{"somef_provenance": {"somef_version": "0.9.5", "somef_schema_version": "1.0.0", "date": "2024-02-27 12:29:19"}, "code_repository": [{"result": {"value": "https://github.com/pytorch/pytorch", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "owner": [{"result": {"value": "pytorch", "type": "Organization"}, "confidence": 1, "technique": "GitHub_API"}], "date_created": [{"result": {"value": "2016-08-13T05:26:41Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "date_updated": [{"result": {"value": "2024-02-27T11:20:29Z", "type": "Date"}, "confidence": 1, "technique": "GitHub_API"}], "license": [{"result": {"value": null, "type": "License", "name": "Other", "url": null, "spdx_id": "NOASSERTION"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "From PyTorch:\n\nCopyright (c) 2016-     Facebook, Inc            (Adam Paszke)\nCopyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\nFrom Caffe2:\n\nCopyright (c) 2016-present, Facebook Inc. All rights reserved.\n\nAll contributions by Facebook:\nCopyright (c) 2016 Facebook Inc.\n\nAll contributions by Google:\nCopyright (c) 2015 Google Inc.\nAll rights reserved.\n\nAll contributions by Yangqing Jia:\nCopyright (c) 2015 Yangqing Jia\nAll rights reserved.\n\nAll contributions by Kakao Brain:\nCopyright 2019-2020 Kakao Brain\n\nAll contributions by Cruise LLC:\nCopyright (c) 2022 Cruise LLC.\nAll rights reserved.\n\nAll contributions from Caffe:\nCopyright(c) 2013, 2014, 2015, the respective contributors\nAll rights reserved.\n\nAll other contributions:\nCopyright(c) 2015, 2016 the respective contributors\nAll rights reserved.\n\nCaffe2 uses a copyright model similar to Caffe: each contributor holds\ncopyright over their contributions to Caffe2. The project versioning records\nall such contribution and copyright details. If a contributor wants to further\nmark their specific copyright on a particular contribution, they should\nindicate their copyright solely in the commit message of the change when it is\ncommitted.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/LICENSE"}, {"result": {"value": "PyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n", "type": "Text_excerpt", "original_header": "License"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "description": [{"result": {"value": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Text_excerpt", "value": "At a granular level, PyTorch is a library that consists of the following components: \n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience | \nUsually, PyTorch is used either as: \n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed. \n", "original_header": "More About PyTorch"}, "confidence": 0.941218172274353, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n \n", "original_header": "A GPU-Ready Tensor Library"}, "confidence": 0.9794966678870781, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "PyTorch has a unique way of building neural networks: using and replaying a tape recorder. \nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc. \nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research. \n", "original_header": "Dynamic Neural Networks: Tape-Based Autograd"}, "confidence": 0.9462716340883892, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "PyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n \n", "original_header": "Python First"}, "confidence": 0.9846566194931397, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "PyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n \n", "original_header": "Imperative Experiences"}, "confidence": 0.9958377853124223, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions. \nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp). \n", "original_header": "Extensions Without Pain"}, "confidence": 0.938855611357411, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "PyTorch is a community-driven project with several skillful engineers and researchers contributing to it. \nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito. \nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n \n", "original_header": "The Team"}, "confidence": 0.9514969999412201, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "PyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system \n"}, "confidence": 0.9757089276240668, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "name": [{"result": {"value": "pytorch", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "full_name": [{"result": {"value": "pytorch/pytorch", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "issue_tracker": [{"result": {"value": "https://api.github.com/repos/pytorch/pytorch/issues", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "forks_url": [{"result": {"value": "https://api.github.com/repos/pytorch/pytorch/forks", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "stargazers_count": [{"result": {"value": 75934, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "keywords": [{"result": {"value": "autograd, deep-learning, gpu, machine-learning, neural-network, numpy, python, tensor", "type": "String"}, "confidence": 1, "technique": "GitHub_API"}], "forks_count": [{"result": {"value": 20553, "type": "Number"}, "confidence": 1, "technique": "GitHub_API"}], "download_url": [{"result": {"value": "https://github.com/pytorch/pytorch/releases", "type": "Url"}, "confidence": 1, "technique": "GitHub_API"}], "programming_languages": [{"result": {"value": "Python", "name": "Python", "type": "Programming_language", "size": 57335545}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "C++", "name": "C++", "type": "Programming_language", "size": 47549803}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Cuda", "name": "Cuda", "type": "Programming_language", "size": 4369667}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "C", "name": "C", "type": "Programming_language", "size": 2406853}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Objective-C++", "name": "Objective-C++", "type": "Programming_language", "size": 1550899}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "CMake", "name": "CMake", "type": "Programming_language", "size": 887603}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Starlark", "name": "Starlark", "type": "Programming_language", "size": 379879}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Shell", "name": "Shell", "type": "Programming_language", "size": 346167}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Assembly", "name": "Assembly", "type": "Programming_language", "size": 336348}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "GLSL", "name": "GLSL", "type": "Programming_language", "size": 197807}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Jupyter Notebook", "name": "Jupyter Notebook", "type": "Programming_language", "size": 183466}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Java", "name": "Java", "type": "Programming_language", "size": 135037}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "PureBasic", "name": "PureBasic", "type": "Programming_language", "size": 115115}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "JavaScript", "name": "JavaScript", "type": "Programming_language", "size": 77536}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Metal", "name": "Metal", "type": "Programming_language", "size": 42755}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Objective-C", "name": "Objective-C", "type": "Programming_language", "size": 36304}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Dockerfile", "name": "Dockerfile", "type": "Programming_language", "size": 32493}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Batchfile", "name": "Batchfile", "type": "Programming_language", "size": 19162}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Makefile", "name": "Makefile", "type": "Programming_language", "size": 9167}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Ruby", "name": "Ruby", "type": "Programming_language", "size": 7912}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "HTML", "name": "HTML", "type": "Programming_language", "size": 5893}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Yacc", "name": "Yacc", "type": "Programming_language", "size": 3848}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "CSS", "name": "CSS", "type": "Programming_language", "size": 2409}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "LLVM", "name": "LLVM", "type": "Programming_language", "size": 1605}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "PowerShell", "name": "PowerShell", "type": "Programming_language", "size": 674}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "GDB", "name": "GDB", "type": "Programming_language", "size": 653}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Smarty", "name": "Smarty", "type": "Programming_language", "size": 376}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"value": "Vim Script", "name": "Vim Script", "type": "Programming_language", "size": 154}, "confidence": 1, "technique": "GitHub_API"}], "releases": [{"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/143208965", "tag": "v2.2.1", "name": "PyTorch 2.2.1 Release, bug fix release", "author": {"name": "atalman", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n- Fix missing OpenMP support on Apple Silicon binaries (https://github.com/pytorch/builder/pull/1697)\r\n- Fix crash when mixing lazy and non-lazy tensors in one operation (https://github.com/pytorch/pytorch/pull/117653) \r\n- Fix PyTorch performance regression on Linux aarch64 (https://github.com/pytorch/builder/pull/1696)\r\n- Fix silent correctness in DTensor `_to_copy` operation (https://github.com/pytorch/pytorch/pull/116426)\r\n- Fix properly assigning `param.grad_fn`  for next forward (https://github.com/pytorch/pytorch/pull/116792)\r\n- Ensure gradient clear out pending `AsyncCollectiveTensor` in FSDP Extension (https://github.com/pytorch/pytorch/pull/116122)\r\n- Fix processing unflatten tensor on compute stream in FSDP Extension (https://github.com/pytorch/pytorch/pull/116559)\r\n- Fix FSDP `AssertionError` on tensor subclass when setting `sync_module_states=True` (https://github.com/pytorch/pytorch/pull/117336)\r\n- Fix DCP state_dict cannot correctly find FQN when the leaf module is wrapped by FSDP (https://github.com/pytorch/pytorch/pull/115592)\r\n- Fix OOM when when returning a AsyncCollectiveTensor by forcing `_gather_state_dict()` to be synchronous with respect to the mian stream. (https://github.com/pytorch/pytorch/pull/118197) (https://github.com/pytorch/pytorch/pull/119716)\r\n- Fix Windows runtime `torch.distributed.DistNetworkError`: [WinError 32] The process cannot access the file because it is being used by another process (https://github.com/pytorch/pytorch/pull/118860)\r\n- Update supported python versions in package description (https://github.com/pytorch/pytorch/pull/119743)\r\n- Fix SIGILL crash during `import torch` on CPUs that do not support SSE4.1 (https://github.com/pytorch/pytorch/issues/116623)\r\n- Fix DCP RuntimeError in `get_state_dict` and `set_state_dict` (https://github.com/pytorch/pytorch/pull/119573)\r\n- Fixes for HSDP + TP integration with device_mesh (https://github.com/pytorch/pytorch/pull/112435) (https://github.com/pytorch/pytorch/pull/118620) (https://github.com/pytorch/pytorch/pull/119064) (https://github.com/pytorch/pytorch/pull/118638) (https://github.com/pytorch/pytorch/pull/119481)\r\n- Fix numerical error with `mixedmm` on NVIDIA V100 (https://github.com/pytorch/pytorch/pull/118591)\r\n- Fix RuntimeError when using SymInt input invariant when splitting graphs (https://github.com/pytorch/pytorch/pull/117406)\r\n- Fix compile `DTensor.from_local` in trace_rule_look up (https://github.com/pytorch/pytorch/pull/119659)\r\n- Improve torch.compile integration with CUDA-11.8 binaries (https://github.com/pytorch/pytorch/pull/119750)\r\n\r\nRelease tracker https://github.com/pytorch/pytorch/issues/119295 contains all relevant pull requests related to this release as well as links to related issues.", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.2.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.2.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.2.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/143208965", "release_id": 143208965, "date_created": "2024-02-15T18:19:00Z", "date_published": "2024-02-22T21:15:00Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/139160052", "tag": "v2.2.0", "name": "PyTorch 2.2: FlashAttention-v2, AOTInductor", "author": {"name": "jcaip", "type": "User"}, "description": "# PyTorch 2.2 Release Notes\r\n\r\n- Highlights\r\n- Backwards Incompatible Changes\r\n- Deprecations\r\n- New Features\r\n- Improvements\r\n- Bug fixes\r\n- Performance\r\n- Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch\u00ae 2.2!  PyTorch 2.2 offers ~2x performance improvements to `scaled_dot_product_attention` via FlashAttention-v2 integration, as well as AOTInductor, a new ahead-of-time compilation and deployment tool built for  non-python server-side deployments.\r\n\r\nThis release also includes improved torch.compile support for Optimizers, a number of new inductor optimizations, and a new logging mechanism called TORCH_LOGS.\r\n\r\n**Please note that we are [deprecating macOS x86 support](https://github.com/pytorch/pytorch/issues/114602), and PyTorch 2.2.x will be the last version that supports macOS x64.**\r\n\r\nAlong with 2.2, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog.\r\n\r\nThis release is composed of 3,628 commits and 521 contributors since PyTorch 2.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.2.  More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.\r\n\r\nSummary:\r\n- `scaled_dot_product_attention` (SDPA) now supports FlashAttention-2, yielding around 2x speedups compared to previous versions.\r\n- PyTorch 2.2 introduces a new ahead-of-time extension of TorchInductor called AOTInductor, designed to compile and deploy PyTorch programs for non-python server-side.\r\n- `torch.distributed` supports a new abstraction for initializing and representing ProcessGroups called device_mesh.\r\n- PyTorch 2.2 ships a standardized, configurable logging mechanism called TORCH_LOGS.\r\n- A number of torch.compile improvements are included in PyTorch 2.2, including improved support for compiling Optimizers and improved TorchInductor fusion and layout optimizations.\r\n- Please note that we are deprecating macOS x86 support, and PyTorch 2.2.x will be the last version that supports macOS x64.\r\n- `torch.ao.quantization` now offers a prototype `torch.export` based flow\r\n\r\n\r\n<table>\r\n  <tr>\r\n   <td>\r\n<strong>Stable</strong>\r\n   </td>\r\n   <td><strong>Beta</strong>\r\n   </td>\r\n   <td><strong>Prototype</strong>\r\n   </td>\r\n   <td><strong>Performance Improvements</strong>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>FlashAttentionV2 backend for scaled dot product attention\r\n   </td>\r\n   <td>PT 2 Quantization\r\n   </td>\r\n   <td>Inductor optimizations\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td> AOTInductor\r\n   </td>\r\n   <td> Scaled dot product attention support for jagged layout NestedTensors\r\n   </td>\r\n   <td>aarch64-linux optimizations (AWS Graviton)\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td> TORCH_LOGS\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>torch.distributed.device_mesh\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>torch.compile + Optimizers\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n</table>\r\n\r\n\\*To see a full list of public 2.2 - 1.12 feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).\r\n\r\n# Tracked Regressions\r\n\r\n### **Performance reduction when using NVLSTree algorithm in NCCL 2.19.3 (#117748)**\r\n\r\nWe have noticed a performance regression introduced to all-reduce in NCCL 2.19.3. Please use version 2.19.1 instead. \r\n\r\n\r\n### **Poor numeric stability of loss when training with FSDP + DTensor (#117471)**\r\n\r\nWe observe the loss will flatline randomly while training with FSDP + DTensor in some instances.\r\n\r\n\r\n# Backwards Incompatible Changes\r\n\r\n###\r\n\r\n### **Building PyTorch from source now requires GCC 9.4 or newer (#112858)**\r\n\r\nGCC 9.4 is the oldest version fully compatible with C++17, which the PyTorch codebase has migrated to from C++14.\r\n\r\n### **Updated flash attention kernel in `scaled_dot_product_attention` to use Flash Attention v2 (#105602)**\r\nPreviously, the v1 Flash Attention kernel had a Windows implementation. So if a user on Windows had explicitly forced the flash attention kernel to be run by using  `sdp_kernel` context manager with only flash attention enabled, it would work. In 2.2, if the `sdp_kernel` context manager must be used, use the memory efficient or math kernel if on Windows.\r\n\r\n```Python\r\nwith torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\r\n  torch.nn.functional.scaled_dot_product_attention(q,k,v)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n# Don't force flash attention to be used if using sdp_kernel on Windows\r\nwith torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=True):\r\n  torch.nn.functional.scaled_dot_product_attention(q,k,v)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Rewrote DTensor (Tensor Parallel) APIs to improve UX (#114732)**\r\nIn PyTorch 2.1 or before, users can use ParallelStyles like `PairwiseParallel` and specify input/output layout with functions like `make_input_replicate_1d` or `make_output_replicate_1d`. And we have default values for _prepare_input and _prepare_output. The UX of Tensor Parallel was like:\r\n```python\r\nfrom torch.distributed.tensor.parallel.style import (\r\n    ColwiseParallel,\r\n    make_input_replicate_1d,\r\n    make_input_reshard_replicate,\r\n    make_input_shard_1d,\r\n    make_input_shard_1d_last_dim,\r\n    make_sharded_output_tensor,\r\n    make_output_replicate_1d,\r\n    make_output_reshard_tensor,\r\n    make_output_shard_1d,\r\n    make_output_tensor,\r\n    PairwiseParallel,\r\n    parallelize_module,\r\n)\r\nfrom torch.distributed.tensor import DeviceMesh\r\n\r\nmodule = DummyModule()\r\ndevice_mesh = DeviceMesh(\"cuda\", list(range(self.world_size)))\r\nparallelize_module(module, device_mesh, PairwiseParallel(_prepare_input=make_input_replicate_1d))\r\n...\r\n```\r\n\r\nStarting from PyTorch 2.2, we simplified parallel styles to only contain `ColwiseParallel` and `RowwiseParallel` because other ParallelStyle can consist of these two. We also deleted the input/output functions, and started using `input_layouts` and `output_layouts` as kwargs instead to specify the sharding layout of both input/output tensors. Finally, added PrepareModuleInput/PrepareModuleOutput style, and no default arguments for layouts in these two styles and users need to specify them to think about the sharding layouts.\r\n```python\r\nfrom torch.distributed.tensor.parallel.style import (\r\n    ColwiseParallel,\r\n    PrepareModuleInput,\r\n    RowwiseParallel,\r\n    parallelize_module,\r\n)\r\nfrom torch.distributed._tensor import init_device_mesh\r\n\r\nmodule = SimpleMLPModule()\r\ndevice_mesh = init_device_mesh(\"cuda\", (self.world_size,)))\r\nparallelize_module(\r\n   module,\r\n   device_mesh,\r\n   {\r\n      \"fqn\": PrepareModuleInput(\r\n                input_layouts=Shard(0),\r\n                desired_input_layouts=Replicate()\r\n             ),\r\n      \"fqn.net1\": ColwiseParallel(),\r\n      \"fqn.net2\": RowwiseParallel(output_layouts=Shard(0)),\r\n   }\r\n)\r\n...\r\n```\r\n\r\n\r\n### **`UntypedStorage.resize_` now uses the original device instead of the current device context (#113386)**\r\n\r\nBefore this PR, `UntypedStorage.resize_` would move data to the current CUDA device index (given by `torch.cuda.current_device()`).\r\nNow, `UntypedStorage.resize_()` keeps the data on the same device index that it was on before, regardless of the current device index.\r\n\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> import torch\r\n>>> with torch.cuda.device('cuda:0'):\r\n...:     a = torch.zeros(0, device='cuda:1')\r\n...:     print(a.device)\r\n...:     a = a.untyped_storage().resize_(0)\r\n...:     print(a.device)\r\ncuda:1\r\ncuda:0\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> import torch\r\n>>> with torch.cuda.device('cuda:0'):\r\n...:     a = torch.zeros(0, device='cuda:1')\r\n...:     print(a.device)\r\n...:     a = a.untyped_storage().resize_(0)\r\n...:     print(a.device)\r\ncuda:1\r\ncuda:1\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n\r\n### **Wrapping a function with set_grad_enabled will consume its global mutation (#113359)**\r\nThis bc-breaking change fixes some unexpected behavior when `set_grad_enabled` is used as a decorator.\r\n\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> import torch\r\n>>> @torch.set_grad_enabled(False)  # unexpectedly, this mutates the grad mode!\r\n    def inner_func(x):\r\n        return x.sin()\r\n\r\n>>> torch.is_grad_enabled()\r\nTrue\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> import torch\r\n>>> @torch.set_grad_enabled(False)  # unexpectedly, this mutates the grad mode!\r\n    def inner_func(x):\r\n        return x.sin()\r\n\r\n>>> torch.is_grad_enabled()\r\nFalse\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Deprecated `verbose` parameter in `LRscheduler` constructors (#111302)**\r\nAs part of our decision to move towards a consolidated logging system, we are deprecating the `verbose` flag in `LRScheduler`.\r\n\r\nIf you would like to print the learning rate during execution, please use `get_last_lr()`\r\n\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True)\r\nfor epoch in range(10):\r\n    train(...)\r\n    val_loss = validate(...)\r\n    # Note that step should be called after validate()\r\n    scheduler.step(val_loss)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min')\r\nfor epoch in range(10):\r\n    train(...)\r\n    val_loss = validate(...)\r\n    # Note that step should be called after validate()\r\n    scheduler.step(val_loss)\r\n\tprint(f\"Epoch {epoch} has concluded with lr of {scheduler.get_last_lr()}\")\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Removed deprecated c10d multi-gpu-per-thread APIs (#114156)**\r\nIn PyTorch 2.1 or before, users can use our multi-gpu c10d collective APIs such as `all_reduce_multigpu`:\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nimport torch.distributed as dist\r\n\r\n\r\ndist.broadcast_multigpu\r\ndist.all_reduce_multigpu\r\ndist.reduce_multigpu\r\ndist.all_gather_multigpu\r\ndist.reduce_scatter_multigpu\r\n...\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\nscheduler = ReduceLROnPlateau(optimizer, 'min')\r\nfor epoch in range(10):\r\n    train(...)\r\n    val_loss = validate(...)\r\n    # Note that step should be called after validate()\r\n    scheduler.step(val_loss)\r\n\tprint(f\"Epoch {epoch} has concluded with lr of {scheduler.get_last_lr()}\")\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\nIn PyTorch 2.2, these APIs are removed because PyTorch Distributed's preferred programming model is one device per thread, as exemplified by the APIs in its document. The multi-GPU functions (which stand for multiple GPUs per CPU thread) have been deprecated since PyTorch 1.13.\r\n\r\n### **Rename `torch.onnx.ExportOutput*` to `ONNXProgram*` (#112263)**\r\nThe torch.onnx.dynamo_export\u2019s output was renamed from torch.onnx.ExportOutput to torch.onnx.ONNXProgram to better align with torch.export.export API terminology which returns a torch.export.ExportedProgram. With this change, any ambiguity that could arise with either API is eliminated.\r\n\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nexport_output: torch.onnx.ExportOutput = torch.onnx.dynamo(...)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nonnx_program: torch.onnx.ONNXProgram = torch.onnx.dynamo(...)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Fix `functional::smooth_l1_loss` signatures to not override `beta` (#109798)**\r\n\r\nPreviously, there were two possible options to pass in `beta` to `smooth_l1_loss`, either as a `SmoothL1LossFuncOption` parameter or a function parameter.\r\n\r\nBefore, the beta specified as a function parameter would override the other beta if it was set, which was unexpected behavior. Now, we throw an error when beta is passed in both cases.\r\n\r\n# Deprecations\r\n\r\n## Autograd API\r\n\r\n### **Deprecate not passing `use_reentrant` kwarg to `torch.utils.checkpoint.checkpoint_sequential` explicitly (#114158)**\r\nThe `use_reentrant` parameter should be passed explicitly. In version 2.4 we will raise an exception if `use_reentrant` is not passed. `use_reentrant=False` is recommended, but if you need to preserve the current default behavior, you can pass `use_reentrant=True`. Refer to docs for more details on the differences between the two variants.\r\nNote that not passing `use_reentrant` kwarg to `torch.utils.checkpoint.checkpoint` has been previously deprecated in a previous release.\r\n\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\na = torch.randn(3, requires_grad=True)\r\nmodules_list = [\r\n    torch.nn.Linear(3, 3),\r\n    torch.nn.Linear(3, 3),\r\n    torch.nn.Linear(3, 3)\r\n]\r\n\r\n# This would produce a warning in 2.2\r\ncheckpoint_sequential(modules_list, 3, a)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n# Recommended\r\ncheckpoint_sequential(modules_list, 3, a, use_reentrant=False)\r\n\r\n# To preserve existing behavior\r\ncheckpoint_sequential(modules_list, 3, a, use_reentrant=True)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Deprecate `\"fallthrough\"` as autograd fallback default (#113166)**\r\nCustom operators that do not have a kernel registered to the Autograd keys (e.g. AutogradCPU and AutogradCUDA) will now produce a warning when used with autograd.\r\nIf your custom operator previously returned floating-point or complex Tensors that do not require grad, they will now require grad as long as grad mode is enabled and the inputs require grad.\r\nFor users who would like the old behavior, register `torch::CppFunction::makeFallthrough()` to your Autograd key, as shown [here](https://gist.github.com/zou3519/d09a5f4b1afe2430af09fea67c6ff2c8).\r\n\r\nThe below example uses the torch library API, but if you are writing an operator in a cpp extension, please read [this](https://docs.google.com/document/d/1W--T6wz8IY8fOI0Vm8BF44PdBgs283QvpelJZWieQWQ/edit#heading=h.pnrqcv6bkfn3) doc for more information.\r\n```Python\r\nimport torch\r\nimport numpy as np\r\n\r\n# Define the operator\r\ntorch.library.define(\"mylibrary::sin\", \"(Tensor x) -> Tensor\")\r\n\r\n# Add implementations for the cpu device\r\n@torch.library.impl(\"mylibrary::sin\", \"cpu\")\r\ndef f(x):\r\n    return torch.from_numpy(np.sin(x.detach().numpy()))\r\nx = torch.randn(3, requires_grad=True)\r\ny = torch.ops.mylibrary.sin(x)\r\ny.sum().backward()\r\n```\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nUserWarning: mylibrary::sin: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd.\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n## Linalg\r\n\r\n### **Deprecate `torch.cross` default behavior (#108760)**\r\nCalling `torch.cross` without specifying the dim arg is now deprecated. This behavior will be changed to match that of `torch.linalg.cross` in a future release.\r\n\r\n## Jit\r\n\r\n### **`NVFuser` functionality has been removed from TorchScript (#110124, #111447, #110881)**\r\nNeural Network Compiler (NNC) has replaced NVFuser as the default GPU fuser for TorchScript in PyTorch 2.1, which also added a deprecation warning for NVFuser. The TorchScript functionality for NVFuser has now been fully removed and is no longer supported.\r\n\r\n## Optimizer\r\n\r\n### **`SparseAdam` constructor will no longer accept raw Tensor type for `params` (#114425)**\r\n`SparseAdam` is now consistent with the rest of our optimizers and only accepts containers instead of individual Tensors/Parameters/param groups.\r\n\r\n<table>\r\n<tr>\r\n<th>2.1</th>\r\n<th>2.2</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nimport torch\r\nparam = torch.rand(16, 32)\r\noptimizer = torch.optim.SparseAdam(param)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\noptimizer = torch.optim.SparseAdam([param])\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n# New Features\r\n\r\n## torch.compile\r\n\r\n### Dynamo\r\n\r\n  - Fully enabled compiled optimizers (#115906)\r\n  - Cudagraphs support for compiled optimizers (#107504)\r\n  - Experimental support for TorchDynamo tracing with DTensor (#108329)\r\n  - Experimental support for torch.compile, activation checkpointing and FSDP (#103953)\r\n  - Dynamo variable trackers are mutable (#113725) - Improves Dynamo compilation time\r\n  - Reduce cache size limit to 8 - Quickly fallback to eager for non-compile friendly functions (#108526)\r\n\r\n### Inductor\r\n\r\n  - CUTLASS Backend with epilogue fusion support (#107802, #107847, #107901, #107931, #108015, #109338, #110890,  #112762)\r\n  - FP8 support (#109168, #112527, #114974, #111122, #112297)\r\n  - Support user defined triton kernel (#111434, #111627, #111939, #111956, #111970, #112228, #112290, #112523, #112752, #113090, #114002, #114475)\r\n  - Support registering out of tree customized pre/post grad pattern matchers (#108615, #113823)\r\n\r\n### torch.export\r\n  - Introduce dynamic_shapes API in favor of constraints (#108448, #112298, #110101, #110638, #110276)\r\n  - Add torch.export.register_dataclass API (#109152)\r\n  - Expose torch.ops.higher_order.map (#111404)\r\n  - Change export to return full ATen IR (not Core ATen) and add a run_decomposition() function to allow users to pass in a decomposition table (or by default it will decompose to the core ATen decomposition table) (#111030, 8be26111f93, #110236, #114714)\r\n\r\n## Build\r\n\r\n- Add Hopper (CUDA arch 9.0a) support (#110587)\r\n\r\n## Python API\r\n\r\n- Add torch.unravel_index (#110580)\r\n- Add multi-dim reductions for `torch.{any,all} (#110310)\r\n- Add file name and size to the serialization metadata logging (#113077)\r\n- Add `torch.distributions.InverseGamma` distribution and fix `sign` bug in `torch.distributions.PowerTransform` (#104501)\r\n- Add torch.utils.deterministic.fill_uninitialized_memory flag (#111377)\r\n\r\n## Profiler\r\n\r\n- Show shapes for lists of tensors in chrome traces (#109751)\r\n- Add `src/dst` information to  NCCL `send`/`recv` (#111811)\r\n- Populate in/out split size information for NCCL all_to_all from CPU to CUDA kernel (#112308)\r\n\r\n## Quantization\r\n\r\n- Add CUTLASS-based support for mixed dtypes matrix multiplication (#110981)\r\n\r\n## Sparse API\r\n\r\n- Add torch.compile support and padding for semi-structured sparsity (#111049, #110583)\r\n- Add CSR tensor with non-contiguous values support to CuSparseSpMatCsrDescriptor (#111742)\r\n- Add scatter_mm and bsr_scatter_mm operations. (#110396, #111796)\r\n- Add is_sparse as a property of MaskedTensor (#110725)\r\n\r\n## NestedTensor API\r\n\r\n- Add unary out-of-place sin / cos support (#107891)\r\n- Add binary out-of-place ge.Scalar / eq.Scalar support (#107892)\r\n- Add binary op support for (B, C, *, *) NT with (C, 1, 1) dense (#107890)\r\n- Add support for cat with dim=0 (#108361)\r\n- Add support for matmul of (B, *, C, D) NT with dense (D, E) (#108370)\r\n- Add support for narrow() on dim=0 (#108362)\r\n- Add support for cat with dim > 0 when representable as jagged (#108428)\r\n- Add public API for constructing NT with jagged layout from tensor list (#111078)\r\n\r\n## Misc\r\n\r\n- Python 3.10 Union operator `|` support for JIT (#109293)\r\n- Allow specifiying inputs as GradientEdge in autograd APIs (#110867, [dev-discuss](https://dev-discuss.pytorch.org/t/highlighting-a-few-recent-autograd-features-h2-2023/1787))\r\n- Use CapturedTraceback symbolizer for C++ exceptions from Python library (#113207)\r\n- Add sparse tensor support to dataloader (#112842)\r\n- Add 0dim Tensor overload for _foreach_div (#113688)\r\n- Add global_step parameter to SummaryWriter.add_hparams (#109572)\r\n\r\n## Fx\r\n\r\n- Add a matcher that supports name to node mapping (#110743)\r\n- Add splitting by tags feature (#109332)\r\n- Allow tracing calls with Python Enum values. (#109507)\r\n- Add function to port FX minified graph to HLO via StableHLO (#109084)\r\n\r\n## ONNX\r\n\r\n- Add symbolic shape support for torch.onnx.dynamo_export(#112179)\r\n- Add optional torch.export.ExportGraphSignature to ONNXProgram (#113477)\r\n- Add `ONNXProgram.__call__` API to run model with ONNX Runtime (#113495)\r\n- Add decomposition support for dynamo_export + ExportedProgram (#112444)\r\n- Add user input mutation support for dynamo_export + ExportedProgram (#114596)\r\n- Add mutated buffer support for dynamo_export + ExportedProgram (#112272)\r\n- Add FakeTensor support for dynamo_export + ExportedProgram  (#114407)\r\n\r\n## CPU\r\n\r\n- Add support for `torch.cpu.set_device()` and `torch.cpu.current_device()` (#110716, #110987)\r\n\r\n## MPS\r\n\r\n- Pixel shuffle unshuffle support (#99306)\r\n- Add lgamma, digamma, and polygamma implementations (#106292)\r\n- Add support for aten::nextafter (#109685)\r\n- Adding weight_norm_interface support for mps (#108008)\r\n- Add searchsorted op (#112829)\r\n- Add bucketize op (#112830)\r\n\r\n## Vulkan\r\n\r\n- Add Vulkan support for several ATen operators:\r\n  - `aten::randlike` (#108086)\r\n  - `aten::randn_like` and `aten::normal` (#109075)\r\n  - `aten::bmm` (#109360)\r\n  - `aten::baddbmm` (#109360)\r\n  - `aten::floor_divide` (#110785, #112190)\r\n  - `aten::mean.dim` (#111609)\r\n  - `aten::var.dim` (#111965)\r\n  - `aten::log` and `aten::log_softmax` (#112828)\r\n  - `aten::layer_norm` (#112322, #114701)\r\n  - `aten::native_layer_norm` (#113573)\r\n- Partial implementation of 1D convolution (only supports stride=1, padding=0, dilation=1 for now) (#112880)\r\n  - Add support for 0-size tensors (i.e. a tensor with a size of 0, for example with sizes {2, 1, 0}) (#111512)\r\n  - Add support for 0-dim tensors (i.e. a tensor with sizes of {})  (#111680)\r\n\r\n# Improvements\r\n\r\n## torch.compile\r\n\r\n### Dynamo\r\n\r\n- Dispatch numpy.take_along_axis to torch.take_along_dim (#108880)\r\n- Force specialization on INT_LIST (#111216)\r\n- Add custom treespec fqn field (#112428)\r\n- Better error handling for cond (#108817)\r\n- Support 'BaseOutput' and subclasses from 'diffusers' in dynamo (#111978)\r\n- Add infinite generators `itertools.{count, repeat, cycle}` (#110967)\r\n- Add support for dict.fromkeys() / OrderedDict.fromkeys() / defaultdict.fromkeys() (#115010)\r\n- Add support for dict.update(seq2) / OrderedDict.update(seq2) / defaultdict.update(seq2) (#115011)\r\n- Add support for dict.copy() / OrderedDict.copy() / defaultdict.copy() (#115012)\r\n- Force synced KJT to trace unbacked SymInt (#108960)\r\n\r\n### Inductor\r\n\r\n- `max-autotune` improvements\r\n    - max-autotune in multi-processes with multi-GPUs (#109126, #109127, #109500)\r\n    - Allow customizing benchmark input generation  (#108242)\r\n- Make codegen stateless (#107320, #107617)\r\n- Add or improve lowering rules for prims.div, reflection_pad2d, full, sub, _local_scalar_dense, index, reflection_pad2d, index_put, unfold (#102809,  #110988, #108166, #108518, #109893, #111015, #111212, #113204, #113259)\r\n- Add or improve decomposition rules for grid_sampler_2d,  full, torch.ops.quantized.embedding_bag_byte_unpack, amax/amin, native_dropout, bmm, mm, complex dtype addition, upsample_nearest_exactNd (#104710, #108443, #109398, #110311, #115040, #109836, #110740, #113749)\r\n- Add reinplacing pass for scatters + incremental fake tensor updating (#106192)\r\n- Add meta-registration for _sparse_semi_structured_linear, _cslt_sparse_mm (#114477, #114685 )\r\n- Provide fallback values for unbacked symint (#109893, #110520)\r\n- Decompose addmm on cpu for a few special cases (e.g. dot product or small matrix vector multiplication)  (#110010, #110456)\r\n- Don't tune beyond 32 warps (which is a CUDA limit) for the coordinate descent tuner (#108997)\r\n- Avoid special characters in cache_dir path (#110945)\r\n- Add DeviceInterface abstraction to make inductor code more device agnostic (#109486 )\r\n- Allow matmul to have flexiable layout when we are not autotuning (#110726)\r\n- Allow backend compiler skipping a frame for transient errors (#111153)\r\n- Handle item() on boolean tensor (#114157)\r\n- Replace `rand[n].generator` with inductor prim if generator=None (#115051)\r\n- Support channel last for XPU convolution in inductor layout optimization path (#111018)\r\n- Early work to improve the static memory planning algorithm (#111402)\r\n- Added support for symbolic shapes in FX graph cache (#111421)\r\n- Added config to specify the shape attribute for the generated svg graphs (#114811)\r\n- Quantization\r\n    - Enable quantization dynamic batch size support (#108550)\r\n    - Enable QConv2d Unary & Binary int8-mixed-bf16 Lowering (#112550, #112551)\r\n    - Enable QLinear int8-mixed-bf16 Lowering (#112486)\r\n    - Enable the lowering of quantized reshape (#114443)\r\n    - Enable the Inductor Lowering of QConv2d post op hardtanh (#114580)\r\n- Improve the CPU backend\r\n    - Support S390X (#111367, #112723)\r\n    - Support MkldnnRnnLayer (#107858)\r\n    - Add GIL release and acquire (#111888)\r\n    - Vectorize support for truediv (#112234)\r\n    - Support QConv (#112373)\r\n    - Vectorize embedding lookup (#114062)\r\n    - Avoid redundant lowp type cast for direct load/store (#115006)\r\n- Improve the Fx pattern matching passes\r\n    - Generalize pointless_cumsum_replacement pattern (#108373)\r\n    - Improve mem efficiency of constant folding (#108421)\r\n    - Make sure unfuse_addmm and addmm patterns don't overlap (#110235)\r\n    - Improve reinplace_scatters pass (#112801)\r\n    - Make pattern-matcher failure diagnostics lazy and add an error message if format string is too long (#112923)\r\n- Foreach kernel compilation time improvement\r\n    - Skip searching getitem in group batch fusion pass reduces optimizer compilation time by 60s (#112088)\r\n    - Re-inplace foreach when safe and allow aliasing during lowering (#112440)\r\n- AOTInductor\r\n    - ABI-compatible mode support\r\n        - Add a C shim layer for libtorch (#109391, #109834)\r\n        - Support _scaled_dot_product_flash_attention fallback (#110085)\r\n        - Add AOTI ABI shim function for repeat_interleave.Tensor (#110745)\r\n        - Add size, stride, storage_offset to RAIIAtenTensorHandle (#110764)\r\n        - Add AOTI ABI shim function for torch.nonzero (#110766)\r\n        - Enable floor_div indexing to work under ABI-compat mode (#113276)\r\n        - Add ABI shim function for torch.scatter (#114027)\r\n        - Support ReinterpretView in ABI mode (#114169)\r\n        - Support at::convolution for AOTInductor (#114961)\r\n    - ProxyExecutor for custom ops support\r\n        - ProxyExecutor skips serializing missing args with default value (#111425)\r\n        - Support List[Tensor] return type (#110182)\r\n        - Proxy Executor for Extern Fallback kernels (#108350)\r\n        - Switch ProxyExecutor to use AtenTensorHandle (#109748)\r\n        - ProxyExecutor supports custom op with tuple output (#110140)\r\n        - ProxyExecutor supports Tuple of Tensor and List[Tensor] in returns (#110187)\r\n        - ProxyExecutor support ReinterpretView inputs (#110451)\r\n        - ProxyExecutor support Dynamic Shape (#110526)\r\n        - Allow using ProxyExecutor for ATen fallbacks (#112976)\r\n        - Use ProxyExecutor for aten op if c-shim is missing (#113918)\r\n    - CPU performance improvement\r\n        - Generate reused thread_locals when tensors probably have static shape (#110892)\r\n        - Cache dtypes and device types at DSO load (#111820)\r\n        - Emit CACHED_TORCH_TYPE only as needed (#113997)\r\n    - UX improvement and refactoring\r\n        - Use array of constants (#111815)\r\n        - Write weight files only if they do not exist yet (#111379)\r\n        - Enforce no_grad for 'run' entry points (#111613)\r\n        - Improve validation for C++ wrapper codegen (#111102)\r\n        - Avoid generating redundant kernel loading code (#110510)\r\n        - Group AOTInductor configs under aot_inductor class (#108369)\r\n        - Include constants in the generated .so file (#108473)\r\n        - Do not hardcode directory with .cubin files (#109151)\r\n        - Add is_cpu for AOTInductorModelContainer (#109287)\r\n        - Pass TorchIR to AOTInductor (#110020)\r\n        - A lightweight model runner (#110158)\r\n        - Remove CUDA dependency for cpp backend (#110409)\r\n        - Delay the fallback kernel naming decision to the codegen time (#113660)\r\n        - Move constant loading logic from Container to Model (#112197)\r\n        - Allow specifying a .so name in the aot_inductor.output_path config (#112651)\r\n        - Improve the two-pass wrapper codegen (#114067)\r\n\r\n###  torch.export\r\n\r\n- Address constant tensors in ExportedPrograms (#113689, #108592)\r\n- Remove replaced symbols from range_constraints (#110644)\r\n- Copy graph module before calling PassManager (#108321)\r\n- Made aot_export_module uses dynamo's fake_mode (#114009, #114381)\r\n- Core ATen Opset\r\n    - Registered additional ATen operators as core (#110882)\r\n    - De-registered full_like and empty_like as core (#110924)\r\n    - Added div.Tensor_mode, div.Scalar_mode, and copy as core operators (#109812)\r\n\r\n## Composability\r\n\r\n- FakeTensors and meta tensors are used to perform shape propagating when tracing out a graph in torch.compile. There were a number of op coverage improvements this release:\r\n  - `masked_scatter` (#108802), `_segment_reduce` (#109359), `foreach` ops (#112281), `linear_backward` (#114359), `scaled_mm` (#112609)\r\n- We have python \u201creference\u201d decompositions for many aten operators. These are used during the tracing step of torch.compile. In a few ways: sometimes they are used to directly decompose operators in the captured graph. Other times, they are used as an alternative to a shape-propagation rule for an operator. There were several improvements to operator coverage in this release\r\n  - `linalg.vecdot` (#108188)\r\n  - `aten.dot`/`vdot` (#108194)\r\n  - `aten.tensor_split.tensor_indices_or_sections` (#107251)\r\n  - `view_as_complex` (#108005)\r\n  - `aten.take_along_dim` (#108185)\r\n  - `scaled_dot_product_attention` (#108180, #108608, #108371, #113102)\r\n  - `unsafe_split{,_with_sizes}` (#109668)\r\n  - `_weight_norm_interface` (#112193)\r\n- We also have an opset known as \u201cCore ATen IR\u201d as defined here. Several ops were either added to core ATen, or had decompositions for them added, that decompose into other core ATen operators:\r\n- decompositions:\r\n  - `std.correction` (#108733)\r\n  - `unsafe_split.Tensor` (#108544)\r\n  - `clamp_min` (#108717)\r\n  - `clamp_max` (#108718)\r\n  - `_unsafe_view` (#108713)\r\n  - `baddbmm` (#108534)\r\n  - `floor_divide` (#110046)\r\n  - `aten.all` (#110093)\r\n  - `split.Tensor` + `unbind` (#110323)\r\n  - `aten.sum` + `aten.squeeze` (#110645)\r\n  - `std` + `std_mean` (#109667)\r\n  - Scaled dot product attention (#117097)\r\n  - New core aten ops:\r\n    - `trunc` (#109319, #109902)\r\n    - `glu` (#110043)\r\n\r\n## Python API\r\n\r\n- Add a UserWarning when using torch.{std,var,std_mean,std_var} with dof<=0 (#109824)\r\n- Add `torch.half` support for `torch.multinomial` on CPU (#104178)\r\n- Add support for serializing `torch.float8_*` dtypes (#114662)\r\n- Add different out dtypes support to `torch.addc{mul,div}` (#112682)\r\n\r\n## torch.nn API\r\n\r\n- Add `__all__` for `torch.nn.utils` (#111026)\r\n- Add Half support for `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d` on CPU (#102079)\r\n- Add Half support for `GroupNorm` on CPU (#100234)\r\n- Add Half support for `torch.nn.functional.{softmax/log_softmax}` on CPU (#103315)\r\n- Add BFloat16 support to `torch.nn.functional.grid_sample` (#112331)\r\n- Add BFloat16 support for `nn.utils.parametrizations.weight_norm` (#114785)\r\n\r\n## Linalg API\r\n\r\n- Add fp16 support for gemm on CPU (#99498)\r\n- Add quantized int4 support for mm (#111403)\r\n\r\n## Optimizer API\r\n\r\n- Allow torch.float64 scalars for forloop + foreach implementations (#115841, #111008)\r\n- Add `NAdam` support for complex dtypes, with `has_complex` shortcut (#110634)\r\n- Set default learning rate (`lr`) value of `SGD` to `1e-3` (#114467)\r\n- Add capturable ASGD impl (#107857)\r\n\r\n## torch.func\r\n\r\n- Add vmap support for various in-place operations (#110692, #113513)\r\n- Add vmap support for `torch.unsafe_chunk` (#110862)\r\n- Add vmap support for `Tensor.index_add_` (#112276)\r\n- Add vmap support for `torch.linspace` and `torch.logspace` (#105451)\r\n- Add vmap support for `torch.linalg.eigh` (#110640)\r\n- Add dynamic shapes support for vmap over `torch.squeeze` and alias  (#107577)\r\n- Add dynamic shapes support for vmap over `torch.is_same_size` and `torch.split_with_sizes` (#111491)\r\n\r\n## Misc\r\n\r\n- Add `torch.utils.checkpoint.set_checkpoint_debug_enabled` (#110728)\r\n- StackDataset batched sampling (#110694)\r\n- Add option to flop counter formula registration to get raw values (#110591)\r\n\r\n## Quantization\r\n\r\n- Bits Types\r\n  - Enable `cat` for bits types (e.g.`torch.bits8`) in cuda (#115044)\r\n  - Enable `copy`/`clone`/`reshape`/`contiguous` operations for bits types (#113508)\r\n- PyTorch 2 Export Quantization:\r\n  - Add reference representation for dynamic quantized linear (#108073)\r\n  - Use `input_qspec_map` for weight quantization of `linear` (#107105)\r\n  - Make annotation util functions return annotated nodes (#107106)\r\n  - Add dequantize operator duplication pass (#107900)\r\n  - Add metadata porting for nodes added by quantization (#107107)\r\n  - Move to BFS instead of DFS to check for connectedness (#108572)\r\n  - Support `int16` quantization (#108453)\r\n  - Support `cat` (#108382), `conv1d` (#109830) and `mul` (#110428) in `XNNPACKQuantizer`\r\n  - Enable constant folding for quantize ops (#109343)\r\n  - Add util function to convert scalars to attrs (#110427)\r\n  - Support `cudnn_batch_norm` (#109908) and `miopen_batch_norm` (#110653) in QAT fusion\r\n  - Preserve source_fn_stack after QAT fusion (#110899, #111515)\r\n  - Cleanup observer insertion logic (#111828) (#112453)\r\n  - Fix QAT conv-bn bias using `DerivedQSpec` (#112159)\r\n  - Refactor QAT q-dq patterns (#112279)\r\n  - Add \"quantization_tag\" as metadata to `fx.Proxy` (#108764)\r\n  - Inductor cpp wrapper: support QLinear (#112378)\r\n  - Enable QAT Quantization flow in `X86InductorQuantizer` (#111280)\r\n  - Add `ConvBNAdd(ReLU)` Annotation (#111281), adaptive_avg_pool2d and flatten (#114442), Hardtanh and ReLU6 for conv2d (#114579) in `X86InductorQuantizer`\r\n  - Enable `oneDNN` QConv (#112010) and QLinear (#112126) FP32/BF16 output\r\n  - Enable `oneDNN` QConv2d with hardtanh post op (#114578)\r\n  - Remove the output Annotation of Conv/Linear in x86InductorQuantizer (#112140)\r\n  - Enable `quantize_per_tensor`/`quantize_per_channel` to accept `bfloat16` input (#112225)\r\n  - Support quantized conv bias in QAT fusion (#112528)\r\n  - Fix custom dtype per channel weight in QAT (#112612)\r\n  - Support `allow_implicit_sharing` flag (#112929)\r\n  - Add `transform_for_annotation` method in `Quantizer` (#113115)\r\n  - Remove add/relu from conv-bn QAT pattern (#113006)\r\n  - Rewrite QAT annotations using `SubgraphMatcherWithNameNodeMap` (#113709)\r\n  - Support conv1d-bn QAT fusion (#113714)\r\n  - `XNNPACKQuantizer` skip quantization for input and output to workaround histogram observer problem (#113405)\r\n  - Add support for QAT dynamic quantization for linear in `XNNPACKQuantizer` (#113288)\r\n- Support Subclasses of `FloatFunctional` in `torch.ao.quantization.prepare` (#109646)\r\n- Enable pickling model prepared with QAT `QConfig` (#109288)\r\n- Suppress empty translation unit warning in QNNPACK (#111475)\r\n- `new_qtensor` support `privateuseone` allocator. (#111464)\r\n- Add support for `float8_e4m3fnuz` and `float8_e5m2fnuz` (#107586)\r\n- Overload vec::dequantize to eliminate rounding error for quantized sigmoid (#114098)\r\n\r\n## NestedTensor API\r\n\r\n- Pickle support for NT (#110219)\r\n- Multiprocessing support for NT (#110292)\r\n- pin_memory support for NT (#110404)\r\n- Implement split_with_sizes backward for NT (#110647)\r\n- Multiprocessing support for NT (#110292)\r\n- Support for as_nested_tensor() with jagged layout + fixed nested_tensor() semantics (#112304)\r\n- Do not generate zero-numel NT by default in helper and improve to_padded_tensor msg (#113162)\r\n- Backward support for broadcasting binary ops (#112519)\r\n- Implement narrow from a regular tensor to jagged tensor (#112770)\r\n\r\n## Distributed\r\n\r\n- c10d\r\n  - Make TCPStore more robust to one-time interruptions. (#108425)\r\n  - Add functional collective `all_to_all_single` and support it in Inductor (#110195)\r\n  - Set `ProcessGroupNCCL` default timeout to 10 min (#110947)\r\n  - Add an explicit `_shutdown` method to ProcessGroupNCCL (#111392)\r\n  - Enable coalescing manager in `DETAIL` debug mode (#111878)\r\n  - Avoid recording stream for all-gather, reduce-scatter, broadcast and scatter (#111431, #112896)\r\n  - Relax tensor contiguity requirement for P2P ops (#114982)\r\n  - Add .boxed() to `c10d::ProcessGroup` and `c10d::Work`'s pybind (#111997)\r\n  - Make `init_process_group` timeout kwarg override `pg_options` (#112611, #113094)\r\n  - Use allocator trace callbacks for `ProcessGroupNCCL` register (#112850)\r\n  - Make `FakeProcessGroup` traceable (#113314)\r\n  - Add Bfloat16 scalar support to gloo backend (#113557)\r\n  - Opportunistically use `ncclCommSplit` when creating new NCCL groups (#114385)\r\n  - Add API `_set_group_name` and `group_name` to track pg names in C++. (#108813)\r\n- Distributed Checkpointing (DCP):\r\n  - Stateful Checkpointing for Distributed (#113867)\r\n- DistributedDataParallel (DDP):\r\n  - Add an API to DDP for dynamically updating the underlying process group. (#113580, #114194)\r\n- DTensor\r\n  - Supported convolution ops (#113123)\r\n  - Add DTensor constructor: `randn` (#108285)\r\n  - Add grad placements kwarg to `to_local` API (#110629)\r\n  - Support `aten.where` and enabled implicit scalar promotion (#110584)\r\n  - Support lt/gt op (#110585)\r\n  - Enable DTensor TP in the inference mode (#110751)\r\n  - Refactor Parallel Style and TP API to improve UX (#111160, #111166, #111176, #111346, #111353, #111625, #111521)\r\n  - Enable embedding sharding in TP API (#111177)\r\n  - Enable adagrad foreach support (#114151)\r\n  - Enable RMSprop optimizer foreach support (#114152)\r\n  - Introduce `full_tensor` API to DTensor (#112224, #113322)\r\n  - Enable foreach operators for adam optimizer (#112108)\r\n  - Don\u2019t use make_fx for strategy propagation (#108262)\r\n  - Add assert of shard dim to be less than tensor ndim (#112404)\r\n  - Add `rand_like`, `randn_like`, `randint_like` ops to shard propagation (#112576)\r\n  - Enable `min`, `max` and `prod` sharding propagation rules (#112403)\r\n  - Add support for layer norm op in DTensor (#113105, #113244)\r\n  - Add foreach_zero_ support (#113897)\r\n  - Make `_Partial`, `Replicate` frozen dataclasses (#113919)\r\n  - Make replicate -> partial DTensor do division instead (#110898)\r\n  - Use new placements for neg dim in `redistribute` (#113924)\r\n  - Use new placements for neg dim in `from_local` (#114134)\r\n  - Use new placements for neg dim in `distribute_tensor` (#113930)\r\n  - Ensure `grad_placements` was tuple (#113925)\r\n  - Support Xla backend in distribute_tensor API (#110275)\r\n- FullyShardedDataParallel (FSDP):\r\n  - Not materialized ignored modules for FSDP (#108032)\r\n  - Not moved ignored params / buffers to device (#108033)\r\n  - Make `checkpoint_wrapper` default to `NO_REENTRANT` (#108435)\r\n  - Make `ModuleWrapPolicy` callable (#109117)\r\n  - Enable `cpu_offload` config for optimizer state_dict (#108434)\r\n  - Enable FSDP on CPU when GPU is still available (#112145, #112144)\r\n  - Add `cpu_only` and `ranks_only` support for `_gather_state_dict` (#112836)\r\n  - Implement `cpu_offload` and `full_state_dict` for `get_state_dict` (#112837)\r\n- TorchElastic:\r\n  - Ensure grandchild processes are restarted correctly (#113231)\r\n  - Avoid terminating parent process if exit code from child isn't valid (#111961)\r\n\r\n## CPU\r\n\r\n- Use cpuinfo to determine c10::ThreadPool thread number (#107010)\r\n- Add Half support for cummax, cummin, cumprod, logcumsumexp, and prod on CPU (#112132)\r\n- Add Half support for poisson and use float for Half cumulative distribution on CPU (#112124)\r\n- Remove memory efficient attention checks (#112375)\r\n- Enable THP for buffer sizes >=2MB  (5a4f1363409)\r\n\r\n## CUDA\r\n\r\n- Add lazy initialization for p2p access function (#108589)\r\n- Add support of CudaHostRegister (#108488)\r\n- Create per thread task pool for mapping memory space (#111545)\r\n- Add AMP support to linalg.vecdot. (#108165)\r\n- bfloat16 support in erfinv (#111257)\r\n- Add Bfloat16 support to CrossKernel.cu (#108941)\r\n- Add bf16 support to replicate padding (#112099)\r\n- Preserve operations order between vectorized and non-vectorized in ln grad input (#111488)\r\n\r\n## Fx\r\n\r\n- Add mechanism for make_fx to not error on data-dependent-ops (#114129)\r\n- Preserve non-forward method during torch package serialization (#114702)\r\n- Add Graph input option for replace_pattern (#112409)\r\n- Allow preserving non-forward methods during deepcopy (#114849)\r\n- Replace node.meta source_fn with source_fn_stack (#108595)\r\n- Fix tree spec matching behavior (#109679)\r\n- Assert that output must be the last node of the FX graph (#114973)\r\n- Misc improvements to visualization + utility (#114984)\r\n- Add stylistic improvements for `fx.split_module` (#113373)\r\n\r\n## Jit\r\n\r\n- Skip builtins while enumerating class methods (#91805)\r\n- Support lovelace for NVRTC (#87611)\r\n- Add expanded symbolic shape support (movedim) (#91696)\r\n\r\n## MPS\r\n\r\n- Add complex support for fill, mul, add, sub (#111885, #111937, #108395, #108394)\r\n- Add support for sgn to MPS backend (#110829)\r\n- Add support for new activation functions (Mish, Softshrink) (#109786) (#110814)\r\n- Generalize toAccumulateType() (#108248)\r\n\r\n## ONNX\r\n\r\n- torch->onnx export support: quantized::linear_relu (#109755)\r\n- Add Half for aten2, logaddexp, logaddexp2, hypot, and nextafter on CPU (#112138)\r\n- Support None in fx.args as torchlib inputs (#108708)\r\n- Support attn_mask fp16 type (#110306)\r\n- A better way to safe guard 2GB model serialization (#111984)\r\n- Fix scalar type promotion between fp16 tensor and fp32 scalar (#113404)\r\n- Add 'aten::rsub' type promotion (#113697)\r\n- Relax unsupported node analysis on complex dtype (#113785)\r\n\r\n## ROCm\r\n\r\n- enable hipSparse const descriptors for version >= 2.4.0 (#110317)\r\n\r\n## Vulkan\r\n\r\n- Improve binary operators to be able to handle the other argument being a 0-dim tensor (#109035)\r\n- Improve binary operators to automatically convert the other argument to float in order to handle mismatched input dtype (#114145)\r\n- Improve aten::addmm and aten::linear to be able to broadcast the bias argument  (#108199)\r\n- Improve aten::layernorm to be able to handle 2D inputs (#110796)\r\n- Improve aten::slice to be able to return 0-size output (#112879)\r\n\r\n# Bug fixes\r\n\r\n## Autograd API\r\n\r\n- Fix in-place custom autograd Functions to not fail when grad returned from backward is undefined (#108353)\r\n- Update custom Function preserve torch function when inputs returned as-is (#109825)\r\n- Do not error when printing view created in no-grad modified in-place in no-grad (#113716)\r\n- Fix `torch.autograd.gradcheck` when `fast_mode=True` and default device is set (#114560)\r\n- Fix `torch.prod` double backward when input tensor contains more than one zero (#113969)\r\n\r\n## Cpp API\r\n\r\n- Check results dtype in index_out (#108167)\r\n- Add the appropriate check on div_value to the cpp frontend (#114671)\r\n- Add input check at the beginning for C++ API `interpolate` (#108506)\r\n- Fix the coredump described by #106702 (#108002)\r\n- Fix torch.nn.GRUCell segfaulting (#108340)\r\n- Add checks to `num_layers` for `RNN`, `LSTM`, `GRU` (#108853)\r\n- `torch::nn::AdaptiveLogSoftmaxWithLoss`: check length of `cutoffs` (#106777)\r\n\r\n## Foreach API\r\n\r\n- Fix 0-size handling for real (#109402)\r\n\r\n## Linalg API\r\n- Fallback to GEMM if mkldnn_matmul fails on aarch64 (#115936)\r\n- Preserve input's NaN values to prevent undefined behavior for `matrix_exp` function (#111539)\r\n\r\n## NestedTensor API\r\n\r\n- Fix torch.load(..., weights_only=True) for NT (#112516)\r\n\r\n## Optimizer API\r\n- `ReduceLROnPlateau` now subclasses `LRScheduler` (#113659)\r\n- Fix `adagrad` sparse handling due to incorrect early exit (#110454)\r\n- Solving pickle error when saving CyclicLR `state_dict` (#110931)\r\n\r\n## Python API\r\n\r\n- Fix type checking of lazy submodule import (#109683)\r\n- Fix unhandled exceptions in `torch.{finfo,iinfo}` calls (#109743)\r\n- Fix torch.{size|stride}(dim=None)` (#111991)\r\n- Fix legacy typed storage warning line pointer (#113601)\r\n- Fix cpu detection error handling (#113771)\r\n\r\n## Sparse API\r\n\r\n- Fix semi-structured sparse shape mismatch bug (#110420)\r\n\r\n## torch.compile\r\n\r\n### Dynamo\r\n\r\n- Add torch.distributed get_rank and get_world_size to constant_fold_functions (#109029)\r\n- Implement traceable torch.tensor when you have SymInt/SymFloat inputs (#109515)\r\n- Avoid throwing exception in ClosingTHPObjectPtr (#109758)\r\n- Fix inductor CI (by updating graph break count) (#110160)\r\n- Error if you try to run Dynamo compiled function under torch.jit.trace (#111321)\r\n- Adjust _list_with_default to also work with SymInt input (#113073)\r\n- Avoid eager imports of classes with custom VariableTrackers (#112319)\r\n- Uniformly use SourcelessBuilder to handle user defined types (#113390)\r\n- Register SymInt-aware meta function for mm out, symintify resize (#113202)\r\n- use sourceless builder for builtin getattr (#113340)\r\n- use sourceless builder for builtin getattr (#113340)\r\n- Don't toggle torch logger to NOTSET if it is not set; always use pre-existing (#113842)\r\n- Fix dict.get with no default (#115048)\r\n- Improve support for list subclasses (#115052)\r\n\r\n### Inductor\r\n\r\n- Properly handle unbacked symint in various scenarios (#109603, #109609, #111803)\r\n- Using floating point 0 rather than integer 0 as default value for tl.load (#113047)\r\n- Avoid passing a None 'generator' argument to aten.rand which does not accept a generator argument (#112240)\r\n- Avoid recursion error because of accumulating too much computation in a pointwise IRNode (#112320)\r\n- Fix 0-sized views of tensors in cudagraphs (#109055)\r\n- Explicitly use the result's dtype for 'other' values in a masked load to avoid unexpected type promotion (#109325)\r\n- Work around triton issue that loading from int1 pointer returns int8 (#110388)\r\n- Avoid max-autotune benchmarking messing up the random number generator (RNG) state (#111381)\r\n- Fix an out of shared memory issue by avoiding a single invalid triton config causes fatal problem (#112916)\r\n- Make TORCH_COMPILE_DEBUG=1 work again (#112917)\r\n- Fix inductor <> ddp_optimizer issue (#108081)\r\n- Fix visualize_overlap for Inductor comm reordering (#113066)\r\n- Fix cat decomp that the first tensor was returned if it is empty and there is only one non-empty tensor (#113514)\r\n- Correctly codegen math.inf in Inductor (#114159)\r\n- Do not promote int to float for torch.mm (#115043)\r\n- Dont pad broadcasting bias dimension in pad mm (#115098)\r\n- Bug fix for the CPU backend\r\n    - Fix argmax with >1 reduction dims (#113168)\r\n    - Fix add/sub with uint8 dtype to avoid unexpected type promotion (#113253)\r\n    - Fix non-contiguous reduction store (#113261)\r\n- Bug fix for the Fx pattern matching passes\r\n    - Fix a bug in the merge getitem cat pattern (#113822)\r\n    - Fix shape mismatch in SDPA pattern matcher (#115038)\r\n- AOTInductor\r\n    - make AOTInductor work with pip installed torch (#108319)\r\n    - Fixing a redefining symbol bug (#110041)\r\n    - Make freezing work with AOTInductor (#110055)\r\n    - Make a free function in AOTInductor header file inline to avoid redefining symbol error (#110445)\r\n    - Fix a weight loading issue when the weight size can be 0 (#114280)\r\n    - Handle empty input args (#114682)\r\n\r\n### torch.export\r\n\r\n- Fix how pass base uses constant fake tensors (#111140)\r\n\r\n\r\n## torch.func API\r\n\r\n- Fix vmap support for `torch.real, torch.imag (#110508)\r\n- Fix vmap support for `torch.isfinite`, `torch.isreal`, and `torch.log_sigmoid` (#110896)\r\n- Fix vmap support for `torch.movedim`, `torch.tensor_split`, `Tensor.to`, `to.*` (#110999)\r\n- Fix vmap support for `torch.flatten`, `torch.linalg.*`, `torch.linear`, `torch.log_softmax`, `torch.logdet`, `torch.special.*` (#110985)\r\n\r\n## torch.nn API\r\n\r\n- Fix precision issues for `torch.nn.LayerNorm` on CPU (#108089)\r\n- Madforward outputs of type `collections.namedtuple` are preserved instead of being changed to `tuple` when there are backward hooks on `nn.Module` (#112433)\r\n- Fixug in mem_eff kernel with attention mask and MQA (bc244ee2cdc)\r\n- Fix allowed dtypes for CUDA devices less than SM80 for `memory_efficient_attention` (#116026)\r\n- Enfced that both input tensors to `nn.CosineEmbeddingLoss` have the same size (#112782)\r\n- Fix type hints for `nn.Module.to` (#108767)\r\n- Fix `torch.nn.utils.rnn.pad_sequence` type hint to allow sequences to be an iterable (#108765)\r\n- Fix `num_batches_tracked` of `nn.BatchNorm{*}D` in load_state_dict to not be reset to 0 if the state_dict does not contain `num_batches_tracked` (#110850)\r\n- Fix `convert_sync_batchnorm` to return `SyncBatchNorm` layer with same training flag as BatchNorm layer being converted (#111998)\r\n- Fix 64-bit indexing support for cross-entropy CUDA kernels (#112096)\r\n\r\n## Build\r\n- Fix finding Intel MKL, LAPACK, cuDNN and cuSPARSELt on Windows (#108040)\r\n- Fix ppc64le clang compilation errors (#106446)\r\n- Compile FBGEMM with ASAN (#111266)\r\n\r\n## Composability\r\n\r\n- FakeTensors and meta tensors are used to perform shape propagating when tracing out a graph in torch.compile. There were a number of op coverage improvements this release:\r\n  - Bugfixes to several aten operator meta implementations.\r\n      - `index_select.out` (#111364)\r\n      - `meta_randperm` (#109721)\r\n      - `qlinear_pointwise` (#112390)\r\n  - Other meta bugfixes: (#108988, #113634, #108989, #111705, #113635)\r\n- There were several bugfixes to our python decompositions and reference implementations of our aten operators this release:\r\n  - Operator specific\r\n    - Several FFT operators (#108360, #109083)\r\n    - `threshold_backward` (#110689)\r\n    - `aten.add` with int32 + scalar input (#113965)\r\n    - `aten.add` non-contiguous out handling (#111758)\r\n    - `aten.normal` with strided layout input (#111205, #112467)\r\n    - `aten.baddbmm` (#109714)\r\n    - `index_add` (#108826)\r\n    - `aten.split_with_sizes` (#113984)\r\n  - General bugfixes\r\n    - fix infinite loop with primtorch and .to(meta) (#109632)\r\n    - Fix registering jit decompositions for jvp for out wrapped decomps (#109367)\r\n    - Fix python decomps for OpOverloadPackets and add tests (#107707)\r\n- fix issue with lift_fresh_copy when using export + compile (#108243)\r\n- Removed spurious warnings from calling `torch.overrides.get_overridable_functions` (#109890)\r\n\r\n## CPU\r\n\r\n- Fix NULL dereference in binary CPU ops (e57f089704a)\r\n- Fix cpuinfo related crash on ppc64 (#110708)\r\n\r\n## CUDA\r\n\r\n- Release GIL in torch.cuda ops wherever possible. (#109159)\r\n- Skipped CUDA Flags if C++ Extension Name includes \"arch\" Substring (#111211)\r\n- Don't set CUDA_HOME when not compiled with CUDA support (#106310)\r\n\r\n## Distributed\r\n\r\n- C10d\r\n  - Fix gloo cuda `sparse_allreduce` dispatch (#111485)\r\n  - Add timeout for master store if clients do not join (#111805)\r\n  - Add `cuda` to MPI backend capabilities (#109614)\r\n  - Fix `send()`/`recv()` to make them respect the timeout same as non-p2p collectives (#109611)\r\n  - Change default `NCCL_ASYNC_ERROR_HANDLING` to `3:SkipCleanUp`  to avoid calling ncclCommAbort which in some cases hangs (#110723)\r\n  - Distributed Checkpointing (DCP):\r\n  - Fix `torch.cpu` has no attribute `current_device` in `checkpoint/optimizer.py` (#110299)\r\n- DTensor:\r\n  - Fix `DTensor.from_local()` returns DTensor with wrong size for uneven sharded tensor (#110781)\r\n  - Make DTensor handle negative dim correctly and fixed TP regression (#111750)\r\n  - Fix pointwise op strategy linearity (#112107)\r\n  - Fix empty shape init for DTensor constructors (#115091)\r\n- FullyShardedDataParallel:\r\n  - Fix non-Node 0 unable receive parameters from Node 0 for HSDP (#108331)\r\n  - Add device to `_shard_utils.py` to explicitly use the correct device from `fsdp_state` (#109631)\r\n  - Propagate `requires_grad` attribute to unsharded params (#109892)\r\n  - Fix logics for fsdp exec order pre fwd record (#110138)\r\n  - Move local optimizer state to FSDP `compute_device` (#110929)\r\n  - Fix the FSDP to not reshard parameters twice (#110948)\r\n  - Fix FSDP to reset prefetch flag upon reshard (#111354)\r\n  - Fix FSDP when `SHARD_GRAD_OP` and `forward_prefetch` is turned on (#110139)\r\n  - Fix FSDP `summon_full_params(..., with_grads=True)` when grad precision is not fp32 (#112746)\r\n  - Fix fsdp state_dict to use run_check=False (#114995)\r\n  - Fix pylance issues for torch.distributed.fsdp (#109922)\r\n- RPC:\r\n  - Fix assertion on vector length during message parsing (#108414)\r\n\r\n## Fx\r\n- Fix `functorch.compile.minifier` error of \u201c'Node' object is not iterable\u201d (#103011)\r\n  - Skip mode issue in minimizer (#109399)\r\n  - Skip the Tensor node in `__annotations__` (#109853)\r\n  - Fixed dict size change during iteration error (#111267)\r\n  - Made sure fx code is valid in python (#113345)\r\n  - Updated symbolic_trace\u2019s nn_module_stack format (#114422)\r\n  - Fixed missing meta for proxy.node (#114659)\r\n- Correctly restore pybind11 error_already_set (#93238)\r\n- Remove proxy tensor's check for data dependent output (#93265)\r\n- Make ShapeEnv deepcopy-able (#93403)\r\n- Fix SubgraphMatcher for case of no anchor found (#86421)\r\n- Fix for partitioner with symbolic shapes (#86425)\r\n- Fix getitem in partitioner and make metadata storage more consistent (#87012)\r\n- Fix magic method try reverse protocol (#88030)\r\n- Fix FakeTensorProp on Module with Parameters or Buffers (#88700)\r\n- Fix PassManager to not use a class variable mutable list (#89108)\r\n- Prevent tracing when we track_tensor_tree (#89139)\r\n- Make all `make_fx` invocations isolated (opaque to higher `make_fx` invocations) by default (#93290)\r\n- Fix matching args in PatternMatcher (#94375)\r\n- Allow FakeTensorProp to run on graphs traced with some None inputs (#94569)\r\n- Copy codegen in legalize_graph (#90023)\r\n- Fix proxy unwrapping for cond() (#91907)\r\n\r\n## Jit\r\n- Fix `optimize_for_inference` to support modules that don't have a forward method (#110013)\r\n- Fix errors found by fuzzing and sanitizers (#108417, #108413, #110303, #110441)\r\n- Fix deprecated python usage for python 3.12 in TorchScript (#113981)\r\n- Support newer versions of LLVM (#110200, #113455)\r\n\r\n## Lazy\r\n\r\n- Fix error when inferring shape in `AdaptiveAvgPool3d` (#109822)\r\n\r\n## Mps\r\n\r\n- Fix and refactor unary/binary ops with non-zero offset or non-contiguous output (#97085)\r\n- Fix memory leak in copy_from_mps_ (#114197)\r\n- Fix crash if nonzero is called concurrently (#108996)\r\n- Fix nll_loss with default ignore_index (#109574)\r\n- Fix sort with empty tensor. (#109584)\r\n\r\n## ONNX\r\n\r\n- Fix module attribute retrieval in ONNX export (#109759)\r\n- Add dynamic input support for MaxPool (#113318)\r\n- Fix op-level debug for complex dtype (#114885)\r\n- Fix indexing for meshgrid op (#109350)\r\n- Fix torch.diagonal for torch.onnx.export when dim1<0 or dim2<0 (#111130)\r\n- Fix scope name when parent scope is empty for torch.onnx.export (#112654)\r\n- Cast \u2018scale\u2019 back to float16 after _attention_scale. (#112554)\r\n- Fix aten::layer_norm for ONNX opset 17 (#114058)\r\n- Add support for negative dim in _index_fill_reshape_helper (#114050)\r\n- Disable opmath type promotion (#113780)\r\n\r\n\r\n## Profiler\r\n- Fix missing dependency in torch.utils.tensorboard (#115598) (#115598)\r\n- Fix torch.utils.benchmark API while use privateuse1. (#108548)\r\n- Ignore some properties when symbolic size/strides exist (#112458)\r\n- Fix description to use nelems rather than size (#114735)\r\n- Use PyCFunction_Check to check both PyCMethod_Type and PyC\u2026 (#110002)\r\n- Disable CUPTI Teardown when using CUDA Graphs (#112507)\r\n- Fix the Chrome trace loading issue with all_to_all input split length > 30 (#113392)\r\n\r\n## Quantization\r\n\r\n- Make mutation test work with quantized tensors (#108935)\r\n- Support transitive sharing for SharedQuantizationSpec (#111172)\r\n\r\n## Releng\r\n\r\n- Fix focus builds of macOS apps on apple silicon. (#96966) (#107816)\r\n- Use jemalloc for CUDA builds (#116900) (#116900)\r\n\r\n## Visualization\r\n\r\n- Fix TensorBoard summary writer encoding for torch.bfloat16 tensors (#108351)\r\n\r\n## Vulkan\r\n\r\n- Fix for a bug in aten::sum.dim_IntList where providing negative dims in the opt_dim argument and setting keepdim=false results in wrong inputs (#111586)\r\n\r\n# Performance\r\n\r\n## Autograd API\r\n\r\n- Avoid saving input for `torch.mean` backward (#109935)\r\n\r\n## Cpp API\r\n\r\n- Add ScalarTensor or 0dim overload for _foreach_add (#111079)\r\n- Vectorize torch.exp2 on CPU and add complex support (#92115)\r\n- Add various performance fixes to c++ STL usage (#94034)\r\n\r\n## Linalg API\r\n\r\n- Speedup `torch.matrix_exp` performance (#110848)\r\n- Improve speedup of `cholesky_solve_backward` using output_mask (#112981)\r\n\r\n## NestedTensor API\r\n\r\n- Reduce overhead in split and chunk for NestedTensor (#108213)\r\n\r\n## Optimizer API\r\n- Use for loop with shortcut in `Optimizer`s to speedup inductor against list comprehensions (e.g. complex conversion) (#110613, #112722)\r\n- Speed up dynamo tracing of optimizer by shortcutting is_sparse iteration in foreach SGD (#110648)\r\n\r\n## Sparse API\r\n\r\n- Add NVIDIA A100 optimized meta parameters to bsr_dense_mm (#111760)\r\n- Improve triton bsr_dense_mm performance on column-major ordered inputs with float32 dtype (#108512)\r\n- Add bsr_dense_addmm triton kernel (#114595)\r\n- Use more performant bsr_scatter_mm within bsr_dense_mm when blocksize is 16. (#111489)\r\n\r\n## torch.compile API\r\n\r\n###  Inductor\r\n\r\n- Support convolution layout optimization for models with SDPA (#112045)\r\n- Scaling down XBLOCK/RBLOCK to increase occupancy for kernels exposing large parallelism and having register pressue (#109275, #109839, #113039, #114284)\r\n- Horizontal fusion for concat (#111437)\r\n- Avoid an extra memory copy for views on ExternKernelAlloc (#108635)\r\n- More memory and performance efficient implementation for Conv+BatchNorm block in eval mode (#109398, #109722)\r\n- Pointwise fuse cat with pointwise inputs or outputs and <= 4 inputs (#111233)\r\n- Add a way to force fusion of int_mm with mul (#111413)\r\n- Add a heuristic to multi-layer reduction to increase the chance that the first splitted reduction can have compatible shape to fuse with a preceding reduction (#111781)\r\n- benchmark fusion: either use this to skip slow fusions or analyze patterns from slow fusions  (#112450)\r\n- optimize sympy expression where div denominator is -1 (#112878)\r\n- Use different conv layout optimization heuristics for inference (#114600)\r\n- Add or improve  Fx pattern matching passes\r\n    - pre grad batch relu fusion (#111146)\r\n    - new split cat pattern detection (#110923)\r\n    - Add split-stack-tahn-unbind pattern detection (#111854)\r\n    - Remove split nodes with split section size one (#112922)\r\n    - Normalize nodes created by users (#113179)\r\n    - post_grad batched linear fusion (#112504)\r\n    - More SDPA patterns (#109156, #110001)\r\n    - Horizontally fusing two matmuls in freezing phase (#111232)\r\n    - A bunch of pattern matcher + indexing fixes (#112476)\r\n- CPU Backend\r\n    - Fallback scatter_add to eager on CPU to avoid bad perf (#108220)\r\n    - Make OneDNN matmul inputs contiguous to avoid degraded performance (#108560)\r\n\r\n## torch.func API\r\n\r\n- Add vmap batching rule for: `bitwise operators` (#91971), `nansum` & `nanmean` (#91372), `all` & `any` (#91966), `torch.linalg.vander` (#91749), `slogdet` (#86815), `torch.index_fill` (#91364), `narrow_copy` (#88130), `view_copy` (#88150), `greater_equal.Scaler` (#91324)\r\n\r\n## CPU\r\n\r\n- S390x complex division (#108516)\r\n- Add Half support for CPU autocast on eager mode (#112484)\r\n- Add scalar conversion using avx instructions for half (#102140)\r\n\r\n## CUDA\r\n\r\n- Release the allocator lock on the slow path (#108367)\r\n- Faster gc_count update for CUDACachingAllocator (#108071)\r\n- baddmm should fall back to addmm for batch=1 (#114992, #114992)\r\n- Speed-up casts to FP8 (#110251)\r\n- int4 mm kernel enhancement (#111460)\r\n- vectorized implementation for layer_norm_grad_input_kernel (#111021)\r\n\r\n## Distributed\r\n\r\n- c10d:\r\n  - Push TCPStore scalability further by staggering client connection and increasing the backlog to 16k. (#109217)\r\n  - Make the minimum wait time in `_store_based_barrier` to be adaptative based on the number of ranks. (#109218)\r\n- DTensor:\r\n  - Fix and improve the sharding cache behavior (#109306, #109428)\r\n  - Switch DTensor and Functional Collective to use optree (#110670)\r\n  - Skip move to device when `device_type` match (#110774)\r\n  - Skip pytree when not necessary (#110132)\r\n  - Introduce cost model for sharding (#109145)\r\n  - Group dispatch unwrapping to a method (#113846)\r\n  - Cache hash for `DTensorSpec` (#113915)\r\n  - Compute and recompute `DTensorSpec` hash lazily (#114322, #114379)\r\n  - Reduce to one `isinstance` call in `is_shard` (#114140)\r\n- FullyShardedDataParallel (FSDP):\r\n  - Fuse allgather for `optim_state_dict` when `use_orig_params` is True (#108298)\r\n  - Make the new optimizer allgather fusion work with fine-tuning models (#110540)\r\n  - Skip the parameter in optim state dict if the parameter does not belong to the current FSDP instance (#112804)\r\n\r\n## Fx\r\n\r\n- Use deque instead of list for BFS (#91139)\r\n- Refactor the dfs cyclic search from recursive to iterative approach (#91042)\r\n\r\n## Vulkan\r\n\r\n- Improve matrix multiplication shader performance by up to 50% through new packing schemes and algorithmic improvements (#112918, #113627, #113883, #113943)\r\n\r\n# Documentation\r\n\r\n## Autograd API\r\n\r\n- Improve docstring issues in various places (#113266)\r\n\r\n## Dataloader API\r\n\r\n- Add and update docstrings for `torch.utils.data` (#112244, #112817, #112765)\r\n\r\n\r\n## Linalg API\r\n- Fix typo in example of torch.linalg.solve_triangular (#112361)\r\n- Remove duplicate sentences in description of torch.linalg.eig (#108230)\r\n- Fix bug in matrix_power documentation (#108585)\r\n\r\n## Optimizer API\r\n\r\n- Update documentation for `PolynomialLR` (#110151)\r\n- Clarify `maximize` option in optimizer.py (#112724)\r\n- Fix docstring errors inside `torch/cuda/` and `torch/optim/` (#112964)\r\n\r\n\r\n## Python API\r\n\r\n- Fix docstring issues in torch.utils (#113335)\r\n- Add docstring to `Timer.adaptive_autorange` (#111612)\r\n- Fix a typo in `torch.cholesky_inverse` documentation (#110364)\r\n- Document `torch.from_file` and `UntypedStorage.from_file` properly (#111688)\r\n- Clarify difference between `Tensor.share_memory_` and `torch.from_file` (#111856)\r\n- Improve `torch.unique` docs (#113424)\r\n- Fix `torch.lgamma` docs (#108719)\r\n- Update `torch.take_along_dim` docs to include `dim=None` case (#109120)\r\n- Fix `torch.searchsorted` docs (#109364)\r\n- Clarify `torch.multinomial` usage (#112892)\r\n\r\n## torch.compile API\r\n\r\n### Inductor\r\n\r\n- Add a tutorial for AOTInductor (#112457)\r\n- Add document for cudagraph_mark_step_begin API (#111722)\r\n\r\n### `torch.export` API\r\n\r\n- Add `torch.cond` doc (#108691)\r\n- Add ir spec (#110394)\r\n- Update docs to say that export returns full ATen IR (#111161)\r\n\r\n## torch.func API\r\n\r\n- Fix per-sample-grads notebook (#107988)\r\n\r\n## torch.nn API\r\n\r\n- Add examples for `nn.CosineEmbeddingLoss`(#108215)\r\n- Fix `attn_bias` in code block in `scaled_dot_product_attention` documentation (#109086)\r\n- Add documentation for `torch.nn.utils.parametrizations.weight_norm` (#113783)\r\n- Improve type annotation for device parameters when a device ordinal is allowed (#113647)\r\n- Update `scaled_dot_product_attention` documentation to point to flash-attn-v2 (#114124)\r\n- Fix extending torch native API docs (#114863)\r\n\r\n## Build\r\n\r\n- Fix doc preview page url at CONTRIBUTING.md (#108580)\r\n- Fix typo in cpp/installing when wheel is used (#111143)\r\n\r\n## Composability\r\n\r\n- Fix ScalarTensor **repr** in Extending PyTorch example (#86330)\r\n- Fix incorrect wrapping of function decorator (#94446)\r\n- Add **all** to torch.{autograd, fx, cuda} submodules (#85343)\r\n\r\n## CUDA\r\n\r\n- Show CUDAExtension example commands as code (#112764)\r\n- Rewrite docs to describe CUDACachingAllocator semantics (#113282)\r\n\r\n## Distributed\r\n\r\n- c10d:\r\n  - Fix TCPStore doc for arg `wait_for_workers` (#111807)\r\n  - Fix the warning messages when `avoidRecordStreams_` so that correct name of Environment variable is shown in the warning message (#108759)\r\n  - Fix an incorrect indent in documentation for `dist.send` (#108273)\r\n  - Fix warnings and descriptions about distributed exceptions in the logging section of PyTorch Distributed document (#110157)\r\n  - Fix `batch_isend_irecv` example incorrect usage (#110408)\r\n  - Clarify the behavior of `apply_optimizer_in_backward` in the document (#110903)\r\n  - Correct docstring errors for torch.distributed files (#112735, #113511, #113523, #112693, #113241, #113216)\r\n  - Print `NCCL_SUFFIX` in NCCL version log at PG init (#112560)\r\n  - Distributed Checkpointing (DCP):\r\n  - Fix the comment `no_dist` for in `load_state_dict` (save -> load) (#112217)\r\n  - Improve DDP checkpoint documentation (#106985)\r\n- DistributedDataParallel (DDP):\r\n  - Fix import in DDP notes (#111833)\r\n  - Add errors when using `_dynamo.optimize_ddp=True` and `_inductor.keep_output_stride=False` together inside DDP (#108235)\r\n- DTensor:\r\n  - Improve TP documentation (#115880, #115974)\r\n  - FullyShardedDataParallel (FSDP):\r\n  - Fix docstring of `FSDP.optim_state_dict_to_load` to reflect right ctors (#108383)\r\n  - Fix docstring for `FSDP.set_state_dict_type` to contain missing Args (#103864)\r\n  - Remove \"on CPU\" in the comment of FSDP initialization doc (#113753)\r\n- TorchElastic:\r\n  - Fix a typo in `rendezvous/registry.py` (#111352)\r\n- RPC:\r\n  - Fix `torch.distributed.rpc` example incorrect usage (#112367)\r\n- Activation checkpointing\r\n  - Clean up comments in activation checkpoint (#86622)\r\n- Distributed (c10d)\r\n  - Improve documentation for various functions (#87018, #94543, #91116,#89905, #86438 )\r\n- DistributedDataParallel\r\n  - Improve Documentation (#86221, #91832)\r\n- RPC\r\n  - Fix non-existing parameters in docstrings in benchmarks (#91115)\r\n- Tensor parallelism and DTensor:\r\n  - Add more clarifications and fix errors in tensor parallelism docs (#94786)\r\n  - Update 2D parallelism API naming and docs (#94771)\r\n- FullyShardedDataParallel\r\n  - Add docs to explain the running the forward pass of of submodules in FSDP (#86343)\r\n  - Clarify warnings to mention collectives (#87478)\r\n  - Remove HSDP Zero-2 from doc (#90503)\r\n  - Improve the comments for FSDP (#92359)\r\n- Distributed Checkpoint\r\n  - Enable documentation for Distributed Checkpoint. (#92813)\r\n- Torch Elastic\r\n  - Fix a minor typo in documentation (#90667)\r\n  - Fix `torch.distributed.run` init connect timeout by comparing `host` with the current IP list (#90221)\r\n\r\n## Mps\r\n\r\n- Resolve docstring errors (#113311)\r\n\r\n## ONNX\r\n\r\n- Update exporter issue report instructions for quantized models (#113494)\r\n- Fix sample code in onnx_dynamo.rst (#114770)\r\n\r\n## Profiler\r\n\r\n- Improve the docstring for export_memory_timeline (#110949)\r\n- Improve torch/csrc/profiler/README.md - stubs, RecordFunction, Autograd interaction (#108470)\r\n\r\n## Quantization\r\n\r\n- Add pt2 export quantization to main doc (#110260)\r\n- Add x86 inductor quantization docs (#112648)\r\n- Use \\odot everywhere instead of mixing \\odot and * for the Hadamard product (#111763)\r\n- Add documentation for `prepare_pt2e`, `prepare_qat_pt2e` and `convert_pt2e` (#110097)\r\n- Docstyle fix for some quantization code (#112992)\r\n- Updating docs for embedding_bag support for fx and eager (#107623)\r\n- fix docstring errors in quantized modules (#112695)\r\n\r\n# Security\r\n\r\n## Releng\r\n\r\n- Use secure setup-ssh action from test-infra (#111922)\r\n- Automate passing Conda PyTorchBot Test Token for release (#111821)\r\n- Migrate MacOS wheel binary builds to ephemeral M1 runners (#110432)", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.2.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.2.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.2.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/139160052", "release_id": 139160052, "date_created": "2024-01-10T22:17:29Z", "date_published": "2024-01-30T17:58:51Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/133972717", "tag": "v2.1.2", "name": "PyTorch 2.1.2 Release, bug fix release", "author": {"name": "atalman", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n- Fix crashes for float16 empty tensors (https://github.com/pytorch/pytorch/pull/115183)\r\n- Fix MPS memory corruption when working with tensor slices (https://github.com/pytorch/pytorch/pull/114838)\r\n- Fix crashes during Conv backward pass on MPS devices (https://github.com/pytorch/pytorch/pull/113398)\r\n- Partially fix nn.Linear behavior on AArch64 platform (https://github.com/pytorch/pytorch/pull/110150)\r\n- Fix cosine_similarity for tensors of different sizes (https://github.com/pytorch/pytorch/pull/109363)\r\n- Package missing headers needed for extension development  (https://github.com/pytorch/pytorch/pull/113055)\r\n- Improve error handling of ``torch.set_num_threads`` (https://github.com/pytorch/pytorch/pull/113684)\r\n- Fix profiling traces generation (https://github.com/pytorch/pytorch/pull/113763)\r\n\r\n\r\nThe Cherry pick tracker https://github.com/pytorch/pytorch/issues/113962 contains all relevant pull requests related to this release as well as links to related issues.", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.1.2", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.1.2", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.1.2", "url": "https://api.github.com/repos/pytorch/pytorch/releases/133972717", "release_id": 133972717, "date_created": "2023-12-12T16:41:07Z", "date_published": "2023-12-15T01:59:57Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/129423939", "tag": "v2.1.1", "name": "PyTorch 2.1.1 Release, bug fix release", "author": {"name": "jerryzh168", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n* Remove spurious warning in comparison ops  (#112170)\r\n* Fix segfault in foreach_* operations when input list length does not match (#112349)\r\n* Fix cuda driver API to load the appropriate .so file (#112996)\r\n* Fix missing CUDA initialization when calling FFT operations (#110326)\r\n* Ignore beartype==0.16.0 within the onnx package as it is incompatible (#111861)\r\n* Fix the behavior of torch.new_zeros in onnx due to TorchScript behavior change (#111694)\r\n* Remove unnecessary slow code in `torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict` (#111687)\r\n* Add `planner` argument to `torch.distributed.checkpoint.optimizer.load_sharded_optimizer_state_dict` (#111393)\r\n* Continue if param not exist in sharded load in `torch.distributed.FSDP` (#109116)\r\n* Fix handling of non-contiguous bias_mask in `torch.nn.functional.scaled_dot_product_attention`  (#112673)\r\n* Fix the meta device implementation for `nn.functional.scaled_dot_product_attention` (#110893)\r\n* Fix copy from mps to cpu device when storage_offset is non-zero (#109557)\r\n* Fix segfault in `torch.sparse.mm` for non-contiguous inputs  (#111742)\r\n* Fix circular import between Dynamo and einops (#110575)\r\n* Verify flatbuffer module fields are initialized for mobile deserialization (#109794)\r\n\r\nThe https://github.com/pytorch/pytorch/issues/110961 contains all relevant pull requests related to this release as well as links to related issues.", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.1.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.1.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.1.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/129423939", "release_id": 129423939, "date_created": "2023-11-08T12:49:29Z", "date_published": "2023-11-15T22:59:33Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/123579416", "tag": "v2.1.0", "name": "PyTorch 2.1: automatic dynamic shape compilation, distributed checkpointing", "author": {"name": "jerryzh168", "type": "User"}, "description": "# PyTorch 2.1 Release Notes\r\n- Highlights\r\n- Backwards Incompatible Change\r\n- Deprecations\r\n- New Features\r\n- Improvements\r\n- Bug fixes\r\n- Performance\r\n- Documentation\r\n- Developers\r\n- Security\r\n\r\n# Highlights\r\nWe are excited to announce the release of PyTorch\u00ae 2.1! PyTorch 2.1 offers automatic dynamic shape support in torch.compile, torch.distributed.checkpoint for saving/loading distributed training jobs on multiple ranks in parallel, and torch.compile support for the NumPy API.\r\n\r\nIn addition, this release offers numerous performance improvements (e.g. CPU inductor improvements, AVX512 support, scaled-dot-product-attention support) as well as a prototype release of torch.export, a sound full-graph capture mechanism, and `torch.export`-based quantization.\r\n\r\nAlong with 2.1, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog. \r\n\r\nThis release is composed of 6,682 commits and 784 contributors since 2.0. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.1.  More information about how to get started with the PyTorch 2-series can be found at our [Getting Started](https://pytorch.org/get-started/pytorch-2.0/) page.\r\n\r\nSummary: \r\n- `torch.compile` now includes automatic support for detecting and minimizing recompilations due to tensor shape changes using automatic dynamic shapes.\r\n- `torch.distributed.checkpoint` enables saving and loading models from multiple ranks in parallel, as well as resharding due to changes in cluster topology.\r\n- `torch.compile` can now compile NumPy operations via translating them into PyTorch-equivalent operations.\r\n- `torch.compile` now includes improved support for Python 3.11.\r\n- New CPU performance features include inductor improvements (e.g. bfloat16 support and dynamic shapes), AVX512 kernel support, and scaled-dot-product-attention kernels.\r\n- `torch.export`, a sound full-graph capture mechanism is introduced as a prototype feature, as well as torch.export-based quantization.\r\n- `torch.sparse` now includes prototype support for semi-structured (2:4) sparsity on NVIDIA\u00ae GPUs.\r\n\r\n\r\n\r\n<table>\r\n  <tr>\r\n<td> <strong> Stable </strong>\r\n\r\n   </td>\r\n   <td> <strong> Beta </strong>\r\n\r\n   </td>\r\n   <td> <strong>Prototype </strong>\r\n\r\n   </td>\r\n   <td> <strong>Performance Improvements </strong>\r\n\r\n  </tr>\r\n\r\n<tr>\r\n\r\n   </td>\r\n   <td> \r\n\r\n   </td>\r\n   <td>Automatic Dynamic Shapes\r\n\r\n   </td>\r\n   <td> torch.export()\r\n\r\n   </td>\r\n   <td>AVX512 kernel support\r\n\r\n</tr>\r\n\r\n<tr>\r\n\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>torch.distributed.checkpoint\r\n\r\n   </td>\r\n   <td>torch.export-based Quantization\r\n\r\n   </td>\r\n   <td>CPU optimizations for scaled-dot-product-attention (SDPA)\r\n</tr>\r\n\r\n<tr>\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>torch.compile + NumPy\r\n\r\n   </td>\r\n   <td>semi-structured (2:4) sparsity\r\n\r\n   </td>\r\n   <td>CPU optimizations for bfloat16\r\n\r\n</tr>\r\n\r\n<tr>\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>torch.compile + Python 3.11\r\n\r\n   </td>\r\n   <td>cpp_wrapper for torchinductor\r\n\r\n   </td>\r\n   <td>\r\n   </td>\r\n</tr>\r\n\r\n<tr>\r\n   <td>\r\n   </td>\r\n   <td>torch.compile + autograd.Function\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n\r\n</tr>\r\n\r\n</tr>\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>third-party device integration: PrivateUse1\r\n\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n</tr>\r\n</table>\r\n\r\n*To see a full list of public 2.1, 2.0, and 1.13 feature submissions click [here](https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing).\r\n\r\nFor more details about these highlighted features, you can look at the release blogpost.\r\nBelow are the full release notes for this release.\r\n\r\n# Backwards Incompatible Changes\r\n\r\n### Building PyTorch from source now requires C++ 17 (#100557)\r\nThe PyTorch codebase has migrated from the C++14 to the C++17 standard, so a C++17 compatible compiler is now required to compile PyTorch, to integrate with libtorch, or to implement a C++ PyTorch extension.\r\n\r\n\r\n### Disable `torch.autograd.{backward, grad}` for complex scalar output (#92753)\r\n\r\nGradients are not defined for functions that don't return real outputs; we now raise an error if you try to call backward on complex outputs. Previously, the complex component of the output was implicitly ignored. If you wish to preserve this behavior, you must now explicitly call `.real` on your complex outputs before calling `.grad()` or `.backward()`.\r\n\r\n#### Example\r\n```python\r\ndef fn(x):\r\n    return (x * 0.5j).sum()\r\n\r\nx = torch.ones(1, dtype=torch.double, requires_grad=True)\r\no = fn(x)\r\n```\r\n\r\n#### 2.0.1\r\n```python\r\no.backward()\r\n```\r\n\r\n#### 2.1\r\n```python\r\no.real.backward()\r\n```\r\n\r\n### Update non-reentrant checkpoint to allow nesting and support `autograd.grad` (#90105)\r\n\r\nAs a part of a larger refactor to `torch.utils.checkpoint`, we changed the interaction activation checkpoint and `retain_graph=True`. Previously in 2.0.1, recomputed activations are kept alive if `retain_graph=True`, in PyTorch 2.1, non-reentrant impl now clears recomputed tensors on backward immediately upon unpack, even if `retain_graph=True`. This has the following additional implications: (1) Accessing `ctx.saved_tensor` twice in the same backward will now raise an error.  (2) Accessing `_saved_tensors` multiple times will silently recompute forward multiple times.  \r\n\r\n#### 2.1\r\n```python\r\nclass Func(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, x):\r\n        out = x.exp()\r\n        ctx.save_for_backward(out)\r\n        return out\r\n\r\n    @staticmethod\r\n    def backward(ctx, x);\r\n        out, = ctx.saved_tensors\r\n        # Calling ctx.saved_tensors again will raise in 2.1\r\n        out, = ctx.saved_tensors\r\n        return out\r\n\r\na = torch.tensor(1., requires_grad=True)\r\n\r\ndef fn(x):\r\n    return Func.apply(x)\r\n\r\n\r\nout = torch.utils.checkpoint(fn, (a,), use_reentrant=False)\r\n\r\ndef fn2(x):\r\n    return x.exp()\r\n\r\nout = torch.utils.checkpoint(fn2, (a,), use_reentrant=False)\r\n\r\nout.grad_fn._saved_result\r\n# Calling _saved_result will trigger another unpack, and lead to forward being\r\n# recomputed again\r\nout.grad_fn._saved_result\r\n```\r\n\r\n### Only sync buffers when `broadcast_buffers` is True (#100729)\r\n* In PyTorch 2.0.1 and previous releases, when users use DistributedDataParallel (DDP), all buffers were synced automatically even if users set flag `broadcast_buffers` to be `False`:\r\n```python\r\nfrom torch.nn.parallel import DistributedDataParallel as DDP\r\nmodule = torch.nn.Linear(4, 8)\r\nmodule = DDP(module) # Buffer is synchronized across all devices.\r\nmodule = DDP(module, broadcast_buffers=False) # Buffer is synchronized across all devices.\r\n...\r\n```\r\n\r\n* Starting with PyTorch 2.1, if users specify the flag `broadcast_buffers` to be `False`, we don\u2019t sync the buffer across devices:\r\n```python\r\nfrom torch.nn.parallel import DistributedDataParallel as DDP\r\nmodule = torch.nn.Linear(4, 8)\r\nmodule = DDP(module) # Buffer is synchronized across all devices.\r\nmodule = DDP(module, broadcast_buffers=False) # Buffer is NOT synchronized across all devices\r\n...\r\n```\r\n\r\n### Remove store barrier after PG init (#99937)\r\n* In PyTorch 2.0.1 and previous releases, after we initialize PG, we always call store based barrier:\r\n```python\r\nfrom torch.distributed.distributed_c10d import init_process_group\r\ninit_process_group(...) # Will call _store_based_barrier in the end.\r\n...\r\n```\r\n\r\n* Starting with PyTorch 2.1, after we initialize PG, the environment variable `TORCH_DIST_INIT_BARRIER` controls whether we call store based barrier or not:\r\n```python\r\nfrom torch.distributed.distributed_c10d import init_process_group\r\nimport os\r\nos.environ[\"TORCH_DIST_INIT_BARRIER\"] = \"1\" # This is the default behavior\r\ninit_process_group(...) # Will call _store_based_barrier in the end.\r\nos.environ[\"TORCH_DIST_INIT_BARRIER\"] = \"0\"\r\ninit_process_group(...) # Will not call _store_based_barrier in the end.\r\n...\r\n```\r\n\r\n### Disallow non-bool masks in `torch.masked_{select, scatter, fill}` (#96112, #97999, #96594)\r\nFinish the deprecation cycle for non-bool masks. Functions now require the `dtype` of the mask to be `torch.bool`.\r\n\r\n```python\r\n>>> # 2.0.1\r\n>>> inp = torch.rand(3)\r\n>>> mask = torch.tensor([0, 1, 0], dtype=torch.uint8)\r\n>>> torch.masked_select(inp, mask)\r\nUserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1855.)\r\n  torch.masked_select(inp, mask)\r\n\r\n>>> torch.masked_select(inp, mask.to(dtype=torch.bool))\r\n# Works fine\r\n\r\n>>> correct_mask = torch.tensor([0, 1, 0], dtype=torch.bool)\r\n>>> torch.masked_select(inp, correct_mask)\r\n# Works fine\r\n\r\n>>> # 2.1\r\n>>> inp = torch.rand(3)\r\n>>> mask = torch.tensor([0, 1, 0], dtype=torch.uint8)\r\n>>> torch.masked_select(inp, mask)\r\nRuntimeError: masked_select: expected BoolTensor for mask\r\n\r\n>>> correct_mask = torch.tensor([0, 1, 0], dtype=torch.bool)\r\n>>> torch.masked_select(inp, correct_mask)\r\n# Works fine\r\n\r\n>>> torch.masked_select(inp, mask.to(dtype=torch.bool))\r\n# Works fine\r\n\r\n```\r\n\r\n### Fix the result of `torch.unique` to make it consistent with NumPy when `dim` is specified (#101693)\r\n\r\nThe `dim` argument was clarified and its behavior aligned to match the one from NumPy to signify which sub-tensor to consider when considering uniqueness. See the documentation for more details, https://pytorch.org/docs/stable/generated/torch.unique.html\r\n\r\n\r\n### Make the Index Rounding Mode Consistent Between the 2D and 3D GridSample Nearest Neighbor Interpolations (#97000)\r\n\r\nPrior to this change, for `torch.nn.functional.grid_sample(mode='nearest')` the forward 2D kernel used `std::nearbyint` whereas the forward 3D kernel used `std::round` in order to determine the nearest pixel locations after un-normalization of the grid. Additionally, the backward kernels for both used `std::round`. This PR fixes the inconsistencies to use `std::nearbyint` which rounds values that are exactly <>.5 to the nearest even which is consistent with the behavior of `torch.round`. Unnormalized indices that are exactly <>.5 will now be rounded to the nearest even instead of being rounded away from 0.\r\n\r\n### Turned input shapes (aka `record_shapes`) off by default for on-demand tracing (#97917)\r\nProfiler traces collected by on-demand tracing via IPC Fabric will have `record_shapes` off my default. \r\n\r\n* In v2.0.1:\r\nBy default, profiler trace files\u2019 `cpu_op` activities will contain metadata fields: Input Dims, and Input type.\r\n\r\n* In v2.1.0:\r\nBy default, profiler trace files\u2019 `cpu_op` activities will no longer contain metadata fields for input shapes. If turned on via Kineto config, it will show metadata fields: Input Dims, Input type and Concrete Inputs.\r\n\r\n### When called with a 0-dim tensor input, **`torch.aminmax`** would previously inconsistently return a 1D tensor output on CPU, but a 0D tensor output on CUDA. This has been fixed, so we consistently return a 0D tensor in both cases. (#96171).\r\n\r\nIn v2.0.1:\r\n\r\n```python\r\n>>> torch.aminmax(torch.tensor(1, device='cpu'), dim=0, keepdim=True)\r\n__main__:1: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:24.)\r\ntorch.return_types.aminmax(\r\nmin=tensor([1]),\r\nmax=tensor([1]))\r\n>>> torch.aminmax(torch.tensor(1, device='cpu'), dim=0, keepdim=False)\r\ntorch.return_types.aminmax(\r\nmin=tensor(1),\r\nmax=tensor(1))\r\n```\r\n\r\nIn v2.1.0:\r\n\r\n```python\r\n>>> torch.aminmax(torch.tensor(1, device='cpu'), dim=0, keepdim=True)\r\ntorch.return_types.aminmax(\r\nmin=tensor(1),\r\nmax=tensor(1))\r\n>>> torch.aminmax(torch.tensor(1, device='cpu'), dim=0, keepdim=False)\r\ntorch.return_types.aminmax(\r\nmin=tensor(1),\r\nmax=tensor(1))\r\n```\r\n\r\n### Change to the default behavior for custom operators registered to the dispatcher, that do not have anything registered to an Autograd dispatch key \r\n\r\nIf you have a custom operator that has a CPU/CUDA kernel registered to the CPU/CUDA dispatch key, but has no implementation at the Autograd key, then:\r\n\r\n**Old behavior:** When calling this operator with tensor inputs that require gradients, the tensor outputs would silently not require gradients.\r\n\r\n**New behavior:** When calling this operator with tensor inputs that do require gradients, the tensor outputs would require gradients (as long as the outputs are floating-point or complex), and will error if you try to backpropagate through them.\r\n\r\nThere is more information on how to recover the old behavior in the PR: (#104481, #105078) \r\n\r\n### `torch.autograd.Function` Raise an error if input is returned as-is and saved for forward or backward in `setup_context` (#98051)\r\n\r\nIf you are writing a custom autograd Function and you have implemented your autograd Function using `setup_context`, and if your forward function returns an input as-is as output, then saving that tensor for forward or backward now raises an error. You should return an alias of the input instead.\r\n\r\n#### 2.0.1\r\n```python\r\nclass Cube(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(x):\r\n        return x ** 3, x\r\n\r\n    @staticmethod\r\n    def setup_context(ctx, inputs, outputs):\r\n        cube, x = outputs\r\n        ctx.save_for_backward(x)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output, grad_x):\r\n        # NB: grad_x intentionally not used in computation\r\n        x, = ctx.saved_tensors\r\n        result = grad_output * 3 * x ** 2\r\n        return result\r\n\r\n```\r\n\r\n#### 2.1\r\n```\r\nclass Cube(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(x):\r\n        return x ** 3, x.view_as(x)\r\n\r\n    ...\r\n```\r\n\r\n# Deprecations\r\n\r\n### Deprecate not specifying the `use_reentrant` flag explicitly when using `torch.utils.checkpoint` (#100551)\r\n\r\nIn PyTorch 2.1, if the `use_reentrant` flag is not explicitly passed, a warning is raised. To retain current behavior, pass `use_reentrant=True`. The default value will be updated to `use_reentrant=False` in the future. We recommend using `use_reentrant=False`.\r\n\r\n#### 2.1\r\n```\r\ntorch.utils.checkpoint(fn, (a,)) # Warns in 2.1\r\n```\r\n\r\n### Deprecate `torch.has_*` attributes (#103279)\r\n\r\nUse the version in the particular backend module at `torch.backends.*` to access these flags.\r\nAlso note that we now properly differente `is_built()` (compile time availability) and `is_available()` (runtime availability) in these modules.\r\n\r\n\r\n### Deprecate `check_sparse_nnz` argument for `torch.autograd.gradcheck` (#97187)\r\n\r\n#### 2.0.1\r\n```\r\ntorch.autograd.gradcheck(fn, inputs, check_sparse_nnz=True)\r\n```\r\n\r\n#### 2.1\r\n```\r\ntorch.autograd.gradcheck(fn, inputs, masked=True)\r\n```\r\n\r\n### `NVFuser` integration with `TorchScript` is deprecated (#105185)\r\n\r\n`NVFuser` replaced Neural Network Compiler (NNC) as the default GPU fuser for TorchScript in PyTorch 1.13. In PyTorch 2.1, TorchScript switched its default fuser back to NNC. Additionally, `NVFuser` for TorchScript is now deprecated. Currently, users can still manually choose to use `NVFuser` instead of NNC, see [fuser options](https://github.com/pytorch/pytorch/blob/v2.1.0-rc3/torch/csrc/jit/OVERVIEW.md#fusers) for details on how to do this.\r\n\r\n# New features\r\n## Release Engineering\r\n- Adding AArch64 wheel builds (#104109)\r\n- CUDA 12.1 support for PyTorch binaries (#107295)\r\n- Compile PyTorch on M1 natively (instead of cross compiling from x86) (#95719)\r\n- Enable UCC Distributed Communication Backend in CI (#100395)\r\n\r\n## Python Frontend\r\n- Enable `torch.device` to be used as a context manager to change the default device (#106514)\r\n- Add `torch.Tensor.dim_order` field to access current dimension permutation (#106835)\r\n- Add `torch.backends.cpu.get_cpu_capability()` to expose cpu properties (#100164)\r\n- Add support for `PrivateUse1` device (out of tree device) for untyped storage (#100868)\r\n- Add `torch._foreach_pow` (#92303)\r\n\r\n## optim\r\n- Provide **NAdamW** implementation through the `decoupled_weight_decay` flag (#103881, #107706)\r\n- Add xpu support for foreach kernels (#106021)\r\n- Add capturable API w/ tests + fix differentiable for **NAdam** (#106615)\r\n- Add pre hooks and post hooks for optimizer `load_state_dict()` and `state_dict()` (#105953, #106209)\r\n\r\n## torch.compile\r\n- Automatic dynamic shapes - `torch.compile` automatically finds dynamic dims and selectively turns on dynamism - (#106188, #98923)\r\n- Support Python 3.11 version with TorchDynamo - (#94098, #94099, #94100, #94101, #94102, #96499, #96500, #96501, #96503, #96504, #96495, #96505, #96506, #96508, #96509, #98032, #98364, #96511, #99934)\r\n- Introduce `TORCH_LOGS` to enable better logging UX for `torch.compile` - (#94858, #98564, #98776, #98795, #100664)\r\n- NumPy support in `torch.compile` (#106211)\r\n- Higher Order ops - TorchDynamo supports higher order ops - (#101707, #104685, #106425, #107459, #107461)\r\n\r\n## Sparse Frontend\r\n- Add prototype implementation of semi-structured (often known as 2:4 sparsity) for NVIDIA's Ampere, and newer, architecture at `torch.sparse.semi_structured` (#100485, #103700, #107398, #103978, #103830, #104608, #101339)\r\n\r\n## Autograd\r\n- Add backward support for out-of-place foreach functions  (#93901)\r\n- Add backward support for for in-place foreach functions (#96405)\r\n- Add backward support on `_foreach_zero_` (#101149)\r\n- Add forward mode AD to out-place foreach functions  (#106320)\r\n- Add forward over backward support for `torch.nn.functional.logsigmoid` (#99288)\r\n- Add forward mode AD to in-place foreach functions (#100695)\r\n- Add forward mode AD for `torch.renorm` (#100798)\r\n\r\n## torch.nn\r\n- Add an optional scale kwarg to `scaled_dot_product_attention` (#95259)\r\n- Add non-recursive `nn.Module.to_empty` option (#104197)\r\n- Add keyword argument to allow disabling bias for `LayerNorm` (#101683)\r\n- Add option to always call `nn.Module` global/non-global forward hooks (#104278)\r\n- Add keyword argument to allow disabling bias for `Transformer` (#101687)\r\n\r\n## torch.export\r\n- Add public APIs to `torch.export`: `export()` (#107609), `dynamic_dim()`(#107635), `constrain_as_{size,value}` APIs (#107735) and `ExportedProgram` (#107852)\r\n- Add support for saving and loading exported programs (#107309, #102707, #107924, #102708, #103274, #107818,\r\n#107938, #107420, #107666, #107386, #102716, #103273, #107888)\r\n- Add [ExportDB page](https://pytorch.org/docs/2.1/generated/exportdb/index.html#exportdb) in pytorch.org  ([#104288)\r\n- Add a verifier for EXIR Aten dialect (#94783, #100019)\r\n\r\n## functorch\r\n- Add experimental support for functorch transforms and `torch.compile` composition (#98328, #106610, #107462, and others)\r\n- Add `functorch.einops.rearrange` (#101957) \r\n\r\n## Distributed\r\n### c10d\r\n- Add `PrivateUse1` for dispatching PyTorch Distributed Collectives to support custom device. (#98137) \r\n- Add new `Store` methods: `append`, `multi_get`, `multi_set`. (#100379)\r\n- Implement coalesced `all_gather_into_tensor` (#101157)\r\n- Implement coalesced `reduce_scatter_tensor` (#103561)\r\n- Add back in `reduce_scatter_tensor_coalesced` (#104345)\r\n- Design a new fake process group aimed at running a single rank with a fake process group without needing multiple processes. A fake process group (not related to `FakeTensor`) is a process group which doesn't actually do any communication, but instead just just hallucinates communication. (#102180, #102238, #104213, #104428)\r\n- Enable barrier to support the specified device (#99589)\r\n- Add xpu to the default device supported by user specified backend (#103410)\r\n- Support third-party devices to use the `init_process_group` method without specifying the backend (#107113) \r\n### Distributed Tensor \r\n- Add `DTensor` constructor function: `ones`/`empty`/`full` (#100933, #101022, #103165) \r\n- Enable `DTensor` based Native sequence parallelism (#94369)\r\n- Enable DDP + TP 2D parallelism (#106583)\r\n- Enable `deviceMesh` to use dispatchable PG to support custom backend (#102336)\r\n- Allow ONNX Runtime (ORT) backend for `DTensor` (#101914)\r\n### FullyShardedDataParallel:\r\n- Introduce `CustomPolicy` in FSDP wrapping (#104986)\r\n- Add FSDP support for creating hybrid-sharded process group for custom backend (#100622)\r\n### DTensor based Distributed Checkpoint\r\n- Add 1D `DTensor` based DCP (#94868)\r\n\r\n## Profiler\r\n- Add a global flag to record concrete shapes, which are Scalar lists in profiler traces (#101043, #101292)\r\n- Add `export_memory_timeline` to save memory timeline plot to file (#96137, #96535)\r\n- Reintroduce forward-backward links in profiler traces with a global flag (#102424, #102492)\r\n- Add Kineto synchronization events in profiler traces (#105187, #105144)\r\n- Add support for `cuLaunchKernel` in profiler traces for triton kernel launches including flow events in profiler traces (#99571)\r\n- Add CUDA runtime events up to CUDA 12.0 for traces, and added flow events for H100\u2019s `cudaLaunchKernelExC` (#106293)\r\n\r\n## ONNX\r\n### New TorchDynamo ONNX Exporter\r\n- Implement a new exporter core infrastructure and expose public APIs for it (#97920, #99940, #99202, #102810, #104736, #104493, #105040, #106228, #99284, #96349, #107245, #100490, #95650, #95676, #96350, #100554, #94878, #99191)\r\n  - `torch.onnx.dynamo_export`\r\n  - `torch.onnx.ExportOptions`\r\n  - `torch.onnx.ExportOutput`\r\n- Add an operator registry (#103943, #106140)\r\n- Add an operator dispatcher (#104679, #105104, #104267, #105972, #106478, #100660,\r\n- Add operator validation (#94920, #97494, #105874, #104268)\r\n- Add Input/Output adapter (#98421)\r\n- Enable `ExportOutput.save` for models larger than 2GB (#107904)\r\n- Add Pre-ONNX FX Passes (#95664, #95935, #98633, #97729, #95929, #98760)\r\n  - Functionalization (#98245, #99667)\r\n  - Explicit type promotion (#104063, #104064, #104229, #104720, #104491, #106178)\r\n  - Export module as function (#105618, #107409)\r\n- Add fake mode export (#103865, #105246, #105247, #105477, #106930, #107836)\r\n  - `torch.onnx.enable_fake_mode()`\r\n- Add SARIF](https://sarifweb.azurewebsites.net/) diagnostic system ([#99668, #99944, #100219, #100299, #100407, #100451, #105231, #105263, #105886, #105889, #105892, #106048, #106592, #106741, #107165, #107653, #107654, #107158)\r\n\r\n### New `torch.compile` ONNX Runtime backend (#107973, #106929, #106589)\r\n```python\r\nUsage: `torch.compile(..., backend=\"onnxrt\")`\r\n    Available when `torch.onnx.is_onnxrt_backend_supported()` returns `True`\r\n    Additional Python package dependencies: `onnx`, `onnxscript`, `onnxruntime`\r\n```\r\n\r\n### Additional TorchScript ONNX exporter operators:\r\n- `aten::_convolution_mode` (#89107)\r\n- `aten::unflatten` (#99056)\r\n- `aten::scaled_dot_product_attention` (#99658)\r\n- `aten::tile` (#99927)\r\n- `aten::atan2` (#100040)\r\n- `aten::broadcast_to` (#101833)\r\n- `aten::scatter_reduce` (#102048)\r\n- `aten::logit` (#102377)\r\n- `aten::hstack`, `aten::vstack` (#102872)\r\n- `aten::atleast_1d`, `aten::atleast_2d`, `aten::atleast_3d` (#103061)\r\n- `aten::logical_not` (#96315)\r\n- `aten::randint` (#105089)\r\n- `aten::norm`: Supports `dtype` argument (#95637)\r\n\r\n### Others\r\n- Add initial support for FP8 ONNX export (#107962)\r\n\r\n## MPS\r\n- Add support for `MPSProfiler` (#100635, #101002, #101692)\r\n- Enable saved models to be loaded directly to MPS through `torch.jit.load` (#102204)\r\n- Introduce `torch.mps.Event()` APIs (#102121)\r\n\r\n## torch.fx\r\n- Add attribute node matching in the subgraph rewriter (#98604)\r\n- Add variadic arg matching in the subgraph matcher (#99431)\r\n- Add a flag to ignore literals with subgraph matcher (#97683)\r\n- Add a prototype `source_fn` based partitioner to partition modules that were flattened in export (#98628, #101121)\r\n- Add `aggressive_merge` to `CapabilityBasedPartitioner` which merges independent subgraphs (#100195)\r\n\r\n## Quantization\r\n- Add various uninterpreted bit tensor data types (`torch.{bits1x8,bits2x4,bits4x2,bits8,bits16}`) (#95860)\r\n- Add basic cuda support for `float8` dtypes (#105807)\r\n- Add Autocast Support for XLA (#96370)\r\n\r\n### Export Quantization:\r\n  - Quantizer and Annotation API (#97994, #101708, #101920, #102054, #102184, #102282, #102439, #102582, #105484, #106922, #107833);\r\n  - `XNNPACKQuantizer` (#98507, #98569, #98560, #99063, #99399, #100566, #101122, #102394, #102395, #102396, #102397, #102398, #102703, #103526, #105551, #106087, #106094, #107872, #107992);\r\n  - `EmbeddingQuantizer` (#103088);\r\n  - `ComposableQuantizer` (#102846);\r\n  - `X86InductorQuantizer` and Kernels (#98730, #98826, #105639, #106836, #106781, #105818,  #104580);\r\n  - Quantization Aware Training (#98568, #100283, #100442, #100525, #100610, #100941, #102224, #102993, #103298, #103731, #103759, #103970, #104110);\r\n  - Reference Quantized Model Representation (#104130, #105707, #104395, #105708, #105783, #105784, #107810);\r\n  - Program Capture (#107941)\r\n\r\n## JIT\r\n- Register ops for `torch.get_cpu_capability`, `Tensor.is_xla`, so they can be captured by torchscript (#100723)\r\n- Provide `__prepare_scriptable__` on non-`nn.Module` classes as an escape hatch to provide a scriptable alternate implementation (#106229)\r\n- Introduce API to `deepcopy` a JIT module onto a new device (#106521)\r\n\r\n## Vulkan\r\n- Add Vulkan support for the following operators: `aten::unsqueeze`for 2d to 3d (#101719), `aten::cat` operator for 1d, 2d, 3d and 4d (#102128), `aten::expand` (#103930), `aten::flip` (#106628), `gelu` (#102762), `aten::masked_fill` (#104444), `aten::pow` (#105550), `at::softmax` 1,2,3 dimension tensors (#105012), `at::softmax` along all dimensions for 4-dim Tensors (#102988), `sum.dim_IntList` (#105612), `sum.dim_IntList` with keepdim (#106159), `at::select.int` operator, 4 dim/rank tensor case (#96228), `aten::stack` (#103344), `aten::uniform` (#102431), `aten::unsqueeze`, 1d->2d, 3d->4d (#102987), `aten::repeat` (#103255), `aten::tile` (#103944), `aten::zero_` (#103042), `aten::zeros` (#103703), `convert_qconv2d_context` (#97714), \"height\" and \"width\" dimension for `select` operator (#94612), `t` and `transpose` operators for 2d, 3d and 4d tensors (#101808), unary ops (#104994), `upsample_bilinear2d` (#98022), `upsample_nearest2d` and `quantized_upsample_nearest2d` (#97467), `quantize_per_tensor` vulkan backend function (#106641), quantized binary ops (`add`/`sub`/`mul`/`div`), and adding graph rewrites for quantized `add`, `mul`, `conv2d` and `conv2d_relu` (#97468)\r\n- Add broadcast support for 4D tensors where the batch and channel of a tensor are different (#104718)\r\n- Templatize `BinaryOp.cpp` (#105380)\r\n\r\n# Improvements\r\n\r\n## Python Frontend\r\n- Support non-ASCII characters in model file paths for `torch.{save,load}` (#99453)\r\n- Enable registering fallthroughs via `torch.library` (#106086)\r\n- Add support for saving and loading with any Endianness to `torch.{save,load}` (#94503) \r\n- Add `torch.storage.UntypedStorage.get_device` method (#99818)\r\n- Add type hint for `torch.__init__.py` (#106214, #103807), `torch.Tensor.retains_grad` (#103528)\r\n- Add support for HPU device for serialization (#101680)\r\n- Add support for XPU device for old-style Tensor classes (#96656), storage resize_ (#105262)\r\n- Mark `torch.bincount` deterministic on CUDA if weights are not given (#105244)\r\n- Properly expose all constraints on the `torch.distributions` (#106458)\r\n- Add `itemsize` and `nbytes` properties to Tensor (#98322)\r\n- Add complex support for `torch.expm1` (#96644)\r\n- Add `nonzero_static` op to pytorch to unblock export (#97417)\r\n- Tweak heuristic for Scaled Dot Product Attention (SDPA) selection based off of data (and a decision tree) (#99644)\r\n- Fix print `tensor.type()` issue. (#96381)\r\n- Improve error messages in `THPVariable_set_grad` (#100683)\r\n- Enable `new_full`'s fill_value argument type to be complex, for more accurate type checking (#91345)\r\n- Add 0-dim (zero dimension) Tensor overload to `_foreach_mul` (#106677)\r\n- Add in-place `_foreach_copy` (#107226)\r\n- Support floating point correction value for std/var operators (#94073)\r\n\r\n## Dataloader and DataPipe\r\n- Fix `validate_input_col` for partial functions (#95067)\r\n- Add support for pin memory on custom device (#97621)\r\n- Fix collation logic (#97789)\r\n- Add context to NotImplementedErrors in dataset.py (#100667)\r\n- Add `__getitems__` to description of Dataset API, and also better support within `Subset` (#100375)\r\n- Adding `StackDataset` (#101338)\r\n- Change DataPipe display name in profiler (#100042)\r\n- Add `copy` option to `fork` DataPipe (#96030)\r\n- Fix missing imports in DataPipe interface file (#97458)\r\n- Do not materialize entire `randperm` in `RandomSampler` (#103339)\r\n\r\n## torch.nn\r\n- Add check that `embedding_bag`'s weight is 2D (#94931)\r\n- Improve error message for instance norm when channels is incorrect (#94624)\r\n- Add generator argument to `nn.init.trunc_normal_` (#100810)\r\n- Improve `clip_grad_norm`  to use `torch.linalg.vector_norm` (#102429)\r\n- Add `uint8` support for CPU images in `interpolate(mode='bicubic\u2019)` (#103252)\r\n- Allow `nn.ChannelShuffle` to run without error on CUDA tensors (#105351)\r\n- Add `nn.CircularPad{3/4/5d` and fixed `no_batch_dim` support for CircularPad (#106632)\r\n- Add `reset_parameters` for `torch.nn.PRelu`  (#106507)\r\n- Use accumulate type to improve accuracy of `grid_sample` on half precision inputs on CUDA (#96586)\r\n- Improve performance for vectorized bilinear interpolate `cpu` `uint8` channels last (#96848))\r\n- Add differentiable `mkldnn_rnn_layer_backward` to support double backward of LSTM (#100627)\r\n- Add `is_causal` API for `TransformerDecoder` (#97166)\r\n- Add `is_causal` hints for `Transformer` (#106143)\r\n- Enable channels last for reflection padding on CPU (#102518, #102597)\r\n- Add `bfloat16` support for reflection and replication padding (#102949)\r\n- Add `SyncBatchNorm` support for custom device (#104250)\r\n- Add channels last 3d support for `BatchNorm` on CPU (#97774)\r\n\r\n## functorch\r\n- Add `torch.vmap` support for `torch.complex` (#96032), overloads of `float_power`, `where`, and `comparison` ops. (#96744), `linalg.lu_factor` (#94328), `ldl_factor` (#97518), `torch.copysign` (#96018), `torch.nn.functional.smooth_l1_loss` (#98357), `nn.functional.huber_loss` (#99235, #99236), special bessel functions (#99543), `torch.nn.functional.{max_pool1d, max_pool3d}` batch_rule (#99517, #99522), `Tensor.index_fill` (#99229), `torch.bucketize` (#95783), `smooth_l1_loss_backward` (#99429)\r\n\r\n## optim\r\n- Merge and improve torch optim optimizer type stubs (#102593)\r\n- Allow fused optimizers to call `_foreach_zero_` in `zero_grad` (#97159)\r\n- Add multi Stochastic Weight Averaging (SWA) support for custom device (#103297)\r\n- Use `torch._foreach_lerp` for SWA update (#103550)\r\n- Add XLA friendly codepath to `single_tensor_adamw` (#102858)\r\n\r\n## Linear Algebra\r\n- `lerp(cpu)`: Add `half` support (#105607)\r\n- `norm(cpu)`: Accumulate in `float` when inputs are `half` or `bfloat16` (#95166)\r\n- `matmul`: Avoid unnecessary copies (#97355)\r\n- `matmul` backwards: Don\u2019t create large intermediary tensors (#95261)\r\n- `addmm`: Call to `mkldnn_matmul` on AArch64 (#91763)\r\n- `addmm(cuda)`: Enable `addmm` + GELU epilogue fusion (#103811)\r\n- `dot/mv/gemm(cpu)`: Accumulate in `float` for `bfloat16` inputs in the fallback path (#96074)\r\n- `bmm`: Heuristics for AArch64 (#107167)\r\n- `baddbmm(cpu)`: Fix grain size setting (#98297)\r\n- `mm(cuda)`: Expose cuBLAS int8@int8 -> int32 (#96685)\r\n- `triu/tril`: complete dtype support. (#101414)\r\n- Enable hipSOLVER in ROCm builds (#97370)\r\n- Improve error message in `ADDMM_META()`. (#105309)\r\n- Allow setting `TORCH_LINALG_PREFER_CUSOLVER=1` to prefer cusolver as linear algebra library globally (#106226)\r\n- `ldl_factor(cuda)`: Enable hipSOLVER backend in ROCM (#102665)\r\n- Add `SymInt` support for `{tensordot,inner,linalg.{matrix_power,tensorinv}}`. (#100356, #101940, #102465)\r\n- Add fake tensor support for SVD. (#100130)\r\n\r\n## Autograd\r\n- Improve `torch.utils.checkpoint` with `use_reentrant=False`:\r\n  - Support recursive checkpointing; allow grad calls within checkpointed function (#90105)\r\n  - Allow the specification of a pair of context functions via `context_fn=` (#96783)\r\n  - Stop recomputation early if possible; enabled by default but also expose a way to disable (#96866)\r\n  - Improve debuggability of activation checkpoint; expose `debug=` and `determinism_check` kwargs (#103859)\r\n- Allow `torch.inference_mode`, `torch.no_grad`, `torch.enable_grad` decorators to be used without parens (#107086)\r\n- Allow `torch.autograd.set_multithreading_enabled` to act as function and context manager  (#105291)\r\n- Add `materialize_grads` parameter to `torch.autograd.grad()` (#97015)\r\n- Allow `torch.autograd.Function` to save non-input leaf tensors for backward (#104039)\r\n- `sampled_addmm`: backward performance improvements (#103544)\r\n\r\n## Sparse\r\n- Add rudimentary support for addmv(strided, CSR, strided) on CPUs without MKL support (#97353, #97730)\r\n- Implement sparse semantics support in gradcheck (#94714, #95405, #96095, #107150)\r\n- Add add(COO, COO) for BFloat16 on CPU (#96767)\r\n- Add support for negative dim to torch.sparse.softmax for COO (#102172)\r\n- Add support for dim to sum for CSR on CPU and CUDA (#99292)\r\n- Add integer overflow checks to sparse tensor invariant checker for large compressed tensor dimensions and large nnz (#102530)\r\n\r\n## Nested Tensor\r\n- Support `zeros_like()` and `randn_like()` for nested tensor (#96527, #96528)\r\n- Add backwards for layer norm for nested tensor (#94781)\r\n- Support elementwise add / mul for \\[B, *\\] nested, \\[B, 1\\] dense (CUDA only) (#95620)\r\n- Enabling FlashAttention for SDPA when given NestedTensor (#95438)\r\n- Add `sub`, `sgn` `abs` ops for nested tensor (#97837)\r\n- Implement last dim `split_with_sizes` for NestedTensor(forward only, non-SymInt-ified) (#97446)\r\n\r\n## Foreach Frontend\r\n- Move tensor grouping to ATen (#103912)\r\n- Disable grouping by dtype and device if compiling (#102771)\r\n- Add fused support for XPU devices (#104517)\r\n\r\n## Build Frontend\r\n- `_mm_prefetch` is for Intel, changed to `__prefetch` for ARM64 (#96638)\r\n- Build PyTorch with `-Wnewline-eof` (#99687)\r\n- conditional `CMAKE_CUDA_STANDARD` (#104240)\r\n- cmake: allow `USE_SYSTEM_ZSTD` (#104611)\r\n\r\n## CPU\r\n- Introduce fast path for equal and concat: (#100024, #106727)\r\n- Add channel last 3d support for `MaxPool3d` on CPU (#97775)\r\n- Add Half support for `logsigmoid`, `threshold`, `elu`, `gelu`, `hardtanh`, `hardsigmoid`, `hardswish`, `hardshrink`, `softshrink`, `leakyrelu`, `softplus`, `glu`, `silu`, `mish`, and `prelu` on CPU (#98745)\r\n- Make `index_add_` error if input source shape is wrong (#100321)\r\n- Enable taskset core pinning in addition to numactl (#96011)\r\n- Add explicit vectorization for Half dtype on CPU (#96076)\r\n- Add `Half` support for sigmoid on CPU (#96077)\r\n- Add `Half` to cat fast path on CPU (#96078)\r\n- Use `float` as accumulate type for reduce Ops: `min`, `max`, `minmax` on CPU (#96079)\r\n\r\n## CUDA\r\n- Support `bf16` dtype for `conv_depthwise3d` and `searchsorted` (#97819, #99426)\r\n- Support integer dtypes for padding (cpu and cuda) (#107755)\r\n- Support complex dtype for Sigmoid Linear Unit (SILU) (#106854)\r\n- Add additional stream priority for cuda streams (#101956)\r\n- Prevent grad scale from overflowing (#98876)\r\n- `nn.EmbeddingBag` bound check (#96022)\r\n- Hide `set_device` change (#94864)\r\n\r\n## MPS\r\n- Add `lerp` implementation (#105470), `logit` (#95162), `hardsigmoid` (#95164),`hypot` (#95196), `xlogy` (#95213), `log_sigmoid`(#95280), `fmax` and `fmin` (#95191), `roll`(#95168), `copysign` (#95552), `pow.Scalar` (#95201), `masked_scatter` (#95743), `index_fill` (#98694), `linalg.vector_norm` (#99811), `histogram` (#96652), `aminmax` (#101691), `aten::erfinv` (#101507), `cumprod` (#104688), `renorm` (#106059), `polar` (#107324)\r\n- Add optional minor argument to `is_macos13_or_newer` (#95065)\r\n- Allow `float16` input to `float32` `LayerNorm` (#96430)\r\n- Add higher order derivatives warning to `max_pool2d` (#98582)\r\n- Expose `mps` package in torch (#98837)\r\n- Prerequisite for MPS C++ extension (#102483)\r\n\r\n## torch.export\r\n- Change attributes of ExportedProgram to properties and add BC decorator #106170\r\n- Throw explicit error when constraining on static values (#101655)\r\n- Store the arguments used to trace the exported program in itself to facilitate (#107906)\r\n- Add kwargs support for export. (#105337)\r\n- `ExportedProgram.transform` updates `graph_signature` automatically (#107792)\r\n- Support preserving calling convention to some modules so that they can be properly unflattened. (#106798)\r\n- Make pass base composable (#103701)\r\n- Remove unused flags in export (#106336)\r\n- Update the core Aten operator set:\r\n  - Add 23 ops to core Aten set (#107766)\r\n  - Remove `split.Tensor` from core Aten (#107938)\r\n- Allow registration of dataclass as pytree node (serialization of it not supported yet) (#106160)\r\n- Support re-exportability (#106531)\r\n\r\n## torch.fx\r\n- Rewrote graph traversal to avoid recursion (#95723)\r\n- Reorder the Fx execution order to in-time `get_attr` rather than putting all `get_attr` ahead (#95014(https://github.com/pytorch/pytorch/pull/95014 ))\r\n- Preserve node.meta for `get_attr` nodes in `fx.Transformer` (#95245)\r\n- Preserve output node metadata (#95426)\r\n- Copy `nn_module_stack` metadata whenever we create a new node when tracing (#95358)\r\n- Prettify assert expr in `self.symbol_to_source` failure (#95972)\r\n- Allow `torch.fx` to take Modules that return dataclass (#99576)\r\n- Add `has_side_effect` to add to a list of side effecting functions (#97288)\r\n- Change placeholder check instanceof PHBase (#102008)\r\n- Add metadata to PHBase placeholders (#102195)\r\n- Make `fx.wrap` idempotent (#104838)\r\n- Enable Python dispatcher when ShapeProp with fake mode (#103512)\r\n\r\n## Quantization\r\n- Add quantization support for `pixel_shuffle`, `pixel_unshuffle`, `narrow`, ConvTranspose, ConvTranspose3d (#94769, #96160, #97126, #97125, #101926)\r\n- Support static quantization for `LSTM` and `MultiheadAttention` (#96343, #96436, #101299, #95636)\r\n- Force weight observer/fake_quant to be on the same device as the weight tensor (#106755)\r\n- Add serialization method for quantized `hardswish` (#94486)\r\n- Enable `quantized_max_pool3d` (#101654)\r\n- Quantization oneDNN backend only support VNNI CPU (#103653)\r\n- Fix bug in `fuse_modules` (#105069)\r\n- Add `torch.matmul` in `FloatFunctional`/`QFunctional` (#106831)\r\n- Support quantized Sub, Multiply in XNNPACK (#104090, #104134)\r\n\r\n## Profiler\r\n### General Profiling\r\n- Profiler permitted CPU events with CUPTI Range Profiler mode (#97048)\r\n- Make Profiler API agnostic with respect to target hardware (#101554, #106142)\r\n- Improve on-demand profiling options for Input Shapes, Memory, Stack, Flops, and Modules (#97380, #97556)\r\n- When `record_inputs=True`, record scalar lists of length <= 30 (#100593)\r\n- Disable Kineto event profiler by default--due to flakiness; fixed thread sanitizer issue; and refactored `stress_test` (#105144)\r\n- Bump Kineto to C++17 (#106293)\r\n- `tb_plugin` to support HDFS and improved memory view (#106672)\r\n- Make on-demand update duration configurable, and improved start time for on-demand tracing (#101952)\r\n### Memory Profiling\r\n- Add support for HTML plot of memory profile via `export_memory_timeline` (#99751, #101316)\r\n- Include more uncategorized events in memory profiles (#101200)\r\n- Add export of raw memory events with timestamp via `export_memory_timeline_raw` (#105094)\r\n\r\n## ONNX\r\n### TorchScript ONNX exporter\r\n- Add `Autocast` support to `MatMul` through explicit cast (#98346)\r\n- Add missing spaces between sentences in warning text (#105527)\r\n- Improve shape inference for `Slice` (#105755)\r\n- Do not run `deduplicate_initializers` when `keep_initializers_as_inputs` is True (#96320)\r\n- Remove legacy diagnostic printing (#106498)\r\n- Re-purpose `name` field of `GraphProto` (#107408)\r\n- Add constant folding for `Softmax` op (#102861)\r\n- Add `autograd_inlining` flag to `torch.onnx.export` (#104067)\r\n- Update opset version warning text (#106830)\r\n\r\n## Distributed\r\n### Activation checkpointing\r\n- Enable `checkpoint_wrapper` acccept `auto_wrap_policy` (#102672) \r\n- Add warns on reentrant use (#102890)\r\n### DistributedDataParallel (DDP)\r\n- Enable delayed all reduce in DDP (#96673)\r\n- Enable DDP native mixed precision (#92882) \r\n- Add an API to remove autograd hooks from DDP (#96490) \r\n- Enable fused optimizer for DDP (#98270) \r\n- Perform input casting in pre-forward (#100131)\r\n- Implement new Store methods in `PrefixStore`. (#100380)\r\n- Unify `_cast_forward_inputs` (#102680) \r\n- Multiple forward support for static graph (#103487)\r\n- Add methods to DDP to check for backward finalization. (#100773)\r\n- Support optim in backward after DDP init (#105991, #105995) \r\n### FullyShardedDataParallel (FSDP)\r\n- Add alignment padding for `use_orig_params=True` (#97667) \r\n- Allow non-uniform `requires_grad` for `use_orig_params=True` (#98221) \r\n- Move only current FSDP's states to GPU during init (#98319) \r\n- Reshard frozen params in backward (#101982) \r\n- Support unfreezing params for reshard-only hook (#104186) \r\n- Standardize meta device init within FSDP (#104189) \r\n- Annotate modules for `fully_shard` (#104363) \r\n- Make `limit_all_gathers=True` default for FSDP (#104900) \r\n- Add `record_function` for explicit prefetching (#105985) \r\n- Optimize away intermediate `div_` for Hybrid Sharding Data Parallel (HSDP) (#106034) \r\n- Check valid param freezing for `ModuleWrapPolicy` (#104427) \r\n- Allow `ModuleWrapPolicy` to take Iterable (#104999) \r\n- Enable async all-reduce for Hybrid Sharding Data Parallel (HSDP) #106080) \r\n- Relax `sharded_grad` assert to allow IDLE state (#96584)\r\n- Copy step tensor so that each parameter has its own step (#96313) \r\n- Make FSDP `optim_state_dict` aware of DDP prefix (#96415) \r\n- Consolidate the arguments and logic of `optim_state_dict` and `optim_state_dict_to_load` (#96534)\r\n- Make it full precision in eval mode (#97645)\r\n- Include duplicate parameters and modules when calling `named_parameters` and `named_modules` (#99448) \r\n- Make `param_groups` optional for FSDP `optim.state_dict` (#99117)\r\n- Support `rank0_only` when `use_orig_params` is True (#99624)\r\n- Consolidate `rank0_only` load logic (#99647)\r\n- Make `fsdp` device-agnostic for custom-backend which implements cuda-semantics (#99024)\r\n- Ensure that customized non-tensor optimizer state can be saved (#99214) \r\n- Avoid elementwise dispatch of gradient unscaling/validation ops in `_foreach_non_finite_check_and_unscale_cpu_` (#100108)\r\n- Do not flatten states when `use_orig_param` is True and sharding is `NO_SHARD` (#100189)\r\n- Make `set_state_type` to `SHARDED_STATE_DICT` compatible with `NO_SHARD` sharding_strategy (#100208)\r\n- Allow each `fully_shard` unit to cast foward inputs for mixed precision config (#100290)\r\n- Restore the `state_dict_config` for `NO_SHARD` (#100855)\r\n- Skip unshard call during checkpointing for `NO_SHARD` sharding strategy (#101095)\r\n- Add `ignored_states` to FSDP/fully_shard (#102056)\r\n- Start to generalize modules to ignore for mixed precision (#102010)\r\n- Implement a workaround for FSDP init issue for 2D Parallel (#104398)\r\n- Improve support for CPU tensors. (#103171)\r\n- Avoid calling `optim.state_dict()` to get the initial empty states (#103609)\r\n- Use `_get_module_fsdp_state_if_fully_sharded_module` for state_dict (#103783)\r\n- Validate `ignored_modules`, `ignored_states` (#104273)\r\n- Check `module.training` for `_root_cast_forward_inputs` (#104223)\r\n- Add Option for eval in fp32/bf16 (#104682)\r\n- The correct way to initialize optimizer states if the corresponding param is empty (#104765)\r\n- Make `optim_state_dict_to_load` work with `use_orig_param=False` + `NO_SHARD` (#107185)\r\n- Expose optimizer `state_dict` config (#105949)\r\n- Enable custom device support in `fsdp` checkpoint (#107289)\r\n### Distributed Tensor (Prototype Release)\r\n- Add `_tenso.zero` function (#95863)\r\n- Enable the `nn.Embedding` op for `DTensor` (#96702, #104820)\r\n- Support creating `DTensor` in submesh (#95458)\r\n- Enable correct behavior of random ops in `DTensor` and Tensor Parallel (#98198, #98577, #103235, #103910, #106535)\r\n- Implement `aten.equal` sharding prop for `DTensor` (#97170)\r\n- Set `cuda` device automatically, and refactor error handling (#97583)\r\n- Set default value for `DTensor` ops on non-participating devices (#95852) \r\n- Change sharding algorithm to be in line with `torch.chunk` (#98722, #106250)\r\n- Add a new `ColwiseParallel` style when `Pairwise` cannot be used directly (#100137)\r\n- Enable more generic attention module sharding for PTD Tensor Parallelism (#100508) \r\n- Adopt strategy based sharding prop in `DTensor` ops (#100607, #101203)\r\n- Support torch.save/load with `DTensor` (#103106)\r\n- Allow `DTensor` to support `cuda`-like device (#102468) \r\n- Add an input resharding wrapper for TP and unit test for 2D + AC (#103334)\r\n- Add validate mesh flag to `DeviceMesh` (#104807) \r\n- Improve allgather unpadding logic (#103219) \r\n- Use stack to manage mesh resources (#101202)\r\n### Distributed (c10d)\r\n- Enable the handling of bool tensors in Gloo. (#105354)\r\n- Enable avoid `recordStream` calls in `ProcessGroupNCCL` an option (#89880)\r\n- Remove stack trace captures from import (#97274) \r\n- Update `_store_based_barrier` implementation to reduce load on rank 0 (#98000) \r\n- Remove lock for `nccl` collective launch for `nccl` 2.0+ (#97904) \r\n- Figure out the correct device to use for object collectives  (#100954) \r\n- Start gloo sequence numbers at 0. (#101422)\r\n- Add missing `torch.distributed.ReduceOp.AVG` in type stubs (#101534)\r\n- Determine collective device from `_get_pg_default_device` rather than from explicit cuda (#101533)\r\n- Enable configuration of NCCL communicators (#97394)\r\n- Make default backend need to check for `nccl` availability (#102470)\r\n- Add `is_backend_available` for c10d backend. (#101945) \r\n- Add one flag value for direct teardown without comm abort (#102599)\r\n- Make it the default that PG do not perform barrier after init (#103033)\r\n- Avoid `workEnqueue` when capturing cuda graph for NCCL process group (#103503) \r\n- Ensure `ncclCommAbort` can abort stuck `ncclCommInitRank` (#103925)\r\n- Allow previously initialized MPI (#105023)\r\n- Increase socket buffer size to allow ProcessGroup init up to 12k ranks (#107878) \r\n- Change `--standalone` to bind to a random port (#107734)\r\n- Initial commit of `collective_utils`  (#101037)\r\n### Distributed Checkpoint\r\n- Upstream `fsspec` storage read/write to PT (#98387) \r\n- Rewrote read slicing to use a wrapper. (#99167)\r\n- Consolidate OSS `FsspecWriter`/`Reader` and internal `FsspecWriter`/`Reader` (#104724)\r\n### Torch Elastic\r\n- Allow elastic agent to fail fast (#99051)\r\n### RPC\r\n- Add size check before calling `.back()` in `rpc/script_call.cpp` (#94297)\r\n\r\n## Dynamo\r\n- Support `nn.Module` forward hooks in Torch Dynamo (#92125)\r\n- Many graph break fixes - (#94949(https://github.com/pytorch/pytorch/pull/94949,  #94658, #102247 and others).\r\n- Translation validator for dynamo guards (#102563)\r\n- Update dynamo `sum` dtype handling to match eager (#103037)\r\n- Switch calling convention back to real tensors (#99320)\r\n\r\n## Inductor\r\n- Support more operators that fallback to eager previously: `rad2deg`, `deg2rad`, `count_nonzero`, `bitwise_right_shift`, `quantized.max_pool2d`, `erfc`, `erfinv`, `all_reduce`, `squeeze_copy`, `aten.prod`, `softshrink`, `aten.unfold`, `diagonal`, `diagonal_copy`, `diagonal_scatter`  ( #98994, #98995, #94997, #105906, #101416, #101863, #93111, #96039, #99484, #105603, #105165, #103755 )\r\n- Add decomposition rules for: `aten.normal_`, `lerp`, `aten.angle`, `unfold_copy`, `aminmax`, `nansum`, `fmin`, `fmax`, `narrow_copy`, `expand_copy`, `view_copy`, `smooth_l1_loss`, `full_like`, `affine_grid_generator`, `aten.dist` ( #91207, #104866 , #105609, #96038, #96039, #102077, #101963, #104709, #105586  )\r\n- `cudagraph` and `cudagraph` trees (#97440 , #98254, #89146, #98529, #102273, #105148  )\r\n- Add convolution triton template (#95556)\r\n- Pattern matcher infrastructure (#97740)\r\n- Use Welford algorithm to compute variance in a single pass (#104725 )\r\n- Value range analysis ( #102611 )\r\n- Do IR validation recursively ( #98887 )\r\n- Merge consecutive splits (#100107 )\r\n- Constant and `index_expr` propagation pass to simplify indexing expressions (#101077 )\r\n- Fallback `max_pool2d_with_indices` to eager rather than fail an assertion if dilation is not 1. (#100531 )\r\n- Don't fuse nodes with long distance to avoid increasing memory usage (#104024 )\r\n- Easier to add third-party backend ( #106874 )\r\n- Improvements on the CPU backend\r\n  - Support vertical reduction (#97644)\r\n  - Support dynamic shape (#97230, #102068 )\r\n  - Support masked load ( #107670 )\r\n  - Enable accuracy testing in CI (#94898)\r\n  - Enable Inductor to support BF16 atomic_add (#96620)\r\n- Improvements for AMD\r\n  - `tl.dot` and MFMA support enabled in ROCm triton for conv/mm lowerings (#107600)\r\n  - Remove ROCm `miopen_batch_norm` fallback, now lowering to triton (#100089)\r\n  - Enable \"reduce-overhead\" compile mode with hipgraph support on ROCm5.6 (#103092)\r\n- Align inductor behavior with eager mode for split_with_sizes (#99702)\r\n- Avoid decomposing `_unsafe_index` in Inductor (#107882)\r\n- Make CI error on inductor fallback when decomp is available (#99473)\r\n- Enable weight prepack for `LSTM` (#103071)\r\n- Enable `fused_attention` pattern matcher (#107128)\r\n- Add `fused_attention` pattern matcher with additional clone (#108141)\r\n\r\n## JIT\r\n- Include more shape information on tensor values in `jit.trace` functions (#95544)\r\n- Allow annotations using generics directly, e.g. `tuple` instead of `Tuple` (#98703)\r\n- Improve source attribution for easier debugging (#95761, #96423, #98606, #100171)\r\n- Shape functions implemented for `stack`, `squeeze.dims`, `cross_entropy_loss`, `conv_transpose` (#92205, #93919, #98078, #97875, #102139)\r\n- Partially support `ForwardRef` type annotations for `NamedTuple` attributes (#96933)\r\n- Optionally ignore UTF-8 decoding error when converting `std::string` to python `str`. (#97282)\r\n- Improvements to flatbuffer serialization and deserialization (#97190, #97298, #99050)\r\n- Support serialization/deserialization of >4GB strings (#99104)\r\n- Enable `torch.jit.load` for custom device (#99535)\r\n- Allow C++ custom class to define `__repr__` (#100724)\r\n\r\n## Misc\r\n- Better function annotations for `nn.functional` (#102918)\r\n- Add safe `weights_only` option to `load_state_dict_from_url` (#98479)\r\n- Enable Transparent Hugepages (THP) for buffer sizes >=2MB (#95963)\r\n- Automatic pulling of `ExtraFilesMap` without explicit mapping. (#99747)\r\n- Remove device transfers from Java Native Interface (JNI) (#105583)\r\n- `profile_plot` generates snapshot objects (#103497)\r\n- `vmap` Support for `torch.tril` and `torch.triu` (#94287)\r\n\r\n# Bug fixes\r\n\r\n## Python Frontend\r\n- Fix docstring setup to allow running PyTorch in python optimize mode (#100750)\r\n- Fix deserialization for `UpsamplingBilinear2d` (#101248)\r\n- Fix `torch.distributions.Dirichlet.log_prob` when `x=0` and `alpha=1` (#103605)\r\n- Fix `torch.distributions.Gumbel.cdf` (#91698\r\n- Fix PEP 484 Violation (#105022)\r\n- Fix bitwise shift operations when shifting out of bounds (#97150)\r\n- Fix `torch.asarray` to use the default device (#106779)\r\n- Fix deepcopy on `torch.Tensor` on MTIA device (#107427)\r\n- Add deterministic path for `Tensor.resize_` (#104300)\r\n- Fix `torch.pow` to handle real negative base and complex exponent (#95198)\r\n- Fix `LayerNorm(bias=False)` error (#108060)\r\n- Don't fastpath conj copy when conj/neg bit mismatch (#108881)\r\n\r\n## Autograd\r\n- Fix `torch.autograd.graph.register_multi_grad_hook` to not keep tensor alive in closure (#102859)\r\n- Fix autograd hooks being spuriously garbage collected by removing incorrect `THP{Cpp,}Function_traverse` `PyObject` traversals (#102860)\r\n- Fix `Tensor::register_hook` behavior on undefined tensors (#105587)\r\n- Handle undefined gradients out-of-place foreach backward (#100256)\r\n- Fix codegen logic for foreach derivatives (#95263)\r\n- Bump version counter for `torch{resize_, resize_as_}` (#96598)\r\n- Bump version counter for `foreach` functions  (#93901)\r\n\r\n## optim\r\n- Fix and enable `complex` x `amsgrad` support for Adam and AdamW (#104989, #104990)\r\n- Fix unpicklable object in `AveragedModel` (#95979)\r\n- Fix parameter list used in `weight_decay` for Adam (#100973)\r\n- Fix `optimizer` `state_dict` casting to allow step to cast for fused/capturable (#102619)\r\n- Update `lr_scheduler.py` to check the type of eta_min (#97003)\r\n- Fix issue with `lr_scheduler` serialization containing bound methods (#102627)\r\n\r\n## torch.nn\r\n- Fix `int()` casting in `torch.nn.RNN` to have correctly traced JIT and ONNX graph. (#92970)\r\n- Fix device handling in  `nn.utils.rnn.unpad_sequence` (#98042)\r\n- Fix `torch.nn.FractionalMaxPool2d` output_size error (#99507)\r\n- Fix inconsistent `torch.nn.MaxPool1d` output on cpu and gpu (#99843)\r\n- Fix device of `lengths` in `pack_padded_sequence` when the default device is GPU (#103967)\r\n- Fix bias overflow for memory efficient attention in `scaled_dot_product_attention` (#107968)\r\n- Update scaled_dot_product_attention dispatch logic to check for sm86 and head_size == 128 for flash attention  (#94921)\r\n- Raise type error message for `interpolate` if `size` contains non-integer elements (#99243)\r\n- Fix a bug in interpolate uint8 AVX2 on non-contiguous input (#101136)\r\n- Fix bug in interpolate when interpolation size is larger than max (#101403)\r\n- Add error if `stateless.functional_call` is called with `nn.DataParallel` (#107403)\r\n- Fixing interpolate on uint8 unsqueezed 3D CL tensor (#100258)\r\n- Add check for 0 to 1 inclusive for elements of target tensor in BCE loss (#97814)\r\n\r\n## functorch\r\n- Fix torch.vmap support for `torch.roll` (#95048)\uff0c `nn.{PixelShuffle, PixelUnshuffle}`(#96493)\r\n- Add better error message for mutating .data under functorch transforms (#94817)\r\n- Fix `functorch.jacrev` support for `torch.take` (#95772) \r\n- Fix `functorch` support for transforming over Tensor indexing (#98748)\r\n- Fix `torch.vmap` support for `torch.searchsorted` (#99698)\r\n- Fix `torch.vmap support for `Tensor.index_put` (#100516) \r\n- Fix UB in `functorch` infrastructure (#101568)\r\n- C++ `autograd.Function` now raises an error with `functorch` transforms instead of being silently incorrect (#103957)\r\n- Fix `nll_loss` batch rule with negative ignore_idx (#106118)\r\n\r\n## Distributed\r\n### Distributed (c10d)\r\n- Fix `kDefaultTimeout` multiple definition build failure in Gloo (#97270)\r\n- Delete lengths offset checks (#98368)\r\n- Drop the GIL when creating a `TCPSTore` to avoid deadlocks. (#100555) \r\n- Fix bug in `process_group_name` when there are duplicate PGs (#100518) \r\n- Fix subprocess group handlig in `scatter_object_list` (#100552)\r\n- Fix the check message of unsupported collectives ops. (#101775)\r\n- Fix `netName` assignment for NCCL Config (#105776)\r\n- Skip timeout in `FileStore` for Windows if the file path is invalid (#103247)\r\n### FullyShardedDataParallel\r\n- Use correct handle training state when prefetching (#98249)\r\n- Fix issue where `fully_shard` may determine compute device incorrectly (#98831)\r\n- Enable FSDP ``use_orig_params=True`` mixed precision training when some ranks have no (non-zero sized) parameter shards (#99175)\r\n- Fix `use_orig_params=True`, CPU offload, `no_sync()` (#100180)\r\n- Fix `device_id` when buffer-only module (#103504)\r\n- Fix `skip-sharded-views` + mixed precision (#105346)\r\n- Ignore buffer type casting in ignored modules (#106766) \r\n- Fix train -> EMA -> eval with mixed precision (#106858)\r\n- Unblock `ignored_states` + auto wrap (for now) (#104418)\r\n- Fix a memory leak in `optim_state_dict` (#96263) \r\n- Fix bug in determining whether parameters need to be materialized (#97488)\r\n- Fix typo when setting FSDP state dict config (#97110)\r\n- Fix osd rank0_only in fsdp (#99136)\r\n- Fix decision logic for `should_cast_forward_inputs in _root_pre_forward()` and `_pre_forward()` (#99546)\r\n- Fix `ignored_states` when they are passed as generators (#102575)\r\n- Fix for optim state dict (#102901)\r\n- Handle corner case of load with multi-backend PG for FSDP `state_dict` (#107172)\r\n### Distributed Tensor (Prototype Release)\r\n- Fix `DeviceMesh` logics in deciding which PG to use (#96861)\r\n- Remove non-generic asserts in `_get_or_create_default_group()` (#96961) \r\n- Fix the default PG condition for `DeviceMesh` (#97384)\r\n- Fix `DTensor` equal op (#99014) \r\n- Use Stride inferred from local tensor in `to_local` bwd (#102630)\r\n- Enable partial tensor add without redistribute (#105939)\r\n- Get rid of `dim_groups` attribute from `DeviceMesh` (#103105)\r\n- Fix `requires_grad` in `distribute_tensor` (#107606)\r\n- Fix `new_empty_strided` op\u2019s crash on the shard placement (#108600))\r\n- Fix `new_empty_strided` op (#108600)\r\n- Fix `requires_grad` callsite (#108358)\r\n\r\n## torch.compile\r\n\r\n### Dynamic Shapes\r\n\r\nA lot of dynamic-shapes bugfixes, too many to enumerate one-by-one. Some important points:\r\n\r\n- Heavy work our sympy-based symbolic reasoning system, including value ranges analysis for proving redundant constraints (#95174, #105877, #104968, #105138, #105139, #97963, #96121, #104557, #106644(https://github.com/pytorch/pytorch/pull/106644 ), #94944)\r\n- Improved symbolic tracing support for operators, including SymInt\u2019ified schemas and SymInt aware operator/backward implementations (#95543, #96100, #97362, #97675) Some specific operators: \r\n    - tile (#106933)\r\n    - randint/randperm (#98968) \r\n    - roll (#99114)\r\n    - fft ops (#99115)\r\n    - philox_rand (#99290)\r\n- Avoid overspecializing in situations where it is not necessary (#96008)\r\n- Don't attempt to use fake tensor fallback to real tensor if there are symbolic sizes (#97148)\r\n- Make `Tensor.__contains__` accept `SymInt/Float/Bool`. (#98933)\r\n- Change Dynamo to not duck-size unspecialized ints (#99010)\r\n- Improved mixed type handling for SymInts (#100008, #100328)\r\n- Support for annotating that SymInts have a constrained range (#103346)\r\n- Don't generate guards that refer to unbacked SymInts (#95732)\r\n- Automatically guard when SymInt is converted to int, instead of erroring (#95479)\r\n- Z3 based translation validation for debugging symbolic reasoning problems (#104827, #106643, #107523, #106645, #101307, #101607)\r\n- Improve constraint violation error reporting, including recommended constraints for export (#102729(https://github.com/pytorch/pytorch/pull/102729 ), #102198, #107470, #107790(https://github.com/pytorch/pytorch/pull/107790 ), #100745, #101636, #101815)\r\n- Improve logs for dynamic shapes using TORCH_LOGS=dynamic (#99277, #98941, #107439)\r\n- If we can't statically prove 32-bit indexing is OK, only add guard if hint exists (#106004)\r\n- Add expect_true for irrefutable guards, greatly improving overall support for error checking involving unbacked ints, and other unbacked symint improvements (#106720, #95216, #106788)\r\n- Support for torch.compile with FakeTensor that has SymInt sizes (#107662)\r\n\r\n### Other bug fixes\r\n\r\nIn addition, we have the following fixes broken down into roughly 4 parts:\r\n\r\n- Primtorch and decomposition bugfixes and improvements\r\n- FakeTensor and meta function bugfixes and improvements\r\n- AOTAutograd bugfixes and improvements\r\n- General \u201ccomposability\u201d bugfixes.\r\n\r\nThe first three cover a large number of general improvements to torch.compile, since torch.compile captures a graph internally by using these major components (fake tensor, prims and decomps, and AOTAutograd, see docs(https://pytorch.org/get-started/pytorch-2.0/)).\r\n\r\n### Primtorch and decompositions bugfixes\r\n\r\nThere were a large number of fixes to the primtorch and ref decompositions, which are used in torch.compile during graph capture. These all fixed quite a few bugs in torch.compile:\r\n\r\n- Sub.scalar decomp: fix primtorch handling for with alpha and float64 arg (#95421)\r\n- Embedding_backward_dense decomp: broadcasting fix (#95499)\r\n- Upsample_bilinear decomp fix (#101682)\r\n- Batch_norm decomp reduce computation when weight or bias is none (#104616)\r\n- _unsafe_index decomp (#106814)\r\n- `Hardshrink`: make decomp composite implicit (#107039)\r\n- normal op decomposition for specializations of the normal op (#106792)\r\n- matmul decomp: update to match eager (#105850)\r\n- prims.collapse: make it a real prim (#91748)\r\n- Diagonal, linalg.diagonal: add refs (#95774)\r\n- addmv decomp (#96264)\r\n- Minimum_value: fix schema (#97327)\r\n- squeeze.dims decomp (#97020)\r\n- cumprod decomp: add ref (#98670)\r\n- Prims.unbind: fix ref if given dimension size is 0 (#100122)\r\n- Aten.arange.default: decompose to to arange.start_step (#99739)\r\n- Philox_rand: add decomps (#100206)\r\n- Elu_backward: fix bad accuracy in decomp (#100284)\r\n- polar decomp: add ref (#100345)\r\n- Batch_norm decomp: fix decomp  when weight/bias is not flattened (#101059)\r\n- aten.fill.Tensor decomp: don\u2019t call .item() (#103880)\r\n- Torch.renorm: add decomp (#103858)\r\n- multi_margin_loss ops: add decomps (#104578)\r\n- aten.logspace decomp: bugfix (#105201)\r\n- multilabel_margin_loss_forward op: add decomps (#105302)\r\n- Torch.{stft,istft} decomps: add ref (#106400)\r\n- Aten.rrelu_with_noise decomp: add ref (#106812)\r\n- Misc fixes:\r\n    - better error message when functionalization cant handle op (#95392)\r\n    - Simplify some decompositions. (#107038)\r\n    - Make the glue compute short circuit only if possible (#94437)\r\n    - Don't use PrimTorch decomposition for empty (#94512)\r\n    - Remove unnecessary TensorMeta rewrap (#95004)\r\n\r\n### FakeTensor and Meta function fixes\r\nFake Tensors and meta functions are used internally to perform \u201cshape inference\u201d during graph capture when running torch.compile. In particular: when we capture a graph of pytorch operators, we\u2019d like detailed information on the shapes of intermediate and output tensors in the graph. There were a large number of bugfixes and improvements to these two subsystems over the last release.\r\n\r\nOperator bugfixes:\r\n\r\n-  _slice_meta (#98326)\r\n- embedding bag (#105924)\r\n- histogramdd (#100624)\r\n- sort (#96719)\r\n- mm (#97533)\r\n- torch.*_like operators (#98160)\r\n- fused_adam (#99436)\r\n- sdpa_backward (#101128)\r\n- aten.bucketize (#104396)\r\n- torch.full (#104451)\r\n- Multi_margin_loss_shape_check (#104851)\r\n\r\nIncreased operator coverage:\r\n\r\n- randperm.default (#99593)\r\n- aten.searchsorted.Tensor (#101637)\r\n- foreach ops (#102225)\r\n- foreach_mul_ (#105107)\r\n- foreach_maximum_.List (#105864)\r\n- aten.min.dim (#96442)\r\n- baddbmm (#96548)\r\n- take (#98451)\r\n- cummax and cummin (#98552)\r\n- logcumsumexp (#98683)\r\n- linalg_qr (#100714)\r\n- solve_triangular (#100829)\r\n- _linalg_eigh (#100964)\r\n- linalg_householder_product (#101315)\r\n- linalg_ldl_factor_ex (#101362)\r\n- linalg_ldl_solve (#101367)\r\n- linalg_lu (#101372)\r\n- linalg_lu_factor_ex (#101375)\r\n- linalg_lu_solve (#101836)\r\n- _linalg_slogdet (#102464)\r\n- lu_unpack (#102937)\r\n- _linalg_solve_ex (#102454)\r\n- linalg_matrix_exp (#102945)\r\n- avg_pool3d and avg_pool3d_backward (#103392)\r\n- rshift and lshift (#103637)\r\n- pad ops (#103815)\r\n- _pdist_forward and _pdist_backward (#103817)\r\n- max_pool ops (#103951)\r\n- adaptive_max_pool ops (#104167)\r\n- multi_margin_loss ops (#104236)\r\n- multilabel_margin_loss ops (#104388)\r\n- _foreach_div_.Scalar, sqrt_.default (#104779)\r\n- `multi_margin_loss`: check weight shape, make contiguous on CPU, add tests (#104852)\r\n- _cholesky_solve_helper and cholesky_solve (#105867)\r\n- _adaptive_avg_pool3d_backward (#105816)\r\n- `argsort.stable` (#106025)\r\n- cholesky (#106115)\r\n- cholesky_inverse (#106120)\r\n- ormqr (#106278\r\n- mode ops (#106273)\r\n- `searchsorted.Scalar` (#106283)\r\n- grid_sampler_3d ops (#106261)\r\n- _cdist_backward (#106680)\r\n- polygamma (#106681)\r\n- upsample_nearest (#106675)\r\n- triangular_solve (#106682)\r\n- special ops (#106683)\r\n\r\n\r\nOther:\r\n\r\n- Support resize on meta storage (#101988)\r\n- meta_tensor] polish error strings in meta registrations ([#95052)\r\n- meta] error checking for inplace ops ([#101532)\r\n- Implement size checking for copy_ with meta tensors (#107779)\r\n- Use safe_is_leaf to test leafness (#102706)\r\n- FakeTensor] Workaround FFT ops with incorrect meta strides ([#106319)\r\n- Better complex support (#98869)\r\n- pt2] remove unused meta_linalg_eigh ([#100965)\r\n- pt2] convert out params in register_meta ([#101344)\r\n- Add missing decompositons/lowerings for logical/bitwise operators (#102566)\r\n- pt2] bug fix: invert condition in checkFloatingOrComplex ([#102944)\r\n- err on dot product for tensors of different sizes (#106572)\r\n\r\n\r\n### AOTAutograd bugfixes\r\n\r\nAOTAutograd is a major component of the torch.compile stack, and received many bugfixes and improvements over the last release.\r\n\r\n- AOTAutograd: fix 'Trying to backward through the graph a second time' error (#98960)\r\n- Handle tracing foreach ops in ProxyTorchDispatchMode. (#99724)\r\n- functionalization: error during mutations on mem overlap (#99919)\r\n- Functionalization of torch.rand/rand_like ops (#97377)\r\n- fix inference mode / PyDispatcher / Functionalize interaction (#103275)\r\n- Refactor (#95991, #96235)\r\n- Dynamic shapes improvements through AOTAutograd (#95975, #96219, #96300, #96653)\r\n- aot_autograd: dont requires_grad on tangents (#96339)\r\n- aot autograd] avoid cloning some inputs unnecessarily when they dont require grad ([#96342)\r\n- aot] disable inference view tracking ([#96478)\r\n- aot autograd: consolidate metadata (#96340)\r\n- Add missing aot_autograd_arg_pos_to_source (#97487)\r\n- Disable logging in pattern matcher calls to AotAutograd (#98936)\r\n- aot_autograd: factor out runtime epilogue from aot_dispatch_base (#100586)\r\n- Disallow module forward input mutation in aot_export (#101834)\r\n- aot_autograd][functional_rng] Change calling convention ([#102344)\r\n- AOTAutograd] add export entrypoints ([#100587)\r\n- aotautograd: fix mutation bug when input is noncontiguous (#102767)\r\n- AOTAutograd] perform comparisons with stride hints ([#103342)\r\n- AOTAutograd] make _unsafe_view() logic happen during the runtime epilogue ([#103919)\r\n- Read out real strides from compilation result, rather than real args (#105010)\r\n- AOTAutograd: correctness fix when tracing custom autograd functions that alias inputs (#102992)\r\n- Add sequence_nr to aot_autograd to map forward ops to their corresponding backward ops (#103129)\r\n- AOTAutograd: allow input mutations on inputs that are non-contiguous (#106460)\r\n- Add some support for detecting false aliasing in AOTAutograd (#106461)\r\n- Add complex dtypes to partitioner (#96297)\r\n\r\n## Sparse\r\n- Fix an unexpected assertion error when nesting check_sparse_tensor_invariants context managers (#95372)\r\n- Fix silent nnz overflow for very large sparse compressed tensors. (#102523)\r\n- Fix CSR/BSR invariant validation on 0 sized batched inputs (#101180)\r\n- Fix zeros_like CSR and BSR tensors with batch dimensions. (#101215)\r\n- Fix autograd issue with identity conversions (#92022)\r\n- Set outputs of col_/crow_/ccol_/row_indices methods as non-differentiable. (#107447)\r\n- Fix silent index downcast from int64 for int32 for add/add_ on CSR/BSR (#95294)\r\n- Fix add/add_ device checks for CSR/BSR (#97520)\r\n- Fix incorrect sparse_dim in COO.zero_() and in binary operations with zero-sized COO operands (#98292)\r\n- Fix sparse.mm derivatives for non-contiguous inputs on CPU (#106127)\r\n\r\n## Linear Algebra\r\n- `baddbmm`: Fix when out has `nan` value for `beta=0` (#96086)\r\n- Add same dtype checks for {`tensordot`, `addmm(cpu)` (even when input has zero `numel`), `baddbmm`}. (#98938, #100274, #102659)\r\n\r\n## Profiler\r\n- Hand-bound `CapturedTraceback` (#107438)\r\n- Fix crash by initializing `kineto_activity` for each event for on-demand profiling (#97550)\r\n- Fix CUPTI lazy re-initialization and CUDA Graphs crash in CUDA 11 with workaround (#101879)\r\n- Fix CUDA IMA for CUPTI and CUDA 12 by disabling CUPTI lazy re-initialization (#107744)\r\n- Fix profiling PT2 w/ dynamic shapes & `record_shapes` (#104320)\r\n- Fix profiling shapes with PT2 + lists of dynamic shapes (#105893)\r\n- Fix an issue where running Kineto daemon and dynolog in docker fails and UUID generation for IPC fabric (#95535)\r\n- Fix static order deinit with `LoggerCollector` (#101952)\r\n- Fix issues in `tb_plugin` regarding Distributed View and NCCL events (#103031)\r\n- Fix `test_profiler_tree` for HIP and enabled individual activity types for RocTracer (#106293)\r\n- Fix flaky `test_memory_timeline_no_id` in `test_memory_profiler.py` (#103441)\r\n\r\n## Quantization\r\n- Fixing quantized `prelu` workflow (#103455)\r\n- Fix issue of lowering weighted functional ops with kwargs (#95865)\r\n- Return `zero_point` from `determine_qparams` as a int64 (#98746)\r\n- Fix errors in `QuantizeAvx512` (#104400)\r\n\r\n## CUDA\r\n- Add broadcastable check to `index_put` (#94849)\r\n- Fix uniform returning end point for `BFloat16` and `Half` (#96962)\r\n- Fix \"Cannot assign index like `x[[1,2], :] = 2` when torch.use_deterministic_algorithms(True)\" (#105833)\r\n- Fixing a bug where allocating a 4GB block results in using 8GB of memory (#95827)\r\n- Take `CUDA_VISIBLE_DEVICES` into account for nvml calls (#94568)\r\n\r\n## Intel\r\n- Avoid FPE when running batch norm with zero batch size. (#95324)\r\n- Fix CPU bitwise shifts for out-of-limit shift values (#96659)\r\n- Use unordered NEQ comparison for `vec512` `operator!=` implementations (#97466)\r\n- Fix `masked_scatter_:` non-contiguous self (#100232)\r\n\r\n## MPS\r\n- Introduce xfail (#95045)\r\n- Error on unsupported types (#95982)\r\n- Add type promotion to `torch.addcmul` (#96164)\r\n- Add `random_` overload (#98333)\r\n- Fix `layer_norm_backward_mps` key (#100295)\r\n- Make grid_sampler_2d available (#101108)\r\n- Fix `bernoulli` for int types (#100946)\r\n- Enable `arange` for `int8` and `uint8` dtypes (#101303)\r\n- Handle deserialization more permissively (#98834)\r\n- Fix mps unary op issue on non densely stored tensors (#105512)\r\n- Fix `torch.std` for negative dimentions (#107754)\r\n- Remove mps specialized path in BCE backward (#95220)\r\n- Fix type casting copy with storage offset (#95573)\r\n- Fix views with 3 or more sliced dimensions (#95762)\r\n- Fix bidirectional LSTM & small one-direction LSTM fix (#95563)\r\n- Fix in-place add and sub with alpha == 0.0 (#96184)\r\n- Fix flip where no dims need to be flipped (#96605)\r\n- Fix LSTM grad_y (#96601)\r\n- Fix the failure with `ReplicatePad3D` (#96988)\r\n- Fix `torch.eye` unsupported bool constant on macOS 12 (#97027)\r\n- Add linear inputs check (#99228)\r\n- Fix gelu exceptions not raised for error inputs (#99237)\r\n- Fix max_pool2d exceptions not raised for error inputs (#99238)\r\n- Fix trace exceptions not raised for error inputs (#99239)\r\n- Add dot input check (#100099)\r\n- Fix index_put with deterministic algorithm enabled (#97660)\r\n- Fix embedding cache key (#101857)\r\n- Fix `softplus` with `f16` input (#101948)\r\n- Fix incorrect distribution of `randperm` with device mps (#104171)\r\n- Fix `argmax` and `argmin` clamp value on MPS (#104374)\r\n- Make `torch.empty*` deterministic by filling with NaN or max int (#104995)\r\n- Correct empty tensor mps all operation (#105218)\r\n- Fix upsample output size tensor (incorrect result in MacOS 14.0) (#105677)\r\n- Fix MPS clamp issue with different dtypes between input and min/max tensors (#105747)\r\n- Fix `copy_ broadcast` (#105617)\r\n- Fix `clamp` with strided outputs/inputs (#97858)\r\n- Restride output strides to contiguous format for inverse op (#102122)\r\n- Remove casts from reduction/cumsum/sort ops starting with macOS 13.3 (#95817)\r\n- Fix `.item()` for multi-dim scalar (#107913, #108410)\r\n\r\n## Vulkan\r\n- Ensure dim is `size_t` (#104201)\r\n- Fix divide-by-zero with padded tensors (#97698)\r\n- Ensure non-zero divisors in Vulkan API Tests [#100909, #100910]\r\n- Fix concat op in feature dimension (#101721)\r\n- Fix bug of `aten::cat` for concatenation of 3D tensors at channel dim with channels as multiple of 4 (#103718)\r\n- Fix the position computation with the consideration of channel padding (#103908)\r\n- Fix quantized cpu to vulkan broken by padding (#97372)\r\n- Fix broadcasting in quantized elementwise ops (#97554)\r\n- Fix lint for `at::softmax` 1,2,3 dimension tensors (#105082)\r\n- Fix static analysis errors in `vulkan_quantized_api_test.cpp` (#97400)\r\n- Reuse broadcast checks instead of `check_inputs` (#105960)\r\n- Fix global and local sizes for `image->bool` copy (#106752)\r\n\r\n## Build\r\n- `USE_FAST_NVCC` Windows (#95206)\r\n- Enable CuDNN v8 frontend in RL (#102284)\r\n\r\n## ONNX\r\n### TorchScript ONNX exporter\r\n- Fixes for operators:\r\n  - Add `cast` operator after `reduce` to match desired dtype (#100700)\r\n  - Simplify `repeat_intereleave` export for scalar-valued `repeat` (#100575)\r\n  - Fix wrong type when exporting `{zeros, ones, full, empty, rand, randn}_like` ops to onnx (#103048)\r\n  - Fix `output_padding` for quantized `tconv` (#104207)\r\n  - Refactor `AvgPool` to support dynamic shapes (#105683)\r\n  - Fix `expand_as` (#95962)\r\n  - Add new `aten::device` variant to TorchScript (#97023)\r\n  - Export dynamic step size for `aten::slice` (#104385)\r\n  - `STFT` Support (#92087)\r\n  - Fix `aten::flatten` conversion with 0d input to onnx `Reshape` and 1d to `Identity` (#104089)\r\n  - Fix output shape mismatch issue of `max_pool` (#106270)\r\n  - Add quantization support to `reshape` and `size` for the ONNX exporter (#106629)\r\n  - Return input itself for non-fp inputs and support decimals for `aten::round` op (#107920)\r\n- Apply `peephole` for eval mode when constant folding is enabled only (#95801)\r\n- Detect `None` constant during jit scalar type analysis (#101608)\r\n- Fix onnx `Gather` constant folding (#101329)\r\n- Fix third-party custom operator support in torchscript exporter (#104785)\r\n- Fix memory leak when exporting models (#107244)\r\n- Fix typo `scipt` -> `script` (#97850)\r\n- Fix circular padding to support dynamic axes (#95647)\r\n- Perform Shape inference on added `Cast` node (#106093)\r\n- Cap opset version at 17 for `torch.onnx.export` (#107829)\r\n- Make `torch/csrc/jit/passes/onnx/unpack_quantized_weights.cpp` data_ptr-correct (#100681)\r\n### TorchDynamo ONNX exporter\r\n  - Fixes for operators:\r\n    - Fix scalar elements in `op.Concat` (#98509)\r\n    - Fix `aten::cat` export when arg include parameters (#105373)\r\n  - Remove duplicated code from previous rebase (#99072)\r\n  - Cover undiscoverable ops `torch.ops.aten` (#99682)\r\n  - Fix type annotation for `fx_to_onnxscript` (#100050)\r\n  - Set `tracing_mode` through `options.dynamic_shapes` and enable dynamic tests in test_fx_to_onnx_runtime.py (#100212)\r\n  - Add `RemoveConstantInputStep` to adapt torch inputs to ONNX inputs (#100252)\r\n  - Fix exported onnx initializer name (#104741)\r\n  - Fix `UnsupportedFxNodesAnalysis` after onnx dispatcher changes (#105156)\r\n  - Support `torch.device` in FX exporter (#105757)\r\n  - Fix passes to reuse existing fake mode if possible (#105764)\r\n  - Exclude `FXSymbolicTracer` from `_assert_fake_tensor_mode` (#107712)\r\n  - Fix initializer naming at `torch.onnx.ExportOutput.save_model_with_external_data` (#105002)\r\n  - Apply `options.dynamic_shapes` to dynamo API usage in fx exporter (#99199)\r\n\r\n## torch.fx\r\n- Fix `split_module` bug with unused keys (#95493)\r\n- Fix tabulate import error (#104468(https://github.com/pytorch/pytorch/pull/104468 ))\r\n- Fix issue with SubgraphMatcher when ignoring literals  (#98458)\r\n- Update InternalMatch in `subgraph_rewriter` after repeated replacements (#99039)\r\n- Fix conv+bn folding issue for mixed dtype (#99696)\r\n- Fix submodules/parameters/buffers preservation when unpickling graph module (#104115)\r\n- Prevent interpreter from altering original node\u2019s meta (#105880)\r\n- Fix split module\u2019s interaction with dead code (#104554)\r\n- Fix split copying over `node.meta` (#107248)\r\n- Fix repr when arg is an `OpOverload` (#102547)\r\n\r\n## Dynamo\r\n### Misc TorchDynamo fixes\r\n- Correctly use PythonPrinter for generating wrapper code referencing SymPy (#96710)\r\n- Fail fast when dynamo attempts to add unspecialized `int`/`float` as additional graph inputs (#96786)\r\n- Simplify module_key creation logic (#94945)\r\n- Generalize summary script to work with more CSV names (#98500)\r\n- Add support for nonzero, some improvements to reduce guards (#95387)\r\n- Update Dynamo.export to preserve names of args & kwargs (#95851)\r\n- Slight cleanup of VariableBuilder giant if condition (#95471)\r\n- Add guards for deterministic algos (#96695)\r\n- Add signpost_event to dynamic_shapes (#103882)\r\n- Add some missing disabled functions (#103662)\r\n- Add support for dictionary with torch object keys. (#103158)\r\n- Add timeout for translation validation instances. (#104654)\r\n- Add Wav2Vec2 HuggingFace support (#103009)\r\n- Add dyanmo backend based on ONNXRuntime (#106589)\r\n- Allow for torch.sym_int to return int while tracing (#104837)\r\n- Allow NumPy code in torch.compile to run on cuda (#104699)\r\n- Avoid cond prefix when naming subgraph of HigherOrderOperators (#101439)\r\n- Avoid graph break on repeat_interleave.self_int (#99528)\r\n- Change dimension constraint summary to log.info (#101584)\r\n- Debug shape guards (#95848)\r\n- Disable dynamo on some opt methods and differentiable optimizer tests (#103066)\r\n- Disable fused adam op compile (#105256)\r\n- Don't apply automatic_dynamic_shapes if we force tensor to be static (#103673)\r\n- Don't specialize torch.Size with specialize_int = False (#96419)\r\n- Dynamo size dim kwargs (#97450)\r\n- Dynamo stride dim kwargs (#97444)\r\n- Enable fused foreach Adam compilation (#104121)\r\n- Enable torch._C._get_privateuse1_backend_name in Dynamo tracing  (#103141)\r\n- Ensure optimizer state references are cleared (#100282)\r\n- Equality assertions (#102256)\r\n- Explicitly fall back to eager with GraphModule with no output for onnx&tvm backends (#99805)\r\n- Extend assert statement to include ListVariable (#100841)\r\n- Fix disable_saved_tensors_hooks - graph break (#106875)\r\n- Fix for tuple construction from tuple iterators (#97862)\r\n- Fix graph break on boolean mask better (#103052)\r\n- Fix incorrectly getting the name of OrderedDict's index in dynamo (#96940)\r\n- Fix isinstance on SymInt in dynamo (#99393)\r\n- Fix lineinfo generation on PY3.11+ (#103525)\r\n- fix module buffers call (#102251)\r\n- Fix number of inputs in onnxrt and tvm backend (#95429)\r\n- Fix optimizer cuda health check graph break (can be done in the compiler) (#102765)\r\n- Fix optimizer grad mode state interaction with dynamo (#103952)\r\n- Fix OrderedDict reconstruction bytecode (#95800)\r\n- Fix the compatible issue of the Dynamo and the PyDev.Debugger. (#96721)\r\n- Fix torch.compile issue with torch.tensor (#96299)\r\n- fix torch.distributions lazy_attribute failure (#103208)\r\n- Fix usages of contextmanager without finally (#96170)\r\n- Flatten exceptions in dynamo (#100779)\r\n- Full default dict support in dynamo (#102202)\r\n- Generate type match guard for torch.Size input (#96421)\r\n- Graph break on differentiable boolean mask setitem (#102843)\r\n- Graph break on operators that fake tensor doesn't support (#97708)\r\n- Guard on default device (#99551)\r\n- Handle calls to typing.cast (#104799)\r\n- Handle dim in size kwargs (#96992]) ([#97098)\r\n- Initialize optimizer in dynamo to avoid graph break and tracing slowness (#102640)\r\n- Keep submodule's name for nn.Sequential when unrolling (#94913)\r\n- Make _CURRENT_TRACING_CONTEXT thread local (#105942)\r\n- Make int unspecialization actually work (#95621)\r\n- Make openxla and opexla_eval backend show up in list_backends (#107905)\r\n- Make Openxla dynamo backend take boxed input (#107260)\r\n- Manually generate guards for optimizer (#103121)\r\n- Node.stack_trace should have innermost frame last (#95592)\r\n- Normalize builtin types to dtypes (#106074)\r\n- Add a flag that allows breaking on NumPy ops (#107687)\r\n- Fix `ndarray.__pow__ `(#107746)\r\n- Return `NotImplemented` for `np.sort(complex)`` (#107710)\r\n- Support `linalg`, `random` and `fft` module (#105320)\r\n- `torch._numpy`:  remove noops and half-implemented nan-functions (#107596)\r\n- Wrap ndarray dunder methods (#107689)\r\n- Pass `torch.compile` mode/options to all backends (#99645)\r\n- Update pre_dispatch tracing: support autocast and no_grad/enable_grad ctx managers, add a pre_dispatch_eager dynamo backend (#103024)\r\n- Preserve CreationMeta when metafying views (#103152)\r\n- Preserve mark_dynamic when cloning inputs (#99617)\r\n- Prevent GraphArg from keeping real tensors live (#100515)\r\n- Propagate mark_dynamic in dynamo compiled outputs (#99634)\r\n- Properly avoid wrapping numbers as tensors before backend (#96193)\r\n- Properly parenthesize dynamo_dynamic_indices test (#99823)\r\n- Properly respect automatic dynamic config for unspec int (#103321)\r\n- Raise warning if user has hooks installed on the module (#94848)\r\n- Record caller frame instead of function frame (#96882)\r\n- Resolve InlinedClosureVariable in InstructionTranslator stack (#106491)\r\n- Rewrite size/stride/numel TensorVariable handling (#103438)\r\n- Simulate torch function enablement state (#105091)\r\n- Simulate tracing tree_map_only  (#104815)\r\n- Simulate treespec flattening/unflattening (#101896)\r\n- Skip if curr_size is None (#101170)\r\n- Support CUDA stream passed from outside of torch.compile decorator (#94627)\r\n- Support getattr for ConstantVariable when compiling with Dynamo (#98153)\r\n- Support module dict iter (#99503)\r\n- Support threading.local getattr (#104292)\r\n- Support unary not on lists (#102210)\r\n- Support wrapping + returning tensor subclasses (#104802)\r\n- Trace through Tensor slots (#107159)\r\n- Turn on add_runtime_assertion by default (#102671)\r\n- Tweak dynamic=False behavior (#105715)\r\n- Update XLA dynamo backend name (#106489)\r\n- Update `exir.pass_base` to use export.pass_base (#106647)\r\n### Misc dynamic shapes fixes\r\n- Add API to mark input tensors static for cudagraphs (#107154)\r\n- Add invariant that all symbolic shapes must be bound in graph (#99089)\r\n- Add support for Inductor + symbolic shapes + training (#93059)\r\n- Add symbolic tracing support to `torch._dynamo.export` (fake input + weights) (#100017)\r\n- Add unbacked symbol support (#98877)\r\n- Always create ShapeEnv, always apply unspec logic (#103302)\r\n- Do not mutate SymNode expressions. (#107492)\r\n- Do not track parameters, do not generate guards (#98350)\r\n- Add dynamic range constraint API (#98779)\r\n- Enable dynamic shapes of torch.nn.Parameter (#105855)\r\n- Further improve symbolic shapes logging (#99159)\r\n- Group constraints by arg (#102096)\r\n- Guard static shapes alongside tensors, instead of from shape_env, in dynamic_shapes=True (#99566)\r\n- Make hash_storage work with size 0/1 storage (#100467)\r\n- Make unspecified ints to range over negative and positive. (#104658)\r\n- Propagate dynamic int on `__setitem__` (#105923)\r\n- Remove redundant `dynamic_dim` (#107815)\r\n- Support bit shifting `SymInt`s (#104318)\r\n- Switch dynamic_shapes to True by default (#103597)\r\n- Warn if guards are added to ShapeEnv after we produced guards  (#97820)\r\n- Don't specialize when indexing by SymInt (#99123)\r\n- Fix specialization when you pass an unspec int into slicing on a Python list. (#104142)\r\n- Flag guard unbacked SymInt/SymFloat support (#94987)\r\n### Benchmark related bug fixes\r\n- Add a flag to benchmarks script to keep the test report directory (#96398)\r\n- Fix amp in inference in benchmarking suite (#103220)\r\n- Handle new inference csv from CI for benchmarking (#98294)\r\n- Small operatorbench improvements (#103110)\r\n### Export related bug fixes\r\n- Add `aot_export` (#101490)\r\n- Add get buffer from exported program (#107809)\r\n- Add support for edge dialect ops in exir/serde (#106371)\r\n- Change `torch._dynamo.export(aten_graph=...)` to allow `pre_autograd` tracing (#98031)\r\n- Error on closed over variables (#99367)\r\n- Enable dynamo export to export identity function (#94962)\r\n- Error when constraining on static values (#101655)\r\n- ExportedProgram (#102259)\r\n- Fix soundness bug with unsupported constraints (#102897)\r\n- Fix specify_constraints signature for exporting module (#101831)\r\n- Improve error message for IO mismatch (#107907)\r\n- Make serializer more composable (#104816)\r\n- Persist `torch.assert` in aten graph (#100101)\r\n- Preserve `meta\"val\"]`` on export ([#95314)\r\n- Raise error on 3.11 dynamo export (#95088)\r\n- Refactor and add same_signature flag to dynamo.export (#106569)\r\n- Refactor dynamic dims api, stateless internals, higher level export API (#96699)\r\n- Remove eliminate_dead_code (#105875)\r\n- Remove fake_mode arg from torch._dynamo.export API (#106345)\r\n- Remove setter for graph_module (#106651)\r\n- Suggest constraints to specify for export based on generated shape guards (#98463)\r\n- Support list output for HigherOrderOperators (#101986)\r\n- Support runtime assertion for inline constraints (#100763)\r\n- Integrate `torch.ops.call_delegate` into the delegate workflow (#92562)\r\n- Wrap more constraint violation cases to UserError (#100897)\r\n### Logger bug fixes\r\n- Rename sym_shapes logger to dynamic (#99335)\r\n- Raise a NameError when accessing non-existent variable (#96418)\r\n- Convert logging f-strings to use % format, part five (#98765)\r\n- Enable passing a dict of module names: log level to set_logs python api (#98989)\r\n- Expose function to retrieve list of registered loggers (#100776)\r\n- Remove unnecessary check when logging artifacts (#99260)\r\n- Revamp guard debug logging (#107505)\r\n- Add assert + test for artifact log booleans (#104907)\r\n- Add fast traceback utilities (#107358)\r\n- Add graph break logging option instead of config flag (#103202)\r\n- Add verbose_guards logging artifact (#107388)\r\n- Do not use unicode quotes (#99446)\r\n- Elevate cudagraphs failure to warning, added lineno to recompiles (#105081)\r\n- Generate error on bad input to equality constraint (#107311)\r\n- Fix outdated log settings in doc (#102285]) ([#102286)\r\n- Make DimConstraints create actionable message (#100103)\r\n- Make sure log tests are run in non-verbose mode (#106496)\r\n- Report guard failures with recompiles logging (#105500)\r\n- Update error message with torch logging instructions (#102892)\r\n- Fix typo in settings regex logging (#97245)\r\n- Improve TORCH_LOGS settings error msg (#97264)\r\n### Minifier related bug fixes\r\n- Add `--check-str` support to after_aot minifier (#104758)\r\n- Teach `requires_bwd_pass` how to interpret int (#98312)\r\n- Add `--offload-to-disk` support to minifier (#100546)\r\n- Improve minifier printing to be more chatty when it makes sense (#100486)\r\n- Make `run_fwd_maybe_bwd` work with int inputs (#99365)\r\n- Misc accuracy improvements on minifier (#100447)\r\n- Print AOT Autograd graph name when accuracy failed (#99366)\r\n- Relax after_aot restriction on no buffers, serialize small constants (#100472)\r\n- Cast copied model rather than update the original model (#101901)\r\n\r\n## Inductor\r\n- Skip triton configs for `mm_plus_mm` that may crash triton ( #96385)\r\n- Avoid fusion with indirect indexing (#96273)\r\n- Use 64-bit indexing for large tensors in triton codegen (#97447)\r\n- Make `aten.constant_pad_nd` always do a copy even when padding is 0 to have consistent behavior (#100082)\r\n- Make argmin/max handle duplicate values in a way consistent with eager  ( #99920)\r\n- Handle negative padding in reflect_pad_backward. ( #100923)\r\n- Make `torch.sign` return the same type as input (#101346)\r\n- Only reuse aliased buffers if there are no more users ( #100332)\r\n- Fix a number of issues with divs in `ValueRangeAnalysis` (#100547)\r\n- Prevent pattern matches across mutation ops in inductor pre-grad FX passes (#101144)\r\n- Avoid caching stale `inner_fn_str/ReadWrites` objects (#106502)\r\n- Correctly infer dtype of `full` (#95593)\r\n- Avoid zero division error for `dropout` (#100222)\r\n- Fix multi output layout error in indexing dtype calculation (#108085)\r\n- Bug fixes for the CPU backend\r\n  - Fix compilation issues on pre clang-10 (#103347)\r\n  - Fix compiler error when trying to vectorize `logit_and` and `logit_or`  (#95361)\r\n  - Properly handle 3D tensor for `Conv2d` ( #99601)\r\n  - Fix reduction crash caused by storing `float` value to `bfloat16` (#102719)\r\n  - Properly hande broadcasting for `bfloat16` (#104319)\r\n  - Fix compilation for TIMM `mobilevit_s` model (#100230)\r\n- Bug fixes for the AMD backend\r\n  - Triton wheel support enabled in non-ROCm environments (#95142)\r\n  - Conditionalise triton mm/conv configs on ROCm to mitigate crashes (#107584)\r\n- Dynamic shape related bug fixes\r\n  - Disable persistent reductions with dynamic shapes since persistent reduction relies on tensor shapes (#98405)\r\n  - Turn off `divisible_by_16` for dynamic shapes (#98471)\r\n  - Make `philox_rand_like` work with dynamic shapes (#95461)\r\n  - Handle `int`/`float` arguments for cpp codegen in inductor (#95533)\r\n\r\n## JIT\r\n- Mark `torch.cuda._exchange_device` op as having side effects (#96364)\r\n- Fix `jit.trace` codegen for out-variants on ops with more than one output (#101563)\r\n- Make `NNC` compatible with LLVM 15-17 (#96762), #98811), #101396), #103824)\r\n- Fix errors found by fuzzing and sanitizers (#94815), #101400), #102156), #103667), #103969), #106041), #103327), #94300)\r\n- Fix handling of >32-bit scalars on 32-bit platforms in `NNC` (#97669)\r\n- Fixes for `NNC`\u2019s variable handling and serialization on big-endian systems (#96951), #95881), #104249)\r\n- Ignore meta-device tensors instead of erroring when loading a model with a target device (#100495)\r\n- Add overloads for `_unsafe_index_put`, `_unsafe_index`  (#104127)\r\n- Match eager result from `torch.round` in `NNC` codegen (#104430)\r\n- Fix lifetime of `JITException` binding (#106401)\r\n- Fix annotation handling for subclasses in python >= 3.10 (#104485)\r\n\r\n## Misc\r\n- Stride bugfix: add overflow check for stride calculation (#94900)\r\n- Set `SavedVariable.is_output` to `true` for `grad_fn->result_` (#105504)\r\n- Handle tail 0-size tensor appropriately in `MultiTensorApply` (#100811)\r\n- Fix `UntypedStorage` pin error (#104355)\r\n- Fix `validate_input_col` for `nn.Module` or `Callable` (#96213)\r\n- Fix segmentation fault in flatbuffers when parsing malformed modules (#95221)\r\n- Fix TorchScript support in `as_nested_tensor` (#97960)\r\n- Reintroduce s390x SIMD support (#99057)\r\n\r\n# Performance\r\n\r\n## General\r\n- Avoid copies in matmul (#76828)\r\n- Improve the precision of `abs()` and `sign()` for large values (#99550)\r\n- Fuse ops in eager cosine_similarity while keeping the stability and the gradients (#104771)\r\n- Add scalar conversion using avx instructions for half (#102140)\r\n- enable `Half` for cat serial kernel (#96021)\r\n- Re-enable AVX512 ATen kernels for compute-intensive ops (#104165)\r\n\r\n## torch.optim\r\n- Minimize the use of intermediates across all foreach optimizers (Adam, AdamW, RAdam, NAdam, Adadelta, Adagrad, Adamax, Rprop, ASGD, RMSprop, SGD) to decrease peak memory (#104780, #104898, #104904, #104910, #104983, #104988, #104991, #105193, #105146, #105161, #105599)\r\n- Only make a shallow copy when loading optimizer state_dict (#106082)\r\n- FusedAdam/FusedAdamW accepts lr: Tensor without h2ds (#106916)\r\n- Use plain power operator in Adam/AdamW when capturing (#104254)\r\n- Optimize EMA implementation (#94820)\r\n\r\n## torch.nn\r\n- Optimize reflection padding performance on CPU (#102254)\r\n- Optimize replication padding performance on CPU (#102255)\r\n- Improve precision and performance for BFloat16 upsampling (#91169)\r\n\r\n## Sparse\r\nImproved performance in the following:\r\n\r\n- `mul(COO, COO)` (#96094, #94596\r\n)\r\n- `COO.coalesce` (#94401, #96765)\r\n- `COO.sparse_mask(COO)` (#96094, #94406, #94596)\r\n- `mm(dense, BSR)`, `addmm(dense, BSR)`, linear with BSR weights (#94825, #94823, #96648, #100634, #100543, #100876, #100882, #98403, #104062)\r\n- `sparse.sum` backward (#98838, #94991)\r\n- relu/threshold backward for COO (#98935)\r\n- `add/add_` for CSR/BSR (#95293)\r\n\r\n## torch.compile\r\n- Implement CSE for guards (#98488)\r\n\r\n## Distributed\r\n### Distributed (c10d)\r\n- Enable `store_barrier` only on the ranks that are part of the process group and not the whole world to make it scalable in PG initiation. (#99931)\r\n\r\n### Distributed Tensor (Prototype Release)\r\n- Improve perf to reduce `DTensor` CPU overhead (#106524, #107181, #107305)\r\n\r\n### FullyShardedDataParallel:\r\n- Speed up first iter order check (#96146, #96220) \r\n- Reduce CPU overhead in FSDP (#96958)\r\n\r\n## CUDA\r\n- Speed up bincount and histc on CUDA (#97090)\r\n- Speed up indexing_backward_kernel with duplicates (#100505)\r\n- Speed up torch.cat on contiguous tensors with wide loads (#102815)\r\n- Speed up LossCTC (#97269)\r\n- Speed up prefix scan algorithm (#103314, #103435), #103502)\r\n- Speed up vectorized_layer_norm (#107287)\r\n\r\n## Intel\r\n- Improve mkldnn matmul performance when one input is contiguous tensor but the strides are not default contiguous strides (#99511)\r\n\r\n# MPS\r\n- Implement NEON accelerated implementation of ERF() (#105610)\r\n- Add encoder coalescing support for native kernels (#99810)\r\n- Add PipelineStateObject caching for advanced indexing kernels  (#99855)\r\n- Squeeze last dimensions, if possible, for 5D (or bigger) reductions to map them to optimal 4D implementation (#99856)\r\n\r\n## Vulkan\r\n- Pad channels when using texture storage instead of \"tight packing\" (#95251)\r\n- Introduce GPU Memory Layout qualifier allow more efficient memory layouts when storing Tensors (#106978)\r\n\r\n## ONNX\r\n- Improve diagnostics performance (#99936, #96348)\r\n- Don't duplicate model weights in ONNX export (#101134)\r\n- Reduce exporter memory usage by removing intermediate values (#101148)\r\n- TorchScript ONNX exporter:\r\n  - `aten::relu6`: avoid unncessary Relu operation (#99022)\r\n\r\n## Inductor\r\n- Match decomposed attention patterns and replace them with eager implementation. This improves perf since eager implementation may use flash attention which do comprehensive fusions. ( #97741, #100609, #107578)\r\n- `matmul` padding (#101913, #102200, #103600  )\r\n- Fuse type casting with triton matmul kernel (#106443, #106516, #107495 )\r\n- Improve loop ordering to generate more coalesced memory access (#106827)\r\n- Enable persistent reductions (#94847, #102444 )\r\n- Layout optimization for convolution ( #99773 )\r\n- Improve max-autotune ( #95554, #95555, #96410, #97219 )\r\n- Coordinate descent tuning: doing coordinate descent search to find promising triton configs that are good for perf.  (#99594, #99403, #103660  )\r\n- Inductor Freezing (#100652)\r\n- Horizontally Fuse Addmm for inference (#100746 )\r\n- Avoid unnecessary copy (#102089 )\r\n- Convert layout of conv weight to channels last ahead of time for inference (#103642)\r\n- Performance improvement for CPU backend: Support mkldnn packed linear to improve bfloat16 performance ( #96954 )\r\n- Performance improvement for dynamic shapes: \r\n  - Support multilayer reduction (#99475, #101915, #106747 )\r\n  - Apply divisible_by_16 flag in more cases for vectorized load and store ( #105743 )\r\n\r\n## Release Engineering\r\n- Add workflow for quick perf comparison for inductor (#96166)\r\n- Run inference accuracy and performance tests with bfloat16 for inductor (#103535)\r\n- Add DALLE2_pytorch to inductor benchmarking workflow with AMP fallback (#104283)\r\n- Run the inductor benchmark suite with dynamic batch only (#97912)\r\n\r\n## torch.export\r\n- Speed up export time by avoiding calling the callable during export. (#107249)\r\n\r\n## JIT\r\n- Improve load times by reducing the number of times re-indexing occurs (#102312)\r\n- Skip the source info in the error report if the source code is too large (#105608)\r\n\r\n\r\n# Documentation\r\n\r\n## CUDA\r\n- Fix `torch.cuda.mem_get_info` doc (#96621)\r\n\r\n## DataPipe\r\n- Add generated docstring to functional form DataPipe (#100503)\r\n- Update docstring for functional form of DataPipes (#100446)\r\n\r\n## torch.fx\r\n- Update fx.pass.graph_drawer usage doc to draw fx graph (#95534)\r\n- Add doc test in graph_drawer.py (#95919)\r\n- Add docs for writing ATen IR passes + FX Pattern matching (#100577)\r\n- Update torch.fx docs (#97058)\r\n- Fix typos under torch/fx directory (#97596)\r\n\r\n## torch.export\r\n- `torch.export` landing page (#108783)\r\n\r\n## Intel\r\n- Add best practices doc for CPU backend (#105051)\r\n\r\n## Linear Algebra\r\n- Fix docs rendering in linalg.{matrix_exp, ldl_factor}. (#101363, #99777)\r\n- Fix examples in linalg.tensorinv. (#105911)\r\n- Improve error message for crashes related to `linalg.eigh` when input matrix is ill-conditioned, in some cusolver versions (#107082)\r\n\r\n## optim\r\n- Document optimizer state_dict() better with an example (#105958)\r\n- Have SGD summary show up in optimizer overview (#107738)\r\n\r\n## Python Frontend\r\n- Improve docs for `torch.gradient` (#98824, torch.complex (#99938), `torch.arange` (#99963), torch.asarray (#100971), torch.{cat,stack} (#103421), torch.randn signature (#102075), `torch.bitwise_right_shift` (#103500), `torch.round`  (#97227), `torch.fake_quantize_per_tensor_affine` (#104453), `torch.manual_seed` (#105175), torch.resolve_neg (#104151), `torch.fake_quantize_per_channel_affine` (#105241, #105955), `torch.bucketize` (#104474), `torch.slice_scatter` (#107849), `RNN` (#106222), `torch.unique` (#108292)\r\n\r\n\r\n## Quantization\r\n- Fix disbale--and other--typos (#95322)\r\n- Fix  return values of `_get_name()` in quantized ConvTranspose (#97678)\r\n- Fix docs for `prepare_fx`/`prepare_qat_fx` (#105979)\r\n- Error when someone calls train/eval on pre_autograd graph (#108143)\r\n- Move dropout replacement to `move_model_to_eval` (#108255)\r\n- Fix and rename `move_model_to_eval` to `move_exported_model_to_eval` (#109027)\r\n\r\n## Inductor\r\n- Improve Discoverability of Inductor Optimizations (#95824 )\r\n\r\n## Release Engineering\r\n- Fix doc-rendering error and deprecate CircleCI docs scripts (#105678)\r\n\r\n## Dynamo\r\n- Add a RST doc for the performance dashboard (#100592)\r\n- Small doc update for torch_compile_debug (#95809)\r\n- Logging documentation updates (#100595)\r\n- Move Dynamo IPEX backend to training/inference category (#108643)\r\n\r\n## nn_frontend\r\n- Remove future deprecation warning from kl_div docs (#96541)\r\n- Fix the docs for `cosine_similarity` (#104772)\r\n- Correct `HingeEmbeddingLoss` documentation (#95140)\r\n- Fix docstring for shape of  `target` for MultiLabelSoftMarginLoss (#107817)\r\n- Document differing behavior of RReLU between training and evaluation (#95624)\r\n\r\n## ONNX\r\n- Remove API reference for TorchScript export diagnostics (#107979)\r\n- Refactor `torch.onnx` documentation (#108379)\r\n\r\n## Distributed\r\n### FullyShardedDataParallel\r\n- Update the doc to be more clear that per-device NCCL stream is per PG (#95705) \r\n- Re-addd why we register the post-backward hook only on the first forward in the case of multiple forwards (#95326) \r\n- Clarify CPU offload implicitly in reshard_doc (#98666) \r\n- Document `optim_state_dict_config` in method (#102657) \r\n- Document `get_state_dict_type` (#102658)\r\n### Distributed (c10d)\r\n- Fix typos in comments under torch/csrc/distributed (#96062)\r\n- Update isend/irecv warning messages for nccl (#95236)\r\n- Add warning about object-based collectives for GPU tensors to docs. (#97702) \r\n### Distributed Checkpoint\r\n- Fix documentation for distributed checkpointing for optimizers (#95264)\r\n- Add fsdp checkpoint example (#95258)\r\n- Update DCP doc to use the updated FSDP optim state_dict APIs (#95303) \r\n- Update documentation to read FileSystemReader instead of FileSystemLoader (#102795)\r\n- Add documentation for HSDP saving using DCP (#104810)\r\n### RPC\r\n- Add missing RRef docs for RPC (#106902)\r\n\r\n## Sparse Frontend\r\n- Improve error message when expand is called on sparse tensor (#98365)\r\n\r\n## Composability\r\n### Dynamic Shapes\r\n- Update dynamic shapes documentation (#109764)\r\n\r\n## Dynamo\r\n- Add docs for `torch.compile(numpy)` (#109710)\r\n\r\n\r\n# Developers\r\n\r\n## torch.fx\r\n- Fix typos in `torch/fx/_compatibility.py` (#97618(https://github.com/pytorch/pytorch/pull/97618 ))\r\n- Add `torch/utils/_stats.py` to stack frame skiplist (#98117)\r\n- Add pre_autograd kwarg to make_fx (#97559)\r\n- Revert torch.fx.interpreter error printing change (#101462)\r\n- Fix pytree error formatting string (#105935(https://github.com/pytorch/pytorch/pull/105935 ))\r\n- Assume SymPy is always installed (#94903)\r\n- Add a more error checking to minifier (#103057)\r\n- Refactor unwrap_proxy() for proxy tensor tracing (#104667)\r\n- Enable ruff's UP rules and autoformat dynamo / functorch and refs (#105432)\r\n- Enable flake8-simplify checks (#97984)\r\n\r\n## Inductor\r\n- Allow overriding the decomposition table in compile_fx API.  ( #95468 )\r\n- Allow saving parameters for compiling a graph and relay later to improve development efficiency (#106952 )\r\n- Support benchmarking kernel perf to gather metrics like latency and memory bandwidth ( #95355, #95506, #95845, #96458, #96461, #97057, #103547)\r\n- Tracking operator count ( #100329 )\r\n- Print the path to the generated wrapper code with TORCH_LOGS=output_code (#99038 )\r\n- Provenance tracking for wrapper code (#105717, #95901  )\r\n- Support inductor OSS perf dashboard (#95685, #99387, #99754, #105221)\r\n\r\n## Composability\r\n\r\n- A number of improvements that make it easier for custom backends to integrate as a pytorch eager mode backend out-of-tree, through the PrivateUse1 DispatchKey\r\n\r\n    - Allow privateuse1 key to be used with legacy constructor (#95748)\r\n    - Add Generator register for the privateuse1 backend (#93920)\r\n    - Optimize the AMP func name in custom_device_mod (#98052)\r\n    - Enable dispatch stub for backend PrivateUse1 (#99611)\r\n    - Support random for custom device (#97420)\r\n\r\n\r\n- Nvfuser python API import fix (#94036)\r\n- Add ability to create library fragments (#98439)\r\n- Core aten IR:\r\n    - Tag functions to core IR in native_functions.yaml (#105849)\r\n    - Add `_native_batch_norm_legit_no_training` to core IR (#107732)\r\n    - Make python decomp for native_batch_norm CompositeImplicitAutograd, remove native_batch_norm from core aten opset (#107791)\r\n    - Avoid extra copies in batchnorm decomposition inference by introducing a new op, _native_batch_norm_legit_no_training (#94946)\r\n    - Add `aten.smooth_l1_loss_backward` to `core_aten_decompositions` (#100267)\r\n    - Add `empty`/`empty_like` to core aten decomps (#105158)\r\n- Fixed missing-prototypes warnings in torch_cpu (Part 1) (#100053)\r\n- Fix typos in checkFloatingOrComplex errors (#102456)\r\n- Allow existing \"Python RAII guards\" to be used as context managers (#102579)\r\n- Replace _prims_common.check with torch._check* (#103240)\r\n- Update core aten decomp table (#105673)\r\n- Generate mypy hints for torch.Tag, add a couple of pointwise ops (#106910)\r\n- aot_autograd: avoid using intermediate_base logic unnecessarily (#97786)\r\n- Fix disable amp for runtime wrapper (#97864)\r\n- aot_autograd: more logging on metadata asserts (#99177)\r\n- Proper handling when outputs are aliased but have identical size/stride/offset metadata (#100430)\r\n- Fix de-dupping metadata computation bug (#100431)\r\n\r\n## Release Engineering\r\n- Rename default branch to main (2418b945763)\r\n- Use PyTorch wheel in Windows CI (#94958)\r\n- Use GPU machine and run GPU tests with Bazel builds (#95721)\r\n- Enable simpler C++ test discovery + running workflow on CI with run_test.py (#99956, #99559)\r\n- Run C++ test_api binary directly in CI slow jobs (#101088)\r\n\r\n## Autograd Frontend\r\n- Add a macro for derivatives formulas that returns multiple outputs and can be specified to save certain tensors conditionally (#103750)\r\n- Fix `torch._C.get_current_graph_task_execution_order` `accumulate_grads` ordering (#105353)\r\n- `torch.autograd._force_original_view_tracking` to work as both context manager and function (#106706)\r\n- Enable `autograd` to be compiled (#103822, #104316)\r\n\r\n## JIT\r\n- Create public interface for `torch.jit` to reduce `pyright` errors (#101678)\r\n\r\n## optim\r\n- Change step from 1D to singleton tensor in `Adam` (#96994)\r\n\r\n## ONNX\r\n- Remove torch dependencies in _beartype (#98958)\r\n- Delay torch.onnx import to after all dynamo sub]components ([#99070)\r\n- Enable xdoctests in CI (#98546)\r\n- Update ONNX submodule from ONNX 1.13.1 with Protobuf 4.21 updates (#96138)\r\n- Run ONNX tests as part of standard run_test script (#99215)\r\n- Skip flaky dynamic tests before ORT==1.15 in fx exporter (#98856)\r\n- Add additional_test_kwargs into test_fx_to_onnx_with_onnxruntime.py (#99434)\r\n- Bump onnx-script version with imported module renaming (#99926)\r\n- Add test_fx_op_consistency.py (#99465)\r\n- Refactor test_op_consistenct.py and test_fx_op_consistency.py (#100172)\r\n- Add xfail into subtests of op consistency and retire fixme (#100173)\r\n- Skip flaky dynamic test in CI (#100297)\r\n- Add supported ops into test_fx_op_consistency - 1st batch (#100265)\r\n- Bump onnx submodule to release 1.14.0 (#101809)\r\n- Bump ORT version to 1.15.0 (#102248)\r\n- Add FX exporter MaxPool tests (#102773)\r\n- FX Dispatcher Test (#103971)\r\n- Bench torch.onnx.dynamo_export and torch.onnx.export under dynamo bench (#103135)\r\n- Separate fx _type_utils from torchscript exporter (#103942)\r\n- Use load_model_from_string (#104533)\r\n- Enable ruff's UP rules and autoformat onnx/ (#105427)\r\n- Suppress ORT warnings in unit tests (#105624)\r\n- Add comment on test_view_dynamic_zero_dim (#105950)\r\n- Bump numpy from 1.21.6 to 1.22.0 in /benchmarks/dynamo/_onnx (ab9ea0d0f25)\r\n- Enable skipped gpt2 test (#94930)\r\n- Clean up outdated skip ort < 1.15 decorator in tests (#105951)\r\n- Add test support for dynamic shapes for torch.onnx.dynamo_export (#106495)\r\n- Update xfail reasons in fx runtime tests (#107257)\r\n- Add unittest for exporting embedding_bag (#105862)\r\n- Add huggingface models into CI tests (#107247)\r\n\r\n## Distributed\r\n### FullyShardedDataParallel\r\n- Log FSDP mixed precision (#97367) \r\n- Add loggings of modules FSDP hooks firing (#102508)\r\n- Print out more useful error message for optim_state_dict (#96860) \r\n- Use `INFO` instead of `DETAIL` for warning logs (#102639)\r\n- Add a summary log when finishing state_dict (#103784)\r\n### Distributed (c10d)\r\n- Fix `sandcastle_skip_if` decorator name is confusing (#95649) \r\n- Add sequence number in PG Wrapper (#97462)\r\n- Print collective in PG Wrapper (#97544)\r\n- Add diff capability in PG Wrapper (#100214)\r\n- Add `TDD`, `NCCL_DEBUG` log (#97692)\r\n- Don't crash on retrieve NCCL DesyncReport (#98470)\r\n- Print stacktrace on collectFullMesh in for Gloo (#98810)\r\n- Add fully_shard debug function to print sharded tree structure, module names and managed param fqns (#99133)\r\n- Enhance error msg in PG Wrapper (#100213)\r\n- Make `ProcessGroupNCCL` work.wait() respect timeout (#100162)\r\n- Add size info to collective logs (#100413)\r\n- Add the logics and interface to log ProcessGroup comms configuration (#104373)\r\n- Make NCCL default logging more friendly. (#105695)\r\n- Add OnCompletion Hook to ProcessGroup (#106988) (#107233) \r\n- Improve input mismatch error msg (#107281) \r\n### DistributedDataParallel\r\n- Add debug logging around DDP mixed precision copies (#96438)\r\n### Distributed Tensor (Prototype Release)\r\n- Add necessary logging to APIs and components for PTD use cases such as `DTensor,` TP and DCP (#101994, #102209, #102278\r\n\r\n## Sparse Frontend\r\n\r\n- Expand sparse.softmax zero nnz tests to cover cases of previously reported FPE (#95646)\r\n- Use nested namespaces in sparse (#97581)\r\n- Fix cuSparse CSR SPMM when using nullptr in csrRowOffsets (#105957)\r\n- Remove CUTLASS extensions merged upstream (#107612)\r\n- Remove CUTLASS extensions merged upstream (#107612)\r\n- Fixes for reference and move (#95942)\r\n- Triton kernels without public API\r\n    - Use missing-prototypes in torch_cpu (#104138)\r\n    - SDPA: Support frontend for BSR masks (#104042)\r\n    - sampled_addmm: Support BSR (#101163)\r\n    - softmax: Support Triton kernel for BSR inputs (#102095)\r\n\r\n# Security\r\n\r\n## Release Engineering\r\n- Move mergebot and other CI/CD workflows to its own secure environment (#107060)\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.1.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.1.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.1.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/123579416", "release_id": 123579416, "date_created": "2023-10-04T17:31:14Z", "date_published": "2023-10-04T17:32:12Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/102257798", "tag": "v2.0.1", "name": "PyTorch 2.0.1 Release, bug fix release", "author": {"name": "drisspg", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n* Fix `_canonical_mask` throws warning when bool masks passed as input to TransformerEncoder/TransformerDecoder (#96009, #96286) \r\n* Fix Embedding bag max_norm=-1 causes leaf Variable that requires grad is being used in an in-place operation #95980\r\n* Fix type hint for torch.Tensor.grad_fn, which can be a torch.autograd.graph.Node or None. #96804\r\n* Can\u2019t convert float to int when the input is a scalar np.ndarray. #97696\r\n* Revisit torch._six.string_classes removal  #97863\r\n* Fix module backward pre-hooks to actually update gradient #97983\r\n* Fix load_sharded_optimizer_state_dict error on multi node #98063\r\n* Warn once for TypedStorage deprecation #98777\r\n* cuDNN V8 API, Fix incorrect use of emplace in the benchmark cache #97838\r\n### Torch.compile:\r\n* Add support for Modules with custom __getitem__ method to torch.compile #97932\r\n* Fix improper guards with on list variables. #97862\r\n* Fix Sequential nn module with duplicated submodule #98880\r\n### Distributed:\r\n*   Fix distributed_c10d's handling of custom backends #95072\r\n*   Fix MPI backend not properly initialized #98545\r\n### NN_frontend:\r\n* Update Multi-Head Attention's doc string #97046\r\n* Fix incorrect behavior of `is_causal` paremeter for torch.nn.TransformerEncoderLayer.forward #97214\r\n* Fix error for SDPA on sm86 and sm89 hardware #99105\r\n* Fix nn.MultiheadAttention mask handling  #98375\r\n### DataLoader:\r\n* Fix regression for pin_memory recursion when operating on bytes #97737\r\n* Fix collation logic #97789 \r\n* Fix Ppotentially backwards incompatible change with DataLoader and is_shardable Datapipes #97287\r\n### MPS:\r\n* Fix LayerNorm crash when input is in float16 #96208\r\n* Add support for cumsum on int64 input  #96733\r\n* Fix issue with setting BatchNorm to non-trainable #98794\r\n### Functorch:\r\n* Fix Segmentation Fault for vmaped function accessing BatchedTensor.data #97237\r\n* Fix index_select support when dim is negative [#97916](https://github.com/pytorch/pytorch/pull/97916)\r\n* Improve docs for autograd.Function support #98020\r\n* Fix Exception thrown when running Migration guide example for jacrev #97746\r\n### Releng:\r\n* Fix Convolutions for CUDA-11.8 wheel builds #99451\r\n* Fix Import torchaudio + torch.compile crashes on exit #96231\r\n* Linux aarch64 wheels are missing the mkldnn+acl backend support  - [https://github.com/pytorch/builder/commit/54931c264ed3e7346899f547a272c4329cc8933b](https://github.com/pytorch/builder/commit/54931c264ed3e7346899f547a272c4329cc8933b)\r\n* Linux aarch64 torchtext 0.15.1 wheels are missing for aarch64_linux platform - [https://github.com/pytorch/builder/issues/1375](https://github.com/pytorch/builder/issues/1375)\r\n* Enable ROCm 5.4.2 manywheel and python 3.11 builds #99552\r\n* PyTorch cannot be installed at the same time as numpy in a conda env on osx-64 / Python 3.11 #97031\r\n* Illegal instruction (core dumped) on Raspberry Pi 4.0 8gb  - https://github.com/pytorch/builder/pull/1370\r\n### Torch.optim:\r\n*  Fix fused AdamW causes NaN loss #95847\r\n* Fix Fused AdamW has worse loss than Apex and unfused AdamW for fp16/AMP #98620\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/97272) should contain all relevant pull requests related to this release as well as links to related issues", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.0.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.0.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.0.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/102257798", "release_id": 102257798, "date_created": "2023-04-24T13:27:27Z", "date_published": "2023-05-08T19:55:19Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/94838221", "tag": "v2.0.0", "name": "PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever", "author": {"name": "drisspg", "type": "User"}, "description": "# PyTorch 2.0 Release notes\r\n\r\n- Highlights\r\n- Backwards Incompatible Changes\r\n- Deprecations\r\n- New Features\r\n- Improvements\r\n- Bug fixes\r\n- Performance\r\n- Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch\u00ae 2.0 ([release note](https://github.com/pytorch/pytorch/releases)) which we highlighted during the [PyTorch Conference](https://www.youtube.com/@PyTorch/playlists?view=50&sort=dd&shelf_id=2) on 12/2/22! PyTorch 2.0 offers the same eager-mode development and user experience, while fundamentally changing and supercharging how PyTorch operates at compiler level under the hood with faster performance and support for Dynamic Shapes and Distributed.\r\n\r\nThis next-generation release includes a Stable version of Accelerated Transformers (formerly called Better Transformers); Beta includes torch.compile as the main API for PyTorch 2.0, the scaled_dot_product_attention function as part of torch.nn.functional, the MPS backend, functorch APIs in the torch.func module; and other Beta/Prototype improvements across various inferences, performance and training optimization features on GPUs and CPUs. For a comprehensive introduction and technical overview of torch.compile, please visit the 2.0 [Get Started page](https://pytorch.org/get-started/pytorch-2.0).\r\n\r\nAlong with 2.0, we are also releasing a series of beta updates to the PyTorch domain libraries, including those that are in-tree, and separate libraries including TorchAudio, TorchVision, and TorchText. An update for TorchX is also being released as it moves to community supported mode. More details can be found in this [library blog](https://pytorch.org/blog/new-library-updates-in-pytorch-2.0/).\r\n\r\nThis release is composed of over 4,541 commits and 428 contributors since 1.13.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.0 and the overall 2-series this year.\r\n\r\nSummary:\r\n\r\n- torch.compile is the main API for PyTorch 2.0, which wraps your model and returns a compiled model. It is a fully additive (and optional) feature and hence 2.0 is 100% backward compatible by definition.\r\n- As an underpinning technology of torch.compile, TorchInductor with Nvidia and AMD GPUs will rely on OpenAI Triton deep learning compiler to generate performant code and hide low level hardware details. OpenAI Triton-generated kernels achieve performance that's on par with hand-written kernels and specialized cuda libraries such as cublas.\r\n- Accelerated Transformers introduce high-performance support for training and inference using a custom kernel architecture for scaled dot product attention (SPDA). The API is integrated with torch.compile() and model developers may also use the [scaled dot product attention](https://pytorch.org/docs/2.0/generated/torch.nn.functional.scaled_dot_product_attention.html) kernels directly by calling the new scaled_dot_product_attention() operator.\r\n- Metal Performance Shaders (MPS) backend provides GPU accelerated PyTorch training on Mac platforms with added support for Top 60 most used ops, bringing coverage to over 300 operators.\r\n- Amazon AWS optimize the PyTorch CPU inference on AWS Graviton3 based [C7g instances](https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7g-instances-powered-by-aws-graviton3-processors/). PyTorch 2.0 improves inference performance on Graviton compared to the previous releases, including improvements for Resnet50 and Bert.\r\n- New prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.\r\n\r\n<table>\r\n  <tr>\r\n   <td>\r\n<strong>Stable</strong>\r\n   </td>\r\n   <td><strong>Beta</strong>\r\n   </td>\r\n   <td><strong>Prototype</strong>\r\n   </td>\r\n   <td><strong>Platform Changes</strong>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>Accelerated PT 2 Transformers\r\n   </td>\r\n   <td>torch.compile\r\n   </td>\r\n   <td>DTensor\r\n   </td>\r\n   <td>CUDA support for 11.7 & 11.8 (deprecating CUDA 11.6)\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>PyTorch MPS Backend\r\n   </td>\r\n   <td>TensorParallel\r\n   </td>\r\n   <td>Python 3.8 (deprecating Python 3.7)\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>Scaled dot product attention\r\n   </td>\r\n   <td>2D Parallel\r\n   </td>\r\n   <td>AWS Graviton3\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>Functorch\r\n   </td>\r\n   <td rowspan=\"2\" >Torch.compile (dynamic=True)\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>Dispatchable Collectives\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>torch.set_default_device and torch.device as context manager\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>X86 quantization backend\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n  <tr>\r\n   <td>\r\n   </td>\r\n   <td>GNN inference and training performance\r\n   </td>\r\n   <td>\r\n   </td>\r\n   <td>\r\n   </td>\r\n  </tr>\r\n</table>\r\n\r\n\\*To see a full list of public 2.0, 1.13 and 1.12 feature submissions click[ here](https://docs.google.com/spreadsheets/d/1H3jazwO8BBCwK8JwLNYspLiHfUrzshEtyqjL-X93I9g/edit#gid=790902532)\r\n\r\n# Backwards Incompatible Changes\r\n\r\n### **Drop support for Python versions <= 3.7 (#93155)**\r\n\r\nPreviously the minimum supported version of Python for PyTorch was 3.7. This PR updates the minimum version to require 3.8 in order to install PyTorch. See [Hardware / Software Support ](https://github.com/pytorch/pytorch/blob/893aa5df3f2a475c91ea8eadb1353812e52fb227/RELEASE.md#python) for more information.\r\n\r\n### **Drop support for CUDA 10 (#89582)**\r\n\r\nThis PR updates the minimum CUDA version to 11.0. See the [getting-started](https://pytorch.org/get-started/locally/) for installation or [building from source](https://github.com/pytorch/pytorch#from-source) for more information.\r\n\r\n### **Gradients are now set to `None` instead of zeros by default in `torch.optim.*.zero_grad()` and `torch.nn.Module.zero_grad()` (#92731)**\r\n\r\nThis changes the default behavior of `zero_grad()` to zero out the grads by setting them to `None` instead of zero tensors. In other words, the `set_to_none` kwarg is now `True` by default instead of `False`. Setting grads to `None` reduces peak memory usage and increases performance. This will break code that directly accesses data or does computation on the grads after calling `zero_grad()` as they will now be `None`. To revert to the old behavior, pass in `zero_grad(set_to_none=False)`.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> import torch\r\n>>> from torch import nn\r\n>>> module = nn.Linear(2,22)\r\n>>> i = torch.randn(2, 2, requires_grad=True)\r\n>>> module(i).sum().backward()\r\n>>> module.zero_grad()\r\n>>> module.weight.grad == None\r\nFalse\r\n>>> module.weight.grad.data\r\ntensor([[0., 0.],\r\n        [0., 0.]])\r\n>>> module.weight.grad + 1.0\r\ntensor([[1., 1.],\r\n        [1., 1.]])\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> import torch\r\n>>> from torch import nn\r\n>>> module = nn.Linear(5, 5)\r\n>>> i = torch.randn(2, 5, requires_grad=True)\r\n>>> module(i).sum().backward()\r\n>>> module.zero_grad()\r\n>>> module.weight.grad == None\r\nTrue\r\n>>> module.weight.grad.data\r\nAttributeError: 'NoneType' object has no attribute 'data'\r\n>>> module.weight.grad + 1.0\r\nTypeError: unsupported operand type(s) for +:\r\n'NoneType' and 'float'\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Update `torch.tensor` and `nn.Parameter` to serialize all their attributes (#88913)**\r\n\r\nAny attribute stored on `torch.tensor` and `torch.nn.Parameter` will now be serialized. This aligns the serialization behavior of `torch.nn.Parameter`, `torch.Tensor` and other tensor subclasses\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n# torch.Tensor behavior\r\n>>> a = torch.Tensor()\r\n>>> a.foo = 'hey'\r\n\r\n>>> buffer = io.BytesIO()\r\n>>> torch.save(a, buffer)\r\n>>> buffer.seek(0)\r\n>>> b = torch.load(buffer)\r\n\r\n>>> print(a.foo)\r\nhey\r\n>>> print(b.foo)\r\nAttributeError: 'Tensor' object has no attribute 'foo'\r\n\r\n# torch.nn.Parameter behavior\r\n>>> a = nn.Parameter()\r\n>>> a.foo = 'hey'\r\n\r\n>>> buffer = io.BytesIO()\r\n>>> torch.save(a, buffer)\r\n>>> buffer.seek(0)\r\n>>> b = torch.load(buffer)\r\n>>> print(a.foo)\r\nhey\r\n>>> print(b.foo)\r\nAttributeError: 'Parameter' object has no attribute 'foo'\r\n\r\n# torch.Tensor subclass behavior\r\n>>> class MyTensor(torch.Tensor):\r\n...   pass\r\n\r\n>>> a = MyTensor()\r\n>>> a.foo = 'hey'\r\n>>> print(a.foo)\r\nhey\r\n\r\n>>> buffer = io.BytesIO()\r\n>>> torch.save(a, buffer)\r\n>>> buffer.seek(0)\r\n>>> b = torch.load(buffer)\r\n>>>print(b.foo)\r\nhey\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n# torch.Tensor behavior\r\na = torch.Tensor()\r\na.foo = 'hey'\r\n\r\n>>> buffer = io.BytesIO()\r\n>>> torch.save(a, buffer)\r\n>>> buffer.seek(0)\r\n>>> b = torch.load(buffer)\r\n>>> print(a.foo)\r\nhey\r\n>>> print(b.foo)\r\nhey\r\n\r\n# torch.nn.Parameter behavior\r\n>>> a = nn.Parameter()\r\n>>> a.foo = 'hey'\r\n\r\n>>> buffer = io.BytesIO()\r\n>>> torch.save(a, buffer)\r\n>>> buffer.seek(0)\r\n>>> b = torch.load(buffer)\r\n>>> print(a.foo)\r\nhey\r\n>>> print(b.foo)\r\nhey\r\n\r\n# torch.Tensor subclass behavior\r\n>>> class MyTensor(torch.Tensor):\r\n...   pass\r\n\r\n>>> a = MyTensor()\r\n>>> a.foo = 'hey'\r\n>>> print(a.foo)\r\nhey\r\n\r\n>>> buffer = io.BytesIO()\r\n>>> torch.save(a, buffer)\r\n>>> buffer.seek(0)\r\n>>> b = torch.load(buffer)\r\n>>>print(b.foo)\r\nhey\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\nIf you have an attribute that you don't want to be serialized you should not store it as an attribute on tensor or Parameter but instead it is recommended to use `torch.utils.weak.WeakTensorKeyDictionary`\r\n\r\n```Python\r\n>>> foo_dict = weak.WeakTensorKeyDictionary()\r\n>>> foo_dict[a] = 'hey'\r\n>>> print(foo_dict[a])\r\nhey\r\n```\r\n\r\n### **Algorithms `{Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, NAdam, RAdam, RMSProp, RProp, SGD}` default to faster `foreach` implementation when on CUDA + differentiable=`False`**\r\n\r\nWhen applicable, this changes the default behavior of `step()` and anything that calls into `adadelta(...)`, `adagrad(...)`, `adam(...)`, `adamax(...)`, `adamw(...)`, `asgd(...)`, `nadam(...)`, `radam(...)`, `rmsprop(...)`, `rprop(...)`, `sgd(...)` directly to use the `foreach` implementation instead of the for-loop for better performance. However, this change can potentially be backward incompatible since there may be small numerical differences between the results computed with the `foreach` implementation and the previous default. The foreach implementation will be the default only if the following conditions are met.\r\n\r\n1. The user has not specified kwargs relating to implementation (`foreach`, `fused`, or `differentiable`),\r\n2. All tensors are native tensors (not subclasses) and on CUDA,\r\n3. `torch.jit.is_scripting` is `False`.\r\n\r\nWhen these conditions are satisfied, the implementation used will match the implementation used when one passes `foreach=True`. The user defined flag for `foreach` will NOT be overwritten in order to preserve user selections. For more details, check the [documentation](https://pytorch.org/docs/stable/optim.html#algorithms). There should be no significant differences between the results returned by these optimizers. To revert to the old behavior, say, for `adam`, pass in `adam(..., foreach=False, ...)` or initialize `Adam` with `Adam(..., foreach=False, ...)`.\r\n\r\nPull Requests: #92306, #92716, #92723,#92724, #92726, #92727, #92728, #92715, #91896, #92730, #90865, #93184, #92181, #92923, #95415, #95818, #95811\r\n\r\n### **`torch.nn.utils.stateless.functional_call` now respects tied weights (#90477)**\r\n\r\nAssume a module has two tied weights, x and x_tied. Previously, invoking `functional_call(module, parameters_and_buffers, args, kwargs=None, *, strict=False)` with a parameter dictionary of only one of the tied weights would result in the other one(s) not being updated.\r\n\r\nWe\u2019ve changed the behavior so that providing one of the tied weights in the parameter dictionary will update all other tied weights. If you would like the behavior in previous versions of PyTorch, please set `tie_weights=False`.\r\n\r\nPlease also see the related deprecation section \"torch.nn.stateless.functional_call in favor of torch.func.functional_call\".\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> class Foo(nn.Module):\r\n...    def __init__(self):\r\n...        super().__init__()\r\n...        self.x = nn.Parameter(torch.zeros([]))\r\n...        self.x_tied = self.x\r\n...\r\n...    def forward(self, inp):\r\n...        return self.x + self.x_tied\r\n\r\n>>> foo = Foo()\r\n>>> params = {'x': torch.ones([])}\r\n>>> result = functional_call(foo, params, torch.randn([]))\r\n>>> print(result)\r\n1.0\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> class Foo(nn.Module):\r\n...    def __init__(self):\r\n...        super().__init__()\r\n...        self.x = nn.Parameter(torch.zeros([]))\r\n...        self.x_tied = self.x\r\n...\r\n...    def forward(self, inp):\r\n...        return self.x + self.x_tied\r\n\r\n>>> foo = Foo()\r\n>>> params = {'x': torch.ones([])}\r\n>>> result = functional_call(foo,\r\n...                         params,\r\n...                         torch.randn([]),\r\n...                         tie_weights=False)\r\n>>> print(result)\r\n1.0\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Require `return_complex` to be passed explicitly to `torch.stft` for real input (#86724)**\r\n\r\n`torch.stft` takes an optional return_complex parameter that indicates whether the output should be a floating point tensor or a complex tensor. `return_complex` previously defaulted to False for real input tensors. This PR removes the default and makes `return_complex` a required argument for real inputs. However, complex inputs will continue to default to `return_complex=True`.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> a = torch.rand(1024)\r\n>>> _ = torch.stft(a, n_fft=128)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> t = torch.rand(1024)\r\n>>> _ = torch.stft(t, n_fft=128, return_complex=False)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Require inputs to `torch.istft` to be complex valued**\r\n\r\n`torch.istft` no longer supports input in the form of real tensors\r\nwith shape `(..., 2)` to mimic complex tensors. Instead, convert\r\ninputs to a complex tensor first before calling `torch.istft`.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> t = torch.rand(65, 33, 2)\r\n>>> _ = torch.istft(t, n_fft=128, length=1024)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> t = torch.rand(65, 33, 2)\r\n>>> _ = torch.istft(t, n_fft=128, length=1024)\r\nRuntimeError: istft requires a complex-valued input\r\ntensor matching the output from stft with return_complex=True.\r\n>>> t_complex = torch.view_as_complex(t)\r\n>>> _ = torch.istft(t_complex, n_fft=128, length=1024)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Change default behavior of sparse tensor construction to not do component verification(#92094)**\r\n\r\nWe now disable the costly component verification of torch.sparse_coo/csr/csc/bsr/bsc/compressed_tensor by default. The user can use the new `check_invariants` flag or `torch.sparse.check_sparse_tensor_invariants` to locally enable component verification. This allows users to constrain these costly checks to specific regions of their code and enables better overall performance. Previously users had no access to public constructors that disable these checks.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> i = [[0, 1, 1],\r\n         [2, 0, 5]]\r\n>>> v =  [3, 4, 5]\r\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\r\nRuntimeError: size is inconsistent with\r\nindices: for dim 1, size is 3 but found index 5\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> i = [[0, 1, 1],\r\n         [2, 0, 5]]\r\n>>> v =  [3, 4, 5]\r\n>>> s = torch.sparse_coo_tensor(i,\r\n...                            v,\r\n...                            (2, 3),\r\n...                            check_invariants=True)\r\nRuntimeError: size is inconsistent with indices: for\r\ndim 1, size is 3 but found index 5\r\n>>> with torch.sparse.check_sparse_tensor_invariants():\r\n...     s = torch.sparse_coo_tensor(i, v, (2, 3))\r\n...\r\nRuntimeError: size is inconsistent with indices: for\r\ndim 1, size is 3 but found index 5\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Remove deprecated functionality from `torch.testing`**\r\n\r\nHistorically, `torch.testing` exposed a lot of private and undocumented functionality publicly. The 2.0 release completes the deprecation cycle for the following items and removes them:\r\n\r\n- `rand` and `randn` (#87970)\r\n- `get_all_device_types` (#87971)\r\n- multiple dtype getters (#87972)\r\n- `make_non_contiguous` (#87973)\r\n\r\n### **Hooks registered on tensor to always run, even if they are the inputs to `.grad()` (#85849)**\r\n\r\nThis is a bug fix. Per the docs, hooks registered to Tensor should fire any time gradients are computed w.r.t. to that tensor. This change corrects the behavior to be consistent with the documentation. See [documentation](https://pytorch.org/docs/2.0/notes/autograd.html#backward-hooks-execution) for more details about backward hooks execution..\r\n\r\n**2.0**\r\n\r\n```Python\r\na = torch.tensor(1., requires_grad=True)\r\nb = a.clone()\r\nb.register_hook(hook)  # the hook registered here didn't fire before!\r\ntorch.autograd.grad(b.clone(), inputs=(b,))\r\n```\r\n\r\n### **`grad_fn` post-hooks can always observe the modifications to gradient by any grad_fn pre-hooks or hooks registered to Tensor, even if this is a leaf tensor (#85849)**\r\n\r\nThis corrects the behavior of hooks to be consistent with the documentation in the case where the tensor is a leaf tensor, i.e. the node is a grad accumulator node. See [documentation](https://pytorch.org/docs/**2.0**/notes/autograd.html#backward-hooks-execution) for more details about backward hooks execution.\r\n\r\n**2.0**\r\n\r\n```Python\r\ndef hook(grad):\r\n   # updates grad\r\n   return grad * 3\r\n\r\ndef hook2(grad_input, grad_output):\r\n   # Before this change, grad_output would NOT see the x3\r\n   print(grad_output)\r\n\r\na = torch.tensor(1., requires_grad=True)\r\nb = a.clone()\r\nacc_grad = b.grad_fn.next_functions[0][0]\r\nacc_grad.register_hook(hook2)\r\nb.register_hook(hook)\r\ntorch.autograd.backward(b.clone(), inputs=(a,))  # hook fire\r\n```\r\n\r\n### **Remove FSDP `params_with_grad` (#87480)**\r\n\r\nIn FSDP, we used to have an API `params_with_grad` for users to get parameters which have gradients from the FSDP module. We decided not to expose this helper because it is not a common paradigm.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nm = FullyShardedDataParallel(module)\r\nm.params_with_grad()\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nm = FullyShardedDataParallel(module)\r\nm.params_with_grad()  # Runtime error thrown\r\n# For work-around, users can still do\r\n[p for p in self.parameters() if p.grad is not None]\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Users doing wildcard import of torch.distributed.fsdp.fully_sharded_data_parallel will no longer get non-public symbols (#87917)**\r\n\r\nUsers could previously import both public and non-public symbols:\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import *\r\nShardingStrategy.FULL_SHARD # Non-public API\r\nFullyShardedDataParallel(module) # public API\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import *\r\nShardingStrategy.FULL_SHARD # Non-public API, this will fail now\r\nFully`Sharded`DataParallel(module) # public API\r\n...\r\n# Users can instead\r\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import (\r\nFullyShardedDataParallel,\r\nShardingStrategy,\r\n)\r\nFullyShardedDataParallel(module, sharding_strategy=ShardingStrategy.FULL_SHARD)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Signature of FSDP `auto_wrap_policy `related APIs were changed in (#88450).**\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nlambda_auto_wrap_policy(m, unwrapped_params=...)\r\ntransformer_auto_wrap_policy(m, unwrapped_params=...)\r\nsize_based_auto_wrap_policy(m, unwrapped_params=...)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nlambda_auto_wrap_policy(m, nonwrapped_numel=...)\r\ntransformer_auto_wrap_policy(m, nonwrapped_numel=...)\r\nsize_based_auto_wrap_policy(m, nonwrapped_numel=...)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Updated `alltoall` signature to be consistent with other c10d APIs (#90569)**\r\n\r\nThe keyword argument names have been changed.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nalltoall(output=..., input=...)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nalltoall(output_tensors=..., input_tensors=...)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Remove unused functions in torch.ao.quantization.fx.utils (#90025)**\r\n\r\nThis commit removes the following unused functions from both the torch.quantization and the\r\ntorch.ao.quantization namespaces:\r\n\r\n- `graph_pretty_str`\r\n- `get_per_tensor_qparams`\r\n- `quantize_node`\r\n- `get_qconv_op`\r\n- `create_qparam_nodes`\r\n- `node_return_type_is_int`\r\n- `is_get_tensor_info_node`\r\n\r\n### **Make `torch.ao.quantization.backend_config.BackendConfig` accept inputs in the right order (#90698)**\r\n\r\nThe existing `BackendConfig` fusion pattern uses a \"reversed nested tuple\" format that is unintuitive.\r\nThis pattern format also complicates the signatures of the user specified \"fuser methods\", which needed to accept arguments in reverse nested order to match\r\nthe patterns:\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nimport torch as nn\r\nimport torch.ao.nn.intrinsic as nni\r\nfrom torch.ao.quantization.backend_config import (\r\n  BackendPatternConfig\r\n)\r\ndef fuse_linear_relu(is_qat, relu, bn_conv):\r\n    (bn, conv) = bn_conv\r\n    return nni.ConvBnReLU2d(conv, bn, relu)\r\n\r\nconfig = (\r\n    BackendPatternConfig((nn.ReLU, (nn.BatchNorm2d, nn.Conv2d)))\r\n    .set_dtype_configs(...)\r\n    .set_fuser_method(fuse_conv_bn_relu)\r\n    .set_fused_module(nni.ConvBnReLU2d)\r\n)\r\n\r\nbackend_config.configs  # returns Dict[Pattern, BackendPatternConfig]\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\ndef fuse_linear_relu(is_qat, conv, bn, relu):\r\n    return nni.ConvBnReLU2d(conv, bn, relu)\r\n\r\nconfig = (\r\n    BackendPatternConfig((nn.Conv2d, nn.BatchNorm2d, nn.ReLU))\r\n    .set_dtype_configs(...)\r\n    .set_fuser_method(fuse_conv_bn_relu)\r\n    .set_fused_module(nni.ConvBnReLU2d)\r\n)\r\n\r\n# Or for backward-compatibility\r\ndef fuse_linear_relu(is_qat, relu, bn_conv):\r\n    (bn, conv) = bn_conv\r\n    return nni.ConvBnReLU2d(conv, bn, relu)\r\n\r\nconfig = (\r\n    BackendPatternConfig()\r\n    ._set_pattern_complex_format((nn.ReLU, (nn.BatchNorm2d, nn.Conv2d)))\r\n    .set_dtype_configs(...)\r\n    .set_fuser_method(fuse_conv_bn_relu)\r\n    .set_fused_module(nni.ConvBnReLU2d)\r\n)\r\n\r\nbackend_config.configs  # returns List[BackendPatternConfig]\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Make the AO codebase compliant with the public vs private API guidelines of pytorch [Public-API-definition-and-documentation](https://github.com/pytorch/pytorch/wiki/Public-API-definition-and-documentation)**\r\n\r\nIf users were using any of the AO private APIs then these would have to be accessed with a preceding `_` to conform with the guidelines.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nget_observer_dict()\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n_get_observer_dict()\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\nPull Requests: (#86029, #87515, #87516, #87517, #87518, #87519, #88392, #88394, #88396, #88397, #87521, #88395, #87883, #88399, #88398, #86022, #86023, #86024, #86025, #86026, #86027, #86028, #86030, #86031, #86032, #86033, #86034, #86037, #90315, #88391, #90554, #87520)\r\n\r\n### **Remove overwrite_output_observer and represent the observer constraints for fixed qparams ops through the existing DTypeWithConstraints mechanism (#88620)**\r\n\r\nThis commit removes `overwrite_output_observer` and `overwrite_output_fake_quantize` overwrite observer settings in the BackendConfig. Instead, we represent the observer constraints for\r\nfixed qparams ops through the existing DTypeWithConstraints mechanism. Note that, however, to be consistent with other DTypeWithConstraints checks, we no longer throw an error if an incorrect observer is specified, but simply ignore the offending QConfig and log a warning instead. This is the BC-breaking part of the change.\r\n**1.13**\r\n\r\n```Python\r\nfrom torch.ao.quantization.qconfig import default_qconfig\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx\r\n\r\nmodel = ModelWithFixedQParamsOps()\r\nqconfig_mapping = QConfigMapping().set_global(default_qconfig)\r\nexample_inputs = ...\r\nprepare_fx(model, qconfig_mapping, example_inputs)\r\n```\r\n\r\nBefore this commit, running the above leads to an exception because the wrong observers are used for fixed qparams ops. After this commit, the above will only encounter a warning,and the fixed qparams ops will not be quantized. In both cases, switching to `get_default_qconfig_mapping` will cause the fixed qparams ops to be quantized.\r\n\r\n### **Remove `torch.ao.quantization.quantization_patterns` and `torch.ao.quantization.fusion_patterns`(#89872)**\r\n\r\nThe following classes under the `torch.ao.quantization.fx.quantization_patterns` namespace are migrated to the `torch.ao.quantization.fx.quantize_handler`\r\nnamespace:\r\n\r\n- `QuantizeHandler`\r\n- `BinaryOpQuantizeHandler`\r\n- `CatQuantizeHandler`\r\n- `ConvReluQuantizeHandler`\r\n- `LinearReLUQuantizeHandler`\r\n- `BatchNormQuantizeHandler`\r\n- `EmbeddingQuantizeHandler`\r\n- `RNNDynamicQuantizeHandler`\r\n- `DefaultNodeQuantizeHandler`\r\n- `FixedQParamsOpQuantizeHandler`\r\n- `CopyNodeQuantizeHandler`\r\n- `GeneralTensorShapeOpQuantizeHandler`\r\n- `CustomModuleQuantizeHandler`\r\n- `StandaloneModuleQuantizeHandler`\r\n\r\nThe following classes under the torch.ao.quantization.fx.fusion_patterns namespace are migrated to the torch.ao.quantization.fx.fuse_handler\r\nnamespace:\r\n\r\n- `DefaultFuseHandler`\r\n- `FuseHandler`\r\n\r\n### **Remove public APIs under the `torch.ao.quantization.fx.backend_config_utils` namespace(#89810)**\r\n\r\nThe following APIs that were mistakenly public under the `torch.ao.quantization.fx.backend_config_utils` namespace are removed in this commit.\r\n\r\n- `get_quantize_handler_cls`\r\n- `get_fusion_pattern_to_fuse_handler_cls`\r\n- `get_native_quant_patterns`\r\n- `get_pattern_to_quantize_handlers`\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\nfrom torch.ao.quantization.fx.backend_config_utils import (\r\n    get_quantize_handler_cls,\r\n    get_fusion_pattern_to_fuse_handler_cls,\r\n    get_native_quant_patterns,\r\n    get_pattern_to_quantize_handlers,\r\n)\r\nall_quant_patterns = get_native_quant_patterns()\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nfrom torch.ao.quantization.fx.quantization_patterns import (\r\n    _get_quantize_handler_cls,\r\n    _get_pattern_to_quantize_handlers,\r\n)\r\nfrom torch.ao.quantization.fx.fusion_patterns import (\r\n    _get_fusion_pattern_to_fuse_handler_cls,\r\n)\r\nfrom torch.ao.quantization.backend_config import (\r\n    get_native_backend_config,\r\n)\r\nall_quant_patterns = _get_pattern_to_quantize_handlers(\r\n    get_native_backend_config()\r\n)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Update torch.{slice|select|diagonal|as_strided}\\_scatter ops to preserve input stride/storage_offset (#91029)**\r\n\r\nThese operators are primarily used by the [functionalization pass](https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965), used in AOTAutograd. Previously, they would always return contiguous tensors. Now, they return a tensor with the same striding as their first argument.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> x = torch.ones(2, 2, 2)\r\n>>> base = x[:, :, 1]\r\n>>> base.stride()\r\n(4, 2)\r\n>>> x = torch.zeros(2, 2, 2)\r\n>>> base = x[:, :, 1]\r\n>>> base.stride()\r\n(4, 2)\r\n>>> torch.diagonal_scatter(base, torch.ones(2)).stride()\r\n# returns a tensor with same strides as base.\r\n(4, 2)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> x = torch.ones(2, 2, 2)\r\n>>> base = x[:, :, 1]\r\n>>> base.stride()\r\n(4, 2)\r\n>>> x = torch.zeros(2, 2, 2)\r\n>>> base = x[:, :, 1]\r\n>>> base.stride()\r\n(4, 2)\r\n>>> torch.diagonal_scatter(base, torch.ones(2)).stride()\r\n# returns a contiguous tensor\r\n(2, 1)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n### **Remove ONNX deprecated monkey patches to torch.Graph (#94747)**\r\n\r\nThe Deprecated monkey patches to `torch.Graph`, `torch.Block` and `torch.Node` are removed\r\n\r\nMonkey patches to the classes `torch.Graph`, `torch.Block` and `torch.Node` from `torch.onnx` have been removed. This means the methods `torch.Graph.op()`, `torch..Graph.at()`, `torch.Block.op()`, `torch.Graph.constant()`, and `torch.Node.__getitem__` are no longer available.\r\n\r\nUsers creating custom symbolic functions for the `torch.onnx` exporter can continue to assume the `g.op()` interface for creating an operator in the graph, which is now exposed via the `GraphContext` class. Users should not assume any other methods from the `GraphContext` class other than those defined natively by `torch.Graph` and `.op()`.\r\n\r\nCode change to existing symbolic functions is not expected with this change.\r\n\r\n### **Add full checker mode in torch.onnx.export (#83186)**\r\n\r\nThis removes boolean value of `full_check` parameter in TORCH API `check_onnx_proto`, and forces `full_check` with warning messages if it fails.\r\n\r\nAlso, the API didn\u2019t check on types in the graph even with `full_check=True` previously. With the change, a warning message will show if the graph contains type error.\r\n\r\n### **C++ API specific BC-Breaking Changes:**\r\n\r\n#### **Deleted torch::deploy from PyTorch Core (#85953)**\r\n\r\n`torch::deploy` has been migrated to over to [MultiPy](https://github.com/pytorch/multipy). Ongoing development will continue in this repository.\r\n\r\n#### **Remove the use of `lazy::View` (#87822)**\r\n\r\nThe view and aliasing infrastructure in lazy tensor core has been deprecated in favor of functionalization.\r\n\r\n#### **Renamed `c10::fromIntArrayRef` to `c10::fromIntArrayRefSlow` and changed call sites (#86235)**\r\n\r\nThe function has been renamed to more accurately reflect its performance characteristics.\r\n\r\n# Deprecations\r\n\r\n## torch.func aka functorch\r\n\r\n### **We\u2019ve deprecated the functorch module in favor of the new torch.func module**\r\n\r\nWe\u2019re excited to announce that, as the final step of upstreaming and integrating functorch into PyTorch, the functorch APIs are now available in the torch.func module. Our function transform APIs are identical to before, but we have changed how the interaction with NN modules work.\r\n\r\nWe\u2019ve deprecated `functorch._` function transforms (e.g. `vmap`, `grad`, `jvp`) in favor of their identical `torch.func._ `counterparts (#92279).\r\nPyTorch has consolidated on `torch.func.functional_call` as the NN module functional API. Please migrate from `functorch.{make_functional, make_functional_with_buffers}` to it. For more details see this [Guide](https://pytorch.org/docs/master/func.migrating.html#functorch-make-functional)\r\nPlease migrate from `functorch.combine_state_for_ensemble` to `torch.func.stack_module_state`. For more details see this [Guide](https://pytorch.org/docs/master/func.migrating.html#functorch-combine-state-for-ensemble)\r\nWe are no longer supporting functorch.compile (also known as AOTAutograd) as a frontend for compilation in PyTorch; we have integrated AOTAutograd into PyTorch\u2019s compilation story. If you are a user, please use `torch.compile()` instead.\r\n\r\n## Python API\r\n\r\n### **Deprecate TypedStorage, its derived classes, and all of their public methods (#85303)**\r\n\r\nTyped storages have been removed from the C++ side and torch.UntypedStorage is used in place. The use of torch.TypedStorage and all of its subclasses is now deprecated.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\ntensor.storage()\r\ntorch.TypedStorage(...)\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\ntensor.untyped_storage()\r\ntorch.UntypedStorage(...)\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\nIf you need to access individual elements in a storage as a particular dtype, you can simply create a tensor to view it:\r\n\r\n```Python\r\ntorch.tensor(storage, dtype=...)\r\n```\r\n\r\n### **Deprecate `tensor.mT`,`tensor.T`,`tensor.mH`,`tensor.H` on 0D-tensors (#92143)**\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n>>> a = torch.tensor(10)\r\n>>> a.T\r\n>>> a.H\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\n>>> a = torch.tensor(10)\r\n>>> a.T\r\nUserWarning: Tensor.T is deprecated on 0-D tensors.\r\nThis function is the identity in these cases.\r\n>>> a.H\r\nUserWarning: Tensor.H is deprecated on 0-D tensors.\r\nConsider using x.conj().\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n## Autograd API\r\n\r\n### **Deprecate decorating classes with torch.no_grad (#89522)**\r\n\r\nDecorating classes with `torch.no_grad` is now deprecated. You should be decorating its functions or methods instead. To preserve the current behavior of class decoration, you can directly decorate the `__init__` method and nothing else.\r\n\r\n<table>\r\n<tr>\r\n<th>1.13</th>\r\n<th>2.0</th>\r\n</tr>\r\n<tr>\r\n<td>\r\n\r\n```Python\r\n@torch.no_grad()\r\nclass Blah():\r\n  pass\r\n```\r\n\r\n</td>\r\n<td>\r\n\r\n```Python\r\nclass Blah():\r\n  @torch.no_grad()\r\n  def __init__(self):\r\n    pass\r\n```\r\n\r\n</td>\r\n</tr>\r\n</table>\r\n\r\n## Linalg\r\n\r\n### **Remove the use of overload at::frobenius_norm(const Tensor&) (#81762)**\r\n\r\nIn continuation with the deprecation process from release 1.12 the tensor overload for this function has been removed. This function was not used in the bindings of Pytorch and should not impact users of `torch.norm`.\r\n\r\n## torch.nn API\r\n\r\n### **Canceling deprecation of `functional.{tanh, sigmoid}` functions (#86905)**\r\n\r\nBoth these ops are heavily used and so will not be removed. Deprecation warnings have been removed.\r\n\r\n### **Deprecated torch.nn.utils.stateless.functional_call in favor of torch.func.functional_call (#92280)**\r\n\r\nWe\u2019ve moved torch.nn.stateless.functional_call under the torch.func module to reflect how it is useful for working with nn.Modules in a functional style. As of PyTorch **2.0**, `torch.func.functional_call` is a drop-in replacement for `torch.nn.stateless.functional_call` and we will remove `torch.nn.utils.stateless.functional_call` in a future version of PyTorch. However, please note that we did change the default behavior of `torch.nn.stateless.functional_call` in PyTorch 2.0 (see \u201ctorch.nn.utils.stateless.functional_call now respects tied weights\u201d under BC-breaking notes).\r\n\r\n## Releng\r\n\r\n### **Deprecated private API torch.\\_six (#94709)**\r\n\r\nRemoved the Python 2 and 3 compatibility library six and future and torch.\\_six.\r\n**2.0**\r\n\r\n```Python\r\n# from torch._six import string_classes\r\nstr\r\n# from torch._six import int_classes\r\nint\r\n# from torch._six import inf, nan\r\nfrom torch import inf, nan\r\n# torch._six.string_classes\r\nstr\r\n```\r\n\r\n## Onnx\r\n\r\n### **Deprecated Caffe2 ONNX exporter support[ #95071](https://github.com/pytorch/pytorch/pull/95071)**\r\n\r\nUsers must use PyTorch 1.x versions to use Caffe2 ONNX exporter. This capability will be completely removed from PyTorch 2.x series.\r\n\r\n# New Features\r\n\r\n## torch.nn API\r\n\r\n- Add `torch.nn.functional.scaled_dot_product_attention()` to allow writing fast Transformer-like functions and use it to speed up `nn.Transformer()` ( #91362, #91066, #90413, #87312, #94008, #89470, #90776, #92189)\r\n- Add hooks for `Module.register_{buffer,module,parameter}` functions (#86148, #87369)\r\n- Add `Module.full_backward_pre_hook` (#86700)\r\n- Add `Module.state_dict_pre_hook` (#90435)\r\n- Add `Module.call_super_init: bool` flag that can be used to ensure `Module` initialization is properly calling parent\u2019s `__init__` (#91819)\r\n\r\n## torch.func\r\n\r\n- Add `functorch` support [for torch.autograd.Function](https://pytorch.org/docs/master/notes/extending.func.html): one is now able to apply function transformations (e.g. vmap, grad, jvp) over torch.autograd.Function. (#92023, #91452, #91222, #90037, #90077, #90966, #89860, #91211, #92030)\r\n- Add support for linearize a-la [jax.linearize](https://jax.readthedocs.io/en/latest/_autosummary/jax.linearize.html#jax.linearize) (#94173)\r\n- Add torch.func.functional_call, a new utility function to work with NN modules. (#89213)\r\n- Add torch.func.stack_module_state, a new utility function to help with model ensembling (#88850)\r\n\r\n## Cuda\r\n\r\n- Introduce CUDA Device Assertions Infrastructure (#84609)\r\n- `Logcumsumexp` for complex dtypes for CUDA (build-time optimized) (#94310)\r\n- Caching allocator tracing (#86241)\r\n- Add Pluggable CUDA allocator backend (#86786)\r\n- Add cudaMallocAsync as an alternative backend for the CUDA allocator (#82682)\r\n\r\n## Cpp API\r\n\r\n- Add `set_to_none` flag for C++ optim endpoint (#92989)\r\n\r\n## NestedTensor API\r\n\r\n- Add support for `tensor.to()` for NestedTensor backend (#87146)\r\n- Add backwards support for `gelu` and `relu` operators (#94776)\r\n- Add support for `torch.neg` operator (#88131)\r\n\r\n## Distributed\r\n\r\n- Distributed Tensor (Prototype Release)\r\n  - PyTorch [DistributedTensor](https://github.com/pytorch/pytorch/blob/master/torch/distributed/_tensor/README.md) (DTensor) is a prototyping effort with distributed tensor primitives to allow easier distributed computation authoring in the SPMD (Single Program Multiple Devices) paradigm. The primitives are simple but powerful when used to express tensor distributions with both sharded and replicated parallelism strategies. PyTorch DTensor empowered PyTorch [Tensor Parallelism](https://pytorch.org/docs/master/distributed.tensor.parallel.html) along with other advanced parallelism explorations. In addition, it also offers a uniform way to save/load state_dict for distributed checkpointing purposes, even when there\u2019re complex tensor distribution strategies such as combining tensor parallelism with parameter sharding in FSDP. (#88176, #88177, #88178, #88179, #88551, #88549, #88550, #89800, #89967, #89968, #89991, #90106, #90241, #90449, #90731, #90732, #90733, #90734, #90735, #91756, #91783, #91785, #91801, #91802, #92069, #92197, #92198, #92290, #92611, #92651, #92652, #92677, #93040, #93160, #93306, #93832, #93957, #94517, #94524)\r\n  - We also design and implement Tensor Parallel & 2D Parallel (Tensor Parallel + FSDP) on top of DistributedTensor. (#88180, #89242, #89467, #89535, #89779, #89878, #93029, #93412, #94421, #94748)\r\n- Distributed Checkpoint\r\n  - PyTorch Distributed Checkpointing (DCP) API was first introduced in PyTorch 1.13 and this will be an official prototype release in PyTorch 2.0. The distributed checkpoint API in PT2.0 decouples the storage layer from the checkpoint planning layer. Planner types are introduced to perform the coordination of storage both locally and globally to plan the save/load process. Checkpointing support for FSDP `sharded_state_dict` is added as well. (#87987, #88698, #89256, #89398, #89399, #89501, #89503, #89537, #89542, #89873, #89964, #90212, #91036, #91092, #91209, #91269, #92553, #92705, #92829, #92869, #92933, #94379, #94501)\r\n- DistributedDataParallel\r\n  - Enable DDP for PyTorch 2.0 (#87549, #88523, #89096, #88460, #88480, #88521, #94749, #93162, #89802, #92986)\r\n- FullyShardedDataParallel\r\n  - Add the option to use the original parameters via `use_orig_params=True` in the FSDP constructor (#84911)\r\n  - Enable the use of TorchDispatch with FSDP (#88014)\r\n  - Hybrid Sharded Data Parallel (#89915)\r\n  - Enable FSDP for PyTorch 2.0 (#88781, #89330, #89523)\r\n- Distributed (c10d)\r\n  - Dispatchable collectives: An improvement to the existing `init_process_group` API which changes backend to an optional argument. For users, this feature will allow for code that runs on both GPU and CPU machines without having to change the backend specification. The dispatchability feature will also allow users to perform both GPU and CPU collectives using the same ProcessGroup, as PyTorch will automatically find an appropriate backend for the tensor type (as of PyTorch 2.0, the default is NCCL for CUDA and Gloo for CPU). Existing backend specifications by users will be honored and will not require change (#83679, #83735, #83810, #83859, #83876, #83916, #84423, #86166, #86368, #86407, #86408, #86409, #88351, #88846, #88889, #88903, #89317, #89318, #89505, #89813, #88330, #91257, #91172)\r\n\r\n## Mps\r\n\r\n- Add native support for:`torch.nn.functional.group_norm`(#91190), `torch.var_mean` (#91190), `torch.nansym`(#93845), `torch.frac`(#86625), `torch.signbit`(#87214), `torch.exp1m`(#87147), `torch.cumsum`(#88319), `torch.trace`(#87910), `torch.nn.Hardswish` (#87952),`torch.inverse`(#90428), `torch.floor_divide`(#91126), `unfold`(#91266), `bincount`(#91267), `nonzero`(#91616), `norm_dtype`and`cdist`(#91643), `unique`and`unique_consecutive`(#88532), `nan_to_num`(#91110), `torch.linalg.cross`(#91642), `randperm`(#91708), `triangular_solve`(#94345), `grid_sampler2d`(#94273), `remainder`(#92139), `addr`(#94538), `fmod`(#94722), `repeat_interleave` (#88649),`sort`and`argSort`(#94697),`range` (#91075)\r\n- Add functions to handle rng and force device synchronization `torch.mps.{get_rng_state, set_rng_state, synchronize, manual_seed, seed}` (#94417)\r\n- Add support for the `mps` device for `torch.Generator` (#91348)\r\n- Add `torch.int64` support for unary ops (#86615)\r\n\r\n## Profiler\r\n\r\n- Improve Memory Profiler(alpha): enhancement to the existing memory profiler that can attribute memory consumptions to activations, gradients, parameters, and optimizer states (#86802, #86853, #86854, #86880, #87006, #87566, #87567, #87568, #88924, #88925, #88926, #89355, #89356, #86355, #88917, #87133, #86753, #86754, #87096, #86909, #87825)\r\n- Add Linux perf event support in profiler (#87866, #87874)\r\n\r\n## Foreach API\r\n\r\n- Implement:\r\n  - `torch._foreach_lerp` (#87562),\r\n  - `fused adamw` (#88015)\r\n  - `_foreach_addc`(div/mul)(\\_).Tensor (#88157)\r\n  - `clamp_min` `clamp_max` (#91384)\r\n  - `adamw` (#88015)\r\n\r\n## Mobile\r\n\r\n- Add XNNPACK Delegate Framework.\r\n  - Enable a XNNPACK graph to be built from the torchscript IR and performing checks (#86980, #87128, #87824)\r\n  - Add flatbuffer serialization support (#87826, #87906, #87907, #87908)\r\n  - Create `Executor` and `Compiler` classes which compiles the XNNPACK graph and preps for execution (#88779, #88778, #88780, #89090)\r\n  - Optimize library includes (#88863, #89231)\r\n  - Add Constant Data which will be used in Convolution (#89445)\r\n- Add support for better benchmarking\r\n  - Add support in lite_predictor benchmark binary to select event lists and perform benchmarking using Linux perf through Kineto profiler (#87876)\r\n  - List all missing ops at once (#94205)\r\n\r\n## Sparse API\r\n\r\n- Add `torch.sparse.check_sparse_tensor_invariants` context manager that allows users to opt into more checks at runtime for better debugging. (#92094)\r\n- Add `check_invariants` flag to `torch.sparse_coo/csr/csc/bsr/bsc/compressed_tensor `to allow users to verify components at construction time. (#92094)\r\n- Add `reduce` flag for CPU to torch.sparse.mm with support for `sum, mean, amax, amin` (#83727)\r\n\r\n## Optimizer API\r\n\r\n- Make `{Adadelta, Adagrad, Adamax, AdamW, ASGD, NAdam, RAdam, RProp}` differentiable (#86096, #86258, #86183)\r\n- Publicly expose \\_LRScheduler to LRScheduler (#88503)\r\n\r\n## Distributions\r\n\r\n- Add a transform for positive-definite matrices. (#76777)\r\n\r\n## Signals\r\n\r\n- Set up new module torch.signal.windows (#85599)\r\n- Add the Nuttall window to signals/ (#90103)\r\n- Implement old singal/windows in Python (#87082, #87330)\r\n\r\n## Quantization\r\n\r\n- Add support for oneDNN backend for server CPU quantized inference (#91056, #88478, #88665, #88668, #88879, #88923, #89188, #91297, #90262, #90364, #91152, #91153, #91154, #91155, #91934, #88661)\r\n- Add new \u2018x86\u2019 backend to be used for quantized CPU inference (#91235, #88799)\r\n\r\n## Vulkan\r\n\r\n- Add Vulkan support for several torch operators:\r\n  - `torch.abs` (#87414)\r\n  - `torch.select` for height and width dimensions (#94612)\r\n- Vulkan optimization passes now automatically apply data transfers between the CPU and GPU for input and output tensors (#87432)\r\n  - If the `requires_backend_transfers` flag of a model is set to `false`, then input tensors do not to be transferred to the GPU (via `tensor.gpu()`) and output tensors do not to be transferred back to the CPU (via `tensor.cpu()`) since these transfers are inserted into the model\r\n  - To avoid inserting data transfers into a model, add `MobileOptimizer.VULKAN_AUTOMATIC_GPU_TRANSFER` under `torch.utils.mobile_optimizer` to the `optimization_blocklist` argument of `optimize_for_mobile` (#92081)\r\n\r\n## ROCm\r\n\r\n- `hipGraph` support for pytorch mainline (#88202)\r\n\r\n## Fx\r\n\r\n- Introduce symbolic shape guards (#87570, #90528, #90665, #90679, #90876, #91058, #93894, #94782)\r\n- Introduce a match filter for SubgraphRewriter (#86430, #87998, #87257)\r\n- Support list-typed args in PatternMatcher (#88656)\r\n- Add `any_chain()` in operator support (#90949)\r\n- Have replace_pattern return replaced nodes (#90244)\r\n\r\n## Jit\r\n\r\n- Allow freezing JIT modules that contain mutable interfaces (#86039, #91020)\r\n- ApplyLinear-BatchNormNd folding during torch.jit.freeze (#86706)\r\n- Add an option to skip loading of debug traces, in order to reduce memory usage (#91430)\r\n- Introduce torch.jit.\\_drop function modifier to avoid compiling a method on a non-nn.Module class (#93012)\r\n- Allow providing a kwargs-like dict of example inputs to torch.jit.trace with the new `example_kwarg_inputs` argument (#81623, #94032)\r\n- Include example input shapes when serializing jit.traced modules to assist with debugging (#90744)\r\n\r\n## Build\r\n\r\n- Add Ada Lovelace (cuda arch sm8.9) support (#87436)\r\n- Add an option to disable TORCH_WARN and TORCH_WARN_ONCE log (#87188)\r\n- Enable memory map file support for Android, Apple, and CXX (#88545)\r\n- Support DNNL_GRAPH_CPU_RUNTIME=TBB build option (#87512)\r\n\r\n## ONNX\r\n\r\n- Verification tool to find mismatch in model export (#89946,[ #89807](https://github.com/pytorch/pytorch/pull/89807),[ #89808](https://github.com/pytorch/pytorch/pull/89808),[ #89947](https://github.com/pytorch/pytorch/pull/89947),[ #94648](https://github.com/pytorch/pytorch/pull/94648))\r\n\r\n## Cudnn\r\n\r\n- Add an environment variable to skip cudnn version compatibility check (#89184)\r\n- Enable cuDNN Frontend v8 API by Default (#91117)\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n- Set std/var correction overloads default value to None (#56398)\r\n- Implement correction argument in torch.masked.{std,var} (#87118)\r\n- Update `torch.squeeze` to allow squeezing multiple dimensions at once (#89017)\r\n- Add support for int32 indices in index/index_put ops (#86309)\r\n- Enable `where` to have cpu scalar args (#87022)\r\n- Add support for NumPy scalars to `torch.tensor.asarray` (#90914)\r\n- Update opt_einsum to have more reasonable defaults (#86985)\r\n- Improve error message for `Tensor.set_` when dtypes mismatch(#88804)\r\n- Enable out variant of `torch.max`(#85926)\r\n- Implement faster gradient clipping using foreach function (#91846)\r\n\r\n## Autograd API\r\n\r\n- Add backward support for `torch.ormqr` (#86800)\r\n- Pre-hooks registered on tensor are guaranteed to run before pre-hooks registered on grad_fn (#85849)\r\n- Add a new overridable method `setup_context` (#89859, #92312)\r\n  - You must use override this method if you plan to use your autograd Function with functorch\r\n  - If you choose to override this method, `forward` should no longer take ctx as an input.\r\n- Add context manager `torch.autograd.set_multithreading_enabled` for disabling multithreading in the autograd engine (#86245)\r\n- Add backward AD support for unary foreach functions (#89591)\r\n\r\n## torch.nn API\r\n\r\n- Add `remove_duplicate` flag to `Module.named_buffers()` method (#84984) and `Module.named_parameters()` (#88090)\r\n- Add kwarg support for `Module` forward-pre and forward hooks (#89389)\r\n- Improve error message for `Transformer()` fast path (#90783) and kernel selection (#90783)\r\n- Add support for `torch.bf16` for `Embedding` (#94163)\r\n- Add `freeze` argument to `Embedding()` (#86769)\r\n- Add `torch.channels_last_3d` support for `SyncBatchNorm()` (#88401)\r\n- Add `torch.bfloat16` support on CPU for `functional.{mish,hardtanh,silu}` (#82460)\r\n- Add support for inputs with different data types for `LayerNorm()` (#81851, #88064), `BatchNorm{1,2,3}d()` (#84410), `GroupNorm()` (#89485, #81852, #88663, #92671, #92668)\r\n- Improve printing of `ModuleList()` (#90452)\r\n- Add `torch.uint8` support for `functional.interpolate()` on CPU (#90771)\r\n- Make `functional.max_pool1d` error checking consistent between CPU and CUDA (#90211)\r\n- Add `SyncBatchNorm()` fallback to `BatchNorm()` when it is used in a non-distributed setting (#89706)\r\n- Add channels-last support for `GroupNorm()` on XPU (#87680)\r\n- Add `is_causal` kwarg to `TransformerEncoder()` layer (#90508)\r\n- Add `prepend` argument to `Module` hooks to register a hook that will be called before the existing ones (#87370)\r\n\r\n## Distributed\r\n\r\n- Activation checkpointing\r\n  - Return `None` from `apply_activation_checkpointing` (#87871)\r\n  - Enable non-reentrant support for `checkpoint_sequential` (#86331)\r\n  - Separate CPU offload activation to its own wrapper (#85459)\r\n- DistributedDataParallel\r\n  - Add `PackedSequence` support when `device_ids` is specified (#86614)\r\n  - Enable DDP to handle custom dataclass forward outputs (#92334)\r\n- Distributed (c10d)\r\n  - Add sequence number support for UCC PG (#85047)\r\n- FullyShardedDataParallel\r\n  - Default to `BACKWARD_PRE` for the backward_prefetch of FSDP (#88428)\r\n  - Skip collective communications for `NO_SHARD` in `clip_grad_norm_` (#89137)\r\n  - Allow handle training state to be both `BACKWARD_PRE` and `BACKWARD_POST` in the post-backward assert (#89791)\r\n  - Limit all gather after pre-unshard (#89057)\r\n  - Include module classes in `ModuleWrapPolicy.__repr__` (#89058)\r\n  - Apply the \"largest\" dtype across all parameters/gradients as defined by PyTorch's type promotion semantics for the total norm returned in `clip_grad_norm_` for low prec grads (#90028)\r\n  - Introduce `ModuleWrapPolicy` for simplicity in FSDP autowrap (#88450)\r\n  - Enable mixed hybrid/non-hybrid sharding strategies (#90846)\r\n  - Re-support model dtype change after FSDP init (#91192)\r\n  - Enable `use_orig_params=True`, `no_sync` and mixed precision to work together (#91193)\r\n  - Enable `summon_full_params(with_grads=True)` (#85738, #87314)\r\n  - Add `keep_low_precision_grads` support when CPU offloading (#86495)\r\n  - Consolidate FSDP `state_dict` `offload_to_cpu` settings (#86211)\r\n  - Add `set_state_dict_type` API to setup `state_dict_type` without using context manager (#86243)\r\n  - Enable the support of `use_orig_param` for FSDP\u2019s `optim_state_dict` (#89898, #89899, #89900)\r\n  - Enable nested FSDP wrapper to use different mixed precision (#90523)\r\n  - Enable input cast skip in MixedPrecision (#90620)\r\n  - Publish `optim_state_dict` and `optim_state_dict_to_load` for FSDP (#90798, #91343, #92744, #92118, #92991, #92992, #93285, #93318, #94109, #94129)\r\n  - Make default input casting in root module only and enable the ability to set different mixed precisions for different submodules (#91365)\r\n- Torch Elastic\r\n  - Update `torchrun` and `TorchElastic` to take optional `local_addr` param to allow skip local IP lookup if specified (#88922)\r\n\r\n## torch.func\r\n\r\n- Update vmap to accept None(s) in out_dim (#91644)\r\n- torch.func.jacrev: Support chunked computation (#89376, #91326)\r\n- vmap: chunk_size support (#91157)\r\n- torch.vmap: Implement checks (rather than internal asserts) for vmap escaped errors (#89585)\r\n- Avoid calling allclose in the backward if there are tensor subclasses (#91444)\r\n- Refactor NN stateless APIs by swapping module tensors (#92536)\r\n\r\n## Cuda\r\n\r\n- Use binary units for CUDA memory summary (#91854)\r\n- Improve perf by avoiding implicit string creation in c10_cuda_check_implementation (#88350)\r\n- Add option to record C++ backtraces in \\_record_memory_history (#86145)\r\n- Set CUDA_MODULE_LOADING to LAZY when not set by the user (#85692)\r\n- Add warning if captured graph is empty (#88754)\r\n- Add option to dump a captured graph for debugging (#85519)\r\n- Add support to foreach torch zero for bfloat16s (#90437)\r\n- Enable bfloat16 for hardtanh_backward_cuda (#91511)\r\n- Use pytree to allow any input format for cuda graph (#90941)\r\n- Add requested_bytes to CUDA Caching Allocator Stats (#88575)\r\n- Add an option to disable reduced precision reductions for BF16 GEMM (#89172)\r\n- Add an env variable to disable addmm_cuda_lt kernel (#91436)\r\n\r\n## Serialization\r\n\r\n- Add XPU backend to support torch.save and torch.load (#89679)\r\n\r\n## Cpp API\r\n\r\n- Reduce ambiguity in Tensor namespace collisions (#92266)\r\n\r\n## Dataloader API\r\n\r\n- Add support for pin memory on xpu device (#86545)\r\n- Add type annotation to `get_worker_info` (#87017)\r\n- Allow prefetch factor to be optional (#88972)\r\n\r\n## NestedTensor API\r\n\r\n- Add add/mul for nested dense [B, *, D], [B, 1, D] case (CUDA-only) (#88289)\r\n- Add support for torch.select over irregular dimensions (#88585)\r\n- Add torch.nested.nested_tensor() constructor (#88213)\r\n\r\n## Complex API\r\n\r\n- Improve complex support for: `torch.nn.functional.conv_transpose3d `(#87967), `torch.log1p` (#89214,#90422), `torch.lerp` (#75584), `torch.logcumsumexp` for CPU (#93153)\r\n- Solve under/overflow for complex division (#92539)\r\n\r\n## Composability\r\n\r\n- Improve coverage of primtorch and torch.\\_ref decompositions: `prims.clone` (#86705), `ndtr, ndtri, log_ndtr, erfcx` (#86077), `NLL loss` (#81128), `conv backward` (#87047), `xlogy and xlog1py` (#77712), `alpha_dropout` (#87989)\r\n- More operations now work with meta tensors: `_adaptive_avg_pool2d_backward` (#86359), (#87074), `avg_pool2d and avg_pool2d_backward` (#87043), `scalar_tensor and argmax` (#88590), `topk` (#88694), `max_pool2d_with_indices_backward` (#88743), `grid_sampler_2d_backward` (#88745), `linalg_cholesky` and `linalg_cholesky_ex` (#89430), `aten._cdist_forward` (#90042), `aten.pixel_shuffle` (#91605)\r\n\r\n## Linalg API\r\n\r\n- Fix typos in messages under aten (#88964)\r\n\r\n## Mobile\r\n\r\n- Improve CoreML logging and dependent libraries.\r\n  - Updated Cocoapods (#88075)\r\n  - Preserved CoreML errors by using special throw macro when encountering CoreML API errors (#86938)\r\n- Clean Up MobileOptimizerType Rewrite Flags Public API and Documentation (#91600)\r\n- Clean up flatbuffer lib dependency and fixed its test to match pkl models (#86041, #93022)\r\n- Type corrections to avoid unnecessary static_casts (#93898)\r\n- Add flake8-logging-format linter (#90805, #94840)\r\n\r\n## Sparse API\r\n\r\n- Add autograd support for `linear` (#86137, #86302), `mm`, `log1p`(#86301, #88155), `to_sparse_*`(#90281)\r\n- Improve support for `sparse_dim`, `dense_dim` (#86203, #86203), `torch.sum`(#86300, #92979), torch.sparse.sampled_addmm`(#86401),`frac`, `deg2rad`, `rad2deg`, `relu`(#88153, #88156, #88442, #86749),`conj()`(#91695),`to_sparse`(#90718),`sparse_mask` (#92248, #94829)\r\n- Add support for per batch index contiguity in CSR/CSC/BSR/BSC (#91243), non-contiguous values in CSR/CSC/BSR/BSC (#91243), non-zero dense_dim to COO/CSC/BSR/BSC/Strided conversions. (#90177), uncoalesced operands to `sparse_mask` (#91964)\r\n- Improve error messages for `indices, values, (c)row_indices, (c)col_indices` (#93149) and `addmm` (#94843)\r\n- Extend gradcheck to BSR and BSC inputs. (#90719)\r\n- Sort BSR indices as part of CSR to BSR conversion (#90918)\r\n\r\n## Cpu\r\n\r\n- Implement aten::native_batch_norm.out for CPU (#88604)\r\n- Log1p for complex in CPU (#89691)\r\n- Enable oneDNN implementation for LSTM (#91158)\r\n\r\n## Package\r\n\r\n- Add better debugging for torch.package (#92939)\r\n\r\n## Quantization\r\n\r\n- Remove weight arg from DTypeConfig for non-weighted ops (#86335)\r\n- Add get_symmetric_qnnpack_qconfig_mapping for XNNPACK quantized ops (#87002)\r\n- Add assert for backend correctness in get_default_qconfig related apis (#86259)\r\n- Replacing List[QConfigMapping] in parallel numeric profiler (#86922)\r\n- Check the fixedqparam op qconfig based on backend_config (#87425)\r\n- Explicitly set default quantized engine instead of relying on the order of supported_qengines (#89804)\r\n- Support setting qconfig by module_type in QConfigMapping in PT 2.0 export flow (#92355)\r\n- Migration of quantization code from torch._ to torch.ao._ (#86171, #86172)\r\n- Improvements to qnnpack fully connected sparse ops (#85243, #85244, #85245, #85246, #85247)\r\n- Support lowering of channel shuffle in FX (#83731)\r\n- Remove explicitly default QConfigMapping settings (#90066)\r\n- quant: make various configs printable (#91419)\r\n- Enable FX quant for patterns like x.view(x.size(...), ...) (#90001)\r\n- X86 qengine always uses fbgemm kernels on OS other than Linux (#93218)\r\n- Change prepare_fx and convert_fx to preserve the GraphModule type of input (#94412)\r\n- update xnnpack to newer version and update API usage in pytorch (#94330)\r\n- Remove \\_input_output_observed from backend_config (#92589)\r\n- Add support for LSTM Structured Pruning prune_functions + pattern (#90801)\r\n- Enable FX static quantization for LSTM (#85068)\r\n- Allow setting fixed quantization params for inner LSTM ops (#88456)\r\n- Add support for GRU in fx graph mode quantization (#91976)\r\n\r\n## ONNX\r\n\r\n- Operator support `col2im` opset 18 (#84594), `mse_loss` (#90717), `aten::contains` (#91660), src/index dynamic axes support for `aten::scatter_add` (#90090), `aten::zero` (#91731), Raise Unsupported for `GridSample` with volumetric 5D input (#92212)\r\n- Pretty print diagnostic logging (#88261)\r\n- Bump onnx to 1.13.1, onnxruntime to 1.14.0 (#90332,[ #94767](https://github.com/pytorch/pytorch/pull/94767))\r\n- Add full graph checker option for `torch.onnx.export` API (#83186)\r\n- Integrate all ONNX operators with a new `JitScalarType` API (#87245)\r\n- Add `share_from_this` to `torch::jit::Graph` (#87343)\r\n- Use optional op to keep None in results for ONNX internal tests (#84789)\r\n- Add support for autograd function inlining in `ONNX_ATEN_FALLBACK` mode (#85736)\r\n- Default runtime type checking to raising errors (#86555)\r\n- Remove the `INT64_MAX` magic numbers (#88341)\r\n\r\n## Fx\r\n\r\n- Refactor graph partition to check for cyclic dependency (#86511)\r\n- Enable nvprims.transpose fusions for nvFuser (#86967)\r\n- Simplify magic method definition code. (#88017)\r\n- Add sym_floor, sym_sqrt, sym_int (#88760)\r\n- Propagate .meta info when replacing subgraphs in fx (#87255)\r\n- Make `torch.fx` compatible with Python-3.11 (#92895)\r\n- Add type(module) to be stored in the module stack (#87149)\r\n- Ensure that symbolic variables incorporate fresh constraints before they're used (#87254)\r\n- Add type annotation to `getitem` node before `split_module` (#88510)\r\n- Implement pass for annotating getitem nodes (#90237)\r\n- Guard Symbol and ShapeGuardPrinter behind HAS_SYMPY (#90704)\r\n- Copy meta field in fx.GraphModule on deepcopy (#92062, #92623)\r\n- Match get_attr when comparing nodes (#91657)\r\n- Make **deepcopy** of fx.GraphModule handle circular reference. (#93038)\r\n- Populate memo in deepcopy BEFORE copying children. (#93295)\r\n\r\n## Mps\r\n\r\n- Add fp16 support for `torch.nn.Linear` (#89774), `torch.nn.GELU` (#86218)\r\n- Add support for empty Tensors in `torch.bitwise_not` (#87286), `torch.nn.LayerNorm` (#94212), many backward functions (#94343), `torch.nn.functional.hardswish` (#94342), `torch.topk` (#91884), `torch.arange` (#94485), `torch.linal.inv` (#94551),\r\n- Improve error message for `nn.Conv2d` when inputs are on different devices (#86303)\r\n- Add support via fallback for `torch.nn.{Fold, UnFold}` (#94491)\r\n- Add support for reduction ops on multiple axis at a time (#91734)\r\n- Add support for `k` greater than 16 for `torch.topk` (#94639)\r\n\r\n## Build\r\n\r\n- Add @pytorch in tools/bazel.bzl (#91424)\r\n- Change visibility for //c10:headers (#91422)\r\n- Simplify OpenMP detection in CMake (#91576)\r\n- Use `@pytorch//` in bazel build files which improves embedding usecases (#89660)\r\n- Enable `USE_CUDA `for bazel build (#92640)\r\n- Add missing default initializers to class members (#94049)\r\n\r\n## Jit\r\n\r\n- Skip builtins while enumerating class methods (#91805)\r\n- Support lovelace for NVRTC (#87611)\r\n- Expanded symbolic shape support (movedim) (#91696)\r\n\r\n## Releng\r\n\r\n- Update CI test environment; Add symbolic functions (#94564)\r\n- Import `Literal`, `Protocol`, and `Final` from standard library `typing` as of Python 3.8+ (#94490)\r\n- Add cpuinfo to collect_env.py for new issues reporting which helps triaging on CPU (#93899)\r\n- Refactor nvfuser build (#89621)\r\n- Add error checking to flaky test bot platform parser (#86632)\r\n- Make LazyGraphExecutor extensible (#87218)\r\n- Delete BUILD_SPLIT_CUDA option (#87502)\r\n- Use faster cache flush in triton benchmarking (#88557)\r\n- Guard global observer init against Edge profiler (#86347)\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n- Fix as_strided_scatter derivative formula(#87646)\r\n- Add bfloat16 support to torch.prod (#87205)\r\n- Disable dimension wrapping for scalar tensors (#89234)\r\n- Fix SIGSEGV on a big-endian machine when reading pickle data (#92810)\r\n- Fix BC-breaking change to reduction arguments `amin`/`amax` (#93091)\r\n- Fix incorrect tensor storage check (#86845)\r\n- Ensure einsum contracts left to right (#87199)\r\n- Add nondeterministic error for `torch.tensor.scatter` (#88244)\r\n- Fix multi-index for `torch.tensor.index_select` over scalar tensor (#94347)\r\n- Add scalar support for `torch.tensor.where` (#92849)\r\n- Improve error message for unsupported argument types (#87601)\r\n- Change as_strided_scatter\u2019s storage offset default to None from 0 (#87481)\r\n- Make `torch.histc` consistent between CPU and CUDA (#87832)\r\n- Add float to list of allowed ops for serialization (#94910)\r\n- Fix numpy1.24 deprecations in unittests ([#93997] (https://github.com/pytorch/pytorch/pull/93997))\r\n- Properly moving segment_reduce to be private as expected (#93166)\r\n\r\n## Autograd API\r\n\r\n- Fix behavior of hooks registered to Tensors that had previously been modified in-place (#92734)\r\n  - Previously hooks registered to a tensor after it is modified in-place would erroneously receive the gradients of the output w.r.t. to that tensor before it is modified in-place if that tensor had previously had a hook registered to it before it was modified in-place.\r\n  - See [documentation](https://pytorch.org/docs/2.0/notes/autograd.html#behavior-of-tensor-hooks-when-tensor-is-modified-in-place) for more details about backward hooks execution when tensors are modified in-place.\r\n- Update saved variable hooks to no longer trigger on wrapped numbers (#87316)\r\n- Modifying a view created in no-grad mode in-place no longer triggers an internal assert (#88243)\r\n- Improve error message when saved tensor is detached inplace (#88860)\r\n- Prevent module full_backward_hook from erroring in double backward (#88357)\r\n- Fix forward AD custom Function non-differentiable outputs (#90787)\r\n- Don't materialize forward grad for non-differentiable types (#91183)\r\n- Return input as-is if marked dirty even when requires_grad=False (#91214)\r\n- Fix saved tensor hooks to propogate errors back to python as-is (#94456)\r\n- Fix NumPy broadcasting for backward of `linalg.solve` (#91456), `linalg.lstsq` (#91460)\r\n- Fix torch.var backward when input numel == correction (#94546)\r\n- Fix CopySlices logic to ensure wrapped node runs properly. (#89812)\r\n\r\n## torch.nn API\r\n\r\n- Fix for RNN-like `Module`s to work with `stateless.functional_call()` (#91111), better error messages (#87442),\r\n- Add missing dim checks `EmbeddingBag` (#85433)\r\n- Fix `Upsample` and `EmbeddingBag` module printing (#93850)\r\n- Fix segfaul in `Conv3D` CPU implementation (#94325)\r\n- Fix overflow issue in `Upsample` (#94290)\r\n- Fix `functiona.pixel_{shuffle,unshuffle}` to consistently return views or not (#86608)\r\n- Fix 64bit indexing `Conv3d()` (#87527), `Upsample()` (#87901)\r\n- Fix preserving requires_grad-ness in fusion utils (#89100)\r\n- Fix support for empty inputs/outputs for `Conv{1,2,3}d()` (#86521), `functional.adaptive_{avg, max}_pool()` (#88906)\r\n- Fix buffer overflow in `Upsample()` (#89252), `MaxUnpool3d()` (#94372)\r\n- Fix `functional.grid_sample()` loss of precision for `torch.float16` inputs (#90427)\r\n- Fix `functional.interpolate()` bicubic interpolation to properly preserve memory format (#90470)\r\n\r\n## torch.func\r\n\r\n- Fix cross to match unbatched behavior (#86926)\r\n- Properly error on complex inputs or outputs in jacrev, jacfwd (#94805)\r\n- Fix batching rule for dropout (#92975)\r\n- Fix vmap and anomaly mode interaction (#92672)\r\n- Fix and update type hints for `make_functional.py` (#91579)\r\n- torch.tril & torch.tril : add out of bound checks (#89384)\r\n- Fix torch.cat batching rule (#86932)\r\n- Fix reduction boxed batching rules (#91109)\r\n\r\n## Cuda\r\n\r\n- Check SM version before calling flash attention with BFloat16 (#86600)\r\n- Add range check to multi margin loss target (#89008)\r\n- Fix NVML visible device parsing (#92315)\r\n- Take `CUDA_VISIBLE_DEVICES` into account for nvml calls (#94568)\r\n- Fix topk IMA (#93095)\r\n- Fix: half reduction with multiple sub-iterators (#85596)\r\n- Fix segfault when swapping custom allocator (#89613)\r\n- Conditionally set device in autograd engine (#91191)\r\n- Store `autocast_gpu_dtype` in `custom_fwd` and `custom_bwd` for BFloat16 autocast (#88029)\r\n- Do not use at::cuda::getDefaultCUDAStream() (#91180)\r\n- Ensure that our error handling runs with the GIL enabled (#92848)\r\n- Fix C10_CUDA_CHECK for failing to capture last cuda error occasionally (#93192)\r\n- Fixes a memory leak by making autocast cache global instead of thread-local (#86492)\r\n- Take `CUDA_VISIBLE_DEVICES` into account for nvml calls (#94568)\r\n- Explicitly set the workspace for cuBLAS handles (#86645)\r\n\r\n## Cpp API\r\n\r\n- Fix CUDNN_PATH handling on Windows (#88898)\r\n- Fix typos in warning/error messages(#88961)\r\n- Remove uneeded checks from embedding bag impl (#92982)\r\n- Fix c++ : segfault in modulelist and moduledict (#93074)\r\n\r\n## Visualization\r\n\r\n- Fix overflow issue in tensorboard image summary (#90423)\r\n- Remove deprecated call to tf.io.gfile.get_filesystem (#89832)\r\n\r\n## NestedTensor API\r\n\r\n- Enable non-contiguous Nested Tensors for BMM inputs for NT on CUDA (#88108), linear backward (#94317)\r\n- Fix bug in unsqueeze_nested stride calculation (#88688)\r\n\r\n## Distributed\r\n\r\n- Distributed(c10d)\r\n  - Fix a static initialization order fiasco in c10d (#90149)\r\n  - Fix `send`, `recv` return type (#92152)\r\n  - Fix MPI backend PG initialization (#92847)\r\n  - Fix header-filter for clang-tidy c10 and apply some fixes to c10 and c10d (#91178)\r\n  - Fix `backend_type` for backend/PG plugin (#93129)\r\n  - Fix UCC PG barrier (#86961)\r\n  - Properly finalize unsuccessful UCC collective posts (#89306)\r\n  - Add pre & post processing for UCC CPU collectives (#89030)\r\n  - Re-enabl `isinstance` with `torch.distributed.ReduceOp` (#87303, #88275)\r\n  - Ameliorate custom `__eq__` for `ReduceOp` (#90088)\r\n  - Fix warning if backend registers timer (#91702)\r\n- DistributedDataParallel\r\n  - Fix DDP when the number of output features is zero (#87793)\r\n- FullyShardedDataParallel\r\n  - Fix `use_orig_params=True` for reentrant activation checkpointing by disabling the post-backward hooks (#87413)\r\n  - Re-establish the wrapped module in `_lazy_init` in case module changing after FSDP constructor (#87837)\r\n  - Fix the incorrect norm calculation for `NO_SHARD` by handling sharded and non-sharded parameters differently in `FSDP.clip_grad_norm_` (#88955)\r\n  - Pass through `ActivationWrapper` directly to the inner wrapped module to fix `state_dict` issues (#87950)\r\n  - Remove the clean of FQNs even for `use_orig_params=True` in FSDP (#91767, #92662)\r\n  - Restrict meta model check to non ignored modules in FSDP (#86766)\r\n  - Fix `keep_low_precision_grads=True` for `use_orig_params=True` (#90027)\r\n  - Fix for `use_orig_params=True` + `no_sync` (#90546)\r\n  - Fix `no_sync`, `use_orig_params=True`, mixed precision, sharded (#92874)\r\n  - Fix input grad propagation when using param mixed precision (#90921)\r\n  - Fix `_mp_shard` in `record_stream` (#91096)\r\n  - Fix \"use-after-free\" in reshard logic (#94859)\r\n  - Fix `clip_grad_norm_` issues (#94835), (#86337)\r\n  - Fix `load_sharded_state_dict` FQN mismatches for shared parameters (#86524)\r\n  - Fix grad zero vs. `None` edge case (#87308)\r\n  - Fix FSDP `state_dict` transformations of modules with persistent buffers failure with mixed precision enabled (#93396)\r\n  - [FSDP] Fix `nn.Parameter` usage for 2D and `use_orig_params=True` (#89782, #89845, #90562)\r\n- RPC\r\n  - FFixixed use after free in tensorpipe agent (#87627)\r\n- Torch Elastic\r\n  - Make TorchElastic timer importable on Windows (#88522)\r\n- Tensor parallel & 2D parallel\r\n  - Fix the logic to trigger load hooks for 2D parallel integration with FSDP. (#86272)\r\n\r\n## Profiler\r\n\r\n- Minor bug fixes for ROCM tracing (#89785, #88207)\r\n\r\n## Foreach API\r\n\r\n- Fix `_foreach_norm` on some tensor sizes (#91844)\r\n- Exempt `_foreach_norm` from autograd_not_implemented_fallback check (#93995)\r\n\r\n## Complex API\r\n\r\n- Fix serialization of `conj` and `neg_view` (#88182)\r\n\r\n## Linalg API\r\n\r\n- Add empty tensor check to \\_compute_linear_combination (#94245)\r\n\r\n## Optimizer API\r\n\r\n- Fix discrepancy between mt vs st impl (#92699)\r\n- Do NOT inplace modify gradients (#92706)\r\n- Fix memory leak in \\_LRScheduler.step() (#85602)\r\n- Look up `group[\"capturable\"]`, not `defaults[\"capturable\"]` in Adam(W) (#94149)\r\n- `FusedAdam(W)` should take `OptState` into account before unscaling grads (#94060)\r\n- Fix LinearLR scheduler start_factor (#86695)\r\n- Keep AveragedModel buffers in sync when use_buffers=False (#84054)\r\n- Fix OneCycleLR error log (#92040)\r\n- Fix SparseAdam consuming iterator (#86210)\r\n- Fix empty grad support for SparseAdam (#86459)\r\n\r\n## Serialization\r\n\r\n- Fix set pickle_module if not specified (#88570)\r\n- Explicitly check filelike arg of `torch.save` (#88867)\r\n- Fix dtype mismatch for unallocated storage deserialization (#91285)\r\n- Add float to list of allowed ops (#94910)\r\n\r\n## Composability\r\n\r\n- Fix segfault in has_torch_function (#88559)\r\n- Fix for usages of **torch_dispatch** with operators that take in an OptionalTensorList argument (#88887)\r\n- Allow direct Tensor constructor to return preexisting PyObject (#92754)\r\n- Add fallthrough kernel for AutogradMeta key (#94603)\r\n- Several fixes to existing primtorch and reference decompositions:\r\n  - `cat`: fix striding (#89332)\r\n  - `prelu`: Fix prelu ref when a.ndim &lt; 2 (#89809)\r\n  - `huber_loss_backward` fix (#86955)\r\n  - `uniform` fix (#90094)\r\n  - `unfold_copy` fix (#86371)\r\n- Fix aliasing for primtorch view meta kernels (#86285)\r\n- Properly compute device for elementwise operations with CPU scalar tensor (#93073)\r\n- Several fixes to existing operators\u2019 meta tensor kernels:\r\n  - aten.\\_embedding_bag (#92549)\r\n  - aten.fill\\_ (#87493)\r\n  - `aten.group_norm` type promotion fix (#86607)\r\n  - aten.\\_cudnn_rnn (#91333)\r\n  - aten.bernoulli (#88676)\r\n  - unsqueeze\\_ (#88675)\r\n- Several bug fixes as part of hardening functionalization, which is used in AOTAutograd:\r\n  - fix detach() in functionalization (#87750)\r\n  - fix `torch.as_strided_scatter_backward` memory initialization (#88342)\r\n  - fix functionalization resize stride compute (#94018)\r\n  - fix x.is_contiguous(channels_last) in functionalization (#94195)\r\n  - fix set\\_() with functionalization (#90722)\r\n  - check for undefined tensors in advanced indexing during functionalization (#90791)\r\n  - fix some composite compliance ops for functionalization (#86470)\r\n  - Make `aten.copy` preserve strides (#89464)\r\n\r\n## Sparse API\r\n\r\n- Fixes to `torch.mm`: (#90763), (#90917), (#91094)\r\n- Fix CSR to CSC conversion when given indices of int32 dtype (#91061)\r\n- Fix `mul` when given CUDA CSR Tensor and scalar (#91239)\r\n- Fix conversion from CSC, BSC to COO to only result in coalesced Tensors when appropriate (#91440)\r\n- Fix numel after resizing a CSR/BSR/CSC/BSC tensor. (#91831)\r\n- Fix `torch.triangular_solve` for CSR on CPU when `unitriangular=True`. (#93352)\r\n\r\n## Distributions\r\n\r\n- Fix philox randn to follow standard normal distribution (#91945)\r\n\r\n## Cpu\r\n\r\n- Fix access to uninitialized memory in VSX vector functions (#89833)\r\n- Fix buffer overflow from AddressSanitizer checks due to inaccurate bfloat16 representation of large integer (#89210)\r\n- Make torch.histc ignore NaNs on CPU (consistent with CUDA) (#85870)\r\n- Fix vectorized trigonometric functions for VSX (#86453)\r\n- Call `symint::sizes()` instead of `sizes()` on convolution error messages. (#89549)\r\n- Make `torch.linspace` result on CPU consistent with numpy (#89048)\r\n- Remove variable_excluded_from_dispatch() assertion from mkldnncommon (#92168)\r\n- `exponential_` few fixes (1) lambda > 0 (2) mkl kernel to continuous (3) better error log on dtype (#92891)\r\n- Vectorize more stable complex division (#93277)\r\n- `cauchy_` few fixes (1) check gamma > 0 (2) better dtype error log (#93314)\r\n\r\n## Intel\r\n\r\n- Fix CPU autocast for torch.cat due to the new type ITensorListRef (#87756)\r\n- Add parameters check for torch.\\_mkldnn_transpose (#85318)\r\n- Fix build with Intel compiler due to c10/util/TypeIndex.h (#89610)\r\n\r\n## Package\r\n\r\n- Treat builtins as default extern module (#88385)\r\n- Support pickle version 4 by adding missing ops (#90223)\r\n- Check spec for module source before falling back to file in package exporter (#90258)\r\n\r\n## Quantization\r\n\r\n- Fix the call to get_executorch_backend_config (#86338)\r\n- Fix weight_dtype and bias_dtype backend_config checks (#86719)\r\n- Respect non_leaf_module_list for activation modules (#88498)\r\n- Fix incorrect integer cast on histogram observer bounds (#90355)\r\n- Improve numerical stability of HistogramObserver (#86522)\r\n- Quant_min typo bugfix in utils.py (#88024)\r\n- Fix fuse_func method overwrite (#87791)\r\n- Fix get_default_qat_qconfig for PT 1.13 (#88876)\r\n- Check the value of numel to avoid segfault (#81547)\r\n- Fix mkldnn quantization issue for weight reorder error (#86876)\r\n- Fix Memory Leak in QNNPACK QSoftmax Op (#89544)\r\n- Copy MHA's batch_first attribute in prepare() (#91680)\r\n- Fix for swap_custom_module_to_observer doing duplicate swaps on the same node.target (#91905)\r\n\r\n## Fx\r\n\r\n- Correctly restore pybind11 error_already_set (#93238)\r\n- Remove proxy tensor's check for data dependent output (#93265)\r\n- Make ShapeEnv deepcopy-able (#93403)\r\n- Fix SubgraphMatcher for case of no anchor found (#86421)\r\n- Fix for partitioner with symbolic shapes (#86425)\r\n- Fix getitem in partitioner and make metadata storage more consistent (#87012)\r\n- Fix magic method try reverse protocol (#88030)\r\n- Fix FakeTensorProp on Module with Parameters or Buffers (#88700)\r\n- Fix PassManager to not use a class variable mutable list (#89108)\r\n- Prevent tracing when we track_tensor_tree (#89139)\r\n- Make all `make_fx` invocations isolated (opaque to higher `make_fx` invocations) by default (#93290)\r\n- Fix matching args in PatternMatcher (#94375)\r\n- Allow FakeTensorProp to run on graphs traced with some None inputs (#94569)\r\n- Copy codegen in legalize_graph (#90023)\r\n- Fix proxy unwrapping for cond() (#91907)\r\n\r\n## ONNX\r\n\r\n- Fix `triu`/`tril` operator export with diagonal input (#86843)\r\n- Skip tensor printing during model tracing (#86223)\r\n- Fix `aten::index_put(self, mask, v)` export when `rank(mask) &lt; rank(self)` (#92862)\r\n- Fix 0d-tensor broadcast export (#87211)\r\n- Fix device type detection based on strings (#86168)\r\n- Fix `scatter_add` with different static shape of src and index (#89787)\r\n- Fix `_pad_circular` export (#86984)\r\n- Fix concat with empty tensors (#87620)\r\n- Disable ONNX `ceil_mode` and `count_include_pad` to align torch `ceil_mode` results in corner case (#87892)\r\n- Fix ignored small eps in layer normalization in fp16 (#89869)\r\n- Fix `unconvertible_ops` as per #89261 (#89299)\r\n- Fix `Gather` replacement in `RNN peephole` (#93120)\r\n- Fix `cat` operator for tensors with unknown rank (#94870)\r\n- Fix scalar type analysis for copied constant (#86716)\r\n- Fix scalar type detection for optional tensors (#94427)\r\n- Fix 'prim::PackPadded' shape inference (#91829)\r\n- Add `onnx::Max` into standard Op for scalar type alignment (#88750)\r\n- Add `setType` from user into `InferredType` and `Reliable` in `ConstantValueMap` (#88622)\r\n- Integrate ONNX ATen Fallback export with the new operator registry (#87735)\r\n- Fix ONNX ATen Fallback integration for `BUILD_CAFFE2=0` builds (#88504)\r\n- Fix `torch.autograd.Function.symbolic` method support (#94746)\r\n- Fix `FindCommonAncestor` in `function_extraction` (#86650)\r\n- Update training state logic to support `ScriptedModule` (#86745)\r\n\r\n## ROCm\r\n\r\n- Fix hipify mapping for cuDeviceGet (#90726)\r\n\r\n## Mps\r\n\r\n- Fix issues with non-contiguous Tensor handling (#86956, #86958)\r\n- Fix issues with ops implementation `torch.median` (#90326, #88807), `torch.{std,var}` `correction` argument (#91203), `torch.index_select` (#94117, #91064), `torch.cumsum` (#94119), `torch.where` (#86240), `torch.nn.Embedding` (#82809), `torch.nn.Softplus` (#88555), `torch.nn.functional.pad` (#89864), `torch.max` (#91520), padding functions (#91522), `torch.nn.functional.upsample` (#91669), pooling functions (#91519, #94348), `torch.nn.{NLLLoss,SmoothL1Loss}` (#94226), `torch.nn.SoftPlus` (#94256), `torch.masked_fill` (#94263), `torch.fill_` (#94479), `torch.median` (#94489), `torch.nonzero` (#94442), `torch.nn.BatchNorm` (#94351), `torch.{min,max}` (#94386), `torch.nn.GELU` (#94529), `torch.nn.LSTM` (#94889), #95137),`torch.nn.Conv2d`(#95078),`torch.nn.functional.bilinear`(#94892),`torch.copy\\_` (#95272),`torch.max_pool2d`(#94963),`torch.div` (#95769)\r\n- Fix issues with `torch.bool` for Unary ops (#91120), scatter ops (#94464),\r\n- Fix issues with `torch.float16` for `torch.nan_to_num` (#94220), `torch.nn.HuberLoss` (#94567)\r\n- Properly raise error for `torch.int64` inputs for `torch.dot` (#94270), `torch.floor_divide` (#94488), `torch.square` (#94766),\r\n- Properly cast `torch.int64` to `torch.int32` for reduction ops and raise warning. (#94484)\r\n- Properly raise unimplemented error for `torch.nn.Conv3d` (#94492),\r\n- Fix data type issues with index_add for non-`torch.float` inputs by casting them to `torch.float` (#88542)\r\n- Fix the high watermark value for unified memory allocation on x86 (#91268)\r\n- Fix handling of ops taking multiple dtypes as input (#91197, #91514)\r\n- Fix handling of channels last for `torch.cat` (#91786, #94662), `torch.Conv2d` (#91822, #94384), `torch.nn.{ELU,ReLU,Hardswish}` (#94664), `torch.nn.BatchNorm` (#94760), `torch.nn.MaxPool2d` (#94877)\r\n- Fix view operations handling (#94259, #94278,#95145, #95762, #95905)\r\n- Fix numerical stability issues with various ops (#94889)\r\n- Fix TORCH_WARN_ONCE (#95559) (#95559)\r\n\r\n## Build\r\n\r\n- Move incorrectly placed closing curly brace of `extern \"C\"` block (#87853)\r\n- Set INTERFACE_LINK_DIRECTORIES on caffe2::mkl (#89359)\r\n- Also include MKL_THREAD_LIB in link libraries for caffe2::mkl (#89378)\r\n- Fix MSVC compiler error in basic_ops.h (#93322)\r\n- Fix a bug that redefines \\_\\_STDC_FORMAT_MACROS (#89310)\r\n- Fix ReplaceWithMaybeCopy test in OSS (#88099)\r\n\r\n## Jit\r\n\r\n- Fix out-of-bounds error in torch.jit.script for functions with many decorators (#87804)\r\n- Assorted fixes for NNC cpu fuser (#85056, #86788, #88798, #89978)\r\n- Set the correct size of aten tensor in presence of MKL-DNN padding (#86767)\r\n- Fix Scalar(bool) handling in toIValue (#87179)\r\n\r\n## Vulkan\r\n\r\n- Fix an issue with Vulkan not being able to be compiled on Windows (#92207)\r\n- Fix a possible empty vector dereference in the Vulkan optimization pass (#92918)\r\n\r\n## Cudnn\r\n\r\n- Fix cudnn RNN reproducibility issue (#90522)\r\n- Fix `benchmark_limit` ignoring failed kernels in FIND (#91032)\r\n\r\n## Releng\r\n\r\n- Set nvfuser default to disabled, keep CI (#86369)\r\n- Add manual cuda deps search logic (#90411)\r\n- Workaround for NumPy builds that ship with a broken Dlpack deleter (#89759)\r\n- Workaround MSVC ICE due to constexpr char\\* template argument (#86288)\r\n- Add define to fix issue with compatibility with latest Windows SDK (#85408)\r\n- Remove invalid git option when updating submodules (#91132)\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n- Improve torch.lerp performance on cpu (#84845)\r\n- Improve torch.istft performance (#88060)\r\n- Call view within einsum to remediate MPS regression (#87135)\r\n- Remove unnecessary calls to python builtins(#94323)\r\n- Improve type hints for Module forward hooks (#92061)\r\n\r\n## Autograd API\r\n\r\n- Use in-place input accumulation fast path for dense Tensors. (#90217)\r\n\r\n## torch.nn API\r\n\r\n- Improve `functional.interpolate()` speed for `torch.channels_last` (#86361, #86361, #90302)\r\n- Improve performance for `functional.multi_head_attention_forward()` (#93234, #89847)\r\n- Improve performance for `TransformerEncoderLayer()` and `MultiheadAttention()` (#87377, #88488, #88831, #88854, #88970, #91171)\r\n- Improve `SyncBatchNorm()` performance by using the right gathering ops (#89521)\r\n- Improve `ConvTransposed2D()` CPU performance for `torch.{float32, bfloat16}` (#92530)\r\n- Improve `functional.local_response_norm()` performance for 3d inputs (#91052)\r\n\r\n## torch.func\r\n\r\n- Add vmap batching rule for: `bitwise operators` (#91971), `nansum` & `nanmean` (#91372), `all` & `any` (#91966), `torch.linalg.vander` (#91749), `slogdet` (#86815), `torch.index_fill` (#91364), `narrow_copy` (#88130), `view_copy` (#88150), `greater_equal.Scaler` (#91324)\r\n\r\n## Cuda\r\n\r\n- Layer norm backward speed gain with warp shuffles (#87445, #87814)\r\n- Avoid unnecessary type casts (#86086)\r\n- Use `atomicAdd` for `bfloat16` in Ampere and above (#84981)\r\n\r\n## Cpp API\r\n\r\n- Vectorize torch.exp2 on CPU and add complex support (#92115)\r\n- Add various performance fixes to c++ STL usage (#94034)\r\n\r\n## NestedTensor API\r\n\r\n- Improve performance for NestedTensor `torch.bmm`(#86856), (#85894)\r\n- Remove unnecessary check in `select_nested` (#89150)\r\n\r\n## Distributed\r\n\r\n- Do not call `pad` in no-padding case(#88769)\r\n\r\n## Complex API\r\n\r\n- Improve complex `lerp` performance (#84844)\r\n\r\n## Mobile\r\n\r\n- Passing serialized XNNPACK model by reference (#89089)\r\n- Fix to add multiple outputs for the CoreML delegate (#88345)\r\n\r\n## Sparse API\r\n\r\n- Improve performance of `mul` when given COO (#86269)\r\n- Improve `to(dtype)` support for all sparse compressed formats (#89055)\r\n- Improve conversion of BSR/BSC to COO using `to_sparse` (#91389)\r\n- Improve `sparse_mask` (#91964)\r\n- Improve `to_dense` backward by removing redundant call to `coalesce` (#92001)\r\n- Improve validation of CSR/CSC/BSR/BSC tensors for low dimensional inputs (#94048)\r\n- Improve torch.sparse.sampled_addmm performance on CPU for CSR inputs (#90978)\r\n\r\n## Optimizer API\r\n\r\n- Improve foreach implementations by pre-grouping tensors to maximize fast path for `{Adadelta, Adagrad, Adam, Adamax, AdamW, ASGD, NAdam, RAdam, RMSProp, RProp, SGD}`(#92048, #92362, #92363, #92349, #92364, #92365, #92369, #92372, #92338)\r\n\r\n## Cpu\r\n\r\n- Optimizations for flip (#89414, #91806,#88989, #90013)\r\n- Add fmsub to vectorization primitives (#86568)\r\n- Optimize GELU BFloat16 Impl in CPU path (#79378)\r\n- Fix `biasadd` OMP perf issue for the packed MKL SGEMM (#92300)\r\n- Optimize LogSoftmax by improving thread-allocation in `_vec_log_softmax_lastdim` (#85398)\r\n- BF16 autocast conv transpose 1d/2d/3d for CPU (#92527)\r\n- Add mkl implementation for exponential on CPU (#69967)\r\n\r\n## Fx\r\n\r\n- Use deque instead of list for BFS (#91139)\r\n- Refactor the dfs cyclic search from recursive to iterative approach (#91042)\r\n\r\n## Mps\r\n\r\n- Increase performance of `torch.add{cmul,cdiv,mm}`(#94214, #94534)`torch.multinomial` (#86342), faster op launch time (#86437), `torch.linear` (#91114), view handling (#91743, #94218), `convolutions`(#94661), `scatter/gather` (#94663)\r\n\r\n## Jit\r\n\r\n- Add BFloat16 dtype support for oneDNN Graph JIT fuser (#85591)\r\n\r\n## Cudnn\r\n\r\n- Improve hot path heuristics performance in V8 (#90811)\r\n\r\n# Documentation\r\n\r\n## Python API\r\n\r\n- Fix various spelling and grammatical errors (#87357, #87583, #88033, #91641, #91871, #86642, #86721, #90110, #87724, #88483, #92049, #92762, #88962)\r\n- Fix the documentation of various functions (#88059, #94545, #86593, #93145, #90071, #87870, #91627, #89910, #79086)\r\n- Fix dev-discuss link in the maintainer docs (#89493)\r\n- Add General Project Policies (#87385)\r\n\r\n## Autograd API\r\n\r\n- Improve autograd documentation (#89401, #93065)\r\n\r\n## torch.nn API\r\n\r\n- Improve documentation for: `MaxPool2d` (#86559), `utils.clip_grad_norm_()` (#91312), `Module()` (#87142), `{Unfold,Fold}()` (#88819), `torch.nn.functional.gelu` (#89061), `functional.conv2d` `padding` (#85004), `functional.leaky_relu()` (#94090), `MaxUnpool{1,2,3}D` (#94629)\r\n\r\n## NestedTensor API\r\n\r\n- Update Persons of Interest (#90069)\r\n- Fix path to nested_tensor in example (#86891)\r\n\r\n## Mps\r\n\r\n- Add 'mps' to the tensor attributes doc page (#86585)\r\n\r\n## Distributed\r\n\r\n- Activation checkpointing\r\n  - Clean up comments in activation checkpoint (#86622)\r\n- Distributed (c10d)\r\n  - Improve documentation for various functions (#87018, #94543, #91116,#89905, #86438 )\r\n- DistributedDataParallel\r\n  - Improve Documentation (#86221, #91832)\r\n- RPC\r\n  - Fix non-existing parameters in docstrings in benchmarks (#91115)\r\n- Tensor parallelism and DTensor:\r\n  - Add more clarifications and fix errors in tensor parallelism docs (#94786)\r\n  - Update 2D parallelism API naming and docs (#94771)\r\n- FullyShardedDataParallel\r\n  - Add docs to explain the running the forward pass of of submodules in FSDP (#86343)\r\n  - Clarify warnings to mention collectives (#87478)\r\n  - Remove HSDP Zero-2 from doc (#90503)\r\n  - Improve the comments for FSDP (#92359)\r\n- Distributed Checkpoint\r\n  - Enable documentation for Distributed Checkpoint. (#92813)\r\n- Torch Elastic\r\n  - Fix a minor typo in documentation (#90667)\r\n  - Fix `torch.distributed.run` init connect timeout by comparing `host` with the current IP list (#90221)\r\n\r\n## torch.func\r\n\r\n- Downgrade the warning about forward-mode AD coverage (#87383)\r\n- Add version selector back to functorch docs (#86602)\r\n- Add documentation for torch.func (#91319)\r\n- Fix AOTAutograd tutorial (#87415)\r\n- Add migration guide from functorch (#91811)\r\n- Improve inplace/view note on copy slices (#89856)\r\n- Add more details to the functorch install page (#86823)\r\n\r\n## Linalg API\r\n\r\n- Add a note on the stability of linalg functions. (#88313)\r\n- Improve documentation for various linalg functions (#89013,#89383, #91129)\r\n\r\n## Composability\r\n\r\n- Fix ScalarTensor **repr** in Extending PyTorch example (#86330)\r\n- Fix incorrect wrapping of function decorator (#94446)\r\n- Add **all** to torch.{autograd, fx, cuda} submodules (#85343)\r\n\r\n## Dataloader API\r\n\r\n- Update dataloader docstring mentioning prefetch factor behavior (#89874)\r\n\r\n## Sparse API\r\n\r\n- Extend documentation for `to_sparse` (#89912)\r\n- Small correction to `torch.sparse` overview documentation(#93258)\r\n\r\n## Optimizer API\r\n\r\n- Improve documentation for various optimizers (#91195, #91196, #91881, #89575, #86629, #92111)\r\n- Add general documentation on our algorithm defaults (#95391)\r\n\r\n## Serialization\r\n\r\n- Fix various spelling and grammatical errors (#90662, #91253)\r\n\r\n## Distributions\r\n\r\n- Improve documentation for various distributions (#91091, #87577)\r\n- Add original sources/references to Wishart.py in distributions (#86543)\r\n\r\n## Quantization\r\n\r\n- Improvements to various READMEs (#89319, #86914,#86523, #89795, #90403)\r\n- Add docstrings for operators defined in torch.ops.quantized_decomposed namespace (#89547)\r\n- Add x86 backend as default backend of server inference (#86794)\r\n- Fix non-existing parameters in docstrings in torch/ao (#90875)\r\n- Move parts of BackendConfig tutorial (#91999)\r\n\r\n## ONNX\r\n\r\n- Fix non-existing parameters in docstrings in torch/onnx (#90593)\r\n- Update diagnostics system (#94565)\r\n\r\n## Releng\r\n\r\n- Enabled xdoctest runner in CI (#83816)\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v2.0.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v2.0.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v2.0.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/94838221", "release_id": 94838221, "date_created": "2023-03-09T22:42:00Z", "date_published": "2023-03-15T19:38:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/86242228", "tag": "v1.13.1", "name": "PyTorch 1.13.1 Release, small bug fix release", "author": {"name": "atalman", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n- RuntimeError by torch.nn.modules.activation.MultiheadAttention with bias=False and batch_first=True #88669\r\n- Installation via pip  on Amazon Linux 2, regression #88869\r\n- Installation using poetry on Mac M1, failure #88049\r\n- Missing masked tensor documentation #89734\r\n- torch.jit.annotations.parse_type_line is not safe (command injection) #88868\r\n- Use the Python frame safely in _pythonCallstack #88993\r\n- Double-backward with full_backward_hook causes RuntimeError #88312\r\n- Fix logical error in get_default_qat_qconfig #88876\r\n- Fix cuda/cpu check on NoneType and unit test #88854 and #88970\r\n- Onnx ATen Fallback for BUILD_CAFFE2=0 for ONNX-only ops #88504\r\n- Onnx operator_export_type on the new registry #87735\r\n- torchrun AttributeError caused by file_based_local_timer on Windows #85427\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/89855) should contain all relevant pull requests related to this release as well as links to related issues\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.13.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.13.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.13.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/86242228", "release_id": 86242228, "date_created": "2022-12-08T18:13:26Z", "date_published": "2022-12-16T00:17:16Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/80850282", "tag": "v1.13.0", "name": "PyTorch 1.13: beta versions of functorch and improved support for Apple\u2019s new M1 chips are now available", "author": {"name": "mikaylagawarecki", "type": "User"}, "description": "# Pytorch 1.13 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Changes\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n* Developers\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.13! This includes stable versions of BetterTransformer. We deprecated CUDA 10.2 and 11.3 and completed migration of CUDA 11.6 and 11.7. Beta includes improved support for Apple M1 chips and functorch, a library that offers composable vmap (vectorization) and autodiff transforms, being included in-tree with the PyTorch release. This release is composed of over 3,749 commits and 467 contributors since 1.12.1. We want to sincerely thank our dedicated community for your contributions.\r\n\r\nSummary:\r\n\r\n* The BetterTransformer feature set supports fastpath execution for common Transformer models during Inference out-of-the-box, without the need to modify the model. Additional improvements include accelerated add+matmul linear algebra kernels for sizes commonly used in Transformer models and Nested Tensors is now enabled by default.\r\n\r\n* Timely deprecating older CUDA versions allows us to proceed with introducing the latest CUDA version as they are introduced by Nvidia\u00ae, and hence allows support for C++17 in PyTorch and new NVIDIA Open GPU Kernel Modules.\r\n\r\n* Previously, functorch was released out-of-tree in a separate package. After installing PyTorch, a user will be able to `import functorch` and use functorch without needing to install another package.\r\n\r\n* PyTorch is offering native builds for Apple\u00ae silicon machines that use Apple's new M1 chip as a beta feature, providing improved support across PyTorch's APIs.\r\n\r\n|Stable\t|Beta\t|Prototype\t|\r\n|---\t|---\t|---\t|\r\n|<ul><li>Better Transformer</li><li>CUDA 10.2 and 11.3 CI/CD Deprecation </li></ul> | <ul><li>Enable Intel\u00ae VTune\u2122 Profiler's Instrumentation and Tracing Technology APIs</li><li>Extend NNC to support channels last and bf16</li><li>Functorch now in PyTorch Core Library</li><li>Beta Support for M1 devices</li></ul>\t| <ul><li>Arm\u00ae Compute Library backend support for AWS Graviton</li><li> CUDA Sanitizer</li></ul> |\r\n\r\nYou can check the blogpost that shows the new features [here](https://pytorch.org/blog/PyTorch-1.13-release/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### **uint8 and all integer dtype masks are no longer allowed in Transformer** **(#87106)**\r\n\r\nPrior to 1.13, `key_padding_mask` could be set to uint8 or other integer dtypes in `TransformerEncoder` and `MultiheadAttention`, which might generate unexpected results. In this release, these dtypes are not allowed for the mask anymore. Please convert them to `torch.bool` before using.\r\n\r\n1.12.1\r\n\r\n```python\r\n>>> layer = nn.TransformerEncoderLayer(2, 4, 2)\r\n>>> encoder = nn.TransformerEncoder(layer, 2)\r\n>>> pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.uint8)\r\n>>> inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1)\r\n# works before 1.13\r\n>>> outputs = encoder(inputs, src_key_padding_mask=pad_mask)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n>>> layer = nn.TransformerEncoderLayer(2, 4, 2)\r\n>>> encoder = nn.TransformerEncoder(layer, 2)\r\n>>> pad_mask = torch.tensor([[1, 1, 0, 0]], dtype=torch.bool)\r\n>>> inputs = torch.cat([torch.randn(1, 2, 2), torch.zeros(1, 2, 2)], dim=1)\r\n>>> outputs = encoder(inputs, src_key_padding_mask=pad_mask)\r\n```\r\n\r\n### **Updated `torch.floor_divide` to perform floor division** **(#78411)**\r\n\r\nPrior to 1.13, `torch.floor_divide` erroneously performed truncation division (i.e. truncated the quotients). In this release, it has been fixed to perform floor division. To replicate the old behavior, use `torch.div` with `rounding_mode='trunc'`.\r\n\r\n1.12.1\r\n\r\n```python\r\n>>> a = torch.tensor([4.0, -3.0])\r\n>>> b = torch.tensor([2.0, 2.0])\r\n>>> torch.floor_divide(a, b)\r\ntensor([ 2., -1.])\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n>>> a = torch.tensor([4.0, -3.0])\r\n>>> b = torch.tensor([2.0, 2.0])\r\n>>> torch.floor_divide(a, b)\r\ntensor([ 2., -2.])\r\n# Old behavior can be replicated using torch.div with rounding_mode='trunc'\r\n>>> torch.div(a, b, rounding_mode='trunc')\r\ntensor([ 2., -1.])\r\n```\r\n\r\n### **Fixed `torch.index_select` on CPU to error that index is out of bounds when the `source` tensor is empty (#77881)**\r\n\r\nPrior to 1.13, `torch.index_select` would return an appropriately sized tensor filled with random values on CPU if the source tensor was empty. In this release, we have fixed this bug so that it errors out. A consequence of this is that `torch.nn.Embedding` which utilizes `index_select` will error out rather than returning an empty tensor when `embedding_dim=0` and `input` contains indices which are out of bounds. The old behavior cannot be reproduced with `torch.nn.Embedding`, however since an Embedding layer with `embedding_dim=0` is a corner case this behavior is unlikely to be relied upon.\r\n\r\n1.12.1\r\n\r\n```python\r\n>>> t = torch.tensor([4], dtype=torch.long)\r\n>>> embedding = torch.nn.Embedding(3, 0)\r\n>>> embedding(t)\r\ntensor([], size=(1, 0), grad_fn=<EmbeddingBackward0>)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n>>> t = torch.tensor([4], dtype=torch.long)\r\n>>> embedding = torch.nn.Embedding(3, 0)\r\n>>> embedding(t)\r\nRuntimeError: INDICES element is out of DATA bounds, id=4 axis_dim=3\r\n```\r\n\r\n### Disallow overflows when tensors are constructed from scalars (#82329)\r\n\r\nPrior to this PR, overflows during tensor construction from scalars would not throw an error. In 1.13, such cases will error.\r\n\r\n1.12.1\r\n\r\n```python\r\n>>> torch.tensor(1000, dtype=torch.int8)\r\ntensor(-24, dtype=torch.int8)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n>>> torch.tensor(1000, dtype=torch.int8)\r\nRuntimeError: value cannnot be converted to type int8 without overflow\r\n```\r\n\r\n### **Error on indexing a cpu tensor with non-cpu indices (#69607)**\r\n\r\nPrior to 1.13, `cpu_tensor[cuda_indices]` was a valid program that would return a cpu tensor. The original use case for mixed device indexing was for `non_cpu_tensor[cpu_indices]`, and allowing the opposite was unintentional (`cpu_tensor[non_cpu_indices]`). This behavior appears to be rarely used, and a refactor of our indexing kernels made it difficult to represent an op that takes in (cpu_tensor, non_cpu_tensor) and returns another cpu_tensor, so it is now an error.\r\n\r\nTo replicate the old behavior for `base[indices]`, you can ensure that either `indices` lives on the CPU device, or `base` and `indices` both live on the same device.\r\n\r\n1.12.1\r\n\r\n```python\r\n>>> a = torch.tensor([1.0, 2.0, 3.0])\r\n>>> b = torch.tensor([0, 2], device='cuda')\r\n>>> a[b]\r\ntensor([1., 3.])\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n>>> a = torch.tensor([1.0, 2.0, 3.0])\r\n>>> b = torch.tensor([0, 2], device='cuda')\r\n>>> a[b]\r\nRuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)\r\n# Old behavior can be replicated by moving b to CPU, or a to CUDA\r\n>>> a[b.cpu()]\r\ntensor([1., 3.])\r\n>>> a.cuda()[b]\r\ntensor([1., 3.], device='cuda:0')\r\n```\r\n\r\n\r\n### Remove deprecated `torch.eig`,` torch.matrix_rank`, `torch.lstsq` (#70982, #70981, #70980)\r\nThe deprecation cycle for the above functions has been completed and they have been removed in the 1.13 release.\r\n\r\n## torch.nn\r\n\r\n### Enforce that the `bias` has the same dtype as `input` and `weight` for convolutions on CPU (#83686)\r\n\r\nTo align with the implementation on other devices, the CPU implementation for convolutions was updated to enforce that the `dtype` of the `bias` matches the `dtype` of the `input` and `weight`.\r\n\r\n1.12.1\r\n\r\n```python\r\n# input and weight are dtype torch.int64\r\n# bias is torch.float32\r\n>>> out = torch.nn.functional.conv2d(input, weight, bias, ...)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# input and weight are dtype torch.int64\r\n# bias is torch.float32\r\n>>> with assertRaisesError():\r\n>>>    out = torch.nn.functional.conv2d(input, weight, bias, ...)\r\n\r\n# Updated code to avoid the error\r\n>>> out = torch.nn.functional.conv2d(input, weight, bias.to(input.dtype), ...)\r\n```\r\n\r\n## Autograd\r\n\r\n### Disallow setting the `.data` of a tensor that `requires_grad=True` with an integer tensor (#78436)\r\n\r\nSetting the  `.data` of a tensor that `requires_grad` with an integer tensor now raises an error.\r\n\r\n1.12.1\r\n\r\n```python\r\n>>> x = torch.randn(2, requires_grad=True)\r\n>>> x.data = torch.randint(1, (2,))\r\n>>> x\r\ntensor([0, 0], requires_grad=True)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n>>> x = torch.randn(2, requires_grad=True)\r\n>>> x.data = torch.randint(1, (2,))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nRuntimeError: data set to a tensor that requires gradients must be floating point or complex dtype\r\n```\r\n\r\n### Added variable_list support to ExtractVariables struct (#84583)\r\n\r\nPrior to this change, C++ custom autograd Function considers tensors passed in TensorList to not be tensors for the purposes of recording the backward graph. After this change, custom Functions that receive TensorList must modify their backward functions to also compute gradients for these additional tensor inputs. Note that this behavior now differs from that of custom autograd Functions in Python.\r\n\r\n1.12.1\r\n\r\n```cpp\r\nstruct MyFunction : public Function<MyFunction> {\r\n    static Variable forward(AutogradContext* ctx, at::Tensor t, at::TensorList tensors) {\r\n      return 2 * tensors[0] + 3 * t;\r\n    }\r\n\r\n    static variable_list backward(\r\n        AutogradContext* ctx,\r\n        variable_list grad_output) {\r\n      return {3 * grad_output[0]};\r\n    }\r\n};\r\n```\r\n\r\n1.13\r\n\r\n```cpp\r\nstruct MyFunction : public Function<MyFunction> {\r\n    static Variable forward(AutogradContext* ctx, at::Tensor t, at::TensorList tensors) {\r\n      return 2 * tensors[0] + 3 * t;\r\n    }\r\n\r\n    static variable_list backward(\r\n        AutogradContext* ctx,\r\n        variable_list grad_output) {\r\n      return {3 * grad_output[0], 2 * grad_output[0]};\r\n    }\r\n};\r\n```\r\n\r\n### Don't detach when making views; force kernel to detach (#84893)\r\n\r\nView operations registered as CompositeExplicitAutograd kernels are no longer allowed to return input tensors as-is. You must explicitly create a new tensor (e.g., using `.alias()`).\r\n\r\n1.12.1\r\n\r\n```cpp\r\ntorch::Tensor view_op(const torch::Tensor& self) {\r\n  return self;\r\n}\r\n```\r\n\r\n1.13\r\n\r\n```cpp\r\ntorch::Tensor view_op(const torch::Tensor& self) {\r\n  return self.alias();\r\n}\r\n```\r\n\r\n## ONNX\r\n\r\n### `torch.onnx.register_custom_op_symbolic` now only registers the symbolic function at the specified opset version (#85636)\r\n\r\nThis updates `register_custom_op_symbolic`'s behavior to *only register the symbolic function at a single version.* This is more aligned with the semantics of the API signature. Previously the API registers a symbolic function to *all* versions up to the specified version. As a result of this change, users will need to register a symbolic function to the exact version when they want to override an existing symbolic function. Users are not affected if (1) an implementation does not exist for the op, or (2) the symbolic function is already registering to the exact version for export.\r\n\r\n1.12.1\r\n\r\n```python\r\n# Assuming an implemented symbolic function `custom_op_function`\r\ntorch.onnx.register_custom_op_symbolic(\"aten::foo\", custom_op_function, 16)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# Assuming an implemented symbolic function `custom_op_function`\r\nfor opset in range(1, 17):\r\n    torch.onnx.register_custom_op_symbolic(\"aten::foo\", custom_op_function, opset)\r\n```\r\n\r\n### Default ONNX opset is updated to 14 (#83284)\r\n\r\nThe update is done in regularly to ensure we are in sync with the onnx updates. Users can specify `opset_version` in `torch.onnx.export` to maintain opset version 13.\r\n\r\n### `torch.onnx.symbolic_registry` is removed (#84382)\r\n\r\nWe removed the `symbolic_registry` module and hid it as an internal implementation detail. Users previously relying on the `register_op` function to register custom symbolic functions should move to use the `torch.onnx.register_custom_op_symbolic` API.\r\n\r\n### `ScalarType` and global variables in `torch.onnx.symbolic_helper` are removed (#82995)\r\n\r\nThe `ScalarType` class in `torch.onnx.symbolic_helper`, along with the global variables `cast_pytorch_to_onnx`, `pytorch_name_to_type`, `scalar_name_to_pytorch`, `scalar_type_to_onnx` and `scalar_type_to_pytorch_type` are removed from the module. Users previously using these global variables for PyTorch JIT-ONNX type conversion in symbolic functions should move to use the `torch.onnx.JitScalarType` class.\r\n\r\n1.12.1\r\n\r\n```python\r\n# 1\r\ntorch.onnx.symbolic_helper.scalar_type_to_onnx[\r\n    symbolic_helper.scalar_type_to_pytorch_type.index(x.dtype)\r\n].value\r\n\r\n# 2\r\ntorch.onnx.symbolic_helper.scalar_name_to_pytorch[element_type] in cast_pytorch_to_onnx.keys()\r\n\r\n# 3\r\ntorch.onnx.symbolic_helper.cast_pytorch_to_onnx[\"Long\"]\r\n\r\n# 4\r\ntorch.onnx.symbolic_helper.cast_pytorch_to_onnx[tensor.type().scalarType()]\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# 1\r\ntorch.onnx.JitScalarType.from_dtype(x.dtype).onnx_type()\r\n\r\n# 2\r\ntorch.onnx.JitScalarType.from_name(element_type).onnx_compatible()\r\n\r\n# 3\r\ntorch.onnx.TensorProtoDataType.INT64\r\n\r\n# 4\r\ntorch.onnx.JitScalarType.from_name(tensor.type().scalarType()).onnx_type()\r\n```\r\n\r\n## Distributed\r\n\r\n### **In c10d collectives, input tensors dtype must now be the same (#84664)**\r\n\r\nWe added a check to validate all dtype across all input tensors. Previously, users were allowed to pass in tensors with diferent dtypes for c10d collectives. Now, passing in tensors with different dtypes will throw a RuntimeError with the following message: \u201cInvalid usage of tensors with different dtypes Found `torch.float` and `torch.half`\u201d. Users can use `tensor.to(dtype={some_dtype})` to fix this.\r\n\r\n1.12.1\r\n\r\n```python\r\n# users could pass inputs having different dtypes\r\n>>> tensor = torch.ones(2, 2) * 7\r\n>>> tensor_h = tensor.half()\r\n>>> tensor_list = [torch.zeros(2, 2) for _ in range(4)] # Assume world_size = 4\r\n# Both cases work.\r\n>>> dist.all_gather(tensor_list, tensor)\r\n>>> dist.all_gather(tensor_list, tensor_h)\r\n...\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# all inputs of c10d collectives need to have the same dtype\r\n>>> tensor = torch.ones(2, 2) * 7\r\n>>> tensor_h = tensor.half()\r\n>>> tensor_list = [torch.zeros(2, 2) for _ in range(4)] # Assume world_size = 4\r\n# Only allow same dtype for all input tensors.\r\n>>> dist.all_gather(tensor_list, tensor) # RuntimeError thrown\r\n...\r\n```\r\n\r\n### **Users doing wildcard imports of torch.distributed.distributed_c10d will no longer get non-public symbols (#84872)**\r\n\r\nWe limit the usage of c10d APIs to public APIs, so if a user does a wildcard import and calls an internal API, it will fail. Please see the example below:\r\n\r\n1.12.1\r\n\r\n```python\r\n# users could import both public and non-public symbols:\r\nfrom torch.distributed.distributed_c10d import *\r\n>>> is_nccl_available() # public API\r\n>>> _check_single_tensor(...) # Non-public API\r\n...\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# users can only import public symbols\r\nfrom torch.distributed.distributed_c10d import *\r\nis_nccl_available() # public API\r\n_check_single_tensor(...) # Non-public API, this will fail now\r\n...\r\n```\r\n\r\n### [Process Group C++ extensions](https://pytorch.org/tutorials/intermediate/process_group_cpp_extension_tutorial.html?highlight=process%20group) must use absolute path when importing ProcessGroup.hpp (#86257), ProcessGroup::Work object moved out of work to its own Work class (#83680):\r\n\r\nDetails of the changes and the updated tutorial can be found in the PyTorch tutorial PR [#2099](https://github.com/pytorch/tutorials/pull/2099)\r\n\r\n1.12.1\r\n\r\n```cpp\r\n// users use relative path to import C++ headers and Work resides in ProcessGroup class\r\n#include <c10d/ProcessGroup.hpp>\r\n#include <c10d/Store.hpp>\r\n#include <c10d/Types.hpp>\r\n#include <c10d/Utils.hpp>\r\n...\r\nclass WorkDummy : public ProcessGroup::Work {\r\n    ...\r\n}\r\n```\r\n\r\n1.13\r\n\r\n```cpp\r\n// users must use absolute path of import C++ files and Work is its own class\r\n#include <torch/csrc/distributed/c10d/ProcessGroup.hpp>\r\n#include <torch/csrc/distributed/c10d/Store.hpp>\r\n#include <torch/csrc/distributed/c10d/Types.hpp>\r\n#include <torch/csrc/distributed/c10d/Utils.hpp>\r\n...\r\n#include <torch/csrc/distributed/c10d/Work.hpp>\r\nclass WorkDummy : public Work {\r\n    ...\r\n}\r\n```\r\n\r\n## Quantization\r\n\r\n### Add required `example_args` argument to `prepare_fx` and `prepare_qat_fx` (#249) (#77608)\r\n\r\nWe added an additional required `example_inputs` argument to `prepare_fx` and `prepare_qat_fx` APIs, this can be used to do type inference to figure out the type information for each of the fx Node in the graph.\r\n\r\n1.12.1\r\n\r\n```python\r\nm = resnet18(...)\r\nm = prepare_fx(m, qconfig_dict)\r\n# or\r\nm = prepare_qat_fx(m, qconfig_dict)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nm = resnet18(...)\r\nm = prepare_fx(m, qconfig_dict, example_inputs=(torch.randn(1, 3, 224, 224),))\r\n# or\r\nm = prepare_qat_fx(m, qconfig_dict, example_inputs=(torch.randn(1, 3, 224, 224),))\r\n```\r\n\r\n### Stop moving models to CPU in quantization convert (#80555)\r\n\r\nPreviously, we automatically moved the model to CPU in `torch.ao.quantization.fx.convert` to work around the issue where certain functions called by convert expect CPU arguments. This commit pushes this responsibility to the caller since it is the user's decision of which device to use.\r\n\r\n1.12.1\r\n\r\n```python\r\nmodel = resnet18(...)\r\nmodel = prepare_fx(model, qconfig_mapping, example_inputs)\r\n# calibrate\r\nmodel = convert_fx(model)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nmodel = resnet18(...)\r\nmodel.cpu()  # if needed\r\nmodel = prepare_fx(model, qconfig_mapping, example_inputs)\r\n# calibrate\r\nmodel = convert_fx(model)\r\n```\r\n\r\n### Replace the `is_reference` flag of the `torch.ao.quantize_fx.convert_fx` function with the `convert_to_reference` function (#80091, #81326)\r\n\r\nThis PR removes the is_reference flag from the existing `convert_fx` API and replaces it with a new `convert_to_reference` function. This separates (1) converting the prepared model to a reference model from (2) lowering the reference model to a quantized model, enabling users to call their custom lowering function for\r\ncustom backends.\r\n\r\n1.12.1\r\n\r\n```python\r\nfrom torch.ao.quantization.quantize_fx import (\r\n    prepare_fx,\r\n    convert_to_reference,\r\n)\r\n\r\nprepared = prepare_fx(model, ...)\r\nreference = convert_to_reference(prepared, ...)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nfrom torch.ao.quantization.quantize_fx import (\r\n    prepare_fx,\r\n    convert_to_reference_fx,\r\n)\r\n\r\nprepared = prepare_fx(model, ...)\r\nreference = convert_to_reference_fx(prepared, ...)\r\n```\r\n\r\n### Add default configs for fixed qparams ops (#80184)\r\n\r\nThis commit adds qconfigs with special observers for fixed qparams ops (operators whose corresponding quantized version has fixed quantized parameters for output) like sigmoid in `get_default_qconfig_mapping` and `get_default_qat_qconfig_mapping`. For correctness, we also require users to use these special observers if we detect these fixed qparams ops in prepare.\r\n\r\n1.12.1 (fails after this PR):\r\n\r\n```python\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx\r\n\r\nmodel = ModelWithFixedQParamsOps()\r\nqconfig_mapping = QConfigMapping()\r\nexample_inputs = ...\r\nprepare_fx(model, qconfig_mapping, example_inputs)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nfrom torch.ao.quantization import get_default_qconfig_mapping\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx\r\n\r\nmodel = ModelWithFixedQParamsOps()\r\nqconfig_mapping = get_default_qconfig_mapping()\r\nexample_inputs = ...\r\nprepare_fx(model, qconfig_mapping, example_inputs)\r\n```\r\n\r\n### Replace `qconfig_dict` with a typed `QConfigMapping` object (#78452, #79618)\r\n\r\nPreviously, FX graph mode quantization configurations were specified through a dictionary of qconfigs. However, this\r\nAPI was not in line with other core APIs in PyTorch. This commit replaces this dictionary with a config object that users will\r\ncreate and pass to prepare and convert. This leads to better type safety and better user experience in notebook settings\r\ndue to improved auto completion.\r\n\r\n1.12.1 (deprecated)\r\n\r\n```python\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx\r\n\r\nqconfig_dict = {\r\n    \"\": qconfig,\r\n    \"object_type\": [\r\n        (torch.nn.Linear, qconfig),\r\n    ],\r\n    \"module_name_regex\": [\r\n        (\"foo.*bar\", qconfig),\r\n    ],\r\n    \"module_name\": [\r\n        (\"mod\", qconfig),\r\n    ],\r\n}\r\n\r\nprepare_fx(model, qconfig_dict)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nfrom torch.ao.quantization import QConfigMapping\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx\r\n\r\nqconfig_mapping = QConfigMapping()\r\n    .set_global(qconfig)\r\n    .set_object_type(torch.nn.Linear, qconfig)\r\n    .set_module_name_regex(\"foo.*bar\", qconfig)\r\n    .set_module_name(\"mod\", qconfig)\r\n\r\nprepare_fx(model, qconfig_mapping)\r\n```\r\n\r\n### Replace `*custom_config_dict` with typed config objects (#79066)\r\n\r\nThis commit replaces the following config dicts with python objects:\r\n\r\n* prepare_custom_config_dict \u2192 PrepareCustomConfig\r\n* convert_custom_config_dict \u2192 ConvertCustomConfig\r\n* fuse_custom_config_dict \u2192 FuseCustomConfig\r\n\r\nThis leads to better type safety and better user experience in\r\nnotebook settings due to improved auto completion.\r\n1.12.1\r\n\r\n```python\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\r\n\r\nprepare_custom_config_dict = {\r\n  \"float_to_observed_custom_module_class\": {\r\n     \"static\": {\r\n         FloatClass: ObservedClass\r\n     }\r\n  },\r\n  \"non_traceable_module_name\": [\"mod1\", \"mod2\"],\r\n  \"non_traceable_module_class\": [class1, class2],\r\n  \"input_quantized_idxs\": [0, 1],\r\n  \"output_quantized_idxs\": [0],\r\n  \"preserved_attributes\": [\"attr1\", \"attr2\"],\r\n}\r\n\r\nconvert_custom_config_dict = {\r\n  \"observed_to_quantized_custom_module_class\": {\r\n     \"static\": {\r\n         FloatClass: ObservedClass\r\n     }\r\n  },\r\n  \"preserved_attributes\": [\"attr1\", \"attr2\"],\r\n}\r\n\r\nmodel = prepare_fx(\r\n    model,\r\n    qconfig_mapping,\r\n    example_inputs,\r\n    prepare_custom_config_dict=prepare_custom_config_dict)\r\n\r\nmodel(data)\r\n\r\nmodel = convert_fx(model, convert_custom_config_dict=convert_custom_config_dict)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nfrom torch.ao.quantization.fx.custom_config import (\r\n    PrepareCustomConfig,\r\n    ConvertCustomConfig,\r\n)\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\r\n\r\nprepare_custom_config = PrepareCustomConfig() \\\r\n    .set_float_to_observed_mapping(float_class, observed_class) \\\r\n    .set_non_traceable_module_names([\"mod1\", \"mod2\"]) \\\r\n    .set_non_traceable_module_classes([class1, class2]) \\\r\n    .set_input_quantized_indexes([0, 1]) \\\r\n    .set_output_quantized_indexes([0]) \\\r\n    .set_preserved_attributes([\"attr1\", \"attr2\"])\r\n\r\nconvert_custom_config = ConvertCustomConfig() \\\r\n    .set_observed_to_quantized_mapping(observed_class, quantized_class) \\\r\n    .set_preserved_attributes([\"attr1\", \"attr2\"])\r\n\r\nmodel = prepare_fx(\r\n    model,\r\n    qconfig_mapping,\r\n    example_inputs,\r\n    prepare_custom_config=prepare_custom_config)\r\n\r\nmodel(data)\r\n\r\nmodel = convert_fx(model, convert_custom_config=convert_custom_config)\r\n```\r\n\r\n### Remove `remove_quant_dequant_pairs` and fix tests (#84203)\r\n\r\nThis PR removed some passes in `convert_fx`, and also fixes the way we quantize layer_norm operator, so the `qconfig` for layer_norm op needs to be updated as well.\r\n\r\n1.12.1\r\n\r\n```python\r\nimport torch\r\nfrom torch.ao.quantization.qconfig_mapping import QConfigMapping, QConfig\r\nfrom torch.ao.quantization.observer import default_weight_observer\r\nfrom torch.ao.quantization.backend_config import (\r\n    DTypeConfig,\r\n    ObservationType,\r\n)\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\r\n\r\nqconfig = QConfig(activation=qconfig.activation, weight=default_weight_observer)\r\nqconfig_mapping = QConfigMapping().set_object_type(torch.nn.LayerNorm, q_config) \\\r\n.set_object_type(torch.nn.functional.layer_norm, q_config)\r\n\r\n# assuming mymodel contains a LayerNorm layer or torch.nn.functional.layer_norm\r\nm = MyModel()\r\nexample_inputs = (torch.rand(3, 3),)\r\nm = prepare_fx(m, qconfig_mapping, example_inputs)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nimport torch\r\nfrom torch.ao.quantization.qconfig_mapping import QConfigMapping, QConfig\r\nfrom torch.ao.quantization.observer import default_placeholder_observer\r\nfrom torch.ao.quantization.backend_config import (\r\n    DTypeConfig,\r\n    ObservationType,\r\n)\r\nfrom torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\r\n\r\nqconfig = QConfig(activation=qconfig.activation, weight=default_placeholder_observer)\r\nqconfig_mapping = QConfigMapping().set_object_type(torch.nn.LayerNorm, q_config) \\\r\n.set_object_type(torch.nn.functional.layer_norm, q_config)\r\n\r\n# assuming mymodel contains a LayerNorm layer or torch.nn.functional.layer_norm\r\nm = MyModel()\r\nexample_inputs = (torch.rand(3, 3),)\r\nm = prepare_fx(m, qconfig_mapping, example_inputs)\r\n```\r\n\r\n### Align observer dtype with reference model spec (#85345)\r\n\r\nBefore this PR, the `dtype` attribute of observers was not clearly defined. It originally meant `interface_dtype` in the eager mode workflow, which is how the codebase before this PR is using it. In the new reference model spec, `dtype` attribute of an observer represents the `dtype` value which needs to be passed into a `quantize` function in the reference model spec. This PR aligns the codebase to this definition of `dtype`.\r\n\r\n1.12.1\r\n\r\n```python\r\ndynamic_quant_observer = PlaceholderObserver.with_args(\r\n    dtype=torch.float, compute_dtype=torch.quint8)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\ndynamic_quant_observer = PlaceholderObserver.with_args(\r\n    dtype=torch.quint8, compute_dtype=torch.quint8)\r\n```\r\n\r\n## Composability\r\n\r\n### **Changed the backend C++ kernel representation for some operators that take in lists of tensors (#73350)**\r\n\r\nIf an operator in ATen takes in a list of tensors, and is marked as \u201cstructured\u201d in native_functions.yaml ([example](https://github.com/pytorch/pytorch/blob/c8889f4e109866610bd1981f03deee8f102b5ce6/aten/src/ATen/native/native_functions.yaml#L1205)), then previously, TensorList was represented as `at::TensorList`, or `c10::ArrayRef<at::Tensor>`. Now, it is represented as a more efficient type: `const ITensorListRef&`.\r\n\r\n1.12.1\r\n\r\n```cpp\r\nat::Tensor cat_kernel(at::TensorList tensors,int64_t dim) {\r\n    ...\r\n}\r\nTORCH_LIBRARY_IMPL(aten, dispatch_key, m) {\r\n    ...\r\n    m.impl(\"cat\", &cat_kernel);\r\n}\r\n```\r\n\r\n1.13\r\n```cpp\r\nat::Tensor cat_kernel(const at::ITensorListRef& tensors,int64_t dim) {\r\n    ...\r\n}\r\nTORCH_LIBRARY_IMPL(aten, dispatch_key, m) {\r\n    ...\r\n    m.impl(\"cat\", &cat_kernel);\r\n}\r\n```\r\n\r\n## C++ API\r\n\r\n### **Lowered randint default dtype to the C++ API (#81410)**\r\n\r\nPrior to 1.13, the default for the `dtype` argument of `torch.randint`, `torch.long`, was set via manual python binding. However, in the C++ API, `torch::randint` would default to the global default data type, which is usually `float`. In 1.13 we changed the default for `dtype` in the C++ API to `int64` in order to match the python API. To reproduce the old behavior, one can set the `dtype` argument.\r\n\r\n1.12.1\r\n\r\n```cpp\r\ntorch::randint(/*low=*/0, /*high=*/10, {2, 3});\r\n```\r\n\r\n1.13\r\n\r\n```cpp\r\n// assuming default dtype is float\r\ntorch::randint(/*low=*/0, /*high=*/10, {2, 3}, torch::kFloat);\r\n```\r\n\r\n### **Enabled `dim=None` for `torch.{std, var, std_mean, var_mean}` (#81845, #82765, #82912)**\r\n\r\nPrior to 1.13, a C++ API call that has argument types `torch::{std, var, std_mean, var_mean}(Tensor, OptionalIntArrayRef, int64_t, bool)` used to resolve to the `{std, var, std_mean, var_mean}.correction` overload. In this release, it resolves to the `{std, var, std_mean, var_mean}.dim` overload. With the `.correction` overload, the third argument of type `int64_t` could be used to pass a correction *\u03b4N* other than 1. In order to call the `{std, var, std_mean, var_mean}.correction` overload in 1.13, the old `int64_t` argument can be wrapped in a `c10::optional`.\r\n\r\n1.12.1\r\n\r\n```cpp\r\n// using std as an example\r\nint64_t correction = 2;\r\ntorch::std(t, /*dim=*/dim, /*correction=*/correction, /*keepdim=*/True);\r\n```\r\n\r\n1.13\r\n\r\n```cpp\r\n// To replicate in 1.13 using std as an example\r\nauto correction = c10::make_optional<int64_t>(2);\r\ntorch::std(t, /*dim=*/dim, /*correction=*/correction, /*keepdim=*/True);\r\n```\r\n\r\n# Deprecations\r\n\r\n## Distributed\r\n\r\nWe are deprecating the following APIs of c10d: `*_coalesced` APIs (#85959), `*_multigpu` APIs (#85961) and `ProcessGroupRoundRobin` (#85158)\r\n\r\nWe added warnings when users call c10d\u2019s `*_coalesced`, `*_multigpu` and `ProcessGroupRoundRobin` APIs. Previously, users can use these APIs without any warnings but now they will see warnings like \u201ctorch.distributed.all_reduce_coalesced will be deprecated. If you must use it, please revisit our documentation later at [https://pytorch.org/docs/master/distributed.html#collective-functions\u201d](https://pytorch.org/docs/master/distributed.html#collective-functions%E2%80%9D). There are still workarounds for `*_coalesced` APIs but no workarounds will be provided for the other two.\r\n\r\n1.12.1\r\n\r\n```python\r\n# users could use the following APIs with no warnings:\r\nall_reduce_coalesced(...)\r\nall_gather_coalesced(...)\r\nbroadcast_multigpu(...)\r\nall_reduce_multigpu(...)\r\nreduce_multigpu(...)\r\nall_gather_multigpu(...)\r\nreduce_scatter_multigpu(...)\r\n...\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# users can still use these APIs but it will come with warnings:\r\nall_reduce_coalesced(...)\r\n# Warnings:\r\n# torch.distributed.all_reduce_coalesced will be deprecated. If you must\r\n# use it, please revisit our documentation later at\r\n# https://pytorch.org/docs/master/distributed.html#collective-functions\"\r\n\r\n# Potential workaround:\r\nreqs = []\r\nwith dist._coalescing_manager(group, reqs):\r\n    reqs.append(dist.all_reduce(tensor1, async_op=True))\r\n    reqs.append(dist.all_reduce(tensor2, async_op=True))\r\nfor req in reqs:\r\n    req.wait()\r\n...\r\n```\r\n\r\n\r\nWe are deprecating passing `optim_input` into the FSDP optimizer state checkpointing APIs. The user can simply not pass the `optim_input` argument, and all behavior is preserved. No fix is needed from users side for now.\r\n\r\n1.12.1\r\n\r\n```python\r\n# the user can use the following APIs with no warnings\r\nfull_optim_state_dict(...)\r\nsharded_optim_state_dict(...)\r\nshard_full_optim_state_dict(...)\r\nflatten_sharded_optim_state_dict(...)\r\nscatter_full_optim_state_dict(...)\r\nrekey_optim_state_dict(...)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\n# users can still use these APIs, but they will come with warnings\r\n# The `optim_input` argument is deprecated and will be removed after PyTorch 1.13.\r\n# You may remove it from your code without changing its functionality.\r\n```\r\n\r\n## LinAlg\r\n\r\n### Deprecate torch.lu in favor of linalg.lu_factor (_#77636_)\r\n\r\nThe new operation has a cleaner API and better docs. The update rule is as follows:\r\n\r\n1.12.1\r\n\r\n```python\r\nLU2, pivots2, info = torch.lu(A, compute_pivots, get_infos=True)\r\nLU1, pivots1, info = torch.lu(A, compute_pivots)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nLU2, pivots2, info = torch.linalg.lu_factor_ex(A, compute_pivots)\r\nLU1, pivots1 = torch.linalg.lu_factor(A, compute_pivots)\r\n```\r\n\r\n### Deprecate torch.lu_solve in favor of linalg.lu_solve(_#77637_)\r\n\r\nThe new operation has a notation consistent with `linalg.solve`, and has an extra parameter `adjoint=False`. The update rule is as follows:\r\n\r\n1.12.1\r\n\r\n```python\r\nX = torch.lu_solve(B, LU, pivots)\r\n```\r\n\r\n1.13\r\n\r\n```python\r\nX = linalg.lu_solve(LU, pivots, B)\r\n```\r\n\r\n## ONNX\r\n\r\n### Monkey patched convenience method on `torch._C.Graph`, `torch._C.Block` and `torch._C.Node` are deprecated. (#83006)\r\n\r\nDeprecated methods include `Graph.op()`, `Graph.constant()`, `Graph.at()`, `Block.op()`, and `Node.__getitem__()`. Previously, these methods are patched into the classes above when users call `torch.onnx.export()` and are typically used in custom symbolic functions. Users can continue to expect `g.op()` and `g.at()` in symbolic functions to work. The `g` parameter has been substituted by the `GraphContext` object (#84728). The methods are now exposed by the `GraphContext` class with APIs unchanged. Users should not rely on the `Graph.op()`, `Graph.constant()`, `Graph.at()`, `Block.op()`, `Node.__getitem__()` methods when they are directly interacting with the C classes. Users should use only the `op()` and `at()` methods of the `GraphContext` object, as other fields in the class will change in future releases.\r\n\r\n# New features\r\n\r\n## Python API\r\n\r\n* Added a deterministic implementation of `scatter_add` on CUDA for all input sizes (#79466)\r\n* Added `torch.concatenate` that aliases `torch.cat` (#85073)\r\n* Added `Tensor.is_cpu()`  that returns whether a tensor is on CPU (#78887)\r\n* Added a `force` kwarg to `Tensor.numpy()` that enables returning a numpy `ndarray` that does not share storage with the tensor (#78564)\r\n* Added `torch.special.{airy_ai, bessel_j0, bessel_j1, bessel_y0, bessel_y1, modified_bessel_i0, modified_bessel_i1, modified_bessel_k0, modified_bessel_k1, scaled_modified_bessel_k0, scaled_modified_bessel_k1, spherical_bessel_j0}` (#78900), (#78901), (#78902), (#78912),  (#78451)\r\n* Added `torch.special.{chebyshev_polynomial_t, chebyshev_polynomial_u, chebyshev_polynomial_v, chebyshev_polynomial_w, hermite_polynomial_h, hermite_polynomial_he, laguerre_polynomial_l, legendre_polynomial_p, shifted_chebyshev_polynomial_t, shifted_chebyshev_polynomial_u, shifted_chebyshev_polynomial_v, shifted_chebyshev_polynomial_w}` (#78196), (#78293), (#78304),  (#78366), (#78352),  (#78357)\r\n* Added `weights_only` option to `torch.load` that restricts load to state_dict only, enabling safe loading. This can  also be set using the `TORCH_FORCE_WEIGHTS_ONLY_LOAD` environment variable (#86812)\r\n\r\n## Build\r\n\r\n* Added `-Werror=unused-but-set-variable` build flag (#79305)\r\n* Added ability to get release versions based on the current tag (#78584)\r\n* Added `-Werror=type-limits` in Bazel CPU build (#79139)\r\n* Added `-Werror=unused-variable` in Bazel CPU build (#79156)\r\n* Added \u2014config=shell to bazelrc file for easier debugging (#79350)\r\n* Added clang `-Wconstant-conversion` to catch errors detected in #75400 (#80461)\r\n* Added `-Werror=non-virtual-dtor` build flag (#81012)\r\n* Turned on pocketfft flag for third-party pocket_fft library (#81670)\r\n* Updated NCCL to v2.13.4-1 (#82775)\r\n* Added `-Wunused-local-typedef` build flag (#86154)\r\n* Increased max python version to include 3.10 (#84815)\r\n\r\n## Complex\r\n\r\n*  Added complex half support for:\r\n    * [CPU] `torch.{index_select, index_add} `(#79217), (#79897).\r\n    * [CUDA]  `torch.roll` (#79970), `torch.fft.{fftshift, ifftshift}` (#79970), `torch.{acos, acosh, asinh, atanh}`, (#80030), `torch.{cos, sinh, cosh, tanh}` (#78718), `torch.sqrt, rsqrt` (#77490), `torch.{triu, tril, diag, trace}`(#78062).\r\n    * [CPU and CUDA] `torch.where` (#78665), `torch.{where, pow, masked_fill, sgn, tan, angle}`(#78665)\r\n* Added complex support for `torch.nn.ConvTranspose1d` (#79694).\r\n\r\n## torch.nn\r\n\r\n* Added `pop` function to `nn.Sequential` and `nn.ModuleList` (#81601)\r\n* Added deepcopy support for parametrized `nn.Module` (#80811)\r\n\r\n## torch.optim\r\n\r\n* Added maximization support via the `maximize` kwarg for `optim.SparseAdam` (#80336), `optim.ASGD`  (#81875), `optim.Rprop` (#81864), `optim.RMSprop` (#80326)\r\n* Added support for differentiable optimizers via the `differentiable` kwarg `optim.SGD` (#80938), `optim.Adam` (#82205), `optim.RMSprop` (#83578)\r\n* Added support for complex number for `optim.Adam` (#80279), `optim.AdamW` (#80280), `optim.Adamax` (#80319), `optim.RMSprop` (#83860), `optim.Rprop` (#83858),\r\n* Handled complex params as independent real params in `optim.{RMSprop, ASGD}` (#83860), (#84472)\r\n*  Added `optim.lr_scheduler.PolynomialLR` (#82769)\r\n\r\n## BetterTransformer\r\n\r\n* Allowed user to assert no mask contiguous check is necessary (#82533)\r\n* Added support for norm_first in nn.TransformerEncoderLayer fast path (#78269)\r\n* Added ustom scaled dot product implementations dense (#85984)\r\n* Added Better Transformer fastpath diagnostics (#81013)\r\n\r\n## ForEach\r\n\r\n* Implemented inplace `foreach` `maximum` and `minimum` (#82523)\r\n\r\n## LinAlg\r\n\r\n* Added `linalg.lu_solve`, `linalg.solve_ex`, `linalg.vecdot`, `linalg.vander` (_#77634_, _#80073_, _#70542_, _#76303_)\r\n\r\n## Sparse\r\n\r\n* Added `torch.sparse.spdiags` for easier creation of diagonal sparse matrices (#78439)\r\n\r\n## torch.fx\r\n\r\n* Enabled symbolic shapes (#82063, #82317, #82209, #83380, #85808, #84113, #84829, #84918, #85185, #85261, #85260, #85754, #85768, #86050, #86098, #86067)\r\n* Created an improved version of subgraph matcher (#82090, #82853, #85444, #85456, #85617)\r\n* Rewrite subgraph_rewriter with subgraph_matcher (#83717)\r\n* Added PassBase for writing passes, PassResult for the return value of passes, and a PassManager for managing the workflow of passes (#79878, #81366, #80531, #82485, #83933, #84094, #84425, #84232)\r\n* Added an FX graph partitioner and fuser (#79439, #80292)\r\n* Added a reinplacing FX pass (#80897, #83626, #83845, #83846)\r\n* Added a CSE pass to the common passes (#81512, #81530, #81742)\r\n* Created DecompositionInterpreter for decomposing aten \u2192 prims after an initial make_fx call (#79989)\r\n* Created a Backend for NvFuser based graph partitioner + Prims (#80591, #81311, #81436, #81911)\r\n* Created a Backend for Cudagraphs from dynamo (#80566)\r\n* Created a type constraint generator to Z3 (#79912, #80084, #80095, #80102, #80110, #80147, #80744, #80799, #80823, #80847, #80909, #80925, #80976, #81159, #81175, #81189, #81190, #81265, #81274, #81344, #81360, #81376, #81445, #81516, #81527, #81714, #82163, #82590, #82597, #82614, #82742, #82856, #82923,#82938,#83087, #83109, #83194, #83334, #83682, #83945)\r\n\r\n## JIT\r\n\r\n* Added new NVFuser Python Frontend Record Keeping for Cache enablement. (#81578)\r\n* Added `torch.ops.nvprims` namespace for nvFuser-specific prims (#82155)\r\n* Enabled fusion of conv with elementwise OP in NNC (#77157)\r\n* Added symbolic shape functions for `conv_transpose2d.input, convolution, convolution_backward` (#77283, #83557, #80860)\r\n* Added support in symbolic shapes for generalized lists of tensor shapes, tuple outputs, optional None, upper and lower bounds (#77389, #83092, #83222, #78679)\r\n* Added support for `aten::_convolution` when it is 2D conv in NNC (#84038)\r\n* Exposed `ProcessGroup::Work.wait()` API to TorchScript (#83303)\r\n\r\n## ONNX\r\n\r\n* Inlined `prim::PythonOp` for Autograd Function Export (#74765)\r\n\r\n## AMD\r\n\r\n* Enabled nvfuser (#82498)\r\n\r\n## CUDA\r\n\r\n* Added CUDA trace Python hooks (#82824)\r\n* Added CUDA Sanitizer (#83984)\r\n* Added support for multiple outputs in python jiterator  (#77921, #78139)\r\n\r\n## Intel\r\n\r\n* Added a launch script with Best Recipe of Deep Learning on Intel Xeon CPU (_#63932_)\r\n* Enabled Intel\u00ae VTune\u2122 Profiler's Instrumentation and Tracing Technology APIs (ITT) to PyTorch (_#63289_)\r\n* Added unified x86 quantization backend (_#84329_)\r\n\r\n## MPS\r\n\r\n* Added `aten::index_add.out` operator for MPS backend (_#79935_)\r\n* Added `aten::prelu operator` for MPS backend (_#82401_)\r\n* Added `aten::bitwise-not` operator native support for MPS backend (_#83678_)\r\n* Added `aten::tensor::index_put` operator for MPS backend (_#85672_)\r\n* Added `aten::upsample_nearest1d` operator for MPS backend (_#81303_)\r\n* Added `aten::bitwise_{and|or|xor}` operators for MPS backend (_#82307_)\r\n* Added `aten::index.Tensor_out` operator for MPS backend (_#82507_)\r\n* Added `aten::masked_select` operator for MPS backend (_#85818_)\r\n* Added `aten::multinomial` operator for MPS backend (_#80760_)\r\n\r\n## Profiler\r\n\r\n* Integrated Execution Graph Observer into PyTorch Profiler (#75358, #79753, #82895, #84285)\r\n* TorchTidy: experimental tool to identify anti-patterns from traces (#79631, #79874, #79993, #80094, #80108, #80572, #81056, #81273, #81501, #81733, #81740, #81921, #82421, #82248, #82261, #82782)\r\n* Added reporting for OOM events to the Pytorch Profiler. (#80050)\r\n\r\n## Vulkan\r\n\r\n* Added Vulkan support for the following operators:\r\n    * `torch.cumsum` (#78554, #81107)\r\n    * `torch.nn.LSTM` (#78943, #79702)\r\n    * `torch.nn.ReplicationPad2d` (#79057, #79291)\r\n    * `torch.nn.threshold` (#78654, #79717)\r\n    * `torch.nn.BatchNorm2d` (#80510)\r\n    * `torch.nn.LayerNorm` (#80980)\r\n    * `torch.nn.GLU` (#80910, #81729)\r\n    * `torch.select` (#81771)\r\n    * `torch.stack` (#81064)\r\n* Prototype implementations for Quantized Tensors were added (#81491). These implementations still need to be exposed to Torchscript, but so far prototype implementations for the following ops have been added:\r\n    * `torch.quantize_per_tensor` (#81492)\r\n    * `torch.dequantize` (#81493)\r\n    * Quantized arithmetic ops (#81494, #81632, #81640, #81641)\r\n    * Quantized 2D convolution (#81495, #81496, #81497)\r\n    * Quantized `Upsample2D` (#81720)\r\n\r\n## Mobile\r\n\r\n* Added support for dtypes and custom classes in model tracer (#84795)\r\n* Extended Flatbuffer to get mobile_info for NMLML workflows (#78306)\r\n* Added serialization/deserialization of Sparse Quantize Linear Packed Params (#80474)\r\n* Added qnnpack bcsr matrix unpacking and use unpacking in Linear module (#80475)\r\n* Added OwnedOrBorrowedVector for QNNPack BCSR Indices/Values (#80476)\r\n\r\n## Distributed\r\n\r\n#### `Distributed Checkpointing` (Prototyping)\r\n* This is a prototyping effort which enables loading and saving PyTorch models from one or more hosts. Models can use features such as DDP, FSDP and ShardedTensor and they can have a different configuration between saving and loading - for example, save from 4 hosts and load from a single host. Distributed checkpointing has an extensibility API that enables full control of how a model is saved; and a pluggable IO backend. (#83781, #83419, #84952, #84881)\r\n\r\n#### `Distributed(c10d)`\r\n\r\n* Made c10d collective ops dispatcher passable. It allows tracing mechanisms such as LazyTensor and AOTAutograd to observe communications, e.g., : broadcast(#76722), allreduce(#79582), allgather (#79669), reduce_scatter (#79683), reduce  (#79686), gather (#79687), scatter (#79688), alltoall (#79691), barrier (#79777), send/recv (#79779).\r\n* Added UCC process group (#79918)\r\n* Enabled uneven input support for `all_gather`  (#83713) and uneven output support for `reduce_scatter` (#87010)\r\n* Added NCCL PreMul Sum to c10d `ReduceOp` (#84243)\r\n\r\n**`DistributedDataParallel`**\r\n\r\n* Made DDP work with Python process group (#79176)\r\n* Enabled Zero1's ddp_with_overlap for hpu backend (#80438)\r\n\r\n#### `FullyShardedDataParallel`\r\n\r\n* Added forward prefetching option in FSDP API (#85177)\r\n* Added fp16 and bf16 hooks for FSDP (#81711)\r\n* Implemented `sharded_optim_state_dict` and `flatten_sharded_optim_state_dict`. (#77628)\r\n* Added rate limiter (#83917) Thanks to IBM Research team, @lchu-ibm for his contributions to FSDP and @hfwen0502 for the experimental testbed that identified the issues.\r\n* Added an option to keep grads in lower prec (#85223)\r\n\r\n#### `torch.distributed.elastic`\r\n\r\n* Added watchdog to TorchElastic agent and trainers (#84081)\r\n\r\n#### `Activation Memory Management` (Prototyping)\r\n\r\n* We offer a new API, `torch.distributed.algorithms.checkpoint.checkpoint_wrapper` to wrap `nn.Modules` with activation checkpointing or activation offloading to easily use and experiment with activation checkpoint techniques without modifying model code. This makes it simpler to leverage activation checkpointing to reduce memory footprint of your training applications and train larger models. (#83035, #78704, #78854, #79830, #80089, #84907, #84908, #85448, #85449)\r\n\r\n## Infra (RelEng)\r\n\r\n* Enabled multigpu unittests on FSDP (#77947)\r\n* Added feature to do rebase (via comment) onto any branch (#78772)\r\n* Added implementation to allow PR collaborators to revert their PRs (#82360)\r\n* Added torchvision onto the commit pins file (#79151)\r\n* Turned on `-Werror=all` with a few exceptions in Bazel build for CUDA (#79306)\r\n* Prepared for running PyTorch tests with TorchDynamo and skips for known failing tests (#80106)\r\n* Added ROCm build to pull request jobs (#80149)\r\n* Added dynamo test configuration (#80342)\r\n* Enabled ROCm CI for trunk test (#80920)\r\n* Added linux cuda 11.7 workflows (#81089)\r\n* Updated CI docker images and jobs to ROCm5.2  (#81168)\r\n* Added UCC PG build in CI (#81583)\r\n* Enabled periodic builds for CUDA 11.7 (#81688)\r\n* Enabled distributed tests for ROCm (#81751)\r\n* Added New TORCH_UCC_BLOCKING_WAIT env variable (#81791)\r\n* Change functorch pin mechanism to test functorch in pytorch/pytorch now that functorch is inside pytorch/pytorch (#81918)\r\n* Added Python 3.11 nightlies for Linux PyPi (Please note that 3.11 binaries are not fully functional) (#82302)\r\n* Updated ROCm nightly builds to rocm5.2 (#82353)\r\n* Add functorch target to cmake (#83464)\r\n* Upgraded CUDNN version for cuda 11.7 (#84964)\r\n* Enabled pytest-shard for functorch (#85321)\r\n* Enabled CI to run test_ops in parallel (#85528)\r\n* Updated trunk CUDA-10.2 to CUDA-11.7 (#85943)\r\n* Added support for building and running Metal tests in CI (#86073)\r\n* Bumped nvidia docker version and using python 3.10 for cuda11.7 (#82472)\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n* Added `float16` support for `torch.{arange, linspace}` (#80492)\r\n* Added integer support to `torch.index_reduce` (#80464)\r\n* Added a `stable` kwarg to `torch.argsort`  that controls the relative order of equivalent elements (#75162)\r\n* Improved stability of `torch.distributions.kl_divergence`  for two Bernoulli distributions (#79944)\r\n* Improved type annotations for `torch.{as_tensor, as_subclass}`  (#86105)\r\n* Added type promotion support for `torch.{addcmul, addcdiv}` (#74234)\r\n* Added `bfloat16` support for `torch.save` with XLA/HPU tensors (#77534)\r\n* Improved wrapper subclass detection for serialization (#81105)\r\n* Updated python API `TensorOption` signatures for consistency with JIT schemas (#82241)\r\n* Allowed disabling of`torch.library.Library` with PYTORCH_DISABLE_LIBRARY (#85190)\r\n* Enabled `dim=None` for `torch.{mean, sum, nanmean, nansum}` (#81286), (#79881), (#82912)\r\n* Added feature to enable registration of extension device modules as a native module under the torch namespace (#78329)\r\n* Added `logsumexp` to `amp.autocast` (#76330)\r\n\r\n## C++ API\r\n\r\n* Allowed `const T&` access to `ListElementReference` (#83177)\r\n* Redirected print messages to `stderr` in `torch.utils.cpp_extension` (#82097)\r\n* Updated CUDA compiler matrix in `torch.utils.cpp_extension` (#82860)\r\n* Added `__all__` to `torch.utils.cpp_extension`, `torch.utils.hooks` and `torch.utils.show_pickle` (#85331)\r\n\r\n## Autograd\r\n\r\n* Added forward AD coverage for `torch.{amin, amax, nansum, nanmean}`  (#80082),  `torch.scatter_reduce` (except `reduction=prod`) (#85000),  `torch.linalg.det` (#79487),  `torch.{elu_, celu_, selu_}` (#83080)\r\n* Added forward-over-reverse AD coverage for `nn.functional.{binary_cross_entropy} `(#77852) , ` nn.functional.{embedding} `(#79699),` nn.functional.{mse_loss, softplus, l1_loss, smooth_l1_loss, prelu, hardswish}` (#78740), `nn.functional.{nll_loss,  batch_norm, layer_norm, group_norm, cross_entropy, soft_min}`  (#84976) `torch.`{`log_softmax, softmax}`(#84976), `torch.amin, amax, nansum` (#80082)\r\n* Added support a stable double backward on `torch.linalg.det` for real inputs (#80217)\r\n* Added support for kwargs input to function when `torch.utils.checkpoint` with `use_reentrant=False` (#80987)\r\n* Added context manager to disable saved tensor hooks: `torch.autograd.graph.disable_saved_tensors_hooks` (#85971)\r\n* Added new cpp custom function API to inform the backward function whether a gradient is necessary to compute: `ctx->needs_input_grad(idx)` (#82544)\r\n* Added all device types in the pybinded DeviceType enum (#83676)\r\n* Added `check_nan` flag to `torch.autograd.detect_anomaly` which enables users to run anomaly mode without nan checking (#83481)\r\n\r\n## Build\r\n\r\n* Specify \"Generic\" BLAS library name to ensure PyTorch can find the BLAS llibrary (#74269)\r\n* Generate CUDAConfig.h only for CUDA builds (#78218)\r\n* Moved build_variables.bzl and ufunc_defs.bzl from pytorch-root/tools/ to PyTorch root directory (#78542)\r\n* Made lintrunner compatible with M1 (#78628)\r\n* BLAS library is linked privately instead of being linked publicly (#78883)\r\n* Updated build targets to include generated enum_tag.cpp (#79668)\r\n* Use miopen_LIBRARIES and rccl_LIBRARIES directly, when they are valid target for RCCL (#80446)\r\n* Deleted Win specific case for CMake older than 3.1 (#81411)\r\n* Split `.cu` to improve compile times (#81193)\r\n* Added `append_cxx_flag_if_supported` macro (#82883)\r\n\r\n## torch.nn\r\n\r\n* Improved `groups` argument validation for `nn.Conv{1,2,3}d` modules (#77919)\r\n* Improved error message for convolution backward fallback kernel (#81538)\r\n* Reduced memory usage of `nn.Module` full backward hooks by removing reference cycles (#80139)\r\n* Improved `kl_div` at boundary and its general implementation (#80334)\r\n* Improved input shape validation for MKL-backed convolution operations (#76526)\r\n* Improved input validation for `nn.AdaptiveAvgPool2d` (#84061)\r\n* Improved `groups` argument validation for `nn.Conv{1,2,3}d` (#85248)\r\n* Improved input index validation for `nn.MaxUnpool{2,3}d` (#78280)\r\n* Improved listing of public APIs for `optim` and `nn` (#80237)\r\n* Added new operator for `nn.Sequential`: `+` (#81170), `extend` (#81179), `insert` (#81402), `+=`, `*` and `*=` (#81279),\r\n* Added deepcopy support for unitialized parameter (#83809)\r\n* Added nondeterministic alert for `nn.MaxUnpool`{`1,2,3}d` (#84766)\r\n* Added Bfloat16 support for the backward pass of `nn.functional.kl_div` on CUDA (#77676)\r\n\r\n## torch.optim\r\n\r\n* Added support for optimizers with more than 2 betas for LRScheduler (#84486)\r\n* Added `fused` kwarg to `optim.Adam` to enable a fused implementation on CUDA (#85739)\r\n\r\n## Composability\r\n\r\n* Significant hardening and improvements to the `functionalize()` API that lives with functorch (#77129, #77126, #77125, #78199, #77132, #77713, #77714, #78819, #78820, #82008, #82009, #81702, #80416, #80418, #80251, #80526, #82326, #81454, #81471, #83542, #83701, #85975)\r\n* Allow `__torch_dispatch__` subclasses and modes to override more tensor metadata: device/size/stride/dim (#77684, #77970, #78646, #78691)\r\n* Improvements to the `torch.library` API, for registering python functions to the pytorch dispatcher:\r\n    * Improved error checking in `torch.library` (#77990)\r\n    * Make `torch.library` decorators return function, to allow for chaining (#78996)\r\n* Ported `cholesky`, `linalg_qr`, `linalg_eigh` and `linalg_eighvalsh` to structured kernels, giving them support with meta tensors (#79300, #79054, #79072)\r\n* Added python decompositions for many torch operators. This adds meta tensor coverage for a large number of pytorch operators (#77930, #79768, #79808, #84062, #84350, #80219, #78350, #79667, #81003, #81420, #81113, #81241, #81765, #82284, #80497, #80358, #80182, #80737, #81734, #81826, #78461, #78468, #78525, #78914, #78919, #79900, #79225, #80964, #83235, #84108, #84451, #78602, #78603, #78527, #78604, #78992, #78993, #78997, #79278, #79341, #79311, #79411, #79581, #81800, #79834, #82309, #79975, #82587, #82603, #83191, #84349, #84460, #85793, #86057)\r\n* Beefed up API for printing out operators registered to the dispatcher (#78995)\r\n* Trued up `c10::FunctionSchema::operator<<` to print native_functions.yaml syntax (#79645)\r\n* Made it so that it is valid to set metadata after detach calls, like `x.detach().resize_(...)` (#83590)\r\n* Optimized `torch.ops.ns.opname.overload` accessor in `__torch_dispatch__` (#85132)\r\n\r\n## Dataloader\r\n\r\n* Added shape checking on argument `weights` for `WeightedRandomSampler` (#78585)\r\n* Added support for `radom_split` to accept percentages as `lengths` (#78877)\r\n* Extended collate function that can register collate functions to handle specific batch types (#85748)\r\n\r\n## Functorch\r\n\r\n* `functorch.jacfwd` now accepts a `randomness` kwarg (#84220)\r\n* Improved the error message when using `vmap` on a function with no Tensor inputs (#83016)\r\n* Relaxed the `Tensor.as_strided` batching rule. This is a primitive used in forward-mode AD (among other things) and improves composability of vmap with other transforms (like jvp).\r\n* `functorch.functionalize`: added support for in-place views on inputs (#83993)\r\n* `functorch.functionalize`: moved this API out of the `functorch.experimental` namespace (#85742)\r\n* Added vmap support for `linalg.cholesky`, `linalg.eigvals`, `linalg.eigvalsh`, `linalg.matrix_norm`, `linalg.matrix_power`, `linalg.norm`, `linalg.tensorinv`, `linalg.solve_triangular`  (#82177)\r\n* Added vmap support for `linalg.solve` (#82814)\r\n* Added vmap support for `linalg.cross` (#83759)\r\n* Added vmap support for `linalg.matrix_rank` (#83760)\r\n* Added vmap support for `linalg.pinv` (#83761)\r\n* Added vmap support for `Tensor.fill_` (#84015)\r\n* Added vmap support for `linalg.lstsq` (#82325)\r\n* Added vmap support for `linalg.lu_solve` (#85175)\r\n\r\n## LinAlg\r\n\r\n* Added a `driver=` kwarg to `torch.linalg.svd` and `svdvals`. Add cusolver gesvdaStridedBatched driver to `linalg.svd` (_#74521_)\r\n* Added opteinsum backend to `torch.einsum` (_#86219_)\r\n* Added path optimize kwarg to `einsum` (#84890)\r\n* Call view instead of sum in `einsum` to remediate MPS regression (#87135)\r\n* Ensure that we contract left to right in `einsum` (#87199)\r\n* Fixed opt_einsum defaults to be more reasonable (#86985)\r\n\r\n## Sparse\r\n\r\n* Added `sparse_dim` and `dense_dim` for batched, hybrid CSR/CSC/BSR/BSC (#80565, #80901)\r\n* Added support for conversion between batched CSR/CSC/BSR/BSC and dense Tensors (#80781, #83084, #83086, #78025, #80354, #82120)\r\n    * Conversion between SparseBsr and Strided (#78025)\r\n    * Added support for BSR <-> Strided Conversion (#80354)\r\n* Added support for conversion between CSR and CSC (#85091)\r\n* Added support for conversion between BSR and BSC (#85091)\r\n* Added partial support for CSR/CSC/BSR/BSC inputs to `mm`, `addmm`, `matmul` and `F.linear` (#85551, #85308, #85379, #85307)\r\n* Added support for COO to `permute` (#79707)\r\n* Added support for ComplexHalf to `torch.nonzero` and `add(dense, CSR)` (#79062)\r\n* Added support for CSC/BSR/BSC to unary zero-preserving functions. (#78173, #85031)\r\n* Added support for batched BSR/BSC to `transpose` (#82122)\r\n* Added support for scalar together with COO inputs to `mul` (#82962)\r\n* Added support for CSC/BSR/BSC to `empty_like` (#82310)\r\n* Added support for batch dims of CSR/CSC/BSR/BSC to `select` (#82119)\r\n\r\n## torch.fx\r\n\r\n* In constant folding, added `device_for_folded_attrs` parameter and sets the `requires_grad` option for a folded tensor (#79067)\r\n* Mode-based tracing in make_fx (#79638, #84238)\r\n* Made executor handle kwargs (#79858)\r\n* Added `ignore_parameters_and_buffers` flag to FxGraphDrawer (#79982)\r\n* Enabled an `is_fx_tracing` flag in the FX tracer (#80255)\r\n* Attached ProxyTorchDispatchMode to ProxyTensor and use it in `__torch_dispatch__` (#82549)\r\n* Used `enable_tracing` flag for ProxyTorchDispatchMode instead of modifying torch dispatch mode stack inner attributes (#82643)\r\n* Improved legalize_graph pass in FX (#82874)\r\n* Implemented `__deepcopy__` for fx.Tracer (#83130)\r\n* Hackde up make_fx to natively support varargs (#83210)\r\n* Updated proxy_tensor.py to support List input/output (#83302)\r\n* Added *_only and all/any pytree utilities (#83316)\r\n* Deleted ProxyTensor wrapper subclass (#83330, #83646)\r\n* Added support for partial decompositions in make_fx (#83770)\r\n* Added metadata field to fx.GraphModule (#84378)\r\n* Added option to maintain the FX graph execution order after splitting_module (#85188)\r\n\r\n## JIT\r\n\r\n* Added PReLU to MKLDNN convertible Ops in JIT optimize_for_inference (#79011)\r\n* Enabled `torch._refs.var` for nvFuser executor (#79517)\r\n* Fixed nvFuser's `where` (tensor, python_scalar, tensor) type promotion (#80347)\r\n* Added ComplexDouble scalar creation bindings to nvFuser's Python API (#80522)\r\n* Added real and imag to NVFuser and its python frontend (#79824)\r\n* Added Nvfuser opt in for decomposition (#81134)\r\n* Added `torch.jit.fuser()` option for disabling all fusers (#81731)\r\n* Added support for symbolic diff for `silu` (#81724)\r\n* Added NVFuser support for (`prims.sign, refs.sign, squeeze, native_batch_norm, transpose`) (#83167, #85562, #84629, #84117)\r\n* Use high precision accumulate buffer for bf16 accumulation in NNC (#84402)\r\n\r\n## Quantization\r\n\r\n* Improved quantization support for `masked_fill` (#78368, #85108)\r\n* Improved quantization support for `index_put` (#78384, #85685)\r\n* Improved quantization support for `LSTM` and `MultiHeadAttention` (#79959, #79956, #79960, #83304, #85068)\r\n* Added support for quantized `matmul` (#83885)\r\n* Introduced a more stable conv_bn fusion for QAT training (#85744)\r\n* Removed warnings from using torch.tensor(value) (#84277)\r\n\r\n## ONNX\r\n\r\n* Added operator support for `torch.tensor_split` (#77437), `torch.lerp` (#78891), `torch.movedim` and `torch.moveaxis` (#78931), `torch.scatter_add` (#79103), `torch.argsort` (#80234), `aten::native_dropout` (#81743), `aten::native_layer_norm` (#81754), `aten::convolution` (#81815), `aten::_log_softmax` (#81804), `aten::layer_norm` for ONNX opset version 17 using LayerNormalization (#84293), `nn.init.normal` (#84149)\r\n* Added quantization support to more single output ops (#83008) `aten::reshape`, `aten::reshape_as`, `aten::t`, `aten::transpose`, `aten::numpy_T`, `aten::expand`, `aten::expand_as`, `aten::embedding`, `aten::embedding_bag`, `aten::view`, `aten::select`, `aten::eq`, `aten::ne`, `aten::gt`, `aten::lt`, `aten::le`, `aten::ge`, `aten::elu`, `aten::selu`, `aten::hardtanh`, `aten::hardswish`, `aten::as_strided`, `quantized::sigmoid`, `quantized::layer_norm`, `quantized::group_norm`, `quantized::leaky_relu`, `quantized::instance_norm`\r\n* ONNX operators are exported with names containing their associated scope from `nn.module` (#82038), (#82039), (#82040)\r\n* Introduced runtime type checking with the beartype library in all public APIs (#83673), (#84091)\r\n* All `torch.onnx` APIs now support runtime type checking when @beartype is present in the Python environment. A warning is emitted when a type mismatch is detected.\r\n* This feature is experimental. To turn all warnings into errors, set the environment variable `TORCH_ONNX_EXPERIMENTAL_RUNTIME_TYPE_CHECK=ERRORS`. To disable this behavior, set `TORCH_ONNX_EXPERIMENTAL_RUNTIME_TYPE_CHECK=DISABLED` which effectively makes it a no-op.\r\n* Improved shape type inference (#78999)\r\n* Turn on ONNX shape inference by default (#82767)\r\n* Enabled data propagation from ONNX (#80730)\r\n* Introduced SARIF (#85428) for `torch.onnx` submodule\r\n* Improved warnings and errors (#78441), (#78309), (#83332), (#85179), (#83007)\r\n* Updated ONNX submodule to 1.12 (#79585)\r\n* Apply Common Subexpression Elimination pass to ONNX export (#85665)\r\n\r\n## AMD\r\n\r\n* Support benchmark flag for MIOpen (#77438)\r\n* Correctly handle the error codes of hipGetDeviceCount (#80405)\r\n* Use torch._C._cuda_getArchFlags to get list of gfx archs pytorch was built for (#80498)\r\n* `torch.cuda.is_bf16_supported()` returns True (#80410)\r\n* Workaround missing hipProfilerStart/Stop (#82778)\r\n* Enabled jiterator on ROCm (#77982)\r\n* Enabled MIOpen fused convolution relu (#82002)\r\n* Restore MIOpen benchmark flag default to true (#82656)\r\n* embedded_interpreter_hip to enable torch::deploy on AMD (#83329)\r\n* Add HIP libs into torch deploy init list & corresponding dependency for CURE benchmark running on AMD (#83434)\r\n\r\n## CUDA\r\n\r\n* Added synchronize hooks (#84427)\r\n* Added CSAN support for CPU synchronizations (#84428)\r\n* Return device count using nvml (#84879)\r\n* Reworked printing tensor aliases in CSAN error message (#85008)\r\n* Added jiterator support when dtype is `complex32` for `tan`, `atan`, `sin`, `asin` (#77802),(#77606)\r\n* Added jiterator support when dtype is complex for `logical_{or, xor}` (#75947)\r\n* Reduced overhead of `get_current_stream` (#78066)\r\n* Added an argument to specify warmup iterations in make_graphed_callables (#78124)\r\n* Small improvements to `device_count` (#85192)\r\n* Memoize `torch.cuda.device_count` (#84878)\r\n* Remove the construction of unused tensors in fallback convolution implementation (#79183)\r\n* `__launch_bounds__` for `torch.mode` with CUDA 11.7 (#79710)\r\n* Removed synchronization for D2H copy with a different dtype  (#80607)\r\n* Added nondeterministic alert to CUDA `cumsum` (#75693)\r\n* Annotated CUDACAchingAllocator snapshots (#82146)\r\n* CUDACachingAllocator snapshots from C++ (#86190)\r\n* Propagate CUDAOutOfMemoryError to Python. (#83146)\r\n* Set cublas workspace size to 4M (#74159)\r\n* Allow changing the cuda allocator settings even after the process started (#84970)\r\n* Fixed exception handling, improve overheads and avoid constructing storage for element size for DLPack (#84612)\r\n* Added BFloat16 for fast layernorm (#83971)\r\n* Added BFloat16 support for `torch.{im2col,col2im}` on CUDA (#84372)\r\n* Added Bfloat16 support for `ReflectionPad` (#84949)\r\n* Added explicit `__all__` to torch.cuda (#85193)\r\n* Set CUDA_MODULE_LOADING to LAZY when not set by the user (#85692)\r\n* Support cuDNN Errata Filter (#73934)\r\n* Allow the number of kernels profiled under torch.backends.cudnn.benchmark = True to be limitedCudnnv8 benchmark limit (#78299)\r\n* Update tests and dispatching for CUDNN V8 API behavior for bfloat16 convs (#81139)\r\n\r\n## Intel\r\n\r\n* [RFC] Enable oneMKL&oneDNN on-demands verbose functionality (_#63212_)\r\n* Updated ideep for NNC post-op (_#82705_)\r\n* Enabled native 1d spatial input for Intel xpu (_#82301_)\r\n* Added loss operators to fp32 cast policy of AutocastCPU (_#81689_)\r\n* Added bfloat16 support for `lerp` on CPU (_#84327_)\r\n* Added `prelu` op and module for quantized CPU backend (_#73491_)\r\n* Enabled mkldnn matmul for aarch64 bf16 devices (#85546)\r\n\r\n## MPS\r\n\r\n* Added ranked tensors for addcmul ops in MPS instead of constants and update MacOS version check (_#78354_)\r\n* Moved MPS compat check into common comparison machinery of `TensorLikePair` (_#77836_)\r\n* Made MPS buildable with either XCode or CommandLineTools (_#79430_)\r\n* Improved MPS `aten::softplus` operator by adding RankedPlaceholder for graph nodes instead of constants (_#81169_)\r\n* Extended MPS Conv1D operation for NHWC format (_#83121_)\r\n* Added support for 1D weights in MPS linear layer (_#85752_)\r\n* Added full support for serialization of MPS Tensors (_#79465_)\r\n* Added support for 1D bias in MPS operation `torch.addmm `(_#81519_)\r\n* Added torch dispatch stub code for MPS backend (_#82612_)\r\n* Use convenience helper function `dispatch1DJob` for MPS native implementations (_#82982_)\r\n* Enabled support in MPS for `torch.adaptive_avgpool_2d` for larger output sizes (_#85726_)\r\n* Extended support in MPS for `torch.constant_pad_nd` for 4D+ padding (_#85991_)\r\n\r\n## Profiler\r\n\r\n* Propagate metadata into `Engine::evaluate_function` event. (#77696)\r\n* Switched to nanoseconds for Result's internal representation (#77697)\r\n* Made profiler table column widths changeable via arguments (#85203)\r\n\r\n## Vulkan\r\n\r\n* Enabled higher dimensional input in `torch.nn.linear` (#81773)\r\n* Vulkan tensor views now infers dim size when -1 is provided as input (#81668)\r\n* Vulkan prepacked op contexts will now release the deserialized CPU tensors from memory upon construction (#83587)\r\n* Vulkan shader codegen is now Windows compatible (#85241)\r\n\r\n## Mobile\r\n\r\n* Allowed tracing multiple input models at once (#84833)\r\n* Leaky `relu` in metal shader (#78544)\r\n* Added detailed error message for iOS test (#79140)\r\n* Remove dcode duplications and refactor (#79184)\r\n* Optionally run fbgemm in tracer (#83531)\r\n* Added hardshrink op to metal backend (#82224)\r\n* New flatbuffer_loader functions that do not depend on flatbuffers.h (#82618)\r\n* Added `max_pool2d`, `linear`, `conv2d` FP32 operator tests for XNNPACK (#83131)\r\n* Removed flatbuffer types/headers from flatbuffer_serializer[_jit].h (#82619)\r\n* Migrated remaining pytorch code to use new flatbuffer_loader.h APIs (#82620)\r\n* Remove flatbuffer types/headers from flatbuffer_loader.h (#82893)\r\n* Use flatbuffer of alternate namespace (#82952)\r\n* Hide flatbuffer build dependencies (#82953)\r\n* Renamed flatbuffer_all to flatbuffers_jit (#82826)\r\n* Renamed flatbuffer_serializer to *_mobile or* _full_jit  (#82827)\r\n* Created flatbuffers_mobile (#82828)\r\n* Added API for profiling backend memory events for Edge CPU profiler (#80350)\r\n* Switched mobile targets to flatbuffers_mobile (#82829)\r\n* Added an option to avoid adding base ops to static op library for Edge (#84360)\r\n* Fixed load_extra_only api for flatbuffers and enable flatbuffers in mobile for OSS properly (#83855)\r\n* Remove unused field 'order_' in nnapi.h (#84067)\r\n\r\n## Distributed\r\n\r\n#### `Distributed(c10d)`\r\n\r\n* c10d API improvements:\r\n    * Introduced util functions in c10d `get_local_rank`, `get_global_rank` and `get_global_ranks` (#82134, #84363)\r\n    * Replaced internal API `_all_gather_base` with a public API `all_gather_into_tensor` (#85686)\r\n    * Replaced internal API `_reduce_scatter_base` with a public API `reduce_scatter_tensor` (#85867)\r\n* Improvements to c10d error messages:\r\n    * Added `ncclGetLastError` (#83724, #85825, #85850)\r\n    * Added closing parentheses to the CollectiveFingerprint (#79723)\r\n    * Added tensor deserializer and included rank and collective type to the error messages (#79724)\r\n    * Adopted `ncclRemoteError` (#85887)\r\n* Passed group ranks and options to third party distributed backends (#73164)\r\n* Enabled NCCL_DESYNC_DEBUG when TORCH_DISTRIBUTED_DEBUG is set to DETAIL (#83881)\r\n* Added a soft error handling mode `NCCL_ASYNC_ERROR_HANDLING=2` that does not crash the process (#84386)\r\n* Upgraded NCCL to 2.14.3 (#85367)\r\n\r\n#### `Distributed Optimizer`\r\n\r\n* Added functionality for save and restore step counter for model averanger in PostLocalSGDOptimizer (#78988)\r\n\r\n#### `DistributedDataParallel`\r\n\r\n* Enabled the static graph to print unused parameters in debug mode for DDP. (#81929)\r\n* Enabled stateful PowerSGD communication hook now can be saved and reloaded to resume training (#79334)\r\n\r\n#### `FullyShardedDataParallel`\r\n\r\n* Allowed different `optim_input` orders across ranks (#78599)\r\n* Added profiling range for FSDP.backward (#78479)\r\n* Enabled NamedTuple support for FSDP (#83055)\r\n* Added FSDP communication hook interface for NO_SHARD strategy (#79833)\r\n* Moved the `sharded_state_dict` logic to the post hook to avoid OOM (#82613)\r\n* Added ability to iterate through dataclasses in fsdp.utils (#82638)\r\n* Enabled passing kwargs to load_state_dict (#83309)\r\n* Used `_init_from_local_tensor` to create ShardedTensor to avoid communication overhead (#82911)\r\n* Added communication hook for sharded strategies (#83254)\r\n* Changed to print exec order only in debug mode (#83868)\r\n* Ensured that all ranks use the same order to iterate through optimizer states (#84654)\r\n* Optimizer states may be on CPU, copied them to GPU before gathering (#84708)\r\n* Handled the `state_dict` on CPU cases (#85640)\r\n* Add `FSDPExtensions` for TP support (#85039)\r\n* Ignored buffers that are non-persistent. (#85740)\r\n* Delayed moving tensor to CPU until necessary for optim_state_dict() (#85761)\r\n* Dequeue one event instead of flushing for rate limit (#86165)\r\n\r\n#### `torch.distributed.elastic`\r\n\r\n* Implemented a named pipe based watchdog timer (#83695)\r\n\r\n## Infra (RelEng)\r\n\r\n* Consolidated all python targets in the tools folder (#80408)\r\n* Improved ios simulator test in CI (#80459)\r\n* Add functorch testing shard in CI (#81283)\r\n* Added functorch shards for windows CI (#82161)\r\n* Added functorch shard for mac x86 tests, linux cu102 tests (#82000)\r\n* Added CI workflow to build official docker images with multiarch (#83437)\r\n* Sharded `trunk / linux-bionic-cuda10.2-py3.9-gcc7 / test (default` from 2 -> 4 (#83424)\r\n* Migrated workflows from 18.04 to 22.04 (#83861)\r\n\r\n\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n* Fixed `dim` out of range check for `logcumsumexp` on CUDA when the source tensor is empty(#78284)\r\n* Added missing `__init__.py` for `torch.utils.jit` (#78629)\r\n* Fixed backward crash for `gather` with an empty index tensor when `sparse_grad=True` (#78698)\r\n* Added type annotations to `torch.distributions.kl_divergence` (#78432)\r\n* Fixed erroneous inclusion of `end` in the output of `torch.arange` for some inputs (#80758)\r\n* Fixed `torch.distributions.Transform` to be pickle-able (#81707)\r\n* Added check that `self` and `mask` are on the same device for `torch.masked_fill` (#82737)\r\n* Fixed potential ref cycle creation in `torch.utils.checkpoint` (#82776)\r\n* Fixed `Tensor.__hash__` for Tensor subclasses (#83174)\r\n* Fixed `torch.cat` for 0-dim tensors with different dtypes (#83391)\r\n* Fixed `torch.equal` on CPU when inputs have different dtypes (#83350)\r\n* Fixed data-dependent shapes in `torch.districutions.{HalfCauchy, HalfNormal}` (#84322)\r\n* Added check that the size of the last dimension of `tau` is less than or equal to that of `input` in `torch.ormqr`  (#85278)\r\n* Added check that `weights` is a 1D tensor in `torch.bincount` (#85881)\r\n* Fixed segfault for `out` arguments that have a large number of dims (#85294)\r\n* Fixed comparison ops with scalar arguments by removing overflow check (#78881)\r\n* Normalized `torch.utils.dlpack` strides to 1 where size of corresponding dimensions < 2 (#83158)\r\n* Added a check in `torch.empty_strided` that `sizes` has the same dimensionality as `strides` (#82422)\r\n* Fixed `torch.istft` default output length to prevent trimming of last element (#80031)\r\n\r\n## C++ API\r\n\r\n* Fixed missing antialiasing path to the interpolation for bicubic mode (#84599)\r\n* Added `IListRefTag::Materialized` to `IListRefIterator` destructor. (#85467)\r\n* Fixed `im2col` by adding a check that `pad_width` and `pad_height` are non-negative (#85541)\r\n* Fixed `check_compiler_ok_for_platform` on non-English locales in `torch.utils.cpp_extension` (#85891)\r\n\r\n## Autograd\r\n\r\n* Corrected the forward AD formula of `torch.sgn` which fixed forward-over-backward for `torch.linalg.svd `and other spectral decompositions, and `torch.norm`, `torch.linalg.{norm, matrix_norm}`(#80082)\r\n* Fixed derivatives of convolution overridable backward (#80840)\r\n* Updated setting non-float non-complex values for forward AD dual tensor to properly error(#78361)\r\n* Fixed forward AD to not set tangent as-is in some situations (#79664, #79653)\r\n* Fixed cpp hooks, retains grad, and `backward(inputs=)` behavior in-place (#79996)\r\n* Relaxed storage layout checks for forward AD when zero-numel tensor (#81055)\r\n* Fixed leak when `create_graph=True` and full backward hook registered (#82788)\r\n* Fixed view and in-place interaction when grad_fn is first accessed in no-grad mode (#83872)\r\n* Updated backward of `torch.stack` to correctly handle implicit real->complex casting (#84993)\r\n* Fixed gradients for `torch.nn.functional.{leaky_relu, threshold}` when inplace=True (#85634)\r\n* Corrected autocasting behavior in  `torch.utils.checkpoint` when use_reentrant=False (#81766)\r\n* Fixed gradcheck when outputs that don't require grad precede those that do (#77743)\r\n* Fixed backward and double backward for `nn.functional.binary_cross_entropy_with_logits` (#80083)\r\n* Fixed derivatives of `norm(p=inf)` (#78105)\r\n* Fixed forward AD when conj-ness of primal and tangent of the dual tensor tensor do not match (#78358)\r\n\r\n## Build\r\n\r\n* Use C++17 for RocksDB 7 header. (#75741)\r\n* Fixed Windows builds with _DEBUG flag (bbe8d019f2)\r\n* Pass WITH_BLAS option from environment to CMake (#78037)\r\n* Remove `-Wno-unused-but-set-variable` for clang 13.0.0 (#79666)\r\n* Fixed variable typo for USE_SYSTEM_PYBIND11. (#80272)\r\n* Fixed compilation errors during build with clang13 (#80916)\r\n* Added missing -fexceptions flags during PyTorch build (#81394)\r\n* Fixed CMake dev warning (#81580)\r\n* Fixed false positive AVX, AVX2 and AVX512 detection with MSVC (#82554)\r\n* Fixed NCCL detection issues of the Gloo library (#82773)\r\n* Fixed objcopy version detection in NCCL cmake process (#82774)\r\n* Fixed build error by changing COLORIZE_OUTPUT option to USE_COLORIZE_OUTPUT in cmake file (#83716)\r\n* Set default value for NCCL make to MAX_JOBS if ProcessorCount returns 0 (#84231)\r\n* Fixed intermittent link errors in NCCL build (#84245)\r\n* Deleted `torch._dl` extension (#84361)\r\n* Used unified source file list for BUCK build (#84770)\r\n\r\n## Complex\r\n\r\n* Fixed the derivative of `torch.acosh` for complex numbers (#80841).\r\n* Removed unused conjugate kernels for real dtypes (2.2MB reduction in CUDA binary size) (#80374).\r\n\r\n## torch.nn\r\n\r\n* Fixed `nn.Embedding` \u2018s `max_norm` argument when forward mode AD is used (#78560)\r\n* Fixed `nn.ChannelShuffle` when given empty Tensors (#77029)\r\n* Fixed `nn.RReLU` backward on CUDA (#80434)\r\n* Fixed spurious warnings in `torch.nn.parallel.*` APIs (#81476)\r\n* Fixed `nn.Conv2d` fallback implementation for single channel inputs and channels last weight (#82392)\r\n* Fixed segfault in adaptive pooling for specific index values (#84010)\r\n* Fixed type annotation in `nn.Conv{1,2,3}d` for in_channels (#84302)\r\n* Fixed `nn.GeLU` for empty inputs (#84926)\r\n* Fixed correctness issues for `nn.Conv2d` on ARM-based machines (#85711)\r\n* Fixed `nn.ParameterList` printing of Tensors on the \u201cmeta\u201d device (#78529)\r\n* Fixed channels-first behavior for `nn.MaxPool3D` on CUDA (#80748)\r\n* Fixed input shape validation `nn.MaxPool1d` (#85594)\r\n* Fixed `nn.Softmax` for large input tensors (#84182)\r\n* Fixed lower and upper bound checks for `nn.RReLU` (#84996)\r\n* Fixed edge cases in `torch.nn.grad` by calling into the c++ backward kernel directly (#81839)\r\n* Fixed `torch.nn.PixelShuffle` for empty inputs (#86262)\r\n* Fixed consistency of output and input dtypes for `torch.nn.BatchNorm` (#84410)\r\n\r\n## torch.optim\r\n\r\n* Fixed `optim.SGD` `maximize` flag when `momentum` is involved (#81859)\r\n* Fixed temporary bug where checkpoints from optimizers created with older PyTorch version could not be loaded (#83588)\r\n* Fixed memory leak in `optim.lr_scheduler.CyclicLR` (#85462)\r\n* Fixed initialization of `lr` in `optim.lr_scheduler.SequentialLR`  (#72856)\r\n\r\n## BetterTransformer\r\n\r\n* Cleaned up native transformer implementation (#78265)\r\n* Added fastpath test for mask check flag (#82999)\r\n* Added check for contiguous well-formed mask (#79927)\r\n* Introduced mask contiguity check function (#79186)\r\n* Fixed issue in softmax.cu with transformer error when mask `seqlen > 1024`  (#83639)\r\n* Disabled Transformer/MHA fast path when autocast is enabled (#84722)\r\n* Moved odd `num_head` in TransformerEncoder to `slow_path` (#83483)\r\n\r\n## Composability\r\n\r\n* Fixed `__torch_function__` bug in getindex that causes an error not set exception (#78781)\r\n* Fixed `__torch_dispatch__` usage with inplace views (#79902)\r\n\r\n## Dataloader\r\n\r\n* Fixed `NoneType` object has no attribute `python_exit_status` when `DataLoader` exits (#83985)\r\n\r\n## Functorch\r\n\r\n* `functorch.grad`: fixed silent correctness issue from calling a view operation on a captured tensor followed by an in-place operation (#85374)\r\n* `functorch.jacrev`, `functorch.jacfwd`: fixed loud in-place errors when passing in inputs to the transforms and mutating them (#84914, #84915)\r\n* `functorch.vmap`: Fixed support for in-place view operations (`Tensor.unsqueeze_`, `Tensor.transpose_`, `Tensor.t_`, `Tensor.squeeze_`) (#82899, #82903, #82972)\r\n* `functorch.vmap`: added an error on incorrect `weight` shape to `torch.nn.functional.prelu` (#83106)\r\n* `functorch.vmap`: fixed support for multinomial (#83838)\r\n* `functorch.vmap`: fixed incorrect support for `conv_transpose` with `groups > 1` (#84938)\r\n* Fixed `vmap` x `vjp` x `vjp` composition for `torch.nn.functional.prelu` (#84939)\r\n* Fixed printing tensors that are not being transformed over inside functorch transforms (#85556)\r\n* Disallowed saved tensor hooks in functorch transforms to avoid silently incorrect behavior(#85972)\r\n* Fixed `cross` to match unbatched behavior (#86926)\r\n\r\n## LinAlg\r\n\r\n* Strengthen the preconditions of `linalg.cross` (_#83798_)\r\n* Fix memory issues in `linalg.lstsq` (_#85357_)\r\n* Fix `linalg.lu_solve`/`torch.unpack` to prevent bad memory usage on CPU (_#85922_)\r\n* Preserve the dim of the input in `matrix_exp`. (_#81330_)\r\n\r\n## Sparse\r\n\r\n* Fixed COO Tensors with less than two non-zero elements to always be marked coalesced. (#82426, #82085)\r\n* Fixed CUDA kernel launch misconfiguration for `mul` on tiny COO tensors (#80254)\r\n* Fixed silent type promotion bug by `select` if given all zero integer COO tensors(#82215)\r\n* Fixed CUDA kernel coverage on 0-sized dense inputs for `torch.sparse.sampled_addmm` (#85194)\r\n\r\n## torch.fx\r\n\r\n* Fixed bug where curly brackets were not properly escaped in FxGraphDrawer (#83604)\r\n* Fixed torch.fx.wrap to use the callable `function.__name__` rather than `function.__code__.co_name` (#84373)\r\n* Added strictness check and made tensors into leaves if input tensors were leaves (#77474)\r\n* Used getattr_recursive instead of getattr when splitting (#80011)\r\n* Stopped ProxyTensor from turning aten::lift tensors into proxy objects (#81024)\r\n* Fixed named_modules to be subscriptable (#81258)\r\n* Fixed `to_folder` by adding custom_builtins to dump (#81433)\r\n* Correctly unpacked constants when used in multi-return output (#82568)\r\n* Replaced module name for torch.ops (#82395)\r\n* Removed unnecessary `import warnings` (#82760)\r\n* Don't constant propagate through nondeterministic functions (#83650)\r\n* Don't extract tensor metadata from sparse tensors (#83669)\r\n* Skipped folding side-effectful functions (#84016)\r\n* Fixed make_fx issue by introducing get_attr into symbolic tracing (#84011)\r\n* Disabled autocast cache during aotdispatch (#84035)\r\n* Modified split_by_tags to retain output order (#84136)\r\n* Made NormalizeArgs preserve node type (#85637)\r\n* Fixed PyTree unpacking carrying forward type annotations (#81906)\r\n\r\n## JIT\r\n\r\n* Fixed conv-batchnorm folding for previously-broken datatype inputs during JIT freezing (#78241)\r\n* Fixed lightweight dispatch OOM error by introducing selective build (#79215)\r\n* Used signed integers in `CalculatedNecessaryArgs` to avoid underflow with schemas where all args have defaults. (#79331)\r\n* Fixed indexing into a tensor with a tuple (#79335)\r\n* Propagate `map_location` arg to `torch.jit.load` in `torch.load` (#78733)\r\n* Improved JIT autodiff heuristics for determining whether outputs require gradients (#78392, #79498)\r\n* Used streams for `import_ir_module` for pickle case to reduce memory usage (#80131)\r\n* Added scripting support for \"start\" kwarg in `enumerate()`  (#80585)\r\n* Turned off arc in CoreML backend, because throwing exceptions in arc code leaks memory (#79928)\r\n* Suppressed virtual-dtor check on llvm_jit to fix NNC build (#81449)\r\n* Fixed annotation extraction for python 3.10 (#81334) (#81334, #81506)\r\n* Fixed `std::out_of_range` when using NNC and `ConstantChunk` input shapes are unknown (#82698)\r\n* Limits constant chunk propagation for pw-node-only in NVFuser (#83083)\r\n* When encountering dynamic types, one should cast it recursively. (#83218)\r\n* Fixed handling of empty dim list in `sum_mean_dim` symbolic shape fn (#83357)\r\n* Check existence of the array ref when tracing `resize_` to avoid `_MapBase::at runtime` error (#81422)\r\n* Fixed `define_constant` pybind signature to match `std::complex` scalar in NVFuser (#83684)\r\n* Cast to signed char to fix aarch64 build (#84429)\r\n* Support `torch.ScriptObject` in `torch::jit::as_object` (#84398)\r\n* NVFuser torchbench patch to take nvprim fallback when no cuda tensors are provided as inputs (#84411)\r\n* Fixed coreml gpu flag not set (#84725)\r\n* Print the real type for function schema arguments (#85103)\r\n* Fixed `torch.jit.trace` check that was causing tracing to fail for MPS inputs (#84850)\r\n* Throw an error instead of segfaulting when passing `None` to futures (#85304)\r\n* Cherry pick sorting patch for NVFuser fusion segmented (#85620)\r\n* Support freezing modules that don't have a forward method (#85779)\r\n\r\n## Quantization\r\n\r\n* Added channel axis bound checking in `fused_moving_avg_obs_fake_quant_*` (#78148)\r\n* Disable use of qnnpack with `ceil_mode` of the `avgpool` op (#79028)\r\n* Improve subpackage import in `torch.nn.quantized` (#84141)\r\n* Fix segmentation fault in `QTensor.choose_qparams_optimized` (#85552)\r\n* Enhance the `_rebuild_qtensor` function to support other device type other than CPU (#78234)\r\n* Fix `at::from_blob_quantized_per_tensor_affine` strides calculation (#79314)\r\n* Fix embedding quantization issue when memory format is not `contiguous` (#82605)\r\n* Fix dispatch declaration bug about quantized op (#83649)\r\n* Moved the order of x86 engine to avoid changing the default qengine (#86631)\r\n\r\n## ONNX\r\n\r\n* Fixed `aten::mul` with Boolean inputs (#81671)\r\n* Fixed `add` and `sub` for non-tensor inputs (#81736)\r\n* Fixed `RReLU` eval mode behavior (#82678)\r\n* Fixed onnx optional node type in for/if block (#83599)\r\n* Fixed `Interpolate`: use `half_pixel` instead of `pytorch_half_pixel`. (#80003)\r\n* Fixed `argmin` and `argmax` edge case consistency with PyTorch. (#79503)\r\n* Shape Type Inference and Propagation\r\n* Fixed shape inconsistency when exporting scalar `log2` (#78701)\r\n* Fixed inconsistent `rand` dtype (#79193)\r\n* Fixed linalg `norm` output's shapes and dtypes (#79506)\r\n* Fixed `any` and `all` outputs' shape (#79371)\r\n* Fixed `prelu` output's shape (#79846)\r\n* Fixed onnx logical functions' dtype (#79339)\r\n* Fixed `hardshrink` and `softshrink` output's shape (#79695)\r\n* Fixed quantization outputs' dtype (#79690)\r\n* Fixed reduce node shape inference (#85765)\r\n* Fixed bug using `std::copy_if` (#80999)\r\n* Fixed default function value in `_optimize_graph` (#83996)\r\n* Fixed constant folding unexpectedly adding folded constant as initializer (#79552)\r\n* Fixed autograd subgraph recording with nested graphs (#82852)\r\n* Disabled autocast cache in exporter (#84219)\r\n* Removed static None graph output (#82623)\r\n* Fixed float point detection for optional tensor (with unknown rank) within a list (#81386)\r\n* Support `device().type()` string comparison with constant (#86168)\r\n* Fixed `scalar_type_analysis` metadata for copied constant (#86716)\r\n* Fixed triu/tril export with diagonal input (#86843)\r\n* Ignore `print(Tensor)` during tracing (#86223)\r\n* Updated training state logic to support ScriptedModule (#86745)\r\n\r\n## AMD\r\n\r\n* Fixed memory cross-border access on the ROCM platform (#76100)\r\n* Set nvfuser default to disabled (#86369)\r\n\r\n## CUDA\r\n\r\n* Fix how we handle host memory in CUDA `getDeviceFromPtr` (#76902)\r\n* Only sync CUDA if the operation is run on GPU (#80328)\r\n* Do not use `thrust::lower_bound` on device (#80746)\r\n* Fix `set_requires_cuda_init` (#81183)\r\n* Fix behaviour of index_add / atomicAdd(bool,bool) (#85100)\r\n* Fix IMA for topk (#83042)\r\n* Use `opmath_t` for activation functions in Activation.cu (#77949)\r\n* Fixed the invalid configuration argument error when running layer norm backward (#80893)\r\n* Support non-standard bools in CUDA unique (#79392)\r\n* Accept non-standard bools in more CUDA kernels (#78957)\r\n* Fix cuda-mode and add more tests (#81898)\r\n* Clear autocast amp cache in CUDA Graphs (#81896)\r\n* Properly compute `batch_element_count` in `warp_softmax`  (#82927)\r\n* Disabled autocast cache in torch.cuda.make_graphed_callables (#84289)\r\n* Store RNG seed for CUDA graphs (#84967)\r\n* Assert `lambda >= 0` in poisson distribution cuda kernel (#85906)\r\n* Work-around 32-bit indexing failures in cuDNN batchnorm (#87861)\r\n* Fixed 3d convolution_add_relu in V8 (#85055)\r\n\r\n## Intel\r\n\r\n* Fixed bug for thnn_conv2d when input's C is 1 and weight is channels last (#82392)\r\n* Fixed oneDNN channels_last path issue (#83653)\r\n* Fixed torch.config can't respect USE_MKLDNN flag issue (#75001)\r\n* Made the data types of output and input consistent for batchnorm (#86784)\r\n* Fixed the issue that cat result would be incorrect for channels-last (#85076)\r\n* Fixed the performance issue that the for-loop before ExternallCall could not be parallelized (#85056)\r\n* Fixed the performance issue that the for-loop before ExternallCall (#86516)\r\n\r\n## MPS\r\n\r\n* Fixed MPS operator torch.full for boolean types (#82575)\r\n* Extend MPS Unary operators for empty tensors which should be a no-op (#82650)\r\n* Fixed MPS operator `torch.scatter` for boolean types (#82685)\r\n* Fixed MPS operator `torch.cat` for boolean inputs (#81480)\r\n* Fixed typo in MPS allocator (#83465)\r\n* Fixed MPS operator torch.full to handle uint8 types (#83697)\r\n* Fixed creation of `MPS::Placeholder` behavior for transposed view operations (#85689)\r\n* Fixed handling of output shape for empty inputs to binary ops in MPS backend (#85836)\r\n* Added support for handling scalar inputs to MPS operations of `torch.scatter` and `torch.gather` (#85842)\r\n* Support for handling compatible inputs to MPS operation of torch.where (#85946)\r\n* Added support for inputs with datatypes Short, Byte & Char to torch.dot MPS operation by casting to int32 when needed (#86140)\r\n* Remove incorrect asserts in MPS backend from Copy.mm file (#86184)\r\n* Added support for handling of 1D inputs for MPS operation `torch.nll_loss` (#81290)\r\n* Get correct size of the view tensor when copying from cpu to mps device (#81730)\r\n* Fix issues exposed in MPS testConsistency tests. The fix includes correct handling of types in smooth l1 loss, 0 dimensions for torch.repeat and empty inputs for torch.cat operations (#81735)\r\n* Handle Integer inputs for MPS linear layer by returning error of unsupported data types (#82183)\r\n* Workaround int8 datatype outputs in MPS for View operations (gather) by casting it to int8 (#82315)\r\n* Improve handling of empty outputs and fix MPS linear layer\u2019s handling of transposed Tensors in test consistency (#83124)\r\n* Fixed handling of conv1D and conv2D MPS operations with non-matching strides/paddings (#83522)\r\n* Fixed handling of MPS::Placeholder when View operation is missing gather graph (#83744)\r\n* Fixed the index handling in MPS for torch.constant_pad_nd operations with single-dimension input (#83745)\r\n* Handle casting for MPS torch.div operation in case of type mismatch (#84742)\r\n* Fix device (MPS) to host (cpu) copy by casting from a smaller dtype to a bigger dtype (#84928)\r\n* Ensure as_strided_tensorimpl is never called with MPS (#85020)\r\n* Fixed integer rounding crash in torch.div MPS operation on M1 (#85016)\r\n* Fixed crash in MPS bitwise ops on Mac x86 platforms. (#85285)\r\n* Fixed crash in MPS Conv1d backward operation for NHWC (#85283)\r\n* Added support for MPS reduction operations of scalar edge-cases (#83743)\r\n* Fixed memory corruption in torch.var operation for MPS (#85571)\r\n* Fixed memory leaks in MPS that cause the MTLBuffers not to be released and cause OOM (#85661)\r\n* Fix test consistency error in MPS due to type mismatch between int8 and uint8 types (#85666)\r\n* Fixed shape issues for torch.clamp op in MPS (#85673)\r\n* Fixed handling of TensorBase shapes for view ops in MPS for case of multiple slices on a Tensor (#85934)\r\n* Fix the dimension of padding to match the input's dimension for MPS Pad operations (#85990)\r\n* Fix non-contiguous to contiguous copy of MPS tensors (#86056)\r\n* Remove `std::cout` from MPS `multinomial` operation (#86246)\r\n* Do not dispatch empty job in bitwise_not (#87286)\r\n* Made copy from CPU always add storageOffset (#86958)\r\n* Revamped `copy_to_mps_` implementation (#86956)\r\n\r\n## Package\r\n\r\n* Added fix for implicit numpy dependency (#78979)\r\n* Allowed torch._C to be recognized a module in torch.package (#80917)\r\n* Ignore return value of function declared with 'warn_unused_result' for torch::deploy (#84862)\r\n* Removed torch::deploy from pytorch (#85953)\r\n\r\n## Profiler\r\n\r\n* Fixed build failure in python 3.10 (#81812)\r\n* Pop `KinetoThreadLocalState` at the start of post processing. (#77996)\r\n* Fixed record function inputs_valid_ check (#78002)\r\n* Weakened ordering check during post processing. (#78563)\r\n* Fixed Python parent id (#79356)\r\n* GIL acquire needed in ValueCache::trimPrefixes (#81061)\r\n* Added ephemeral inputs to the value cache. (#81958)\r\n* Fixed profiling with record_shapes=True and nested tensor (#82854)\r\n* Proper reset execution graph data in remove callback registration (#82910)\r\n* Solved two syntax issues when dumping execution graph result to json file. (#81854)\r\n* Set end time on python events when profiling stops. (#83621)\r\n* Don't try to collect strides for non-strided tensors (#83935)\r\n* Add null handling to `AppendOnlyList::copy` memcpy path. (#83963)\r\n* Add quoted metadata API to remove empty trace cpu_op metadata (#84128)\r\n* Make `RecordQueue` manage the lifetime of `PythonTracer`. (#83964)\r\n* Don't assign in AppendOnlyList::emplace_back (#85716)\r\n* Fixed traversal utility (#85717)\r\n* Fixed python object reference counting (#85847)\r\n\r\n## Visualization\r\n\r\n* Removed dependency on `torch.onnx` in `graph` (#82628)\r\n* Updated `Image.ANTIALIAS` to `Image.Resampling.LANCZOS` in summary (#85679)\r\n\r\n## Vulkan\r\n\r\n* Fixed the `aten::cat` operator registration (#78806)\r\n* Fixed a bug in GRU where incorrect behaviour was being observed when `H_in != H_out` (#78945)\r\n* FIxed a possibly null pointer dereference in the `aten::mm` operator when using passing an empty bias (#79701)\r\n* Code under `ATen/native/vulkan/api` was essentially rewritten (more details below) and as a result of these refactors, it is now possible to concurrently execute multiple Vulkan models due to correct synchronization when recording to a Vulkan command buffer (#80959)\r\n\r\n## Mobile\r\n\r\n* Moved saving storage to the last step. (#78024)\r\n* Fixed build For Model Tracer (#84755)\r\n* Skip TestNNAPI tests if QNNPACK is not supported (#82882)\r\n* Extended LinearPackedParamsBase **getstate**/**setstate** deadline in `check_forward_backward_compatibility.py` Allowlist (#81135)\r\n* Removed LinearPackedParamsBase **getstate**/**setstate** from `check_forward_backward_compatibility.py` Allowlist (#81048)\r\n* Fixed `ao::sparse::BCSR` missing in qlinear serialize and deserialize when USE_FBGEMM and USE_PYTORCH_QNNPACK are not set (#81256)\r\n* Updated `model_ops.yaml` (#82444)\r\n* Fixed signed/unsigned compare for Metal (#86068)\r\n* Re-added benchmarking files to ios TestApp (#85539)\r\n\r\n## Distributed\r\n\r\n#### `Distributed(c10d)`\r\n\r\n* Ensured tensors are contiguous for autograd enabled `all_gather`. (#79747)\r\n* Fixed data race condition of `batch_isend_irecv` (#82450)\r\n* Fixed `distributed_test.py` flakiness by turning off async_errror_handling (#78797)\r\n* Reenabled `isinstance` with `torch.distributed.ReduceOp` (#87303)\r\n\r\n#### `DistributedDataParallel`\r\n\r\n* Enabled `AllReduceCommHook` to accept `instrusive_ptr` (#80975)\r\n\r\n#### `FullyShardedDataParallel`\r\n\r\n* Fixed `full_optim_state_dict()` hang (#80712)\r\n* Fixed exec order validation for ignored modules across ranks (#79533)\r\n* Cleaned prefixes when searching for params / buffers to ignore (#78278)\r\n* Returned original module when fsdp wrapped model call .module (#78671)\r\n* Fixed a small bug of pre_backward_hook params prefetch (#78851)\r\n* Fixed param name prefixes for ignored modules (#79955)\r\n* Fixed FSDP when not all outputs get gradient in backward (#80245)\r\n* Fixed that MP config not being passed to FSDP (#80869)\r\n* Fixed FSDP device_id when CPU offloading (#82892)\r\n* Fixed FSDP not all outputs used in loss (#83195)\r\n* Fixed the FQN not found issue for load sharded_state_dict when using activation checkpoint (#84253)\r\n* Fixed `pin_memory()` for CPU offloading (#85048)\r\n* Fixed memory regression! (#85087)\r\n* Implemented a short-term fix to remove `optim_input` (#84201)\r\n\r\n#### `torch.distributed.elastic`\r\n\r\n* Ensured that exit code is propagated from Child to parent process (#81408)\r\n\r\n#### `torch.distributed.rpc`\r\n\r\n* Only initialize CUDA if there are devices specified in `init_rpc` (#80180)\r\n* Fixed the wrong usage of `RRefContext::handleException` by having a new API `RRefContext::handleExceptionSilent` (#83166)\r\n* Changed to avoid initializing storage for empty Optionals (#78947)\r\n\r\n## Infra (RelEng)\r\n\r\n* Made bazel changes to make \u201cbazel query ...\u201d work (#78870)\r\n* Fixed C API to be compatible with latest Python 3.11 beta (Please note that 3.11 binaries are not fully functional)  (#81242)\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n* Fixed use of temporary buffers for tensors in `torch.save`. (#80404)\r\n* Fixed and improved the efficiency of the backward for `torch.xlog{*}` functions. (#82713)\r\n* Vectorized `.copy()` acting between different dtypes on CPU (#80905)\r\n* Vectorized `bfloat16` conversions on CPU (#80906)\r\n\r\n## Autograd\r\n\r\n* Codegened autograd nodes no longer is smarter about which gradients to compute (#82544)\r\n* Made the derivative of masked_fill more efficient (#83515)\r\n* `torch.where` no longer materializes a zero-filled tensor in its backward (#83043)\r\n\r\n## torch.nn\r\n\r\n* Speed up `nn.Module` constructor by not calling custom `setattr` (#77098)\r\n* Speed up CPU `nn.BatchNorm` implementation by using `torch.zeros()` directly (#82558)\r\n* Speed up `nn.Module.load_state_dict` (#85743)\r\n\r\n## BetterTransformer\r\n\r\n* Added nn.module activation support in BetterTransformer (#78394), in addition to functional support which is not available in Torchscript\r\n* Added mask identifier for multiplexed src_mask/src_key_padding_mask in BT (#81947)\r\n* Added a small fastpath test for native multi-head attention (#81432)\r\n\r\n## Composability\r\n\r\n* Release GIL when doing shared memory copies on Tensors (#85389)\r\n* Some micro-optimizations in `RecordFunction`, the core util used by the profiler (#76266)\r\n* `c10::detail::ReplaceAll`: avoid some unnecessary allocations (#79915)\r\n\r\n## Dataloader\r\n\r\n* Moved loop content into a function to ensure we don't preserve `Tensor` in `pin_memory` thread (#83595)\r\n\r\n## LinAlg\r\n\r\n* Simplified and optimized `linalg.solve` (_#74046_)\r\n* Improved heuristics for `linalg.lu_solve` when B is a matrix (_#79838_)\r\n* Small optimization of `linalg.cholesky` (_#81316_)\r\n* Prefer contiguous output from mkldnn_bf16_gemm (_#82968_)\r\n* CPUBlas: Use mkldnn optimized BFloat16 matmul for gemm (_#65840_)\r\n* Updated and improved the heuristics for `linalg.lu_solve` (_#73878_)\r\n* Optimized `linalg.householder_product` backward to be more memory-efficient (_#84627_)\r\n\r\n## Sparse\r\n\r\n* Improved `to_sparse_bsr` for batched dense inputs (#83085)\r\n* Improved `to_dense` for CSC (#79635)\r\n* Improved `index_select` performance for COO input on CUDA (#77551)\r\n* Improved `mul(COO, COO)` performance with broadcasting in dense dims. (#83428, #85336)\r\n\r\n## JIT\r\n\r\n* Improved coreml load time by loading cpu model first, while asynchronously loading a model (#80941)\r\n* Improved `torch::jit::as_{module,object}` performance (#84399)\r\n* Replaced `IValue::toString()->string()` with `IValue::toStringRef()` (#85437)\r\n\r\n## Quantization\r\n\r\n* Allow contiguous inputs run into `qcat_nhwc_stub` when dim is last dimension (#72575)\r\n* Enable qlinear dynamic parallelization with fbgemm (#84033)\r\n\r\n## CUDA\r\n\r\n* Fixed perf regression introduced in #70943 (#78588)\r\n* Improved small sort performance on CUDA (#79627)\r\n* Use cub::BlockRadixSort to improve medium length sort performance (#79628)\r\n* Use cub::BlockRadixSort to improve medium length sort performance (#79628)\r\n* Increased size limit on calling CublasLt in addmm by 32x (#82922)\r\n* Don't synchronize single element any/all reductions (#84465)\r\n* Added col2im_batched kernel (#84543)\r\n* Exposed fast get_current_stream (#78165)\r\n* Pool cudaEvents in CUDACachingAllocator (#78279)\r\n\r\n## Intel\r\n\r\n* Optimize the copy of BFloat16 to Float and Float to BFloat16 (_#79685_)\r\n* Improve performance of ONEDNN backend (_#84470_)\r\n* Optimize softmax backward and logsoftmax backward _#80114_\r\n* Improve sort multi-core perf by adjusting grain_size w.r.t. dim_size (_#74897_)\r\n* Add fast path of `qmean`/`qstd` for quantized CPU (_#80579_)\r\n* Use direct memcpy in `qcat` when all the inputs and output share the same scale and zero_point (_#71903_)\r\n* Vectorize scalar remainder in quantized kernel for normalization (_#79673_)\r\n* Enhance add_out_dense_sparse_cpu for hybrid sparse tensor (_#23057_)\r\n\r\n## MPS\r\n\r\n* Performance improvements for the MPS backend by changing commitAndWait to commit & fixing high memory consumption for View operations. Also improved scalar handling in MPS Allocator (_#81951_)\r\n* Improved performance for MPS backend by reducing the number of command buffers created and hence CPU overhead. It uses commitAndContinue feature in MPS (_#81338_)\r\n* Added direct MPS implementation for constant_pad_nd operation which improved performance as the generic implementation was heavily reliant on View ops which are slow (_#82366_)\r\n* Removed checks that incur unnecessary syncs for MPS device with tensor.item() (_#82505_)\r\n* Enabled Graph caching in MPS for torch random ops with Philox engine (_#85833_)\r\n* Added specialized memory pool for scalar values in MPS which improved performance in torchbench networks (_#85817_)\r\n* Improved memory usage and performance by utilizing garbage collector and adaptive commit feature in MPS (_#86119_)\r\n\r\n## Profiler\r\n\r\n* Optimize getStepCallbacks for common case of no active callbacks for kineto (#77804)\r\n* Use custom AppendOnlyList for op_events to reduce the number of atomic operations (#78643)\r\n\r\n## Vulkan\r\n\r\n* When waiting on the result of a `VkFence`, busy polling is now used instead of a single call to `VkWaitForFences` with no timeout. This can improve benchmark performance by up to 50% by ensuring that the CPU stays at a high frequency when waiting for work on the GPU to complete (#81470)\r\n\r\n## Mobile\r\n\r\n* Added compilation_preference & relax_f32_to_f16 APIs (#78758)\r\n* Made flatbuffer loads faster if loading as mobile module. (#78998)\r\n* Stream pkl (#79931)\r\n* Used Apple's Accelerate framework for blas acceleration (#80449)\r\n* Read via FileAdapter when loading files in torch if not flatbuffer for lite_interpreter (#84028, #84296)\r\n\r\n# Documentation\r\n\r\n## Python API\r\n\r\n* Fixed `torch.as_array` documentation formatting (#78485)\r\n* Fixed default value for `storage_offset` in `torch.as_strided` documentation (#78202)\r\n* Removed warning in documentation that `torch.real` is only supported on complex types (#78644)\r\n* Improved reproducibility documentation for the random number generator and `torch.use_deterministic_algorithms` (#78849)\r\n* Fixed example in documentation for serialization (#79454)\r\n* Fixed `torch.linspace` documentation for dtype (#81371)\r\n* Fixed typo in documentation for `torch.distributions.Dirichlet` (#82062)\r\n* Fixed example  in `torch.dist` documentation (#82104)\r\n* Updated `torch.narrow` documentation to reflect that `start` can be a Tensor (#85180)\r\n* Added documentation for `pin_memory` and `layout` arguments to `torch.new_{zeros, ones, full}` (#85605)\r\n* Added documentation for `pin_memory` argument to `torch.{rand, randn}` (#85219),  (#85221)\r\n* Added argument default values to documentation for `torch.rot90` (#85610)\r\n* Removed `out` argument from documentation for `torch.squeeze` (#85222)\r\n* Fixed `torch.log` example (#78776)\r\n* Fixed `torch.argmin` docs for `keepdim` argument (#78888)\r\n* Updated examples in documentation for `torch.use_deterministic_algorithms` (#82003)\r\n* Changed docstring type `callable` to `Callable` for consistency (#82487)\r\n* Added documentation for `torch.narrow_copy` (#84493)\r\n* Improved documentation for `torch.signbit` (#78349)\r\n* Added doc string for `torch.library.Library.impl` (#81047)\r\n* Renamed `_Typed/_UntypedStorage` to `Typed/UntypedStorage` and updated documentation for `torch.storage` (#82438)\r\n* Added documentation for `torch.unflatten()` (#81399)\r\n\r\n## Autograd\r\n\r\n* Improved autograd custom function docs (#81340)\r\n* Added randomness case to the autograd notes (#78617)\r\n\r\n## Complex\r\n\r\n* Added a note on CUDA 11.6 (#80363)\r\n\r\n## torch.nn\r\n\r\n* Fixed docstring and image for `nn.LeakyReLU`  (#78508, #79102), `nn.ELU` (#78909), `nn.GRU` (#79380), `nn.Hardswish` (#70993), `nn.GeLU` (#85790)\r\n* Fixed docstring for `nn.CrossEntropyLoss` (#79568 and #82538), `nn.MultiMarginLoss` (#84513)\r\n* Fixed high level `nn.init` module doc to reflect that all functions run with `torch.no_grad` (#80882)\r\n* Fixed docstring for `nn.Module.state_dict` (#83104)\r\n* Updated docstring for `scale_factor` in `nn.functional.interpolate` (#80807)\r\n\r\n## torch.optim\r\n\r\n* Fixed docstring for `optim.lr_scheduler.ChainedScheduler` (#79775)\r\n* Fixed docstring for `optim.swa_utils.SWALR` (#79836)\r\n\r\n## Composability\r\n\r\n* Fix `MetadataTensor` example in `__torch_function__` docs (#78073, #78707)\r\n\r\n## Functorch\r\n\r\n* Fixed the model description in the functorch ensembling notebook (#83603)\r\n* Fixed indentation in functorch limitations docs (#85346)\r\n* Updated functorch installation instructions (#85854)\r\n* Fixed functorch whirlwind tour notebook to be runnable (#85974)\r\n* Documented new installation instructions for functorch (#86823)\r\n\r\n## LinAlg\r\n\r\n* Improve `torch.lu_unpack` docs (#77635)\r\n* Fix inconsistent default `rcond` value (#82887)\r\n\r\n## Sparse\r\n\r\n* Updated `scatter_add_` documentation to fix argument name (#80223)\r\n* Updated `torch.sparse` docs to better cover CSR/CSC/BSR/BSC (#82108)\r\n* Added torch.sparse overview section (#85265)\r\n* Updated documentation for `mm` family ops and `F.linear` to note limited sparse support (#86220)\r\n\r\n## torch.fx\r\n\r\n* Fixed decomposition example (#79807)\r\n* Added `__all__` to various submodules in torch.fx, distributions, distributed, package (#80367)\r\n* Added warning about DCE in FX being unsound with mutation (#81818)\r\n\r\n## Quantization\r\n\r\n* Replace `qconfig_dict` with `QConfigMapping` in docs (#78533)\r\n* Corrects typo in quantization docs (#81687)\r\n* Additonal fixes for `quantize_fx` docs (#84587)\r\n* Add example for the error message for fixed qparam ops (#84666)\r\n* Add types for scale and zero_point tensor for `torch.fake_quantize_per_channel_affine` docs (#85733)\r\n* Updated quantization docs to show per channel support for `conv1d` (#81349)\r\n* Add `torch.ao.nn.quantizeable` modules documentation (#79957)\r\n* Add more detailed docs for `torch.ao.quantization.quantize_fx.{prepare_fx|prepare_qat_fx|convert_fx}` (#83132)\r\n\r\n## ONNX\r\n\r\n* Added a table of unsupported aten operators in the documentation (#84496)\r\n\r\n## CUDA\r\n\r\n* Fixed jiterator doc format (#78471)\r\n* Use generic amp autocast in example and specified dtype (#79579)\r\n* Fixed small typo in cuda.rst (#84012)\r\n* Added user facing documentation for CSAN (#84689)\r\n* Fixed broken docstring for `set_float32_matmul_precision` (#78949)\r\n\r\n## MPS\r\n\r\n* Update Persons Of Interest file for MPS (_#81757_)\r\n* Update backends.rst for MPS (_#82525_)\r\n\r\n## Package\r\n\r\n* PackageExporter does not have file_structure (#79948)\r\n* Updated package.rst to not include hermetic claim (#81019)\r\n* Fixed typos in `torch.package` documentation (#82994)\r\n* Fixed typo in torch/package/_mock.py (#84508)\r\n\r\n## Distributed\r\n\r\n#### `Distributed(c10d)`\r\n\r\n* Fixed some links in torch/distributed/CONTRIBUTING.md  (#79855)\r\n* Updated dist.scatter() documentation (#86069)\r\n* Fixed docstring of `scatter_object_list` (#84596)\r\n* Fixed doc string in `reduce_scatter` (#84983)\r\n\r\n#### `DistributedDataParallel`\r\n\r\n* Corrected the DDP wrap example by removing pg in DDP wrap (#83034)\r\n\r\n#### `FullyShardedDataParallel`\r\n\r\n* Improved auto wrap policy doc (#78400)\r\n* Corrected comments in FSDP for gradient averaging (#80456)\r\n* Updated `ShardingStrategy` and `_free_full_params()` docs (#80894)\r\n* Added mentioning of `optim_input` to be removed after 1.13 in the BC breakage warning (#85963)\r\n\r\n#### `torch.distributed.rpc`\r\n\r\n* Updated distributed/CONTRIBUTING.md to remove ProcessGroupAgent references and add test instructions (#78625)\r\n\r\n## Infra (RelEng)\r\n\r\n* Added some documentation about the stats uploading process for CI (#79504)\r\n* Fixed release doc builds (#79865)\r\n* Updated release.md with release candidate validation steps (#79889)\r\n\r\n# Developers\r\n\r\n## Autograd\r\n\r\n* Added the ability to register a hook to grad_fn with `.register_prehook`(#83226)\r\n\r\n## Build\r\n\r\n* Modified nccl_dependency to take dev mode (#79169)\r\n* Moved pytorch buck targets to shared build (#79330)\r\n* Added kineto and flatbuffers to OSS BUCK (#79860)\r\n* Updated llvm deps for Buck build (#79919)\r\n* Moved aten targets to shared buck file (#79966)\r\n* Updated buck_setup.sh (#80467)\r\n* Minor fix for shared build (#80739)\r\n* Deleted CCACHE_DISABLE and SCCACHE_DISABLE from nccl.cmake file (#84007)\r\n\r\n## Composability\r\n\r\n* `TorchDispatchMode` and `TorchFunctionMode` extension points have been added. They are similar to their `__torch_function__` and `__torch_dispatch__` counterparts, but can be used as context managers that intercept **all** torch operator calls, including factory functions. These API\u2019s are still experimental and aren\u2019t quite user facing yet, and we will add more documentation as they are hardened. See [this post](https://dev-discuss.pytorch.org/t/what-and-why-is-torch-dispatch/557) for more details.   (#78214, #78822, #78847, #84774, #83925, #79143, #77667, #80992, #80995, #80998, #82647, #83372)\r\n* A large amount of hardening to `FakeTensor` and `FakeTensorMode`, a `__torch_dispatch__` style mode that allows you to run shape/dtype/device inference. This is similar to the \u201cmeta\u201d device, but fake tensors also faithfully store device metadata, and the logic lives in python. (#77969, #77972, #77971, #78516, #78090, #78836, #78895, #78536, #78677, #78522, #78523, #78972, #79170, #80115, #80193, #80544, #81739, #82281, #82574, #82066, #82449, #82337, #82571, #82593, #82172, #84387, #85065, #82846, #85658, #85759, #85920)\r\n* Added some new tags and beefed up tags support for operators in the dispatcher:\r\n    * Add data_dependent_output tag (#83312)\r\n    * Add nondeterministic tags in tags.yaml and add the nondeterministic_seeded tag to all functions in native_functions.yaml defined as nondeterministic by alias_analysis.cpp (#81440)\r\n    * Allow specifying operator tags when registering an operator to the dispatcher (#79322)\r\n    * add `inplace_view` tag to `resize_()` (#82667)\r\n* Make string serialization of C++ FunctionSchema consistent with torchgen.model.FunctionSchema (#77926)\r\n* Added support for custom namespaces in `torchgen` (#78015, #79733, #81362, #81581)\r\n* Generate kernels for codegen\u2019d `out=` operators (#78626, #81437)\r\n* Added a new alias dispatch key for functional to view op decompositions (#79615)\r\n* Added an env var for dispatcher debug logging (#81846, #82277)\r\n* Fixed printing of DispatchKey in operator not found message (#81637)\r\n* Added test that all BackendComponents are covered by toString (#81713)\r\n* Refactored functionality and backend keys to reduce duplication (#81752)\r\n* Made factory functions `CompositeExplicitAutograd`, so they show up as primitives in `__torch_dispatch__` (#82470)\r\n* Added an `OpOverload.decompose()` API, for running an operator\u2019s decomposition if one exists (#83075)\r\n* Fixed our dispatcher schema parser when parsing tensor list alias annotations (#84005)\r\n* Allowed subclasses of `c10::TensorImpl()` to override non-virtual tensor methods (#84806)\r\n* Made pytorch headers consumable from c++20 code bases (#79985)\r\n* Added meta device support to `_UntypedStorage` and `_TypedStorage` (#78008)\r\n\r\n## torch.fx\r\n\r\n* Added debug statements for small ACC subgraphs elimination (#80117)\r\n* Checked node type before fetching users (#80166)\r\n* Detected ProxyTensor layering violations (#80994)\r\n* Increased stack level for get_attr warning (#81041)\r\n* Preserved a node\u2019s stack trace (#82670, #83050, #83558, #83706, #83960)\r\n* For quantization, removed `WEIGHT_INDEX_DICT` and `BIAS_INDEX_DICT` and replaced with `node_arg_is_weight` and `node_arg_is_bias` (#83263, #83848)\r\n* Asserted that ProxyTensorMode does not accidentally bake in constants (#83297)\r\n* Improvements to FX Minimizer (#83833)\r\n* Ported matmul compositeimplicitautograd impl into core (#85239)\r\n* OpInfo for Slice (#85554)\r\n* Raised errors in fx.Interpreter with Node info (#85810)\r\n\r\n## Quantization\r\n\r\n* Enabled support for quantized fill of nhwc tensors (#79025)\r\n* Tests for code snippets in quantization docs (#79923)\r\n* Eliminate Named tensor warnings in XNNPACK and QNNPACK (#77762)\r\n* Added earlier termination and improved error message for calling `min` and `max` ops on per channel quantized tensors. (#79036)\r\n* Added warnings to quantized dynamic conv and linear ops when `reduce_range=true` (#79273)\r\n* Add assertions to fix `torch::jit::load bugs` (#79192)\r\n* Optionally clamp weights post quantization (#83438)\r\n\r\n## ONNX\r\n\r\n* `onnx.verification` Tool to verify exported model discrepancy between sets of inputs (#78323)\r\n* Symbolic function registration is now done via decorators (#84709)\r\n* `g.op` methods now exposed via the GraphContext class (#84728)\r\n* Initial version of diagnostics infrastructure. (#85107)\r\n* Add dtype check in onnx verification (#79263)\r\n\r\n## Intel\r\n\r\n* Added native impl for group norm on quantized CPU for channels-last inputs: (_#70520_)\r\n* Added `qscheme` check for quantization observer (_#80126_)\r\n* Added oneDNN graph fuser context API and unittest (_#82491_)\r\n* Added eltwise OPs for NNC: `mish` and `elu` (_#80586_)\r\n* Support BF16ImmPtr (_#84041_)\r\n* Enabled fusion of conv with elementwise OP for NNC (_#77157_)\r\n* Channels last propagation within NNC fusion group (_#76948_)\r\n* Lowering function generates the output buffer with the specified stride for NNC(_#76529_)\r\n* Simplified IfThenElse and CompareSelect within for-loop for NNC (_#76793_)\r\n* Do not pull in _autocast_* ops into NNC (_#85140_)\r\n\r\n## MPS\r\n\r\n* Improve MPS test by extending `test_no_warnings_on_input` by capturing any output (_#79163_)\r\n* Add testcase in test_mps for circular mode in torch.pad (_#81455_)\r\n* Fixed build warnings while building with MPS on Mac platforms (_#83048_)\r\n* Add per-op MPS gradient tests and update skips for TestConsistency (_#84242_)\r\n\r\n## Profiler\r\n\r\n* New event representation in profiler (#77693, #77694, #77695, #78163, #79173, #81965, #80797, #81319, #81320, #81321, #81322, #80822, #82993)\r\n* Build call tree for profiled events (#77698, #80810)\r\n* Copy rollbear/strong_type to `c10/util` (#78162)\r\n* Collect Layout and expose TensorMetadata (#81155)\r\n* Added support for storing scalar values in profiling (#81843)\r\n* Added support for Device (#82787)\r\n* Added SOFT_ASSERT to gracefully recover from invariant violations (#82689)\r\n* Added support for accessing strides and scalars (#80072)\r\n* Record nn.Module's parameters (#83209)\r\n* Extend Python bindings (#83622)\r\n* Capture storage data pointer (#84276)\r\n* Cleaned up Tensor representation (#85161)\r\n* Compute unique IDs for Tensors (#85162)\r\n* set_class util (part 1 of Record Optimizer) (#84779)\r\n* Tracking Optimizer (part 2 of Record Optimizer) (#84920)\r\n* Optimizer param_groups (part 3 of Record Optimizer) (#85784)\r\n* Optimizer states (part 4 of Record Optimizer) (#85840)\r\n* Extend ID assignment to allocations and frees (#85719)\r\n* Made `name` a property. (#85720)\r\n* Added dtype to `_TensorMetadata` (#85721)\r\n* Updated python binding type annotations (#85722)\r\n* Started moving python bindings out of autograd (#82584)\r\n\r\n## Vulkan\r\n\r\n* Vulkan operators that use prepacking have switched from using individual `OpContext` classes with `PackedContext` classes that inherit from a generic `VulkanOpContext` class which should reduce boilerplate code when implementing new ops that require prepacking (#78814, #78815, #78816, #78817, #78818, #82730, #83526)\r\n* Code under `ATen/native/vulkan/api` was essentially rewritten to improve code organization and readability. The refactor implements RAII patterns for the classes used to represent Vulkan handles to facilitate proper resource management and re-designed how the `Context` class functions in order to enable concurrent execution of multiple Vulkan models. The stack of PRs containing these refactors can be found at #80699\r\n* Lint is now enforced in the `ATen/native/vulkan` (#81390)\r\n* The VulkanMemoryAllocator version used was upgraded to 3.0.1, which now lives under `third_party` (#81472, #83906, #83934)\r\n* Shader layouts are now automatically generated based on the GLSL code (#81715, #81716)\r\n\r\n## Distributed\r\n\r\n#### `torch.distributed`\r\n\r\n* Added **all** to torch.distributed and tensorboard submodules (#80444)\r\n* Added **all** to torch.{fx, distributed, backends} submodules (#85079)\r\n* Added **all** to fx, fistributed and cuda submodules (#85080)\r\n* Added **all** to torch.distributed, futures, fx, nn, package, benchmark submodules (#80520)\r\n* Added **all** to torch.distributed submodules (#80523)\r\n* Eliminated code duplication in distributed rendezvous (#81577)\r\n* Refactored distributed to use absolute header path (#85780)\r\n\r\n#### `torch.distributed.elastic`\r\n\r\n* Added **all** for torch.nn.modules, torch.distributed.elastic, torch.nn.utils submodules (#80240)\r\n* Fixed macos public bindings failures (#80970)\r\n\r\n#### `Distributed(c10d)`\r\n\r\n* Logged full rank fingerprint mismatches in ProcessGroupWrapper (#79901)\r\n* Added environment parse function that supports default value (#85563)\r\n* Added host and port to TCPStore pyi definition (#84636)\r\n* Added underlying_store property for PrefixStore (#84640)\r\n* Enabled per-thread ProcessGroup for testing (#84153)\r\n* Moved ProcessGroup::Work into a separate class (#83680)\r\n* Install c10d headers with absolute path (#86257)\r\n\r\n## Infra (RelEng)\r\n\r\n* Migrated off xenial gcc5.4 from merge rules (#78137)\r\n* Added functionality for rebasebot to rebase onto viable/strict branch (#78276)\r\n* Pinned protobuf version to 3.20.1 in docker CI build (#78369)\r\n* Removed gcc5.4 from docker/build.sh (#78405)\r\n* Removed gcc5.4 jobs from CircleCI config (#78555)\r\n* Added merge rules for \u201cpytorch distributed\u201d module (#78751)\r\n* Added onnx / test to required merge rules (#78790)\r\n* Added userbenchmark support to TorchBench CI (#78794)\r\n* Installed torchdynamo as part of most CI jobs (#79051)\r\n* Removed linux-xenial-py3_7-clang7-asan from merge rules (#79088)\r\n* Ran torchdynamo tests on PyTorch Linux CI (#79099)\r\n* Centralized commit pins in a folder (#79150)\r\n* Moved CUDA flags out of --per_file_copts into the cu_library macro (#79414)\r\n* Moved CI to cuda-11.6 (#79921)\r\n* Enabled pytest to run test_ops, test_ops_gradients, test_ops_jit in non linux cuda environments (#79898)\r\n* Upgraded pytorch nightly docker python version to 3.8 (#80051)\r\n* Updated Dockerfile to install cmake as part of conda install (#80258)\r\n* Re-enabled vulkan test (#81368)\r\n* Enhanced mergebot with the feature of posting the PR Comment on cancel (#82744)\r\n* Changed nccl build to be single-threaded (#83173)\r\n* Added process for maintaining Build + CI contributors list (#83869)\r\n* Implemented mechanisms to block land checks if the PR hasn't been approved yet (#84239)\r\n* Allowed External Scripts (e.g. vscode) To Discover and Execute unittest Tests (#85584)\r\n* Updated the pinned torchdynamo hash to `6ead5cae0d1234aa64db06fe230ef56e12ec76fe` (#85683)\r\n* Updated the pinned torchvision hash to `d7d90f56117ce0955332846a5f90b8d1346c4c09` (#85776)\r\n* Modified all functions (except factory functions) to support SymInt and update xla hash to `f2b36df6a1a80137eff7644e6d0f4eeb7ff429d6` (#86078)\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.13.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.13.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.13.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/80850282", "release_id": 80850282, "date_created": "2022-10-24T20:00:35Z", "date_published": "2022-10-28T16:54:00Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/73783724", "tag": "v1.12.1", "name": "PyTorch 1.12.1 Release, small bug fix release", "author": {"name": "atalman", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n## Optim\r\n- Remove overly restrictive assert in adam https://github.com/pytorch/pytorch/pull/80222\r\n\r\n## Autograd\r\n- Convolution forward over reverse internal asserts in specific case https://github.com/pytorch/pytorch/issues/81111\r\n- 25% Performance regression from v0.1.1 to 0.2.0 when calculating hessian https://github.com/pytorch/pytorch/pull/82504\r\n\r\n## Distributed\r\n- Fix distributed store to use add for the counter of DL shared seed  https://github.com/pytorch/pytorch/pull/80348\r\n- Raise proper timeout when sharing the distributed shared seed  https://github.com/pytorch/pytorch/pull/81666\r\n\r\n## NN\r\n- Allow register float16 weight_norm on cpu and speed up test  https://github.com/pytorch/pytorch/pull/80600\r\n- Fix weight norm backward bug on CPU when OMP_NUM_THREADS <= 2 https://github.com/pytorch/pytorch/pull/80930 \r\n- Weight_norm is not working with float16 https://github.com/pytorch/pytorch/issues/80599\r\n- New release breaks torch.nn.weight_norm backwards pass and breaks all Wav2Vec2 implementations https://github.com/pytorch/pytorch/issues/80569\r\n- Disable src mask for transformer and multiheadattention fastpath https://github.com/pytorch/pytorch/pull/81277\r\n- Make nn.stateless correctly reset parameters if the forward pass fails  https://github.com/pytorch/pytorch/pull/81262\r\n- torchvision.transforms.functional.rgb_to_grayscale() + torch.nn.Conv2d() don`t work on 1080 GPU https://github.com/pytorch/pytorch/issues/81106\r\n- Transformer and CPU path with src_mask raises error with torch 1.12 https://github.com/pytorch/pytorch/issues/81129\r\n\r\n## Data Loader\r\n- [Locking lower ranks seed recepients https://github.com/pytorch/pytorch/pull/81071\r\n\r\n## CUDA\r\n- os.environ[\"CUDA_VISIBLE_DEVICES\"] has no effect https://github.com/pytorch/pytorch/issues/80876\r\n- share_memory() on CUDA tensors no longer no-ops and instead crashes https://github.com/pytorch/pytorch/issues/80733\r\n- [Prims] Unbreak CUDA lazy init https://github.com/pytorch/pytorch/pull/80899\r\n- PyTorch 1.12 cu113 wheels cudnn discoverability issue https://github.com/pytorch/pytorch/issues/80637\r\n- Remove overly restrictive checks for cudagraph https://github.com/pytorch/pytorch/pull/80881\r\n\r\n## ONNX\r\n- ONNX cherry picks https://github.com/pytorch/pytorch/pull/82435\r\n\r\n## MPS\r\n- MPS cherry picks https://github.com/pytorch/pytorch/issues/80898\r\n\r\n## Other\r\n- Don't error if _warned_capturable_if_run_uncaptured not set https://github.com/pytorch/pytorch/pull/80345\r\n- Initializing libiomp5.dylib, but found libomp.dylib already initialized.  https://github.com/pytorch/pytorch/issues/78490\r\n- Assertion error - _dl_shared_seed_recv_cnt - pt 1.12 - multi node  https://github.com/pytorch/pytorch/issues/80845\r\n- Add 3.10 stdlib to torch.package https://github.com/pytorch/pytorch/pull/81261\r\n- CPU-only c++ extension libraries (functorch, torchtext) built against PyTorch wheels are not fully compatible with PyTorch wheels  https://github.com/pytorch/pytorch/issues/80489\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.12.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.12.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.12.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/73783724", "release_id": 73783724, "date_created": "2022-08-02T23:50:01Z", "date_published": "2022-08-05T19:35:19Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/70696589", "tag": "v1.12.0", "name": "PyTorch 1.12: TorchArrow, Functional API for Modules and nvFuser, are now available", "author": {"name": "soulitzer", "type": "User"}, "description": "# PyTorch 1.12 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.12! This release is composed of over 3124 commits, 433 contributors. Along with 1.12, we are releasing beta versions of AWS S3 Integration, PyTorch Vision Models on Channels Last on CPU, Empowering PyTorch on Intel\u00ae Xeon\u00ae Scalable processors with Bfloat16 and FSDP API. We want to sincerely thank our dedicated community for your contributions. \r\n\r\nSummary:\r\n\r\n* Functional Module API to functionally apply module computation with a given set of parameters\r\n* Complex32 and Complex Convolutions in PyTorch \r\n* DataPipes from TorchData fully backward compatible with DataLoader \r\n* Functorch with improved coverage for APIs\r\n* nvFuser a deep learning compiler for PyTorch\r\n* Changes to float32 matrix multiplication precision on Ampere and later CUDA hardware\r\n* TorchArrow, a new beta library for machine learning preprocessing over batch data\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n**Updated type promotion for `torch.clamp`** ([#77035](https://github.com/pytorch/pytorch/pull/77035))\r\n\r\nIn 1.11, the \u2018min\u2019 and \u2018max\u2019 arguments in `torch.clamp` did not participate in type promotion, which made it inconsistent with `minimum` and `maximum` operations. In 1.12, the \u2018min\u2019 and \u2018max\u2019 arguments participate in type promotion.\r\n\r\n1.11\r\n\r\n```python\r\n>>> import torch\r\n>>> a = torch.tensor([1., 2., 3., 4.], dtype=torch.float32)\r\n>>> b = torch.tensor([2., 2., 2., 2.], dtype=torch.float64)\r\n>>> c = torch.tensor([3., 3., 3., 3.], dtype=torch.float64)\r\n>>> torch.clamp(a, b, c).dtype\r\ntorch.float32\r\n```\r\n\r\n1.12\r\n\r\n```python\r\n>>> import torch\r\n>>> a = torch.tensor([1., 2., 3., 4.], dtype=torch.float32)\r\n>>> b = torch.tensor([2., 2., 2., 2.], dtype=torch.float64)\r\n>>> c = torch.tensor([3., 3., 3., 3.], dtype=torch.float64)\r\n>>> torch.clamp(a, b, c).dtype\r\ntorch.float64\r\n```\r\n\r\n## Complex Numbers\r\n\r\n### Fix complex type promotion ([#77524](https://github.com/pytorch/pytorch/pull/77524))\r\n\r\nUpdates the type promotion rule such that given a complex scalar and real tensor, the value type of real tensor is preserved \r\n\r\n1.11\r\n\r\n```python\r\n>>> a = torch.randn((2, 2), dtype=torch.float)\r\n>>> b = torch.tensor(1, dtype=torch.cdouble)\r\n>>> (a + b).dtype\r\ntorch.complex128\r\n```\r\n\r\n1.12\r\n\r\n```python\r\n>>> a = torch.randn((2, 2), dtype=torch.float)\r\n>>> b = torch.tensor(1, dtype=torch.cdouble)\r\n>>> (a + b).dtype\r\ntorch.complex64\r\n```\r\n\r\n## LinAlg\r\n\r\n### Disable TF32 for matmul by default and add high-level control of fp32 matmul precision ([#76509](https://github.com/pytorch/pytorch/pull/76509))\r\n\r\nPyTorch 1.12 makes the default math mode for fp32 matrix multiplications more precise and consistent across hardware. This may affect users on Ampere or later CUDA devices and TPUs. See the PyTorch [blog](https://dev-discuss.pytorch.org/t/pytorch-and-tensorfloat32/504) for more details. \r\n\r\n## Sparse\r\n\r\n### Use ScatterGatherKernel for scatter_reduce (CPU-only) ([#74226](https://github.com/pytorch/pytorch/pull/74226), [#74608](https://github.com/pytorch/pytorch/pull/74608))\r\n\r\nIn 1.11.0, unlike `scatter` which takes a `reduce` kwarg or `scatter_add`, `scatter_reduce` was not an in-place function. That is, it did not allow the user to pass an output tensor which contains data that is reduced together with the scattered data. Instead, the scatter reduction took place on an output tensor initialized under the hood. Indices of the output that were not scattered to were filled with reduction inits (or 0 for options \u2018amin\u2019 and \u2018amax\u2019).\r\n\r\nIn 1.12.0, `scatter_reduce` (which is in beta) is in-place to align with the API of the related existing functions `scatter`/`scatter_add`. For this reason, the argument `input` in 1.11.0 has been renamed `src` in 1.12.0 and the new `self` argument now takes a destination tensor to be scattered onto. Since the destination tensor is no longer initialized under the hood, the `output_size` kwarg in 1.11.0 that allowed users to specify the size of the output at dimension `dim` has been removed. Further, in 1.12.0 we introduce an `include_self` kwarg which determines whether values in the `self` (destination) tensor are included in the reduction. Setting `include_self=True` could, for example, allow users to provide special reduction inits for the scatter_reduction operation. Otherwise, if `include_self=False,` indices scattered to are treated as if they were filled with reduction inits.\r\n\r\nIn the snippet below, we illustrate how the behavior of `scatter_reduce` in 1.11.0 can be achieved with the function released in 1.12.0.\r\n\r\nExample:\r\n\r\n```python\r\n>>> src = torch.arange(6, dtype=torch.float).reshape(3, 2)\r\n>>> index = torch.tensor([[0, 2], [1, 1], [0, 0]])\r\n>>> dim = 1\r\n>>> output_size = 4\r\n>>> reduce = \"prod\"\r\n```\r\n\r\n1.11\r\n\r\n```python\r\n>>> torch.scatter_reduce(src, dim, index, reduce, output_size=output_size)\r\n`tensor([[ 0., 1., 1., 1.],\r\n        [ 1., 6., 1., 1.],\r\n        [20., 1., 1., 1.]])`\r\n```\r\n\r\n1.12\r\n\r\n```python\r\n>>> output_shape = list(src.shape)\r\n>>> output_shape[dim] = output_size\r\n# reduction init for prod is 1\r\n# filling the output with 1 is only necessary if the user wants to preserve the behavior in 1.11\r\n# where indices not scattered to are filled with reduction inits\r\n>>> output = src.new_empty(output_shape).fill_(1)\r\n>>> output.scatter_reduce_(dim, index, src, reduce)\r\n`tensor([[ 0., 1., 1., 1.],\r\n        [ 1., 6., 1., 1.],\r\n        [20., 1., 1., 1.]])`\r\n```\r\n\r\n## torch.nn\r\n\r\n### `nn.GroupNorm`: Report an error if `num_channels` is not divisible by `num_groups` ([#74293](https://github.com/pytorch/pytorch/pull/74293))\r\n\r\nPreviously, `nn.GroupNorm` would error out during the forward pass if `num_channels` is not divisible by `num_groups`. Now, the error is thrown for this case during module construction instead.\r\n\r\n1.11\r\n\r\n```python\r\nm = torch.nn.GroupNorm(3, 7)\r\nm(...)  # errors during forward pass\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nm = torch.nn.GroupNorm(3, 7)  # errors during construction\r\n```\r\n\r\n### `nn.Dropout2d`: Return to 1.10 behavior: perform 1D channel-wise dropout for 3D inputs\r\n\r\nIn PyTorch 1.10 and older, passing a 3D input to `nn.Dropout2D` resulted in 1D channel-wise dropout behavior; i.e. such inputs were interpreted as having shape `(N, C, L)` with N = batch size and C = # channels and channel-wise dropout was performed along the second dimension.\r\n\r\n1.10\r\n\r\n```python\r\nx = torch.randn(2, 3, 4)\r\nm = nn.Dropout2d(p=0.5)\r\nout = m(x)  # input is assumed to be shape (N, C, L); dropout along the second dim.\r\n```\r\n\r\nWith the introduction of no-batch-dim input support in 1.11, 3D inputs were reinterpreted as having shape `(C, H, W)`; i.e. an input without a batch dimension, and dropout behavior was changed to drop along the first dimension. This was a silent breaking change.\r\n\r\n1.11\r\n\r\n```python\r\nx = torch.randn(2, 3, 4)\r\nm = nn.Dropout2d(p=0.5)\r\nout = m(x)  # input is assumed to be shape (C, H, W); dropout along the first dim.\r\n```\r\n\r\nThe breaking change in 1.11 resulted in a lack of support for 1D channel-wise dropout behavior, so `Dropout2d`  in PyTorch 1.12 returns to 1.10 behavior with a warning to give some time to adapt before the no-batch-dim interpretation goes back into effect.\r\n\r\n1.12\r\n\r\n```python\r\nx = torch.randn(2, 3, 4)\r\nm = nn.Dropout2d(p=0.5)\r\nout = m(x)  # input is assumed to be shape (N, C, L); dropout along the second dim.\r\n            # throws a warning suggesting nn.Dropout1d for 1D channel-wise dropout.\r\n```\r\n\r\nIf you want 1D channel-wise dropout behavior, please switch to use of the newly-added `nn.Dropout1d` module instead of `nn.Dropout2d`. If you want no-batch-dim input behavior, please note that while this is not supported in 1.12, a future release will reinstate the interpretation of 3D inputs to `nn.Dropout2d` as those without a batch dimension.\r\n\r\n### **`F.cosine_similarity`: Improve numerical stability ([#31378](https://github.com/pytorch/pytorch/pull/31378))**\r\n\r\nPreviously, we first compute the inner product, then normalize. After this change, we first normalize, then compute inner product. This should be more numerically stable because it avoids losing precision in inner product for inputs with large norms. Because of this change, outputs may be different in some cases.\r\n\r\n## Composability\r\n\r\n**Functions in torch.ops.aten.{foo} no longer accept `self` as a kwarg**\r\n\r\n`torch.ops.aten.{foo}` objects are now instances of `OpOverloadPacket` (instead of a function) that have their `__call__` method in Python, which means that you cannot pass `self` as a kwarg. You can pass it normally as a positional argument instead.\r\n\r\n1.11\r\n\r\n```python\r\n>>> torch.ops.aten.sin(self=torch.ones(2))\r\n    tensor([0.8415, 0.8415])\r\n```\r\n\r\n1.12\r\n\r\n```python\r\n# this now fails\r\n>>> torch.ops.aten.sin(self=torch.ones(2))\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: __call__() got multiple values for argument 'self'\r\n# this works\r\n>>> torch.ops.aten.sin(torch.ones(2))\r\ntensor([0.8415, 0.8415])\r\n```\r\n\r\n**__torch_dispatch__ now traces individual op overloads instead of op overload packets (**[**#72673**](https://github.com/pytorch/pytorch/pull/72673)**)**\r\n\r\n`torch.ops.aten.add` actually corresponds to a bundle of functions from C++, corresponding to all over the overloads of add operator (specifically, `add.Tensor`, `add.Scalar` and `add.out`). Now, `__torch_dispatch__` will directly take in an overload corresponding to a single aten function.\r\n\r\n1.11\r\n\r\n```python\r\nclass MyTensor(torch.Tensor):\r\n    ....\r\n    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\r\n        # Before, func refers to a \"packet\" of all overloads\r\n        # for a given operator, e.g. \"add\"\r\n        assert func == torch.ops.aten.add\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nclass MyTensor(torch.Tensor):\r\n    ....\r\n    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):\r\n        # After, func refers to an individual operator overload,\r\n        # e.g. \"add.Tensor\"\r\n        assert func == torch.ops.aten.add.Tensor\r\n        # you can recover the old behavior with func.overloadpacket\r\n        assert func.overloadpacket == torch.ops.aten.add\r\n```\r\n\r\n## Profiler\r\n\r\n### Disable forward-backward correlation ([#72904](https://github.com/pytorch/pytorch/pull/72904))\r\n\r\nThe forward-backward correlation is no longer captured as to workaround a profile crash. This feature may be reenabled in a future release after the underlying issue is fixed.\r\n\r\n```python\r\nwith torch.profiler.profile() as p:\r\n    loss = model(inputs)\r\n    loss.backward()  # Invoke autograd\r\n\r\n# The exported chrome trace will not have forward-backward flow events. (arrows)\r\np.export_chrome_trace(...)\r\n```\r\n\r\n## Mobile\r\n\r\n### Remove support for bytecode version 3 ([#57775](https://github.com/pytorch/pytorch/pull/57775))\r\n\r\nThe minimum supported bytecode version is being bumped from 3 to 4. We no longer support version 3 bytecode models because the bytecode version was bumped from 3 to 4 more than half a year ago, and there was code in operator loading that performed differently on one operator on the global bytecode version 3. \r\n\r\nIf the model is generated before Oct 5, 2020, please use the following lines to update the model to the latest version:\r\n\r\n1.12\r\n\r\n```python\r\nimport torch\r\nfrom torch.jit.mobile import _get_model_bytecode_version\r\n\r\nold_model_path = \"old_model.ptl\"\r\nnew_model_path = \"new_model.ptl\"\r\n\r\n# Load full jit model\r\njit_model = torch.jit.load(old_model_path)\r\n# Save model for mobile \r\njit_model._save_for_lite_interpreter(new_model_path)\r\n# Verify the model can be loaded\r\nmobile_m = _load_for_lite_interpreter(new_model_path)\r\n\r\n# Get bytecode version from the new model\r\nbytecode_version = _get_model_bytecode_version(new_model_path)\r\nprint(f\"bytecode version is {bytecode_version}\")\r\n\r\n```\r\n\r\n### Remove redundant FSDP prefix and change default auto wrap policy name to avoid confusion ([#76858](https://github.com/pytorch/pytorch/pull/76858), [#73791](https://github.com/pytorch/pytorch/pull/73791))\r\n\r\n`FullyShardedDataParallel`'s optional param name \u2018fsdp_auto_wrap_policy\u2019 (1.11) changed to \u2018auto_wrap_policy\u2019 (1.12). \u2018default_auto_wrap_policy\u2019 (1.11) is changed to \u2018size_based_auto_wrap_policy\u2019 (1.12). \r\n\r\nIn 1.11, when wrapping a model with FSDP instead of:\r\n\r\n```python\r\nmodel = MyModel()\r\nwrapped_model = FullyShardedDataParallel(\r\n    model,\r\n    **fsdp_auto_wrap_policy**=functools.partial(\r\n        default_auto_wrap_policy,\r\n        min_num_params=0,  # wrap all modules\r\n    )\r\n   ...\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nmodel = MyModel()\r\nwrapped_model = FullyShardedDataParallel(\r\n    model,\r\n   **auto_wrap_policy**=functools.partial(\r\n        size_based_auto_wrap_policy,\r\n        min_num_params=0,  # wrap all modules\r\n    )\r\n   ...\r\n```\r\n\r\n## Quantization\r\n\r\n### TorchScript models exported prior to PyTorch 1.6 using quantized Linear, GRU and LSTM operators will no longer work ([#72680](https://github.com/pytorch/pytorch/pull/72680), [#72522](https://github.com/pytorch/pytorch/pull/72522)) \r\n\r\nTorchScript models created with PyTorch 1.5 or earlier and using the operators `quantized::linear_prepack_legacy`, `linear_prepack_fp16_legacy`, `quantized::linear_unpack.legacy`, or `quantized::linear_unpack_fp16.legacy` will no longer work and need to be re-exported. Please use PyTorch [Quantization](http://%20https//pytorch.org/docs/stable/quantization.html) to quantize the Linear module instead.\r\n\r\n## ONNX\r\n\r\n## Infra (Releng)\r\n\r\n* Bump minimum CMake version to 3.13 ([#76312](https://github.com/pytorch/pytorch/pull/76312))\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n**Deprecated torch.testing.make_non_contiguous** ([#72705](https://github.com/pytorch/pytorch/pull/72705))\r\n\r\n`torch.testing.make_non_contiguous` is being deprecated and will be removed in a future release. Depending on the use case there are different replacement options: If you are using `make_non_contiguous` in the PyTorch test suite, you can use ``torch.testing._internal.common_utils.noncontiguous_like``\r\n\r\n1.11\r\n\r\n```python\r\na = torch.randn(1, 2, 3)\r\ntorch.testing.make_non_contiguous(a)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\na = torch.randn(1, 2, 3)\r\ntorch.testing._internal.common_utils.noncontiguous_like(a)\r\n```\r\n\r\nIf you are using `make_non_contiguous` in combination with a creation function to create a noncontiguous tensor with random values, you can use `make_tensor`.\r\n\r\n1.11\r\n\r\n```python\r\na = torch.randn(1, 2, 3)\r\ntorch.testing.make_non_contiguous(a)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\ntorch.testing.make_tensor(..., noncontiguous=True)\r\n```\r\n\r\nIf you are using `make_non_contiguous` with a specific tensor, you can use `torch.repeat_interleave`\r\n\r\n1.11\r\n\r\n```python\r\na = torch.tensor([[1., 2.], [1., 2.]])\r\ntorch.testing.make_non_contiguous(a)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\na = torch.tensor([[1., 2.], [1., 2.]])\r\ntorch.repeat_interleave(input, 2, dim=-1)[..., ::2]\r\n```\r\n\r\n## Build\r\n\r\n## LinAlg\r\n\r\n### Deprecate torch.lu ([#73804](https://github.com/pytorch/pytorch/pull/73804))\r\n\r\n`torch.lu()` is deprecated in favor of `torch.linalg.lu_factor()` and `torch.linalg.lu_factor_ex()`. `torch.lu()` will be removed in a future PyTorch release. If you were previously using `get_infos=False` (this is the default), you should use `torch.linalg.lu_factor` instead:\r\n\r\n1.11\r\n\r\n```python\r\nLU, pivots = torch.lu(A, compute_pivots) \r\n```\r\n\r\n1.12\r\n\r\n```python\r\nLU, pivots = torch.linalg.lu_factor(A, compute_pivots) \r\n```\r\n\r\nIf you were previously using `get_infos=True` you should use `torch.linalg.lu_factor_ex`:\r\n\r\n1.11\r\n\r\n```python\r\nLU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nLU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots) \r\n```\r\n\r\n### Deprecate torch.lu_solve ([#73806](https://github.com/pytorch/pytorch/pull/73806))\r\n\r\n`torch.lu_solve()` is deprecated in favor of `torch.linalg.lu_solve()`. `torch.lu_solve()` will be removed in a future PyTorch release.\r\n\r\n1.11\r\n\r\n```python\r\nX = torch.lu_solve(B, LU, pivots)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nX = torch.linalg.lu_solve(LU, pivots, B) \r\n```\r\n\r\n### Remove deprecated torch.solve ([#70986](https://github.com/pytorch/pytorch/pull/70986))\r\n\r\n`torch.solve` which was deprecated in a previous release is now being removed. You should use  `torch.linalg.solve`. instead. Note that `torch.linalg.solve` has its arguments reversed and does not return the LU factorization. To get the LU factorization see `torch.lu`, which can be used with `torch.lu_solve` or `torch.lu_unpack`.\r\n\r\n1.11\r\n\r\n```python\r\nX = torch.solve(B, A).solution\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nX = torch.linalg.solve(A, B)\r\n```\r\n\r\n## torch.nn\r\n\r\n### `nn.Module`: Deprecate positional args for `state_dict()` ([#72780](https://github.com/pytorch/pytorch/pull/72780))\r\n\r\n`state_dict` can currently be called in two ways: `destination`, `prefix`, and `keep_vars` can be passed as positional arguments, or as kwargs. The ability to do the former is being deprecated and will be removed in a future release. You should pass the arguments in as kwargs only. \r\n\r\n## Composability\r\n\r\n**Deprecated `__torch_function__` as instance method for more functions** ([#74829](https://github.com/pytorch/pytorch/pull/74829))\r\n\r\n`__torch_function__` should be defined as a class method. Defining `__torch_function__` as a plain method has already been previously deprecated for the functions handling `__torch_function__` in Python. This change makes it so that that is also the case for functions that handle `__torch_function__` in c++.\r\n\r\n1.11\r\n\r\n```python\r\nclass Bad():\r\n    def __torch_function__(self, *args, **kwargs):\r\n        pass\r\nt = Bad()\r\ntorch.abs(t)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\nclass Good():\r\n    @classmethod\r\n    def __torch_function__(cls, *args, **kwargs):\r\n        pass\r\nt = Good()\r\ntorch.abs(t)\r\n```\r\n\r\n## Quantization\r\n\r\n### Deprecate `torch.jit.quantized` ([#72690](https://github.com/pytorch/pytorch/pull/72690))\r\n\r\nInstead of using functions defined in `torch.jit.quantized,` please use [PyTorch Quantization](https://pytorch.org/docs/stable/quantization.html) to dynamically quantize Linear/RNNCell/LSTMCell/GRUCell/LSTM modules. It\u2019s both supported in [Eager Mode Quantization](http://%20https//pytorch.org/docs/stable/quantization.html#dynamic-quantization) and [FX Graph Mode Quantization](https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html)\r\n\r\n1.11\r\n\r\n```python\r\n>> torch.jit.quantized.QuantizedLSTMCell(...)\r\n```\r\n\r\n1.12\r\n\r\n```python\r\n>> torch.jit.quantized.QuantizedLSTMCell(...)\r\n   \"torch.jit.QuantizedLSTMCell is deprecated and will be removed in an upcoming\r\n    PyTorch release. Please use the torch.nn.quantized.dynamic.LSTMCell instead.\"\r\n```\r\n\r\n## Infra (Releng)\r\n\r\n* Removed CUDA 11.1 binary builds ([#73376](https://github.com/pytorch/pytorch/pull/73376))\r\n* Removed CUDA 11.5 binary builds ([#76257](https://github.com/pytorch/pytorch/pull/76257))\r\n\r\n# New features\r\n\r\n## Python API\r\n\r\n* Added new device `mps` that can be used to leverage GPU acceleration on macOS platform with Apple Native Silicon (M1) or discrete AMD GPUs. ([blogpost with details](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/))\r\n* Added `torch.special.log_ndtr` ([#74795](https://github.com/pytorch/pytorch/pull/74795))\r\n* Added `torch.distributions.transforms.{SoftplusTransform,CumulativeDistributionTransform}` ([#52300](https://github.com/pytorch/pytorch/pull/52300), [#72495](https://github.com/pytorch/pytorch/pull/72495))\r\n* Promoted `torch.testing` to stable ([#73348](https://github.com/pytorch/pytorch/pull/73348))\r\n* Added `maximize` flag for `optim.Adadelta`([#75330](https://github.com/pytorch/pytorch/pull/75330))\r\n\r\n## Build\r\n\r\n* Distributed torchgen as part of PyTorch package ([#76306](https://github.com/pytorch/pytorch/pull/76306))\r\n* Added BUILD_LAZY_CUDA_LINALG option ([#73447](https://github.com/pytorch/pytorch/pull/73447))\r\n* Introduced an environment variable to change c10 log level ([#71746](https://github.com/pytorch/pytorch/pull/71746))\r\n\r\n## Complex Numbers\r\n\r\n* Added a new data-type `torch.complex32` to help computing with complex datatype with lower memory usage at the cost of lower precision. Note that this is an experimental feature ([#78245](https://github.com/pytorch/pytorch/pull/78245)) and the major focus in this release was to support operators under `torch.fft` on CUDA.  Besides those operators we have added support and testing for following limited set of ops **(NOTE: few operators are only supported on CUDA)**: `Tensor.copy_, torch.complex, torch.testing.make_tensor, cat, Tensor.fill_, Tensor.item, torch.atleast_1d, torch.atleast_2d, torch.atleast_3d, torch.dsplit, torch.vsplit, torch.hsplit, torch.hstack, torch.dstack, torch.vstack, Tensor.conj, torch.add, torch.sub, torch.mul, torch.sub, torch.div, torch.view, torch.view_as, torch.real, torch.imag, torch.neg, Tensor.__getitem__, torch.sum, torch.prod, torch.abs, torch.sgn, torch.exp, torch.log, torch.eq, torch.masked_fill, torch.index_put, torch.rand, torch.randn, torch.full, torch.empty, torch.ones, torch.zeros, torch.block_diag, Tensor.chunk, Tensor.clone, Tensor.contiguous, torch.diag_embed, torch.diagonal, torch.as_strided, torch.column_stack, Tensor.T, Tensor.H, Tensor.mT, Tensor.mH, Tensor.narrow, torch.isfinite, torch.isinf, torch.isreal, torch.flatten, Tensor.chalf, torch.empty_like, torch.movedim` ( [#73847](https://github.com/pytorch/pytorch/pull/73847), [#74667](https://github.com/pytorch/pytorch/pull/74667), [#74854](https://github.com/pytorch/pytorch/pull/74854), [#75010,](https://github.com/pytorch/pytorch/pull/75010)[#75156](https://github.com/pytorch/pytorch/pull/75156), [#75311](https://github.com/pytorch/pytorch/pull/75311), [#75498](https://github.com/pytorch/pytorch/pull/75498), [#76132](https://github.com/pytorch/pytorch/pull/76132), [#76158](https://github.com/pytorch/pytorch/pull/76158), [#75592](https://github.com/pytorch/pytorch/pull/75592), [#76615](https://github.com/pytorch/pytorch/pull/76615), [#77179](https://github.com/pytorch/pytorch/pull/77179), [#77339](https://github.com/pytorch/pytorch/pull/77339), [#77446](https://github.com/pytorch/pytorch/pull/77446), [#77483](https://github.com/pytorch/pytorch/pull/77483), [#77479](https://github.com/pytorch/pytorch/pull/77479), [#77192](https://github.com/pytorch/pytorch/pull/77192), [#76724](https://github.com/pytorch/pytorch/pull/76724), [#77404](https://github.com/pytorch/pytorch/pull/77404)).\r\n* Operators in `torch.fft` now support tensors with `torch.complex32` dtype (CUDA only) ([#74857](https://github.com/pytorch/pytorch/pull/74857)).\r\n* `torch.complex32` tensor now participate in type-promotion ([#76893](https://github.com/pytorch/pytorch/pull/76893))\r\n* Added `torch.chalf` alias for `torch.complex32` and `Tensor.chalf` method ([#75320](https://github.com/pytorch/pytorch/pull/75320)).\r\n* Added proper print support for `torch.chalf` tensors ([#76614](https://github.com/pytorch/pytorch/pull/76614)).\r\n* Added support for complex convolution (data-types supported: `torch.complex32, torch.complex64, torch.complex128`)\r\n    * `torch.nn.functional.conv1d` and `torch.nn.Conv1d` ([#75310](https://github.com/pytorch/pytorch/pull/75310))\r\n    * `torch.nn.functional.conv2d` and `torch.nn.Conv2d` ([#75412](https://github.com/pytorch/pytorch/pull/75412))\r\n    * `torch.nn.functional.conv3d` and `torch.nn.Conv3d` ([#75581](https://github.com/pytorch/pytorch/pull/75581))\r\n\r\n## LinAlg\r\n\r\n* Added `torch.linalg.ldl_factor_ex` and `torch.linalg.ldl_solve` ([#69828](https://github.com/pytorch/pytorch/pull/69828))\r\n* Added `linalg.vander` ([#76303](https://github.com/pytorch/pytorch/pull/76303))\r\n* Added `linalg.lu` ([#67833](https://github.com/pytorch/pytorch/pull/67833))\r\n* Added `linalg.lu_solve` ([#72935](https://github.com/pytorch/pytorch/pull/72935))\r\n\r\n## Meta API\r\n\r\n* Added meta tensor kernels for the following operators:\r\n    *  `mse_loss` ([#72294](https://github.com/pytorch/pytorch/pull/72294)), `amax` ([#72124](https://github.com/pytorch/pytorch/pull/72124)), `normal` ([#70089](https://github.com/pytorch/pytorch/pull/70089)) `squeeze()` + `unsqueeze` ([#73440](https://github.com/pytorch/pytorch/pull/73440)), `unfold` ([#75717](https://github.com/pytorch/pytorch/pull/75717)), `clamp_min/max`, ([#76926](https://github.com/pytorch/pytorch/pull/76926)), `index_copy` ([#67329](https://github.com/pytorch/pytorch/pull/67329)), `linalg_cross` ([#72413](https://github.com/pytorch/pytorch/pull/72413)), `amin` ([#73581](https://github.com/pytorch/pytorch/pull/73581))\r\n* Enabled the ability to register Python decompositions for operators as meta kernels, get meta support for `where` and `huber_loss` ([#77353](https://github.com/pytorch/pytorch/pull/77353))\r\n* Registered meta functions through Python for `dot/group_norm/instance_norm/var_mean/index_reduce/matmul/bernoulli/adaptive_avg_pool` ([#77499](https://github.com/pytorch/pytorch/pull/77499))  `index_select/abs/min/max` ([#76916](https://github.com/pytorch/pytorch/pull/76916)), `reflection_pad2d` ([#77681](https://github.com/pytorch/pytorch/pull/77681)), `square` ([#77682](https://github.com/pytorch/pytorch/pull/77682)), `log_sigmoid_forward` ([#77739](https://github.com/pytorch/pytorch/pull/77739)), several more ops ([#77362](https://github.com/pytorch/pytorch/pull/77362))\r\n\r\n## torch.nn\r\n\r\n* `nn.Dropout1d`: New module for 1D channel-wise dropout ([#79545](https://github.com/pytorch/pytorch/pull/79545))\r\n* `nn.Module`: Public API for stateless / functional module computation ([#75834](https://github.com/pytorch/pytorch/pull/75834))\r\n* `nn.Module`: Support for hooks that run after state dict loading ([#76823](https://github.com/pytorch/pytorch/pull/76823), [#77392](https://github.com/pytorch/pytorch/pull/77392))\r\n* Added support for tensor subclasses as parameters ([#73459](https://github.com/pytorch/pytorch/pull/73459), [#77655](https://github.com/pytorch/pytorch/pull/77655))\r\n\r\n## torch.fx\r\n\r\n* Core\r\n    * Allowed `Tracer` to record usages of `Buffer`s ([#73612](https://github.com/pytorch/pytorch/pull/73612))\r\n    * Introduced experimental MetaTensorTracer ([#76003](https://github.com/pytorch/pytorch/pull/76003))\r\n    * Introduced `Tracer` the ability to trace different forward functions ([#77502](https://github.com/pytorch/pytorch/pull/77502))\r\n\r\n## Composability\r\n\r\n* Many features, improvements and fixes to Python tensor subclasses based on `__torch_function__` and  `__torch_dispatch__`\r\n    * Added `__torch_function__` mode, which allows you to override the meaning of all `__torch_function__` overrideable functions within a dynamic scope. ([#75154](https://github.com/pytorch/pytorch/pull/75154))\r\n    * Added `enable_torch_dispatch_mode`, which allows nesting of different `__torch_dispatch__` modes. ([#75965](https://github.com/pytorch/pytorch/pull/75965))\r\n    * Added a default implementation of `__torch_dispatch__` ([#73684](https://github.com/pytorch/pytorch/pull/73684))\r\n    * Added support `super().__torch_dispatch__` with arguments list ([#74509](https://github.com/pytorch/pytorch/pull/74509), [#74720](https://github.com/pytorch/pytorch/pull/74720))\r\n    * Miscellaneous `__torch_function__` fixes ([#75484](https://github.com/pytorch/pytorch/pull/75484), [#75110](https://github.com/pytorch/pytorch/pull/75110))\r\n    * Added `__torch_function__` override protocol supporting to some factory functions ([#75639](https://github.com/pytorch/pytorch/pull/75639))\r\n    * Fixed propagation of warnings when using `__torch_dispatch__`. ([#74357](https://github.com/pytorch/pytorch/pull/74357))\r\n    * Removed spurious warning when using disabled torch function ([#75826](https://github.com/pytorch/pytorch/pull/75826))\r\n    * Added the ability to snapshot TLS for \u201chas-a\u201d use cases of `__torch_dispatch__` ([#72623](https://github.com/pytorch/pytorch/pull/72623), [#74577](https://github.com/pytorch/pytorch/pull/74577))\r\n    * Fixed serialization and deep copying for wrapper subclasses ([#73078](https://github.com/pytorch/pytorch/pull/73078))\r\n    * Allowed `is_contiguous()` to be overridden in `__torch_dispatch__` ([#77906](https://github.com/pytorch/pytorch/pull/77906))\r\n* Added a \u201cfunctionalization\u201d program transform, that can be used to remove mutation + aliasing ops from PyTorch programs, while maintaining program semantics. Currently while most of the logic for the pass lives in core, the pass is exposed as an API through `functorch`. You can run it with `functorch.experimental.functionalize()`. Example usages can be found [here](https://github.com/pytorch/functorch/blob/130582ce47d30aec58713bb25eb2911b908aa616/test/test_eager_transforms.py#L2909).  ([#75913](https://github.com/pytorch/pytorch/pull/75913), [#76083](https://github.com/pytorch/pytorch/pull/76083), [#76084](https://github.com/pytorch/pytorch/pull/76084), [#73442](https://github.com/pytorch/pytorch/pull/73442), [#77285](https://github.com/pytorch/pytorch/pull/77285),  [#73441](https://github.com/pytorch/pytorch/pull/73441),  [#75302](https://github.com/pytorch/pytorch/pull/75302), [#75818](https://github.com/pytorch/pytorch/pull/75818), [#75819](https://github.com/pytorch/pytorch/pull/75819), [#76125](https://github.com/pytorch/pytorch/pull/76125), [#76318](https://github.com/pytorch/pytorch/pull/76318), [#77358](https://github.com/pytorch/pytorch/pull/77358))\r\n* Added a new `torch.library` API to allow users to override kernels for existing C++ ops through Python ([#75905](https://github.com/pytorch/pytorch/pull/75905), [#76892](https://github.com/pytorch/pytorch/pull/76892))\r\n* Allowed creating new libraries and defining new operators from Python ([#76250](https://github.com/pytorch/pytorch/pull/76250), [#77690](https://github.com/pytorch/pytorch/pull/77690))\r\n* Added experimental API\u2019s for registering and looking up Python decompositions for many aten operators: `from torch._decomp import register_decomposition, get_decompositions`. ([#76311](https://github.com/pytorch/pytorch/pull/76311), [#76814](https://github.com/pytorch/pytorch/pull/76814))\r\n    * Many decompositions have also been added to this table ([#76621](https://github.com/pytorch/pytorch/pull/76621), [#77329](https://github.com/pytorch/pytorch/pull/77329), [#77219](https://github.com/pytorch/pytorch/pull/77219), [#76633](https://github.com/pytorch/pytorch/pull/76633), [#76855](https://github.com/pytorch/pytorch/pull/76855), [#76714](https://github.com/pytorch/pytorch/pull/76714), [#76763](https://github.com/pytorch/pytorch/pull/76763), [#77473](https://github.com/pytorch/pytorch/pull/77473), [#77807](https://github.com/pytorch/pytorch/pull/77807), [#77500](https://github.com/pytorch/pytorch/pull/77500))\r\n\r\n## Sparse\r\n\r\n* Added factory functions for sparse CSC, BSR, and BSC tensors ([#76634](https://github.com/pytorch/pytorch/pull/76634), [#76623](https://github.com/pytorch/pytorch/pull/76623), [#75946](https://github.com/pytorch/pytorch/pull/75946), [#75961](https://github.com/pytorch/pytorch/pull/75961), [#75831](https://github.com/pytorch/pytorch/pull/75831), [#76651](https://github.com/pytorch/pytorch/pull/76651))\r\n* Added `ccol_indices` and `row_indices` methods for CSC and BSC tensors. ([#77503](https://github.com/pytorch/pytorch/pull/77503))\r\n* Added `to_sparse_csc` with support for 2D Strided and 2D CSC input ([#77521](https://github.com/pytorch/pytorch/pull/77521))\r\n* Added `to_sparse_bsr`  with support for 2D CSR input ([#77366](https://github.com/pytorch/pytorch/pull/77366))\r\n* Added `index_reduce` ([#76997](https://github.com/pytorch/pytorch/pull/76997), [#75981](https://github.com/pytorch/pytorch/pull/75981), [#76296](https://github.com/pytorch/pytorch/pull/76296))\r\n\r\n## CUDA\r\n\r\n* Add Jiterator support when dtype is complex for `sigmoid`, `exp`, `sqrt`, `rsqrt`, `log`, `log10`, `log2`, `addcmul`, `abs`, `addcdiv`, `sgn`, `neg` , `logical_and`, `angle`([#73643](https://github.com/pytorch/pytorch/pull/73643), [#73776](https://github.com/pytorch/pytorch/pull/73776), [#73781](https://github.com/pytorch/pytorch/pull/73781), [#74160](https://github.com/pytorch/pytorch/pull/74160), [#74161](https://github.com/pytorch/pytorch/pull/74161), [#74533](https://github.com/pytorch/pytorch/pull/74533), [#74455](https://github.com/pytorch/pytorch/pull/74455), [#74827](https://github.com/pytorch/pytorch/pull/74827), [#74814](https://github.com/pytorch/pytorch/pull/74814),  [#74863](https://github.com/pytorch/pytorch/pull/74863), [#75123](https://github.com/pytorch/pytorch/pull/75123),  [#76692](https://github.com/pytorch/pytorch/pull/76692))\r\n* Add Jiterator support when dtype is complex for the backward of `sigmoid` and `tanh `([#76289](https://github.com/pytorch/pytorch/pull/76289), [#74948](https://github.com/pytorch/pytorch/pull/74948))\r\n* Add Jiterator support for `kaiser_window` , `prod` ([#73734](https://github.com/pytorch/pytorch/pull/73734), [#75231](https://github.com/pytorch/pytorch/pull/75231))\r\n* Enable simple reductions with Jiterator ([#75231](https://github.com/pytorch/pytorch/pull/75231))\r\n* Updated to cuDNN v8 API with cuDNN benchmark, convolution bwd / transposed convolution fwd, `bfloat16`, conv-bias-activation fusion ([#60755](https://github.com/pytorch/pytorch/pull/60755))\r\n* Added Python Interface for Jiterator ([#76394](https://github.com/pytorch/pytorch/pull/76394))\r\n* Added Jiterator with Python Registration ([#77121](https://github.com/pytorch/pytorch/pull/77121))\r\n* Prepared Jiterator code template for multiple outputs ([#77902](https://github.com/pytorch/pytorch/pull/77902))\r\n* For CUDA graphs, added `torch.cuda.is_current_stream_capturing` ([#77789](https://github.com/pytorch/pytorch/pull/77789))\r\n\r\n\r\n## Vulkan\r\n\r\n* Added Vulkan support for Gated Recurrent Units (`torch.nn.GRU`) ([#72692](https://github.com/pytorch/pytorch/pull/72692), [#73599](https://github.com/pytorch/pytorch/pull/73599))\r\n* Added Vulkan support for the linear interpolation op (`torch.lerp`) ([#76544](https://github.com/pytorch/pytorch/pull/76544))\r\n\r\n## Profiler\r\n\r\n* Added support both global (experimental) and thread local profiling ([#75525](https://github.com/pytorch/pytorch/pull/75525), [#76078](https://github.com/pytorch/pytorch/pull/76078), [#76239](https://github.com/pytorch/pytorch/pull/76239))\r\n\r\n## Mobile\r\n\r\n* Added support for different memory formats of Tensors in NNC ([#72873](https://github.com/pytorch/pytorch/pull/72873))\r\n* Upgraded mobile model bytecode to V9 and provide backporting to previous versions ([#71662](https://github.com/pytorch/pytorch/pull/71662))\r\n\r\n## Distributed\r\n\r\n* `ShardedTensor` and `tensor parallel` \r\n    * This is a prototyping effort which consists of having a new class to represent how one `torch.tensor` is being sharded across multiple GPUs or hosts and a high level APIs for users to specify how to shard, enabling basic tensor ops for `ShardedTensor` and enabling optimizer for `ShardedTensor`. In addition, we have added PartialTensor, ReplicatedTensor and checkpoint with ShardedTensor ([#63997](https://github.com/pytorch/pytorch/pull/63997), [#65511](https://github.com/pytorch/pytorch/pull/65511), [#65671](https://github.com/pytorch/pytorch/pull/65671), [#65855](https://github.com/pytorch/pytorch/pull/65855), [#66012](https://github.com/pytorch/pytorch/pull/66012), [#66351](https://github.com/pytorch/pytorch/pull/66351), [#66464](https://github.com/pytorch/pytorch/pull/66464), [#66603](https://github.com/pytorch/pytorch/pull/66603), [#66604](https://github.com/pytorch/pytorch/pull/66604), [#67057](https://github.com/pytorch/pytorch/pull/67057), [#67188](https://github.com/pytorch/pytorch/pull/67188), [#67199](https://github.com/pytorch/pytorch/pull/67199), [#67799](https://github.com/pytorch/pytorch/pull/67799), [#68021](https://github.com/pytorch/pytorch/pull/68021), [#68096](https://github.com/pytorch/pytorch/pull/68096), [#68607](https://github.com/pytorch/pytorch/pull/68607), [#68771](https://github.com/pytorch/pytorch/pull/68771), [#68786](https://github.com/pytorch/pytorch/pull/68786), [#68806](https://github.com/pytorch/pytorch/pull/68806), [#69226](https://github.com/pytorch/pytorch/pull/69226), [#69493](https://github.com/pytorch/pytorch/pull/69493), [#69569](https://github.com/pytorch/pytorch/pull/69569), [#69725](https://github.com/pytorch/pytorch/pull/69725), [#69874](https://github.com/pytorch/pytorch/pull/69874), [#69945](https://github.com/pytorch/pytorch/pull/69945), [#69946](https://github.com/pytorch/pytorch/pull/69946), [#70145](https://github.com/pytorch/pytorch/pull/70145), [#70228](https://github.com/pytorch/pytorch/pull/70228), [#70266](https://github.com/pytorch/pytorch/pull/70266), [#70331](https://github.com/pytorch/pytorch/pull/70331), [#70476](https://github.com/pytorch/pytorch/pull/70476), [#71445](https://github.com/pytorch/pytorch/pull/71445), [#72062](https://github.com/pytorch/pytorch/pull/72062), [#72130](https://github.com/pytorch/pytorch/pull/72130), [#73309](https://github.com/pytorch/pytorch/pull/73309), [#76360](https://github.com/pytorch/pytorch/pull/76360), [#76477](https://github.com/pytorch/pytorch/pull/76477), [#72733](https://github.com/pytorch/pytorch/pull/72733), [#73392](https://github.com/pytorch/pytorch/pull/73392), [#76199](https://github.com/pytorch/pytorch/pull/76199), [#75374](https://github.com/pytorch/pytorch/pull/75374), [#71624](https://github.com/pytorch/pytorch/pull/71624), [#74040](https://github.com/pytorch/pytorch/pull/74040), [#73529](https://github.com/pytorch/pytorch/pull/73529), [#74941](https://github.com/pytorch/pytorch/pull/74941), [#73703](https://github.com/pytorch/pytorch/pull/73703), [#75712](https://github.com/pytorch/pytorch/pull/75712), [#73873](https://github.com/pytorch/pytorch/pull/73873), [#75991](https://github.com/pytorch/pytorch/pull/75991), [#75844](https://github.com/pytorch/pytorch/pull/75844), [#76824](https://github.com/pytorch/pytorch/pull/76824), [#76897](https://github.com/pytorch/pytorch/pull/76897), [#77185](https://github.com/pytorch/pytorch/pull/77185), [#77191](https://github.com/pytorch/pytorch/pull/77191), [#76758](https://github.com/pytorch/pytorch/pull/76758), [#77209](https://github.com/pytorch/pytorch/pull/77209), [#77214](https://github.com/pytorch/pytorch/pull/77214), [#77367](https://github.com/pytorch/pytorch/pull/77367), [#77580](https://github.com/pytorch/pytorch/pull/77580), [#77626](https://github.com/pytorch/pytorch/pull/77626), [#77800](https://github.com/pytorch/pytorch/pull/77800), [#77707](https://github.com/pytorch/pytorch/pull/77707), [#78056](https://github.com/pytorch/pytorch/pull/78056))\r\n        * Design proposal: [ShardedTensor](https://github.com/pytorch/pytorch/issues/55207) and [Sharding APIs](https://github.com/pytorch/pytorch/issues/72138). [Example](https://github.com/pytorch/examples/tree/main/distributed/sharded_tensor) for a Megatron-LM style tensor parallel.\r\n* FullyShardedDataParallel\r\n    * Added `FlatParameter` to track the information of a flat parameter ([#69241](https://github.com/pytorch/pytorch/pull/69241))\r\n    * Enabled `summon_full_params` for FSDP. ([#71225](https://github.com/pytorch/pytorch/pull/71225))\r\n    * Added `no_sync()` context manager ([#72446](https://github.com/pytorch/pytorch/pull/72446))\r\n    * Implemented `apply()` ([#72925](https://github.com/pytorch/pytorch/pull/72925))\r\n    * Implemented local_state_dict and load_local_state_dict ([#73300](https://github.com/pytorch/pytorch/pull/73300))\r\n    * Implemented `full_state_dict` ([#73324](https://github.com/pytorch/pytorch/pull/73324))\r\n    * Implemented `clip_grad_norm` for FSDP ([#73405](https://github.com/pytorch/pytorch/pull/73405))\r\n    * Added grad accumulation without `no_sync()` ([#73535](https://github.com/pytorch/pytorch/pull/73535))\r\n    * Added `full_optim_state_dict` ([#74215](https://github.com/pytorch/pytorch/pull/74215))\r\n    * Implemented `reshard_flatten_tensor` ([#75192](https://github.com/pytorch/pytorch/pull/75192))\r\n    * Added `scatter_full_optim_state_dict()` ([#75517](https://github.com/pytorch/pytorch/pull/75517))\r\n    * Implemented `sharded_state_dict` and `load_sharded_state_dict` ([#77356](https://github.com/pytorch/pytorch/pull/77356))\r\n    * Enabled mixed precision in FSDP ([#75024](https://github.com/pytorch/pytorch/pull/75024))\r\n    * Changed to allow specification of modules to ignore when wrapping with FSDP([#75431](https://github.com/pytorch/pytorch/pull/75431))\r\n    * Added `FullStateDictConfig` to allow full state dict checkpoint with rank0 only and CPU offload ([#75908](https://github.com/pytorch/pytorch/pull/75908))\r\n    * Added validation to ensure FSDP units execute consistently across ranks ([#75902](https://github.com/pytorch/pytorch/pull/75902))\r\n    * Added support for initialization of modules on meta device ([#75880](https://github.com/pytorch/pytorch/pull/75880))\r\n    * Added support for no sharding config for DDP-style parallelism ([#76736](https://github.com/pytorch/pytorch/pull/76736))\r\n    * Changed to allow FSDP to specify device sharded wrapped module should be placed on ([#77321](https://github.com/pytorch/pytorch/pull/77321))\r\n    * Enabled FSDP parameter sync ([#77492](https://github.com/pytorch/pytorch/pull/77492))\r\n    * Made sharding strategy configurable and support zero2 algorithm ([#73819](https://github.com/pytorch/pytorch/pull/73819))\r\n    * Added a shard aware grad scaler for FSDP+MixedPrecision ([#76918](https://github.com/pytorch/pytorch/pull/76918))\r\n    * Enabled FSDP full state dict to work for non root FSDP modules via post load hooks ([#76912](https://github.com/pytorch/pytorch/pull/76912))\r\n    * Added `always_wrap` policy ([#73687](https://github.com/pytorch/pytorch/pull/73687))\r\n    * Provided an auto wrap policy for common transformer models ([#76455](https://github.com/pytorch/pytorch/pull/76455))\r\n* DistributedDataParallel\r\n    * Added support for hierarchical model averaging ([#73285](https://github.com/pytorch/pytorch/pull/73285))\r\n* torch.distributed.rpc\r\n    * Changed to allow for optional `world_size` argument in `init_rpc` ([#73372](https://github.com/pytorch/pytorch/pull/73372))\r\n    * Changed to allow newly joined ranks to communicate with existing ranks ([#73373](https://github.com/pytorch/pytorch/pull/73373))\r\n    * Changed to allow existing ranks to communicate with newly joined ranks ([#74035](https://github.com/pytorch/pytorch/pull/74035))\r\n    * Added graceful shutdown for dynamic RPC members ([#74561)](https://github.com/pytorch/pytorch/pull/74561)\r\n\r\n## JIT/TorchScript\r\n\r\n* Added autocasting of values from fp32 to lower precision floats in `torch.jit.freeze`  ([#74178](https://github.com/pytorch/pytorch/pull/74178))\r\n* `torch.jit.set_fusion_strategy` is now a public API, allowing one to set if they want fusion based on static or dynamic tensor sizes ([#72639](https://github.com/pytorch/pytorch/pull/72639))\r\n* Added support for compiling `tensor.__getitem__()` ([#73952](https://github.com/pytorch/pytorch/pull/73952))\r\n* TorchScript uses a fuser to combine multiple operator calls into a single kernel. In 1.12 the default fuser for NVIDIA GPUs is switched to NVFuser, which supports a wider range of operators and has demonstrated improved throughput compared to NNC, the previous fuser. ([#74361](https://github.com/pytorch/pytorch/pull/74361), [#77010](https://github.com/pytorch/pytorch/pull/77010), [#77395](https://github.com/pytorch/pytorch/pull/77395), [#72127](https://github.com/pytorch/pytorch/pull/72127), [#73627](https://github.com/pytorch/pytorch/pull/73627), [#75235](https://github.com/pytorch/pytorch/pull/75235), [#75539](https://github.com/pytorch/pytorch/pull/75539), [#75558](https://github.com/pytorch/pytorch/pull/75558), [#75646](https://github.com/pytorch/pytorch/pull/75646), [#76226](https://github.com/pytorch/pytorch/pull/76226), [#76459](https://github.com/pytorch/pytorch/pull/76459), [#76604](https://github.com/pytorch/pytorch/pull/76604), [#76563](https://github.com/pytorch/pytorch/pull/76563), [#77001](https://github.com/pytorch/pytorch/pull/77001), [#77017](https://github.com/pytorch/pytorch/pull/77017), [#77471](https://github.com/pytorch/pytorch/pull/77471), [#77777](https://github.com/pytorch/pytorch/pull/77777), [#77884](https://github.com/pytorch/pytorch/pull/77884), [#76790](https://github.com/pytorch/pytorch/pull/76790), [#76343](https://github.com/pytorch/pytorch/pull/76343), [#76605](https://github.com/pytorch/pytorch/pull/76605), [#76769](https://github.com/pytorch/pytorch/pull/76769), [#77158](https://github.com/pytorch/pytorch/pull/77158), [#74339](https://github.com/pytorch/pytorch/pull/74339))\r\n* Added option to save extra files in `torch.jit.save_jit_module_to_flatbuffer` ([#77870](https://github.com/pytorch/pytorch/pull/77870))\r\n\r\n## Quantization\r\n\r\n* Added oneDNN quantization backend ([#69820](https://github.com/pytorch/pytorch/pull/69820))\r\n* Added oneDNN quant backend ([#74137](https://github.com/pytorch/pytorch/pull/74137))\r\n\r\n## ONNX\r\n\r\n* Added support to exporting additional ops: \r\n    * `Cross`, `Cdist` and `Pairwise Distance` ([#75278](https://github.com/pytorch/pytorch/pull/75278))\r\n    * `bucketize` ([#74856](https://github.com/pytorch/pytorch/pull/74856))\r\n    * `pixel unshuffle` ([#72499](https://github.com/pytorch/pytorch/pull/72499))\r\n    * `embedding_renorm` ([#72738](https://github.com/pytorch/pytorch/pull/72738))\r\n    * `aminmax` ([#75714](https://github.com/pytorch/pytorch/issues/75714))\r\n    * `amax` and `amin` ([#75268](https://github.com/pytorch/pytorch/pull/75268))\r\n    * `grid_sample` ([#76159](https://github.com/pytorch/pytorch/issues/76159))\r\n* Added support to exporting quantized models ([#72986](https://github.com/pytorch/pytorch/issues/72986), [#73102](https://github.com/pytorch/pytorch/issues/73102), [#75697,](https://github.com/pytorch/pytorch/pull/75697) [#76002,](https://github.com/pytorch/pytorch/pull/76002) [#76055,](https://github.com/pytorch/pytorch/pull/76055) [#73336,](https://github.com/pytorch/pytorch/pull/73336)[#77393,](https://github.com/pytorch/pytorch/pull/77393) [#75920,](https://github.com/pytorch/pytorch/pull/75920) [#75921](https://github.com/pytorch/pytorch/pull/75921))\r\n* Added support to optional type. See tests in PR for examples. ([#73284](https://github.com/pytorch/pytorch/issues/73284))\r\n* Added support to ATen fallback for non-Caffe2 implementations of ONNX ([#74759](https://github.com/pytorch/pytorch/pull/74759), [#75468,](https://github.com/pytorch/pytorch/pull/75468) [#74680,](https://github.com/pytorch/pytorch/pull/74680) [#73954](https://github.com/pytorch/pytorch/pull/73954))\r\n\r\n## Infra (Releng)\r\n\r\n* Added support for ROCm 5.0 ([#72895](https://github.com/pytorch/pytorch/pull/72895))\r\n* Added LibTorch builds for ROCm ([#57506](https://github.com/pytorch/pytorch/pull/57506))\r\n* Added support for CUDA 11.6 ([#75518](https://github.com/pytorch/pytorch/pull/75518))\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n* Improved numerical stability of `torch.distributions.wishart.Wishart` ([#72993](https://github.com/pytorch/pytorch/pull/72993))\r\n* Added `mode` property to `torch.distributions.Distribution` ([#76690](https://github.com/pytorch/pytorch/pull/76690))\r\n* Added `foreach` flag for `torch.optim.{Adadelta, Adagrad, Adamax, Adam, ASGD, NAdam, RAdamSGD, Rmsprop, Rprop, AdamW}` ([#69980](https://github.com/pytorch/pytorch/pull/69980), [#69981](https://github.com/pytorch/pytorch/pull/69981), [#69982](https://github.com/pytorch/pytorch/pull/69982), [#70295](https://github.com/pytorch/pytorch/pull/70295), [#70481](https://github.com/pytorch/pytorch/pull/70481), [#70229](https://github.com/pytorch/pytorch/pull/70229), [#70230](https://github.com/pytorch/pytorch/pull/70230), [#70231](https://github.com/pytorch/pytorch/pull/70231), [#70482](https://github.com/pytorch/pytorch/pull/70482), [#70483](https://github.com/pytorch/pytorch/pull/70483), [#70484](https://github.com/pytorch/pytorch/pull/70484))\r\n* Added out variant for `torch.softmax` and `torch.log_softmax` ([#75833](https://github.com/pytorch/pytorch/pull/75833))\r\n* Added handling for r=0 case for `torch.combinations` ([#70270](https://github.com/pytorch/pytorch/pull/70270))\r\n* Added XPU support for `torch.autocast` ([#75250](https://github.com/pytorch/pytorch/pull/75250))\r\n* Added support for Tensor source for `.set_(storage, offset, size, strides)` ([#77007](https://github.com/pytorch/pytorch/pull/77007))\r\n* Changed to register `torch.return_types.*` as pytree nodes ([#75915](https://github.com/pytorch/pytorch/pull/75915))\r\n* Added typing for `torch.return_type` ([#74199](https://github.com/pytorch/pytorch/pull/74199))\r\n* Set correct module for APIs in the `torch` module ([#75801](https://github.com/pytorch/pytorch/pull/75801))\r\n* Improved `NotImplementedError` verbosity for `torch.distributions.kl_divergence` ([#72845](https://github.com/pytorch/pytorch/pull/72845))\r\n* Added maximize flag to `torch.optim.Adagrad` ([#75968](https://github.com/pytorch/pytorch/pull/75968))\r\n* `optim.{Adagrad, Adam, Adamax, AdamW, RAdam}`: Updated `step` in functional optimizers and pass `state_steps` instead of `state` ([#71333](https://github.com/pytorch/pytorch/pull/71333))\r\n* Improved `torch.lerp` numerical precision by doing intermediate math in opmath_t ([#76062](https://github.com/pytorch/pytorch/pull/76062))\r\n* Changed to alias `torch.finfo.tiny` to `torch.finfo.smallest_normal` ([#76292](https://github.com/pytorch/pytorch/pull/76292))\r\n\r\n## C++ API\r\n\r\n* Added catch for overflows in calculating storage byte size for `col2im `([#73719](https://github.com/pytorch/pytorch/pull/73719))\r\n* Implemented center padding for `stft` ([#73432](https://github.com/pytorch/pytorch/pull/73432))\r\n\r\n## Autograd\r\n\r\n* Added forward AD support for `torch.{atan2, dist, logsumexp, log_softmax, norm, polar, put softmax}` ([#73741](https://github.com/pytorch/pytorch/pull/73741), [#74205](https://github.com/pytorch/pytorch/pull/74205), [#75027](https://github.com/pytorch/pytorch/pull/75027), [#75326](https://github.com/pytorch/pytorch/pull/75326), [#77421](https://github.com/pytorch/pytorch/pull/77421))\r\n* Added forward AD support for `torch.nn.functional.{cross_entropy, pairwise_dist, nll_loss, normalize}` ([#73741](https://github.com/pytorch/pytorch/pull/73741), [#74205](https://github.com/pytorch/pytorch/pull/74205))\r\n* Added forward AD support for `torch.cholesky_inverse` ([#75033](https://github.com/pytorch/pytorch/pull/75033))\r\n* Added forward AD and forward-over-reverse support for FFTs ([#75326](https://github.com/pytorch/pytorch/pull/75326))\r\n* Added forward AD support for `torch.nn.functional.{embedding,prelu, bilinear, rrelu, logsigmoid}` ([#77421](https://github.com/pytorch/pytorch/pull/77421))\r\n* Added forward AD support for `torch.nn.BCELoss` ([#77755](https://github.com/pytorch/pytorch/pull/77755))\r\n* Added forward AD support for `Tensor.__rsub__` ([#75326](https://github.com/pytorch/pytorch/pull/75326))\r\n* Added forward AD support for `torch.clamp` when bounds are tensors ([#74042](https://github.com/pytorch/pytorch/pull/74042))\r\n* Added forward AD support for `torch.nn.functional.{dropout, glu}`([#75288](https://github.com/pytorch/pytorch/pull/75288), [#77186](https://github.com/pytorch/pytorch/pull/77186))\r\n* Added forward-over-reverse for `torch.nn.functional.`{`leaky_relu, glu, elu, selu, celu}` ([#75294](https://github.com/pytorch/pytorch/pull/75294), [#77309](https://github.com/pytorch/pytorch/pull/77309), [#75297](https://github.com/pytorch/pytorch/pull/75297))\r\n* Improved forward and backward derivative `torch.{linalg.cholesky, cholesky}` ([#76032](https://github.com/pytorch/pytorch/pull/76032))\r\n* Improved forward and backward derivative of `torch.linalg.qr` ([#76115](https://github.com/pytorch/pytorch/pull/76115))\r\n* Added complex autograd support for  `torch.cholesky_inverse` ([#75033](https://github.com/pytorch/pytorch/pull/75033))\r\n* Added double backward support for `torch.nn.functional.binary_cross_entropy` wrt target ([#77416](https://github.com/pytorch/pytorch/pull/77416))\r\n* Improved error message for `torch.nn.functional.batch_norm` when `running_{mean,var}` have forward grad defined ([#73655](https://github.com/pytorch/pytorch/pull/73655))\r\n* Improve error message when forward AD is not supported ([#75105](https://github.com/pytorch/pytorch/pull/75105))\r\n* Added forward AD and forward-over-reverse support for `torch.nn.functional.max_unpool` ([#68625](https://github.com/pytorch/pytorch/pull/68625))\r\n* Added autograd support for `masked_softmax` ([#71502](https://github.com/pytorch/pytorch/pull/71502))\r\n\r\n## Build\r\n\r\n* Fixed pybind deprecation warnings ([#72376](https://github.com/pytorch/pytorch/pull/72376))\r\n* Enabled win-arm64 ([#72424](https://github.com/pytorch/pytorch/pull/72424))\r\n* Moved magma utils to its own header ([#73058](https://github.com/pytorch/pytorch/pull/73058))\r\n* Turned on -Wsign-compare ([#74996](https://github.com/pytorch/pytorch/pull/74996))\r\n* Made all `.pyi.in` files exportable from torch/_C/ folder ([#74962](https://github.com/pytorch/pytorch/pull/74962))\r\n* Improved Jinja2 for docs/cpp build set to version 3.0 ([#74718](https://github.com/pytorch/pytorch/pull/74718))\r\n* Added CMake option for using static MKL libraries ([#73069](https://github.com/pytorch/pytorch/pull/73069))\r\n* CPU Kernel: Changed to use per-operator headers ([#71137](https://github.com/pytorch/pytorch/pull/71137))\r\n* CUDA Kernels: Changed to use per-operator headers ([#71212](https://github.com/pytorch/pytorch/pull/71212))\r\n\r\n## Dataloader\r\n\r\n* Added `pin_memory_device` to Dataloader to pin `Tensor` to the corresponding GPU device ([#65402](https://github.com/pytorch/pytorch/pull/65402))\r\n\r\n## ForEach\r\n\r\n* Improved numerical precision for `ForEach` L1 and L2 norm by using  `OpMathType` tensor for intermediate results ([#68107](https://github.com/pytorch/pytorch/pull/68107))\r\n\r\n## Meta API\r\n\r\n* Changed to skip superfluous storage allocations while constructing meta tensors ([#65331](https://github.com/pytorch/pytorch/pull/65331))\r\n\r\n## torch.nn\r\n\r\n* Made `nn.init.orthogonal_` no-op for empty input ([#75553](https://github.com/pytorch/pytorch/pull/75553))\r\n* `nn.{Conv1d, Conv2d, Conv3d}`: Added support for complex datatypes ([#75310](https://github.com/pytorch/pytorch/pull/75310), [#75412](https://github.com/pytorch/pytorch/pull/75412), [#75581](https://github.com/pytorch/pytorch/pull/75581))\r\n* `nn.Conv2d`: Added bfloat16 support for mkl-dnn backend ([#55864](https://github.com/pytorch/pytorch/pull/55864))\r\n* `nn.Conv2d`: Added support for channels last memory format on CPU for mkl-dnn backend, naive algorithm, and dilated algorithm ([#55584](https://github.com/pytorch/pytorch/pull/55584), [#68101](https://github.com/pytorch/pytorch/pull/68101), [#70665](https://github.com/pytorch/pytorch/pull/70665))\r\n* `nn.EmbeddingBag`: Added half precision support on CPU ([#74844](https://github.com/pytorch/pytorch/pull/74844))\r\n* `nn.FractionalMaxPool*d`: Added support `0`s in `out_size` ([#73634](https://github.com/pytorch/pytorch/pull/73634))\r\n* `nn.Module`: Changed to throw error for non-dict inputs to `load_state_dict()` ([#77197](https://github.com/pytorch/pytorch/pull/77197))\r\n* `nn.{PixelShuffle, PixelUnshuffle}`: Added support for channels last memory format ([#50573](https://github.com/pytorch/pytorch/pull/50573))\r\n* `nn.PReLU`: Enabled fp32/bfloat16 forward and backward for mkl-dnn backend ([#60427](https://github.com/pytorch/pytorch/pull/60427))\r\n* `F.elu`: Improve numerical precision by using `opmath` and `expm1` ([#77062](https://github.com/pytorch/pytorch/pull/77062))\r\n* `F.{hardshrink, hardsigmoid, hardswish, logsigmoid,  smooth_l1_loss, softplus, softshrink}, nn.{BatchNorm, GLU, Upsample}`: Add bfloat16 support on CPU ([#62558](https://github.com/pytorch/pytorch/pull/62558), [#63134](https://github.com/pytorch/pytorch/pull/63134), [#77496](https://github.com/pytorch/pytorch/pull/77496), [#61944](https://github.com/pytorch/pytorch/pull/61944), [#76935](https://github.com/pytorch/pytorch/pull/76935))\r\n\r\n## torch.fx\r\n\r\n* FX/graph_drawer\r\n    * Added args/kwargs and users ([#73464](https://github.com/pytorch/pytorch/pull/73464))\r\n    * Added `skip_node_names_in_args` option, default to `True` ([#73815](https://github.com/pytorch/pytorch/pull/73815))\r\n* Core\r\n    * Refactor FX codegen into extensible Codegen object ([#72566](https://github.com/pytorch/pytorch/pull/72566))\r\n    * Modified `replace_all_uses_with` to allowing filtering of nodes to update([#73763](https://github.com/pytorch/pytorch/pull/73763))\r\n    * Made `immutable_list` and `immutable_dict` work with pytrees ([#73766](https://github.com/pytorch/pytorch/pull/73766))\r\n    * Added `Assert None concrete_args` and improve error messages ([#74662](https://github.com/pytorch/pytorch/pull/74662))\r\n* In minimizer, made args work in the `uru10x10_to_trt_eval` script ([#74707](https://github.com/pytorch/pytorch/pull/74707))\r\n* For split_module, changed to return mapping of qualified names from split_module() ([#73564](https://github.com/pytorch/pytorch/pull/73564))\r\n* For shape propagation, made shapes and args/kwargs concrete for minimizer ([#75291](https://github.com/pytorch/pytorch/pull/75291))\r\n\r\n## Sparse\r\n\r\n* Added CUDA support for `scatter_reduce` ([#74606,](https://github.com/pytorch/pytorch/pull/74606)[#74607](https://github.com/pytorch/pytorch/pull/74607))\r\n* Added 2D Strided, 2D CSR, 2D CSC, 2D COO support to `to_sparse_csr` ([#77521](https://github.com/pytorch/pytorch/pull/77521))\r\n* Added ND Strided, 2D CSC support to `to_dense` ([#74486](https://github.com/pytorch/pytorch/pull/74486), [#77521](https://github.com/pytorch/pytorch/pull/77521))\r\n* Added 2D CSC support to `to_sparse`  ([#73642](https://github.com/pytorch/pytorch/pull/73642), [#77521](https://github.com/pytorch/pytorch/pull/77521))\r\n* Added support for batched CSR to `sparse_csr_tensor` ([#74542](https://github.com/pytorch/pytorch/pull/74542))\r\n* Added support for `__str__` for CSC, BSR, and BSC tensors ([#77530](https://github.com/pytorch/pytorch/pull/77530), [#76650](https://github.com/pytorch/pytorch/pull/76650))\r\n* Updated transpose to return CSC when given CSR ([#77615](https://github.com/pytorch/pytorch/pull/77615))\r\n* Added support for CSR gradients for CSR tensors ([#75435](https://github.com/pytorch/pytorch/pull/75435))\r\n* Added CSC support to `addmm`, `addmv`, `mm` ([#77615](https://github.com/pytorch/pytorch/pull/77615))\r\n* Added autograd for CSR inputs to `torch.sparse.sampled_addmm` ([#68084](https://github.com/pytorch/pytorch/pull/68084))\r\n* Added autograd for CSR inputs to `torch.sparse.addmm and torch.sparse.mm` ([#76591](https://github.com/pytorch/pytorch/pull/76591))\r\n* Added Half/BFloat16 support for to_dense and coalesce methods. ([#72397](https://github.com/pytorch/pytorch/pull/72397))\r\n* Added CSR support to `mul` ([#74266](https://github.com/pytorch/pytorch/pull/74266), [#77177](https://github.com/pytorch/pytorch/pull/77177))\r\n* Added CSR support to `sum` ([#74766](https://github.com/pytorch/pytorch/pull/74766))\r\n* Added BSR support to `addmm`, `addmv`, `triangular_solve` ([#77255](https://github.com/pytorch/pytorch/pull/77255))\r\n* Added batched CSR support to `torch.sparse.sampled_addmm` on CUDA ([#77243](https://github.com/pytorch/pytorch/pull/77243))\r\n* Added CSR support for `torch.sparse.sampled_addmm` on CPU ([#76589](https://github.com/pytorch/pytorch/pull/76589))\r\n* Added CSR support to `torch.select` ([#76228](https://github.com/pytorch/pytorch/pull/76228))\r\n* Added CSR support to `Tensor.to` ([#76400](https://github.com/pytorch/pytorch/pull/76400))\r\n* Added CSC support to `torch.empty` ([#77508](https://github.com/pytorch/pytorch/pull/77508))\r\n* Added CSC, BSR, BSC support to `torch.clone` ([#77512](https://github.com/pytorch/pytorch/pull/77512))\r\n* Added CSC, BSR, BSC support for `copy_`  ([#77605](https://github.com/pytorch/pytorch/pull/77605))\r\n* Added (Strided, CSR) input support to `torch.mm` ([#73686](https://github.com/pytorch/pytorch/pull/73686))\r\n* Added CSR support to `torch.sparse.mm` ([#73075](https://github.com/pytorch/pytorch/pull/73075))\r\n* Added (Strided, CSR, CSR) support to `addmm` on CPU ([#73076](https://github.com/pytorch/pytorch/pull/73076))\r\n* Added runtime beta support warning to CSR, CSC, BSR, BSC tensors ([#75594](https://github.com/pytorch/pytorch/pull/75594), [#75865](https://github.com/pytorch/pytorch/pull/75865))\r\n* Added `bool` support to `coalesce` and `to_dense`  ([#74495](https://github.com/pytorch/pytorch/pull/74495))\r\n* Added `half` support to `sparse_mask` ([#76862](https://github.com/pytorch/pytorch/pull/76862))\r\n* Added AMD Navi 21 support to `coalesce` ([#73548](https://github.com/pytorch/pytorch/pull/73548))\r\n\r\n## AMD\r\n\r\n* Enabled `atomicAddNoRet()` for all gfx targets. ([#75451](https://github.com/pytorch/pytorch/pull/75451))\r\n* Enabled miopen for RNNs with dropout. ([#75429](https://github.com/pytorch/pytorch/pull/75429))\r\n* Used `ncclAllToAll` for ROCm ([#75128](https://github.com/pytorch/pytorch/pull/75128))\r\n* Navi21 Enablement: fix TI `num_threads` for ROCm,  Depthwise kernels, Embedding kernels, Normalization kernels, Softmax kernels, Tensor kernels, Index, Repeat and Sort kernels, Range and Multinomial Kernels ([#69942](https://github.com/pytorch/pytorch/pull/69942), [#72682](https://github.com/pytorch/pytorch/pull/72682), [#72809](https://github.com/pytorch/pytorch/pull/72809), [#73543](https://github.com/pytorch/pytorch/pull/73543),  [#73545](https://github.com/pytorch/pytorch/pull/73545), [#73546](https://github.com/pytorch/pytorch/pull/73546), [#73549](https://github.com/pytorch/pytorch/pull/73549), [#73550](https://github.com/pytorch/pytorch/pull/73550))\r\n* Added ROCm version api within CMake ([#69481](https://github.com/pytorch/pytorch/pull/69481))\r\n* Enabled `sort` operator BF16 support ([#72854](https://github.com/pytorch/pytorch/pull/72854))\r\n* Enabled HIP IPC ([#74383](https://github.com/pytorch/pytorch/pull/74383))\r\n* Enabled `topk` operator for `bfloat16` dtype ([#71913](https://github.com/pytorch/pytorch/pull/71913))\r\n* Added HIP_HOME/include.lib in cpp_extensions ([#75548](https://github.com/pytorch/pytorch/pull/75548))\r\n\r\n## CUDA\r\n\r\n* PyTorch: added support to NVTX range_start and range_end ([#70030](https://github.com/pytorch/pytorch/pull/70030))\r\n* Show friendly error message when forgetting `init` in `torch.cuda` ([#72404](https://github.com/pytorch/pytorch/pull/72404))\r\n* PyTorch GPU Allocator: better use of blocks with rounding of allocation sizes ([#74213](https://github.com/pytorch/pytorch/pull/74213))\r\n* CUDACachingAlloc/GPUInference: implemented garbage collection without GPU sync ([#74261](https://github.com/pytorch/pytorch/pull/74261))\r\n* CUBLAS/TF32: added environment variable to allow override of `allow_tf32_cublas` ([#77114](https://github.com/pytorch/pytorch/pull/77114))\r\n\r\n## Intel \r\n\r\n* Bfloat16\r\n  * Added BFloat16 support for `torch.{nn.PReLU, nn.Upsample,nn.GLU, randperm, multinomial, poisson, nn.ELU, nn.SELU, nn.CELU, nn.LogSigmoid, nn.Hardsigmoid, nn.Hardshrink, nn.Softshrink, nn.Hardswish, nn.Softplus, nn.SmoothL1Loss, histc, atan2, logcumsumexp, diag, fmod, cumsum, cumprod, nn.utils.weight_norm , nn.BatchNorm2d}` and allow autocast enabled ([_#63634,_](https://github.com/pytorch/pytorch/pull/63634) [_#58297,_](https://github.com/pytorch/pytorch/pull/58297) [_#61944,_](https://github.com/pytorch/pytorch/pull/61944) [_#63215 ,_](https://github.com/pytorch/pytorch/pull/63215) [_#62546,_](https://github.com/pytorch/pytorch/pull/62546) [_#63134_](https://github.com/pytorch/pytorch/pull/63134), [_#72694,_](https://github.com/pytorch/pytorch/pull/72694) [_#61897,_](https://github.com/pytorch/pytorch/pull/61897) [_#73845,_](https://github.com/pytorch/pytorch/pull/73845) [_#74410,_](https://github.com/pytorch/pytorch/pull/74410) [_#68725_](https://github.com/pytorch/pytorch/pull/68725))\r\n    * Improved `torch.nn.functional.log_softmax` on CPU when dim != -1 on both float32 and bfloat16 ([_#64726_](https://github.com/pytorch/pytorch/pull/64726))\r\n    * Improved `torch.nn.functional.layer_norm` bfloat16 performance on CPU ([_#71376_](https://github.com/pytorch/pytorch/pull/71376))\r\n    * Improved autocast cpu documentation ([_#68567_](https://github.com/pytorch/pytorch/pull/68567))\r\n* Channels last\r\n    * Add channels-last support for `torch.nn.{conv2D(kernel slow_conv_dilated2d and thnn_conv2d, mkldnn as backend), GroupNorm, PixelShuffle, PixelUnshuffle}`([_#70665_](https://github.com/pytorch/pytorch/pull/70665), [_#68101_](https://github.com/pytorch/pytorch/pull/68101), [_#55584,_](https://github.com/pytorch/pytorch/pull/55584) [_#50573,_](https://github.com/pytorch/pytorch/pull/50573) [_#555864_](https://github.com/pytorch/pytorch/pull/555864))\r\n* OneDNN\r\n    * Upgraded oneDNN to v2.6.0, ([_#75398_](https://github.com/pytorch/pytorch/pull/75398))\r\n    * Added JIT graph fuser for oneDNN Graph API (v0.5) ([_#76622_](https://github.com/pytorch/pytorch/pull/76622))\r\n* Quantization\r\n    * Improve {`qcat_nhwc, qupsample_bilinear2d, qupsample_nearest2d, qbatch_norm2d, qmax_pool2d, qavg_pool2d`} performance on multi-core ([_#69667_](https://github.com/pytorch/pytorch/pull/69667), [_#69601_](https://github.com/pytorch/pytorch/pull/69601), [_#69600,_](https://github.com/pytorch/pytorch/pull/69600) [_#69599_](https://github.com/pytorch/pytorch/pull/69599), [_#69598_](https://github.com/pytorch/pytorch/pull/69598), [_#69517_](https://github.com/pytorch/pytorch/pull/69517))\r\n    * Add oneDNN as backend for quantization ([_#69820_](https://github.com/pytorch/pytorch/pull/69820)) \r\n* Improved `torch{norm,argmax,argmin, scatter, gather}` performance on CPU ([_#64479_](https://github.com/pytorch/pytorch/pull/64479), [_#64478_](https://github.com/pytorch/pytorch/pull/64478))\r\n* Improved `torch.nn.functional{log_softmax``, softmax}` performance on CPU ([_#73953_](https://github.com/pytorch/pytorch/pull/73953))\r\n* Expanded graph rewrite to handle `conv_transpose3d` ([_#76888_](https://github.com/pytorch/pytorch/pull/76888))\r\n* Expanded coverage of convolution folding in conv\u2192mul\u2192add\u2192bn ([_#75724_](https://github.com/pytorch/pytorch/pull/75724))\r\n* Added MKLDNN support for `PReLU` ([_#60427_](https://github.com/pytorch/pytorch/pull/60427))\r\n\r\n## Composability \r\n\r\n* Added `torch.nn.init` to list of functions overridable by `__torch_function__` ([#76014](https://github.com/pytorch/pytorch/pull/76014))\r\n* Relaxed dtype restrictions on `torch.Tensor`([#73850](https://github.com/pytorch/pytorch/pull/73850))\r\n\r\n## Profiler\r\n\r\n* Enabled iteration tracking for kineto ([#72292](https://github.com/pytorch/pytorch/pull/72292))\r\n* Added support for input sequence ID tracking for NVTX profiler ([#70264](https://github.com/pytorch/pytorch/pull/70264))\r\n* Re-enabled user-annotations in PyTorch ([#75601](https://github.com/pytorch/pytorch/pull/75601))\r\n* Added support to configure Kineto CUPTI profiler from PyTorch profiler interface ([#75616](https://github.com/pytorch/pytorch/pull/75616))\r\n\r\n## Vulkan\r\n\r\n* Added an interface to obtain execution time data for GPU shader kernels when executing Vulkan operators ([#75829](https://github.com/pytorch/pytorch/pull/75829))\r\n\r\n## Mobile\r\n\r\n* Improved Android instrumentation test and update README ([#72736](https://github.com/pytorch/pytorch/pull/72736))\r\n* Improved unsupported scalar type error message for Android ([#74660](https://github.com/pytorch/pytorch/pull/74660))\r\n\r\n\r\n\r\n## JIT/TorchScript\r\n\r\n* `torch.jit.trace` now treats `tensor.numel()` as `aten::numel`, instead of a constant value ([#74081](https://github.com/pytorch/pytorch/pull/74081))\r\n* When printing out the types of a JIT Dict, with a tuple key, we now print out the types of the tuple if it is simple ([#76164](https://github.com/pytorch/pytorch/pull/76164))\r\n* Added support for basic ops support for complex numbers in JIT, We now support op(complex, Tensor) for the following: add (+), mul (*), eq (==), ne (!=), sub (-), div (/) ([#73286](https://github.com/pytorch/pytorch/pull/73286))\r\n* TorchScript now preserves the original exception message when rethrowing a Python-based exception ([#77093](https://github.com/pytorch/pytorch/pull/77093))\r\n* Modified the conditions for conv folding in `torch.jit.freeze` to allow for folding arguments that can be promoted to floating point (eg integer tensor arguments) ([#73278](https://github.com/pytorch/pytorch/pull/73278))\r\n* Reduced size of JIT debug.pkl files by only storing unique traces ([#76688](https://github.com/pytorch/pytorch/pull/76688))\r\n* `torch.jit.save` and `torch.jit.load` are now supported for meta tensors ( aka `torch.Tensor(device=\"meta\")`) ([#73435](https://github.com/pytorch/pytorch/pull/73435))\r\n\r\n## Architecture Optimization\r\n\r\n* Added default symmetric qconfig for QNNPACK ([#74396](https://github.com/pytorch/pytorch/pull/74396))\r\n\r\n## Quantization\r\n\r\n* Core (Quantized Tensor, Operator, Modules)\r\n    * Added QAT fused `Linear-Bn1d` ([#72431](https://github.com/pytorch/pytorch/pull/72431), [#72796](https://github.com/pytorch/pytorch/pull/72796))\r\n    * Added 4 bit support for embedding quantized module (re-land PR 69769) ([#72276](https://github.com/pytorch/pytorch/pull/72276))\r\n    * Enabled slicing on per-channel quantized tensors (support is limited to the a contiguous sliced tensor) and corresponding test case ([#71269](https://github.com/pytorch/pytorch/pull/71269))\r\n    * Added `qint32` quantization support ([#72472](https://github.com/pytorch/pytorch/pull/72472))\r\n    * Added explicit entries for for functional and module conv and linear support into `get_default_qconfig_dict`&`get_default_qat_qconfig_dict` ([#73528](https://github.com/pytorch/pytorch/pull/73528))\r\n    * Added default symmetric QAT qconfig for QNNPACK ([#74507](https://github.com/pytorch/pytorch/pull/74507))\r\n    * Added Quantized `Matmul` Op (Naive Implementation) ([#71783](https://github.com/pytorch/pytorch/pull/71783))\r\n    * Added Quantized `Softmax` Op (Naive Implementation) ([#75415](https://github.com/pytorch/pytorch/pull/75415))\r\n    * Using QNNPACK in Quantized `Softmax` Op ([#75799](https://github.com/pytorch/pytorch/pull/75799))\r\n* Eager Mode Quantization\r\n    * Added 4 bit support for eager mode quantization flow ([#72277](https://github.com/pytorch/pytorch/pull/72277))\r\n* FX Graph Mode Quantization\r\n    * Added workflow support for `torch.matmul` quantization ([#72444](https://github.com/pytorch/pytorch/pull/72444))\r\n    * Added support `conv1d` and its fusion variants in QAT ([#74506](https://github.com/pytorch/pytorch/pull/74506))\r\n    * Decoupled `prepare_*fx `from training/eval modes ([#75401](https://github.com/pytorch/pytorch/pull/75401))\r\n    * Added quantized Softmax workflow integration ([#75106](https://github.com/pytorch/pytorch/pull/75106))\r\n    * Renamed `default_affine_fixed_qparams_observer` and `default_symmetric_fixed_qparams_observer` ([#76637](https://github.com/pytorch/pytorch/pull/76637))\r\n\r\n## ONNX\r\n\r\n* Updated default `opset_version` to 13. The previous default was 9. To get the old behavior, just specify `opset_version=9` when calling ``torch.onnx.export``. Going forward we plan to update the default regularly to \"latest as of 18 months ago\". ([#73898](https://github.com/pytorch/pytorch/issues/73898))\r\n* De-duplicated initializers to reduce ONNX model size for shared parameters ([#69547,](https://github.com/pytorch/pytorch/pull/69547) [#74247](https://github.com/pytorch/pytorch/pull/74247))\r\n* Changed to capture annotated attributes for local function ([#72883](https://github.com/pytorch/pytorch/pull/72883))\r\n* Improve error and warning messages ([#71342,](https://github.com/pytorch/pytorch/pull/71342) [#73255,](https://github.com/pytorch/pytorch/pull/73255) [#73770,](https://github.com/pytorch/pytorch/pull/73770) [#73265](https://github.com/pytorch/pytorch/pull/73265))\r\n* Added support to exporting `torch.minimum` with different dtype combinations ([#76022](https://github.com/pytorch/pytorch/issues/76022)) \r\n* Improved `Expand` shape inference ([#72985](https://github.com/pytorch/pytorch/pull/72985))\r\n* Added broadcast to `matmul` shape inference ([#72990](https://github.com/pytorch/pytorch/pull/72990))\r\n* Rewrote linspace symbolic to improve numerical stability ([#73610](https://github.com/pytorch/pytorch/pull/73610))\r\n* Enabled `topk` export with non-int64 k ([#73761](https://github.com/pytorch/pytorch/pull/73761))\r\n* Enabled `numel` tracing ([#74081](https://github.com/pytorch/pytorch/pull/74081))\r\n* Added constant folding for `onnx::ReduceProd` ([#74082](https://github.com/pytorch/pytorch/pull/74082))\r\n* Added support to equality checks on devices ([#77203](https://github.com/pytorch/pytorch/issues/77203))\r\n* Added support to dynamic dimensions in `Squeeze` and `Unsqueeze` ([#73104](https://github.com/pytorch/pytorch/pull/73104))\r\n\r\n## torch.package\r\n\r\n* Added Python Version to `Torch.Package` metadata ([#74610](https://github.com/pytorch/pytorch/pull/74610))\r\n* Added utility for determining where bad modules may come from ([#74998](https://github.com/pytorch/pytorch/pull/74998))\r\n\r\n## Distributed\r\n\r\n* torch.distributed\r\n    * Refactored `TORCH_DISTRIBUTED_DEBUG` implementation ([#73166](https://github.com/pytorch/pytorch/pull/73166))\r\n    * Set default value of TCPStore world_size to None in pybind definition ([#77277](https://github.com/pytorch/pytorch/pull/77277))\r\n    * Added orthogonalization with QR factorization ([#72043](https://github.com/pytorch/pytorch/pull/72043))\r\n    * Added pickling support for WorkerInfo ([#73371](https://github.com/pytorch/pytorch/pull/73371))\r\n    * Added support for RRefs that contain `threading.Thread` ([#74462](https://github.com/pytorch/pytorch/pull/74462))\r\n    * Added check for mismatch in number of parameters in `verify_params_across_processes` ([#74113](https://github.com/pytorch/pytorch/pull/74113))\r\n    * Added support for backend to register reducer timer ([#71700](https://github.com/pytorch/pytorch/pull/71700))\r\n    * Made ProcessGroupNCCL load torch_ucc.so when TORCH_UCC_LIBRARY_PATH is set ([#69552](https://github.com/pytorch/pytorch/pull/69552))\r\n    * Added support for non-contiguous inputs for `nn.functional.all_gather/reducescatter/gather` ([#75276](https://github.com/pytorch/pytorch/pull/75276))\r\n    * Added the use of batched operations for PowerSGD ([#76041](https://github.com/pytorch/pytorch/pull/76041))\r\n    * Changed to create UCC ProcessGroup when `ucc_lib` available ([#69564](https://github.com/pytorch/pytorch/pull/69564))\r\n    * Changed to generalize param verification and broadcast ([#76374](https://github.com/pytorch/pytorch/pull/76374))\r\n    * Changed to use a more reliable signaling mechanism to stop TCPStore background threads ([#76973](https://github.com/pytorch/pytorch/pull/76973))\r\n    * Added support to disabling post-local gradient sync ([#76723](https://github.com/pytorch/pytorch/pull/76723))\r\n    * Removed call into Python API without GIL being held in c10d ([#72928](https://github.com/pytorch/pytorch/pull/72928))\r\n* FullyShardedDataParallel\r\n    * Fixed `summon_full_params` when not sharded ([#72572](https://github.com/pytorch/pytorch/pull/72572))\r\n    * Fixed 0-dim tensor optim state device ([#75243](https://github.com/pytorch/pytorch/pull/75243))\r\n    * Fixed the synchronization of `all_gather` stream in `summon_full_params` ([#73314](https://github.com/pytorch/pytorch/pull/73314))\r\n    * Added state_dict() save/reload in parity test ([#73366](https://github.com/pytorch/pytorch/pull/73366))\r\n    * Changed to use `unflatten_parameter` in `_summon_full_parameters` ([#72467](https://github.com/pytorch/pytorch/pull/72467))\r\n    * Changed to use `summon_full_params` in `get_full_params` ([#73242](https://github.com/pytorch/pytorch/pull/73242))\r\n    * Added generic arguments for `state_dict` ([#73323](https://github.com/pytorch/pytorch/pull/73323))\r\n    * Added generic argument forward for `load_local_state_dict` ([#73325](https://github.com/pytorch/pytorch/pull/73325))\r\n    * Made `summon_full_params` a public method ([#73116](https://github.com/pytorch/pytorch/pull/73116))\r\n    * Generalized `fsdp_modules()` ([#73553](https://github.com/pytorch/pytorch/pull/73553))\r\n    * Introduced a utility API to allow users easily to set `state_dict_type` ([#73716](https://github.com/pytorch/pytorch/pull/73716))\r\n    * Added an option to summon on rank 0 only in  `summon_full_params` ([#73903](https://github.com/pytorch/pytorch/pull/73903))\r\n    * Enabled offload full params to CPU in `summon_full_params` ([#73904](https://github.com/pytorch/pytorch/pull/73904))\r\n    * Removed `_lazy_init()` in rebuild full params ([#74263](https://github.com/pytorch/pytorch/pull/74263))\r\n    * Changed to override `named_parameters()` for clean names in `summon_full_params()` ([#74333](https://github.com/pytorch/pytorch/pull/74333))\r\n    * Changed to strip FSDP info in `summon_full_params` context, similar to `named_params` in `named_buffers` ([#74517](https://github.com/pytorch/pytorch/pull/74517))\r\n    * Change to use param name as key in `full_optim_state_dict` ([#74879](https://github.com/pytorch/pytorch/pull/74879))\r\n    * Enabled re-key between param names/IDs for `full_optim_state_dict` ([#74912](https://github.com/pytorch/pytorch/pull/74912))\r\n    * Changed to register `state_dict` hooks for `FlatParamsWrapper` even if params_list is empty ([#74860](https://github.com/pytorch/pytorch/pull/74860))\r\n    * Made `apply_to_tensors` support `OrderedDict` type ([#75560](https://github.com/pytorch/pytorch/pull/75560))\r\n    * Added `rank0_only` to `full_optim_state_dict()` ([#75516](https://github.com/pytorch/pytorch/pull/75516))\r\n    * Made `summon_full_params` a static method ([#75423](https://github.com/pytorch/pytorch/pull/75423))\r\n    * Added support for PackedSequence type for `apply_for_tensors` ([#76265](https://github.com/pytorch/pytorch/pull/76265))\r\n    * Made mixed precision API configurable ([#76423](https://github.com/pytorch/pytorch/pull/76423))\r\n    * Validated exec order using `compute_device` ([#76664](https://github.com/pytorch/pytorch/pull/76664))\r\n    * Improved dict inversion in `_get_param_name_to_param` to be faster([#76665](https://github.com/pytorch/pytorch/pull/76665))\r\n    * Changed to ignore params if not in `Optim` state dict ([#76671](https://github.com/pytorch/pytorch/pull/76671))\r\n    * Changed to include buffers in `ignored_modules` ([#76784](https://github.com/pytorch/pytorch/pull/76784))\r\n    * Moved param/buffer name computation to constructor for `ignored_modules` ([#76994](https://github.com/pytorch/pytorch/pull/76994))\r\n    * Changed to not clone buffers and ensure that we offload buffers to CPU if specified ([#77000](https://github.com/pytorch/pytorch/pull/77000))\r\n    * Profiling range for `FSDP.forward` ([#76899)](https://github.com/pytorch/pytorch/pull/76899)\r\n    * Disabled the default behavior of moving CPU module to GPU ([#77720](https://github.com/pytorch/pytorch/pull/77720))\r\n    * Fixed `_get_param_to_unflat_param_names()` for shared params ([#75430](https://github.com/pytorch/pytorch/pull/75430))\r\n* ShardedTensor (prototype)\r\n    * Changed to use absolute imports for ShardMetadata instead ([#73678](https://github.com/pytorch/pytorch/pull/73678))\r\n    * Fixed the metadata error in `init_from_local_shards` with deepcopy ([#73400](https://github.com/pytorch/pytorch/pull/73400))\r\n    * Fixed view op and matrix ops unit test ([#77706](https://github.com/pytorch/pytorch/pull/77706))\r\n* torch.distributed.rpc\r\n    * Improved logging from 'unknown destination worker' ([#75811](https://github.com/pytorch/pytorch/pull/75811))\r\n    * Improved logging for store.wait error ([#76548](https://github.com/pytorch/pytorch/pull/76548))\r\n    * Added support for RPC Meta device ([#76882](https://github.com/pytorch/pytorch/pull/76882))\r\n    * Changed to keep stacktrace when rewriting AttributeError ([#73720](https://github.com/pytorch/pytorch/pull/73720))\r\n* DistributedDataParallel\r\n    * Improved debug level and logging ([#72455](https://github.com/pytorch/pytorch/pull/72455))\r\n    * Removed bucket replicas ([#73567](https://github.com/pytorch/pytorch/pull/73567))\r\n    * Made `HierarchicalModelAverager` a subclass of `averagers.ModelAverager` ([#74564](https://github.com/pytorch/pytorch/pull/74564))\r\n    * Made code simplification for `_find_process_group` function ([#75007](https://github.com/pytorch/pytorch/pull/75007))\r\n    * Made distributed raise `ImportError` when not available ([#75975](https://github.com/pytorch/pytorch/pull/75975))\r\n* torch.distributed.elastic\r\n    * Created a final agent barrier to shutdown process properly ([#74931](https://github.com/pytorch/pytorch/pull/74931))\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n* Fixed type promotion for `torch.where` ([#76691](https://github.com/pytorch/pytorch/pull/76691))\r\n* Fixed `torch.clamp` to correctly propagate nans ([#77306](https://github.com/pytorch/pytorch/pull/77306))\r\n* Fixed `torch.unique` to preserve input size when dim is zero-length ([#75764](https://github.com/pytorch/pytorch/pull/75764))\r\n* Fixed  `torch.ravel` to also return contiguous outputs for non-contiguous inputs([#71771](https://github.com/pytorch/pytorch/pull/71771))\r\n* Fixed `CosineAnnealingLR` to resume last learning rate on restart ([#60339](https://github.com/pytorch/pytorch/pull/60339))\r\n* Fixed invalid shape error for `torch.fft.{irfft2,irfft2} `([#73012](https://github.com/pytorch/pytorch/pull/73012))\r\n* Fixed `torch.set_default_dtype` to no longer crash with invalid dtype ([#72405](https://github.com/pytorch/pytorch/pull/72405))\r\n* Fixed `torch.tril` edge case ([#75335](https://github.com/pytorch/pytorch/pull/75335))\r\n* Fixed `torch.broadcast_shapes` to not handle shapes with negative dimensions. ([#72999](https://github.com/pytorch/pytorch/pull/72999))\r\n* Fixed `torch.logsumexp` integral to float type promotion ([#77480](https://github.com/pytorch/pytorch/pull/77480))\r\n* Fixed `torch.amax` and `torch.amin` for empty tensors if dim arg not provided. ([#73914](https://github.com/pytorch/pytorch/pull/73914))\r\n* Disallowed calling `.tolist` on tensors with nullptr storage ([#75990](https://github.com/pytorch/pytorch/pull/75990))\r\n* Fixed `.tolist` to work correctly work for 0 element tensors ([#76335](https://github.com/pytorch/pytorch/pull/76335))\r\n* Adjusted the stubs for PyCharm autocompletion of the `Tensor` methods. ([#76712](https://github.com/pytorch/pytorch/pull/76712))\r\n* Fixed `Optimizer.zero_grad` type annotation ([#76998](https://github.com/pytorch/pytorch/pull/76998))\r\n* Fixed  `torch.distributions.lkj_cholesky` device error ([#73980](https://github.com/pytorch/pytorch/pull/73980))\r\n* Fixed misplaced type annotation for `torch.distributions.transforms.CatTransform` ([#73747](https://github.com/pytorch/pytorch/pull/73747))\r\n* Fixed `torch.clamp` scalar overloads to propagate nan ([#77371](https://github.com/pytorch/pytorch/pull/77371))\r\n* Fixed advanced indexing assignment when  `use_deterministic_algorithms(True)` for non-contiguous tensors ([#76220](https://github.com/pytorch/pytorch/pull/76220))\r\n* Fixed `**=` operator ([#76900](https://github.com/pytorch/pytorch/pull/76900))\r\n* Fixed `to` to properly support permutation ([#77610](https://github.com/pytorch/pytorch/pull/77610))\r\n\r\n## C++ API\r\n\r\n* Used the same checks in all `grid_sampler` functions ([#75164](https://github.com/pytorch/pytorch/pull/75164))\r\n* Fixed `mean` bug for integral tensors ([#76584](https://github.com/pytorch/pytorch/pull/76584))\r\n* Added missing import to fix crash on loading cpp extension ([#75736](https://github.com/pytorch/pytorch/pull/75736))\r\n\r\n## Autograd\r\n\r\n* Fixed forward AD formula for `torch.angle` ([#77267](https://github.com/pytorch/pytorch/pull/77267))\r\n* Fixed `torch.{minimum, maximum}` forward AD formula for float32 ([#75277](https://github.com/pytorch/pytorch/pull/75277))\r\n* Fixed forward-mode AD formula for `torch.nn.functional.binary_cross_entropy_with_logits` ([#76322](https://github.com/pytorch/pytorch/pull/76322))\r\n* Fixed gradients for norm related ops at zero when p < 1 to mask out nans ([#75103](https://github.com/pytorch/pytorch/pull/75103))\r\n* Fixed forward-over-reverse for convolution to no longer fail in some cases ([#75298](https://github.com/pytorch/pytorch/pull/75298))\r\n* Fixed `torch.autograd.gradcheck` to run with requires_grad=False when `check_forward_ad=True` ([#72309](https://github.com/pytorch/pytorch/pull/72309))\r\n* Fixed `requires_grad`-ness to be propagated for all backends when tensors are deep-copied ([#76256](https://github.com/pytorch/pytorch/pull/76256))\r\n* Fixed `torch.autograd.grad` to automatically needs an extra tuple when handling single outputs and `is_grads_batched=True` ([#75779](https://github.com/pytorch/pytorch/pull/75779))\r\n* Updated forward AD metadata check to skip stride check when size is 0 ([#77269](https://github.com/pytorch/pytorch/pull/77269))\r\n* Fixed deadlock an edge case in autograd ([#73961](https://github.com/pytorch/pytorch/pull/73961))\r\n* Allow forking until a worker thread is created in autograd engine ([#72689](https://github.com/pytorch/pytorch/pull/72689))\r\n* Removed some spurious warnings in the autograd engine  ([#72542](https://github.com/pytorch/pytorch/pull/72542))\r\n* Fixed issue with `torch.utils.checkpoint.checkpoint` when both `use_reentrant` and` preserve_rng_state` set to `False` ([#76890](https://github.com/pytorch/pytorch/pull/76890))\r\n* Fixed Python indexing set item to scalar tensor preserve autograd graph ([#78746](https://github.com/pytorch/pytorch/pull/78746))\r\n\r\n## Build\r\n\r\n* Added TORCH_CUDA_CU_API to CUDABlas functions ([#72340](https://github.com/pytorch/pytorch/pull/72340))\r\n* Fixed doc build for release branches ([#72567](https://github.com/pytorch/pytorch/pull/72567))\r\n* Moved AndroidNightly to GHA ([#74243](https://github.com/pytorch/pytorch/pull/74243))\r\n* Changed `numModules` type to `unsigned` ([#74978](https://github.com/pytorch/pytorch/pull/74978))\r\n* In Kineto, Changed to not search for CUPTI in default paths ([#76188](https://github.com/pytorch/pytorch/pull/76188))\r\n* Changed to use TensorPipe libuv in Gloo ([#77312](https://github.com/pytorch/pytorch/pull/77312))\r\n\r\n## Complex Numbers\r\n\r\n* Fixed segmentation fault when real and imaginary attributes of a tensor are set to a number ([#73867](https://github.com/pytorch/pytorch/pull/73867))\r\n* Fixed complex to real casting warning in the backward\u2019s pass for Real\u2192Complex `copy` ([#75805](https://github.com/pytorch/pytorch/pull/75805))\r\n* Make `torch.addcmul` and `torch.addcdiv` support different complex and non-complex type args together ([#74234](https://github.com/pytorch/pytorch/pull/74234))\r\n* Fixed `torch.isfinite` for complex to avoid overflow when real and imaginary values are finite but abs is infinite ([#76606](https://github.com/pytorch/pytorch/pull/76606)).\r\n* Fixed complex abs/angle output format ([#77585](https://github.com/pytorch/pytorch/pull/77585))\r\n\r\n## Dataloader\r\n\r\n* Reset worker cycle for persistent DataLoader to ensure determinism across epochs ([#73675](https://github.com/pytorch/pytorch/pull/73675))\r\n\r\n## LinAlg\r\n\r\n* Fixed SVD error code handling for OpenBLAS 0.3.15+ and MKL 2022+([#72357](https://github.com/pytorch/pytorch/pull/72357))\r\n* Fixed addmm_cpu for int64 ([#75200](https://github.com/pytorch/pytorch/pull/75200))\r\n\r\n## Meta API\r\n\r\n* Fixed meta kernel for `normal_` when `std` is equal to 0 ([#70085](https://github.com/pytorch/pytorch/pull/70085))\r\n* Fixed `torch.kaiser_window` : meta for window_length > 1 ([#73733](https://github.com/pytorch/pytorch/pull/73733))\r\n* Fixed meta kernel for `normal` ([#77740](https://github.com/pytorch/pytorch/pull/77740))\r\n\r\n## torch.nn\r\n\r\n* `F.pad`: Silence error when unused fill value is zero ([#76307](https://github.com/pytorch/pytorch/pull/76307))\r\n* `nn.{Conv1d, Conv2d, Conv3d}`: Properly initialize `grad_weight` in `raw_cudnn_convolution_backward_weight_out` ([#72157](https://github.com/pytorch/pytorch/pull/72157))\r\n* `nn.Conv2d`: Fix channels last propagation for naive algorithm ([#77347](https://github.com/pytorch/pytorch/pull/77347))\r\n* `nn.ConvTranspose*d`: Fix to support no-batch-dim inputs with `output_size` ([#76151](https://github.com/pytorch/pytorch/pull/76151))\r\n* `nn.CrossEntropyLoss`: Support no-batch-dim input with probability target ([#77653](https://github.com/pytorch/pytorch/pull/77653))\r\n* `nn.CrossEntropyLoss`: Fix to avoid floating point exception for zero-size inputs ([#73837](https://github.com/pytorch/pytorch/pull/73837))\r\n* `nn.GroupNorm`: Ensure `num_groups > 0` in `native_group_norm` ([#75270](https://github.com/pytorch/pytorch/pull/75270))\r\n* `nn.MaxPool2d`: Properly support dilation in channels last kernel ([#76597](https://github.com/pytorch/pytorch/pull/76597))\r\n* `nn.ParameterList`: Fix `__dir__` implementation ([#74997](https://github.com/pytorch/pytorch/pull/74997))\r\n* `nn.{ParameterList, ParameterDict}`: Support containing any kind of object ([#70499](https://github.com/pytorch/pytorch/pull/70499))\r\n* `nn.RReLU`: Fix to support empty tensor inputs ([#70496](https://github.com/pytorch/pytorch/pull/70496))\r\n* `nn.utils.rnn.pad_sequence`: Fix regression; support tensor input for `sequences` ([#72436](https://github.com/pytorch/pytorch/pull/72436))\r\n* `nn.utils.stateless.functional_call`: Properly support setting attributes during forward ([#77137](https://github.com/pytorch/pytorch/pull/77137))\r\n\r\n## torch.fx\r\n\r\n* Core\r\n    * Made `map_aggregate`/`map_arg` work for NamedTuple ([#73198](https://github.com/pytorch/pytorch/pull/73198))\r\n    * Fixed tracing for OpOverload ([#73940](https://github.com/pytorch/pytorch/pull/73940))\r\n    * Fixed codegen for bare generic type annotations ([#74135](https://github.com/pytorch/pytorch/pull/74135))\r\n    * Modified `__deepcopy__` to also copy _codegen ([#75851](https://github.com/pytorch/pytorch/pull/75851))\r\n    * Fixed unnecessary recursion in `GraphModule.__call__` ([#76068](https://github.com/pytorch/pytorch/pull/76068))\r\n    * Changed to prevent infinite recursion in GraphModule ([#73866](https://github.com/pytorch/pytorch/pull/73866))\r\n    * Changed to preserve codegen on FX graph in transformer ([#74189](https://github.com/pytorch/pytorch/pull/74189))\r\n* operator_schemas\r\n    * Added back check for OpOverload ([#73978](https://github.com/pytorch/pytorch/pull/73978))\r\n    * Fixed normalize_function to consider OpOverloads ([#76469](https://github.com/pytorch/pytorch/pull/76469))\r\n    * Fixed for normalizing signature for op overloads ([#77182](https://github.com/pytorch/pytorch/pull/77182))\r\n* For testing, added `super()` calls for FX TestCases ([#74216](https://github.com/pytorch/pytorch/pull/74216))\r\n* For split_module, made split_module preserve proper placeholder names ([#74736](https://github.com/pytorch/pytorch/pull/74736))\r\n\r\n## Sparse\r\n\r\n* Fixed ignored beta value for sparse inputs to `torch.addmm` with non-MKL build ([#72430](https://github.com/pytorch/pytorch/pull/72430))\r\n* Fixed float16/bf16 support for sparse inputs to `torch.addmm` ([#72559](https://github.com/pytorch/pytorch/pull/72559))\r\n* Fixed CUDA error for `torch.mul` when given COO Tensors with zero sized dense dimensions ([#73428](https://github.com/pytorch/pytorch/pull/73428))\r\n* Fixed incorrect results of `torch.sparse.sampled_addmm` for noncontiguous inputs ([#76590](https://github.com/pytorch/pytorch/pull/76590))\r\n* Fixed runtime generation of doc strings for torch._masked functions by making them static instead ([#72865](https://github.com/pytorch/pytorch/pull/72865))\r\n\r\n## CUDA\r\n\r\n* Created jiterator cache dirs recursively ([#74592](https://github.com/pytorch/pytorch/pull/74592))\r\n* Fixed bincount to use acc scalar for the bounds ([#76979](https://github.com/pytorch/pytorch/pull/76979))\r\n* Avoid `collections` deprecation warning ([#72239](https://github.com/pytorch/pytorch/pull/72239))\r\n* Disabled cuBLASLt when batch is too large. ([#73533](https://github.com/pytorch/pytorch/pull/73533))\r\n* Abated spurious resize warnings in `MultiMarginLoss` on CUDA ([#75000](https://github.com/pytorch/pytorch/pull/75000))\r\n* Added missing AT_CUDA_CHECK in CUDAGraph.cpp ([#74392](https://github.com/pytorch/pytorch/pull/74392))\r\n* CUDA graphs\r\n    * Fixed OOM inside graph capture_begin ([#76247](https://github.com/pytorch/pytorch/pull/76247))\r\n    * Changed to allow Adam and AdamW to be capture-safe ([#77862](https://github.com/pytorch/pytorch/pull/77862))\r\n\r\n## Intel\r\n\r\n* Fixed Caffe2 convolution issue in AVX512 when using oneDNN v2.5.2 ([_#73290_](https://github.com/pytorch/pytorch/pull/73290))\r\n\r\n## Composability \r\n\r\n* Fixed formatting of scalar tensors for the `meta` device (don't call item) ([#74376](https://github.com/pytorch/pytorch/pull/74376))\r\n* Fixed to metadata preservation for Python tensor subclasses: preserve Python dispatch keys when copying tensor metadata ([#75644](https://github.com/pytorch/pytorch/pull/75644))\r\n* Fixed data race on `TensorImpl::wns_pyobj_` accesses with non-GIL protected threads ([#75563](https://github.com/pytorch/pytorch/pull/75563))\r\n* Fixed for Python garbage collector can sometimes deallocate a tensor, even when C++ still has strong references to it ([#75933](https://github.com/pytorch/pytorch/pull/75933))\r\n* Added better error checking to `TensorImpl::size_between_dim_`. ([#76719](https://github.com/pytorch/pytorch/pull/76719))\r\n* Changed to ensure that `torch.memory_format` instances are singletons ([#77543](https://github.com/pytorch/pytorch/pull/77543))\r\n\r\n## Profiler\r\n\r\n* Avoided picking up old CUPTI headers ([#72761](https://github.com/pytorch/pytorch/pull/72761))\r\n* Kineto submodule update and fixes ([#75206](https://github.com/pytorch/pytorch/pull/75206))\r\n* Fixed segfault in AppendOnlyList ([#78084](https://github.com/pytorch/pytorch/pull/78084))\r\n\r\n## Vulkan\r\n\r\n* Fixed a bug in the Vulkan implementation of `aten::tanh` where inputs of large magnitudes would result in numerically unstable results ([#73107](https://github.com/pytorch/pytorch/pull/73107))\r\n* Fixed a bug in the Vulkan implementation of `aten::add`, `aten::sub`, `aten::mul`, and `aten::div` where passing in a single element tensor as a second argument would result in an assertion error ([#73108](https://github.com/pytorch/pytorch/pull/73108))\r\n\r\n## Mobile\r\n\r\n* Changed to protect against threading errors when tracing models with parallel operators ([#73327](https://github.com/pytorch/pytorch/pull/73327))\r\n* Changed to ensure error messages are preserved from Metal and CoreML Backend ([#77430](https://github.com/pytorch/pytorch/pull/77430), [#76236](https://github.com/pytorch/pytorch/pull/76263))\r\n* Changed to ensure the iOS test app is working correctly ([#74090](https://github.com/pytorch/pytorch/pull/74090))\r\n* Fixed off-by-one error in tupleIndex ([#72447](https://github.com/pytorch/pytorch/pull/72447))\r\n* Fixed error in export of models containing nested NamedTuple ([#75996](https://github.com/pytorch/pytorch/pull/75996))\r\n\r\n## Distributed\r\n\r\n* torch.distributed\r\n    * Fixed process group wrapper check for Gloo ([#72657](https://github.com/pytorch/pytorch/pull/72657) (https://github.com/pytorch/pytorch/pull/72657))\r\n    * Changes to catch CUDA library runtime error (driver shutting down) during the exit of ProcessGroup ([#74258](https://github.com/pytorch/pytorch/pull/74258) (https://github.com/pytorch/pytorch/pull/74258))\r\n    * Fixed NCCL version string ([#73333](https://github.com/pytorch/pytorch/pull/73333) (https://github.com/pytorch/pytorch/pull/73333))\r\n    * Add retry DNS lookup failures ([#74641](https://github.com/pytorch/pytorch/pull/74641) (https://github.com/pytorch/pytorch/pull/74641))\r\n    * Validated that tensors are contiguous in ProcessGroupNCCL ([#77809](https://github.com/pytorch/pytorch/pull/77809) (https://github.com/pytorch/pytorch/pull/77809))\r\n    * Fixed sign-compare in c10d/Utils.hpp ([#75081](https://github.com/pytorch/pytorch/pull/75081) (https://github.com/pytorch/pytorch/pull/75081))\r\n    * Fixed NCCL gather outputs on non-root ranks ([#75535](https://github.com/pytorch/pytorch/pull/75535) (https://github.com/pytorch/pytorch/pull/75535))\r\n    * Fixed batch_isend_irecv ([#74701](https://github.com/pytorch/pytorch/pull/74701) (https://github.com/pytorch/pytorch/pull/74701))\r\n    * Disabled RPC profiling for kineto profilers ([#76234](https://github.com/pytorch/pytorch/pull/76234) (https://github.com/pytorch/pytorch/pull/76234))\r\n    * Typo fix in generated module name ([#76880](https://github.com/pytorch/pytorch/pull/76880) (https://github.com/pytorch/pytorch/pull/76880))\r\n    * Fixed broadcast for channels-last tensors ([#79071](https://github.com/pytorch/pytorch/pull/79071) (https://github.com/pytorch/pytorch/pull/79071))\r\n* DistributedDataParallel\r\n    * Disabled bucketing for the first iteration ([#72843](https://github.com/pytorch/pytorch/pull/72843) (https://github.com/pytorch/pytorch/pull/72843))\r\n    * Fixed SyncBatchNorm for empty inputs ([#74944](https://github.com/pytorch/pytorch/pull/74944) (https://github.com/pytorch/pytorch/pull/74944))\r\n    * Added a guard for non CPU/CUDA devices ([#75247](https://github.com/pytorch/pytorch/pull/75247) (https://github.com/pytorch/pytorch/pull/75247))\r\n    * Fixed bug where *getstate* of DDP looks for self._replicated_tensor_module when not using ReplicatedTensor. ([#76349](https://github.com/pytorch/pytorch/pull/76349) (https://github.com/pytorch/pytorch/pull/76349))\r\n    * Fixed post_localSGD_optimizer by calling optim.step only once when there are multiple param groups or params ([#74737](https://github.com/pytorch/pytorch/pull/74737) (https://github.com/pytorch/pytorch/pull/74737))\r\n    * Fixed PostLocalSGDOptimizer and ModelAverager average ([#74894](https://github.com/pytorch/pytorch/pull/74894) (https://github.com/pytorch/pytorch/pull/74894))\r\n* ShardedTensor (prototype)\r\n    * Fixed Sharding spec inference to avoid invalid chunk sharding to be inferred as chunkshardingspec ([#75296](https://github.com/pytorch/pytorch/pull/75296) (https://github.com/pytorch/pytorch/pull/75296))\r\n* FullyShardedDataParallel\r\n    * Fixed no_sync() + FULL_SHARD root all-gather behavior ([#75901](https://github.com/pytorch/pytorch/pull/75901) (https://github.com/pytorch/pytorch/pull/75901))\r\n    * Fixed exec order validation (static variable issue) ([#76273](https://github.com/pytorch/pytorch/pull/76273) (https://github.com/pytorch/pytorch/pull/76273))\r\n    * Fixed local_state_dict and state_dict_type bugs ([#77101](https://github.com/pytorch/pytorch/pull/77101) (https://github.com/pytorch/pytorch/pull/77101))\r\n    * Fixed FSDP wrapping for batchnorm when mixed precision enabled ([#77234](https://github.com/pytorch/pytorch/pull/77234) (https://github.com/pytorch/pytorch/pull/77234))\r\n    * Fixed CheckpointWrapper state_dict to enable wrapped modules loaded into non-checkpointed wrapped module ([#77224](https://github.com/pytorch/pytorch/pull/77224) (https://github.com/pytorch/pytorch/pull/77224))\r\n    * Changed to relax exec order valid. to only forward pass ([#76556](https://github.com/pytorch/pytorch/pull/76556) (https://github.com/pytorch/pytorch/pull/76556))\r\n    * Changed to not check forward order in eval mode ([#77195](https://github.com/pytorch/pytorch/pull/77195) (https://github.com/pytorch/pytorch/pull/77195))\r\n    * Changed to pass device_id into recursive_wrap for FSDP ([#77491](https://github.com/pytorch/pytorch/pull/77491) (https://github.com/pytorch/pytorch/pull/77491))\r\n\r\n## JIT/TorchScript\r\n\r\n* torch.jit.fuser(\"fuser1\") is supposed to enable NNC fusion, but it currently only enables gpu fusion. This will enable CPU fusion as well. ([#74078](https://github.com/pytorch/pytorch/pull/74078))\r\n* Fixed bug where when parsing a Python TernaryIf expression (`x if y else z`)  was not being parsed into TorchScript using `torch.jit.script` as right associative ([#68416](https://github.com/pytorch/pytorch/pull/68416))\r\n* Got rid of TorchScript sparse tensor is experimental warning. ([#73874](https://github.com/pytorch/pytorch/pull/73874))\r\n* Custom post-processing passes registered through `torch::jit::RegisterPass` now have access to profiled Tensor Type Specializations ([#71748](https://github.com/pytorch/pytorch/pull/71748))\r\n* When registering a custom print handler for `prim::print()` inside `torch.deploy`, we restore the default print handler when all Python environments are destroyed to prevent errors from not having a Python environment. ([#74513](https://github.com/pytorch/pytorch/pull/74513))\r\n* When running `torch.jit.freeze` on the backward passes of conv (`conv_bn`) with reduced precision (eg `bfloat16`) , fusions will respect the precision of the original op, instead of promoting to `float32`  ([#77042](https://github.com/pytorch/pytorch/pull/77042))\r\n* Loosened `torch.jit.script` type checks that were too strict for the `torch.nn.LPPool2D` and `torch.nn.functional.lp_pool2d` functions ([#73287](https://github.com/pytorch/pytorch/pull/73287))\r\n* `torch.nn.ParameterList` is now subscriptable in TorchScript  ([#75479](https://github.com/pytorch/pytorch/pull/75479))\r\n\r\n## Quantization\r\n\r\n* Fixed `get_module_type` for fusion ([#72735](https://github.com/pytorch/pytorch/pull/72735))\r\n* Fixed bug in QuantWrapper with DeQuant qconfig ([#73671](https://github.com/pytorch/pytorch/pull/73671))\r\n* Fixed observer insertion through dtype propagation ([#73274](https://github.com/pytorch/pytorch/pull/73274))\r\n* Only do reference module swapping for floating point fused modules ([#74231](https://github.com/pytorch/pytorch/pull/74231))\r\n* Fixed dynamic weighted op lowering when input is used multiple times ([#74364](https://github.com/pytorch/pytorch/pull/74364))\r\n* Fixed `get_default_qconfig_dict` for fused modules ([#75838](https://github.com/pytorch/pytorch/pull/75838))\r\n* Fixed bug for ave pooling in FX quant ([#73054](https://github.com/pytorch/pytorch/pull/73054))\r\n* Fixed FX QAT for untraceable modules ([#74277](https://github.com/pytorch/pytorch/pull/74277))\r\n* Fixed `qmin`/`qmax` when using customized \u2018qrange\u2019 ([#74717](https://github.com/pytorch/pytorch/pull/74717))\r\n\r\n## ONNX\r\n\r\n* Fixed repeat interleave when repeats and dim is 1 ([#73760](https://github.com/pytorch/pytorch/pull/73760))\r\n* Fixed ONNX gather shape inference ([#73607](https://github.com/pytorch/pytorch/pull/73607))\r\n* Fixed 1d case flatten export ([#74595](https://github.com/pytorch/pytorch/pull/74595))\r\n* Fixed opset_version checked before set ([#76928](https://github.com/pytorch/pytorch/pull/76928))\r\n* Fixed an assertion failure involving Slice ([#72989](https://github.com/pytorch/pytorch/pull/72989))\r\n* Fixed LSTM reshape shape inference regression ([#72532](https://github.com/pytorch/pytorch/pull/72532))\r\n* Fixed Caffe2 ONNX export for environment with newer ONNX ([#75718)](https://github.com/pytorch/pytorch/pull/75718/)\r\n* Refactored test/onnx/test_onnx_export.py for better code reuse ([#76851](https://github.com/pytorch/pytorch/pull/76851))\r\n* Fixed `aten::to(\"cpu\")` and `aten::to(device=\"cpu\")` ([#76498](https://github.com/pytorch/pytorch/pull/76498))\r\n* Fixed BatchNormalization for invalid dtype ([#74875](https://github.com/pytorch/pytorch/pull/74875))\r\n* Added Autocast support for `einsum` ([#71916](https://github.com/pytorch/pytorch/pull/71916))\r\n\r\n## torch.package\r\n\r\n* Deploy: added dummy metadata for builtin packages ([#76211](https://github.com/pytorch/pytorch/pull/76211))\r\n* Enabled module modification during repackaging ([#71520](https://github.com/pytorch/pytorch/pull/71520))\r\n* Added test case for repackaging parent module ([#72367](https://github.com/pytorch/pytorch/pull/72367))\r\n* Fixed orderedimporter dummy package check ([#72533](https://github.com/pytorch/pytorch/pull/72533))\r\n* Improved error message for module detection on saving pass ([#73106](https://github.com/pytorch/pytorch/pull/73106))\r\n* Changed to allow torch/csrc/deploy/interpreter/Optional.hpp to be allowed into the wheel distribution ([#74643](https://github.com/pytorch/pytorch/pull/74643))\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n* Improved `torch.topk` performance on CUDA ([#74267](https://github.com/pytorch/pytorch/pull/74267))\r\n* Added SIMD horizontal reduce to improve `torch.log_softmax` and `torch.softmax` performance on CPU ([#73953](https://github.com/pytorch/pytorch/pull/73953))\r\n* Made small optimizations for `torch.view` ([#72626](https://github.com/pytorch/pytorch/pull/72626))\r\n* Optimized dim reduce performance on `torch.{norm,` `argmax, argmin}` ([#72083](https://github.com/pytorch/pytorch/pull/72083))\r\n* Improved CPU performance for `torch.log_softmax` when dim != -1 on both float32 and bfloat16 ([#72163](https://github.com/pytorch/pytorch/pull/72163))\r\n* Improved `torch.softmax` `dim=-1` performance on bfloat16 by adding more fusion ([#76278](https://github.com/pytorch/pytorch/pull/76278))\r\n* Removed duplicate call to objective function in strong Wolfe line search in `L-BFGS` optimizer. ([#72773](https://github.com/pytorch/pytorch/pull/72773))\r\n\r\n## Autograd\r\n\r\n* Optimized code-generated in-place forward AD formulas ([#74017](https://github.com/pytorch/pytorch/pull/74017))\r\n* Added a fast path for `torch.{stack, cat}` forward AD computation when tangents are zero-filled ([#75590](https://github.com/pytorch/pytorch/pull/75590))\r\n* Reduced forward AD recomputation for `linalg.{eig,eigh,svd}` when function returns multiple outputs ([#75583](https://github.com/pytorch/pytorch/pull/75583))\r\n\r\n## Sparse\r\n\r\n* Improved performance of `index_select` for COO inputs on CPU ([#72710](https://github.com/pytorch/pytorch/pull/72710))\r\n* Improved performance of `index_add` on CUDA ([#76996](https://github.com/pytorch/pytorch/pull/76996))\r\n\r\n## Dataloader\r\n\r\n* Improved the performance of `BatchSampler` ([#76951](https://github.com/pytorch/pytorch/pull/76951))\r\n\r\n## AMD\r\n\r\n* Enabled foreach fast path ([#74417](https://github.com/pytorch/pytorch/pull/74417))\r\n* Reverted cat operator performance work-around ([#74129](https://github.com/pytorch/pytorch/pull/74129))\r\n\r\n## CUDA\r\n\r\n* Removed sync in embedding ([#70943](https://github.com/pytorch/pytorch/pull/70943))\r\n* Added fused addmm path in linear for contiguous 3D input ([#72728](https://github.com/pytorch/pytorch/pull/72728))\r\n* Changed to use cub 1.15's latest scan-by-key algorithm to replace thrust for `Embedding.cu` and `EmbeddingBag.cu` ([#66580](https://github.com/pytorch/pytorch/pull/66580))\r\n* Changed to use `cub::DeviceSelect::UniqueByKey` for EmbeddingBackward ([#68376](https://github.com/pytorch/pytorch/pull/68376))\r\n* Changed to use cuBLASLt interface for bias fusion ([#72148](https://github.com/pytorch/pytorch/pull/72148))\r\n* Set workspace size for cuBLASLt interface 1M ([#73439](https://github.com/pytorch/pytorch/pull/73439))\r\n* Added fastAtomicAdd to scatter_add [v2] ([#75545](https://github.com/pytorch/pytorch/pull/75545))\r\n* Added a new optimized cuDNN RNN algorithm for small RNN hidden_size ([#73211](https://github.com/pytorch/pytorch/pull/73211))\r\n* Avoided CPU Sync in SyncBatchNorm When Capturing CUDA Graphs ([#78810](https://github.com/pytorch/pytorch/pull/78810)) ([_commit_](https://github.com/pytorch/pytorch/commit/2652da29ab6c0d690bfb543bee958f50c0b86451))\r\n* Added Autocast CPU doc ([#68567](https://github.com/pytorch/pytorch/pull/68567))\r\n* Documented CUDA 11.5 windows issue ([#73013](https://github.com/pytorch/pytorch/pull/73013))\r\n* Added `__all__` for `torch.cuda.memory` ([#76490](https://github.com/pytorch/pytorch/pull/76490))\r\n\r\n## Composability \r\n\r\n* Improved performance for forward-mode AD with `at::sub`: added ZeroTensor fast-path ([#75587](https://github.com/pytorch/pytorch/pull/75587))\r\n\r\n## torch.nn\r\n\r\n* `nn.EmbeddingBag`: Removed out-of-bounds check to improve CUDA performance ([#74767](https://github.com/pytorch/pytorch/pull/74767))\r\n* `nn.GELU`: Added support tanh-based approximation ([#61439](https://github.com/pytorch/pytorch/pull/61439))\r\n* `nn.GroupNorm`: Improved channels last performance on CPU ([#69067](https://github.com/pytorch/pytorch/pull/69067))\r\n* `nn.LayerNorm`: Improved bfloat16 performance on CPU ([#71376](https://github.com/pytorch/pytorch/pull/71376))\r\n* `nn.LayerNorm`: Added mixed data type mode for forward path ([#73844](https://github.com/pytorch/pytorch/pull/73844))\r\n* `nn.MultiheadAttention`: Fast path using nested tensors for inference under specific conditions ([#77924](https://github.com/pytorch/pytorch/pull/77924), [#77761](https://github.com/pytorch/pytorch/pull/77761))\r\n* `nn.MultiheadAttention`: Fuse the `attn_mask` addition ([#73219](https://github.com/pytorch/pytorch/pull/73219), [#72871](https://github.com/pytorch/pytorch/pull/72871)))\r\n* `nn.MultiheadAttention`: Native fast path under specific conditions ([#75809](https://github.com/pytorch/pytorch/pull/75809), [#76333](https://github.com/pytorch/pytorch/pull/76333), [#72944](https://github.com/pytorch/pytorch/pull/72944), [#72941](https://github.com/pytorch/pytorch/pull/72941), [#72671](https://github.com/pytorch/pytorch/pull/72671), [#72375](https://github.com/pytorch/pytorch/pull/72375), [#72458](https://github.com/pytorch/pytorch/pull/72458), [#72464](https://github.com/pytorch/pytorch/pull/72464), [#72463](https://github.com/pytorch/pytorch/pull/72463))\r\n* `nn.MultiheadAttention`: Preserve identity relationships among query, key, and value for `batch_first=True` ([#73053](https://github.com/pytorch/pytorch/pull/73053))\r\n* `nn.utils.weight_norm`: Added native CPU kernel ([#73845](https://github.com/pytorch/pytorch/pull/73845))\r\n* `F.grid_sample`: Improved backward pass scaling with input size for 3d implementation ([#71759](https://github.com/pytorch/pytorch/pull/71759))\r\n\r\n## Benchmark\r\n\r\n* Added binary to benchmark model load speed ([#74700](https://github.com/pytorch/pytorch/pull/74700))\r\n\r\n## Profiler\r\n\r\n* Optimized Profiler overhead and improve scalability ([#71538](https://github.com/pytorch/pytorch/pull/71538), [#73409](https://github.com/pytorch/pytorch/pull/73409), [#73855](https://github.com/pytorch/pytorch/pull/73855), [#74151](https://github.com/pytorch/pytorch/pull/74151), [#74241](https://github.com/pytorch/pytorch/pull/74241), [#74484](https://github.com/pytorch/pytorch/pull/74484), [#74888](https://github.com/pytorch/pytorch/pull/74888))\r\n* Optimized RecordFunction machinery ([#75807](https://github.com/pytorch/pytorch/pull/75807), [#76017](https://github.com/pytorch/pytorch/pull/76017), [#76016](https://github.com/pytorch/pytorch/pull/76016))\r\n\r\n## Mobile\r\n\r\n* Reduced unnecessary reference count bumps while parsing ByteCode. ([#72523](https://github.com/pytorch/pytorch/pull/72523))\r\n\r\n## Quantization\r\n\r\n* Improved multi-core performance of `qavg_pool2d` ([#69517](https://github.com/pytorch/pytorch/pull/69517))\r\n* Improved multi-core performance of `qmax_pool2d` ([#69598](https://github.com/pytorch/pytorch/pull/69598))\r\n* Improved multi-core performance of `qbatch_norm2d` ([#69599](https://github.com/pytorch/pytorch/pull/69599))\r\n* Improved multi-core performance of `qupsample_nearest2d` ([#69600](https://github.com/pytorch/pytorch/pull/69600))\r\n* Improved multi-core performance of `qupsample_bilinear2d` ([#69601](https://github.com/pytorch/pytorch/pull/69601))\r\n* Improved `qcat_nhwc` performance on both multi-core and single-core ([#69667](https://github.com/pytorch/pytorch/pull/69667))\r\n* Added Optimized QInt8 Quantize Tensor Arm ([#76245](https://github.com/pytorch/pytorch/pull/76245))\r\n\r\n# Documentation\r\n\r\n## Python API\r\n\r\n* Updated `torch.amp` document with CPU Training/Inference Examples ([#77244](https://github.com/pytorch/pytorch/pull/77244))\r\n* Updated `torch.utils.dlpack.from_dlpack` documentation ([#70543](https://github.com/pytorch/pytorch/pull/70543))\r\n* Fixed indexing of class names in docs for `torch.{device,` `dtype, layout, memory_format}` ([#73632](https://github.com/pytorch/pytorch/pull/73632))\r\n* Fixed `torch.asarray` docs and add test case ([#73736](https://github.com/pytorch/pytorch/pull/73736))\r\n* Removed misleading statement in `optim.Optimizer` docs ([#76967](https://github.com/pytorch/pytorch/pull/76967))\r\n* Fixed nesterov momentum equation for `torch.optim.SGD` ([#76639](https://github.com/pytorch/pytorch/pull/76639))\r\n* Added missing zero-ing step in `torch.optim.Rprop` algorithm ([#75555](https://github.com/pytorch/pytorch/pull/75555))\r\n* Fixed docs about type promotion of `torch.`{`bitwise_left_shift,bitwise_right_shift}` ([#77613](https://github.com/pytorch/pytorch/pull/77613))\r\n* Fixed docstring for `torch.roll` ([#74880](https://github.com/pytorch/pytorch/pull/74880))\r\n* Added docs for `torch.scatter_reduce` ([#73125](https://github.com/pytorch/pytorch/pull/73125))\r\n* Automatically generate docstring for `torch.distributions.kl_divergence` ([#72845](https://github.com/pytorch/pytorch/pull/72845))\r\n* Miscellaneous documentation improvements ([#74796](https://github.com/pytorch/pytorch/pull/74796), [#76369](https://github.com/pytorch/pytorch/pull/76369))\r\n\r\n## C++ API\r\n\r\n* Exposed documentation for `unfold` ([#74224](https://github.com/pytorch/pytorch/pull/74224))\r\n\r\n## Autograd\r\n\r\n* Fixed error in \u201cAutograd Mechanics\u201d doc\u2019s eval mode section ([#74807](https://github.com/pytorch/pytorch/pull/74807))\r\n* Added \u201cGradients for non-differentiable functions\u201d section in \"Autograd Mechanics\" doc to explain how gradients are chosen in edge cases ([#76898](https://github.com/pytorch/pytorch/pull/76898))\r\n* Added  link to \"Custom function double backward tutorial\" from \"Extending Pytorch\" page ([#72584](https://github.com/pytorch/pytorch/pull/72584))\r\n* Documented forward AD interaction with grad mode ([#72216](https://github.com/pytorch/pytorch/pull/72216))\r\n* Fixed code examples to run successfully ([#74044](https://github.com/pytorch/pytorch/pull/74044))\r\n\r\n## Dataloader\r\n\r\n* Updated DataLoader docstring about `prefetch_factor` to reflect right amount of batches prefetched by `DataLoader` ([#74558](https://github.com/pytorch/pytorch/pull/74558))\r\n* Fixed docstring for `collate_fn` ([#76594](https://github.com/pytorch/pytorch/pull/76594))\r\n\r\n## LinAlg\r\n\r\n* Extrapolated on equiv between linalg @ and solve ([#71769](https://github.com/pytorch/pytorch/pull/71769))\r\n* Updated `torch.lu_unpack` docs ([#73803](https://github.com/pytorch/pytorch/pull/73803))\r\n\r\n## torch.nn\r\n\r\n* `nn.CosineEmbeddingLoss`: Use correct cosine similarity term instead of cosine distance ([#75188](https://github.com/pytorch/pytorch/pull/75188))\r\n* `nn.Hardtanh`: Use `min_val` and `max_val` in function definition ([#75789](https://github.com/pytorch/pytorch/pull/75789))\r\n* `nn.KLDivLoss`: Fixed `log_target` example ([#74945](https://github.com/pytorch/pytorch/pull/74945))\r\n* `nn.``LazyModuleMixin` Fixed typo in docs ([#76269](https://github.com/pytorch/pytorch/pull/76269))\r\n* `nn.LSTM`: Clarified docs for outputs vs. hidden states ([#74291](https://github.com/pytorch/pytorch/pull/74291))\r\n* `nn.Module`: Fixed docs by moving `_version` class variable after docstring ([#72912](https://github.com/pytorch/pytorch/pull/72912))\r\n* `nn.Module`: Fixed docstring typo for `get_submodule()` ([#73018](https://github.com/pytorch/pytorch/pull/73018))\r\n* `nn.Module`: Fixed URL for creating GitHub issues ([#73411](https://github.com/pytorch/pytorch/pull/73411))\r\n* `nn.RNN`: Fixed math notation for linear projections ([#77082](https://github.com/pytorch/pytorch/pull/77082))\r\n* `nn.Transformer`: Detailed 3D tensor shape for masks ([#75552](https://github.com/pytorch/pytorch/pull/75552))\r\n* `nn.TripletMarginLoss`: Fixed formatting error ([#76629](https://github.com/pytorch/pytorch/pull/76629))\r\n* `F.{conv3d, conv_transpose3d, fold, linear}, nn.{AdaptiveAvgPool3d, AvgPool1d, MultiMarginLoss, PairwiseDistance, TripletMarginLoss}`: Fixed doc formatting regressions ([#73014](https://github.com/pytorch/pytorch/pull/73014))\r\n* `F.multi_head_attention_forward`: Added to functional rst ([#72675](https://github.com/pytorch/pytorch/pull/72675))\r\n* `F.multi_head_attention_forward`: Fixed math formatting, misc edit ([#74181](https://github.com/pytorch/pytorch/pull/74181))\r\n* `F.pad`: Fixed supported input shapes in docs ([#76117](https://github.com/pytorch/pytorch/pull/76117))\r\n* `nn.init.trunc_normal_`: Added to `nn.init` docs ([#76896](https://github.com/pytorch/pytorch/pull/76896))\r\n* `nn.utils.clip_grad_norm_`: Fixed return value description ([#76230](https://github.com/pytorch/pytorch/pull/76230))\r\n* `nn.Convolution`: Added note on complex support ([#](https://github.com/pytorch/pytorch/pull/78351)[78351](https://github.com/pytorch/pytorch/pull/78351))\r\n\r\n## torch.fx\r\n\r\n* Added better error message for FX when using concrete_args ([#76600](https://github.com/pytorch/pytorch/pull/76600))\r\n\r\n## Composability\r\n\r\n* Added docs for Python Registration ([#79481](https://github.com/pytorch/pytorch/pull/79481))\r\n\r\n## Sparse\r\n\r\n* Added missing entry for `torch.sparse.sampled_addmm` on website ([#72312](https://github.com/pytorch/pytorch/pull/72312))\r\n\r\n## Mobile\r\n\r\n* Documentation improvement in test_backend_with_compiler (52c516ecb8)\r\n* Added README for mobile model test ([#76385](https://github.com/pytorch/pytorch/pull/76385), [#76409](https://github.com/pytorch/pytorch/pull/76409))\r\n\r\n## Distributed\r\n\r\n* torch.distributed\r\n    * Clarified the input of PostLocalSGDState ([#72792](https://github.com/pytorch/pytorch/pull/72792))\r\n    * Added a reference to hierarchical SGD for Model Averaging ([#73823](https://github.com/pytorch/pytorch/pull/73823))\r\n    * Updated documentation about NCCL environment variables ([#74006](https://github.com/pytorch/pytorch/pull/74006))\r\n    * Added `TORCH_CPP_LOG_LEVEL` to the docs ([#76625](https://github.com/pytorch/pytorch/pull/76625))\r\n* FullyShardedDataParallel\r\n    * Improved the documentation of state_dict ([#73453](https://github.com/pytorch/pytorch/pull/73453))\r\n    * Updated `full_optim_state_dict` warning ([#75109](https://github.com/pytorch/pytorch/pull/75109))\r\n    * Added warning when fail to clone ([#74946](https://github.com/pytorch/pytorch/pull/74946))\r\n    * Added mixed precision doc ([#76130](https://github.com/pytorch/pytorch/pull/76130))\r\n    * Added warnings for shared params and updated doc ([#77726](https://github.com/pytorch/pytorch/pull/77726))\r\n    * Fixed `state_dict_type()` example ([#77848](https://github.com/pytorch/pytorch/pull/77848))\r\n    * Reworded device placement warning ([#77850](https://github.com/pytorch/pytorch/pull/77850))\r\n    * Updated `state_dict()` docstring ([#77853](https://github.com/pytorch/pytorch/pull/77853))\r\n* torch.distributed.rpc\r\n    * Added note in RPC docs about retries. ([#73601](https://github.com/pytorch/pytorch/pull/73601))\r\n* DistributedDataParallel\r\n    * Updated the comment for Forward and Backward Hook ([#74063](https://github.com/pytorch/pytorch/pull/74063))\r\n    * Added documentation for c10d log levels ([#73361](https://github.com/pytorch/pytorch/pull/73361))\r\n* torch.distributed.elastic\r\n    * Added documentation clarifying that `torchrun` is a console script to `torch.distributed.run` ([#73598](https://github.com/pytorch/pytorch/pull/73598))\r\n\r\n## TorchScript\r\n\r\n* Corrected torch.jit.Attribute docs to say that it needs to be used in subclasses of torch.jit.ScriptModule, not torch.nn.Module ([#74653](https://github.com/pytorch/pytorch/pull/74653))\r\n\r\n## Quantization\r\n\r\n* Added docs for `torch.quantize_per_tensor_dynamic` ([#72311](https://github.com/pytorch/pytorch/pull/72311))\r\n* Fixed typo in quantization docs ([#73511](https://github.com/pytorch/pytorch/pull/73511))\r\n* Grammatically updated quantization tech doc ([#74436](https://github.com/pytorch/pytorch/pull/74436))\r\n* Added best practices for quantization accuracy debugging ([#77536](https://github.com/pytorch/pytorch/pull/77536))\r\n* Improved rendered documentation for backend_config_dict ([#77535](https://github.com/pytorch/pytorch/pull/77535))\r\n* Autogenerated quantization backend configs for documentation ([#75126](https://github.com/pytorch/pytorch/pull/75126))\r\n* Added more docs for quantization.rst ([#75998](https://github.com/pytorch/pytorch/pull/75998))\r\n* Fixed formatting for quantization.rst ([#76223](https://github.com/pytorch/pytorch/pull/76223))\r\n\r\n## ONNX\r\n\r\n* Added the developing PyTorch ONNX exporter wiki doc link ([_#72663_](https://github.com/pytorch/pytorch/pull/72663))\r\n* Added list of supported ATen ops to [_torch.onnx_](https://pytorch.org/docs/master/onnx.html#list-of-supported-operators) page ([_#74397_](https://github.com/pytorch/pytorch/pull/74397))\r\n\r\n## Visualization\r\n\r\n* `torch.utils.tensorboard.writer:` Added missing 'dataformats' argument to 'add_image' docs. ([#48834](https://github.com/pytorch/pytorch/pull/48834))\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.12.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.12.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.12.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/70696589", "release_id": 70696589, "date_created": "2022-06-27T17:41:56Z", "date_published": "2022-06-28T16:48:06Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/61381447", "tag": "v1.11.0", "name": " PyTorch 1.11, TorchData, and functorch are now available", "author": {"name": "bdhirsh", "type": "User"}, "description": "# PyTorch 1.11 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.11. This release is composed of over 3,300 commits since 1.10, made by 434 contributors. Along with 1.11, we are releasing beta versions of TorchData and functorch. We want to sincerely thank our community for continuously improving PyTorch.\r\n\r\n\r\n* TorchData is a new library for common modular data loading primitives for easily constructing flexible and performant data pipelines. [_View it on GitHub_](https://github.com/pytorch/data). \r\n* functorch, a library that adds composable function transforms to PyTorch, is now available in beta. [_View it on GitHub_](https://github.com/pytorch/functorch).\r\n* Distributed Data Parallel (DDP) static graph optimizations available in stable.\r\n\r\nYou can check the blogpost that shows the new features [here](https://pytorch.org/blog/pytorch-1.11-released/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### Fixed python `deepcopy` to correctly copy all attributes on `Tensor` objects ([#65584](https://github.com/pytorch/pytorch/pull/65584))\r\n\r\nThis change ensures that the `deepcopy` operation on Tensor properly copies all the attributes (and not just the plain Tensor properties).\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\na = torch.rand(2)\r\na.foo = 3\r\ntorch.save(a, \"bar\")\r\nb = torch.load(\"bar\")\r\nprint(b.foo)\r\n# Raise AttributeError: \"Tensor\" object has no attribute \"foo\"\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\na = torch.rand(2)\r\na.foo = 3\r\ntorch.save(a, \"bar\")\r\nb = torch.load(\"bar\")\r\nprint(b.foo)\r\n# 3\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### **`steps` argument is no longer optional in `torch.linspace` and `torch.logspace`**\r\n\r\nThis argument used to default to 100 in PyTorch 1.10.2, but was deprecated (previously you would see a deprecation warning if you didn\u2019t explicitly pass in `steps`). In PyTorch 1.11, it is not longer optional.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# Works, but raises a deprecation warning\r\n# Steps defaults to 100\r\na = torch.linspace(1, 10)\r\n# UserWarning: Not providing a value for linspace's steps is deprecated\r\n# and will throw a runtime error in a future release.\r\n# This warning will appear only once per process.\r\n# (Triggered internally at  ../aten/src/ATen/native/RangeFactories.cpp:19\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# In 1.11, you must specify steps\r\na = torch.linspace(1, 10, steps=100)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Remove `torch.hub.import_module` function that was mistakenly public ([#67990](https://github.com/pytorch/pytorch/pull/67990))\r\n\r\nThis function is not intended for public use.\r\nIf you have existing code that relies on it, you can find an equivalent function at `torch.hub._import_module`.\r\n\r\n## C++ API\r\n\r\n### **We\u2019ve cleaned up many of the headers in the C++ frontend to only include the subset of `aten` operators that they actually used ([#68247](https://github.com/pytorch/pytorch/pull/68247), [#68687](https://github.com/pytorch/pytorch/pull/68687), [#68688](https://github.com/pytorch/pytorch/pull/68688), [#68714](https://github.com/pytorch/pytorch/pull/68714), [#68689](https://github.com/pytorch/pytorch/pull/68689), [#68690](https://github.com/pytorch/pytorch/pull/68690), [#68697](https://github.com/pytorch/pytorch/pull/68697), [#68691](https://github.com/pytorch/pytorch/pull/68691), [#68692](https://github.com/pytorch/pytorch/pull/68692), [#68693](https://github.com/pytorch/pytorch/pull/68693), [#69840](https://github.com/pytorch/pytorch/pull/69840))**\r\n\r\nWhen you `#include` a header from the C++ frontend, you can no longer assume that every `aten` operators are transitively included. You can work around this by directly adding `#include <ATen/ATen.h>` in your file, which will maintain the old behavior of including every `aten` operators.\r\n\r\n###  **Custom implementation for `c10::List` and `c10::Dict` move constructors have been removed (**[**#69370**](https://github.com/pytorch/pytorch/pull/69370)**)**\r\n\r\nThe semantics have changed from \"make the moved-from List/Dict empty\" to \"keep the moved-from List/Dict unchanged\"\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"cpp\">\r\nc10::List<string> list1({\"3\", \"4\"});\r\nc10::List<string> list2(std::move(list1));\r\nstd::cout << list1.size() // 0\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"cpp\">\r\nc10::List<string> list1({\"3\", \"4\"});\r\nc10::List<string> list2(std::move(list1)); // calls copy ctr\r\nstd::cout << list1.size() // 2\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## CUDA\r\n\r\n### **Removed `THCeilDiv` function and corresponding `THC/THCDeviceUtils.cuh` header ([#65472](https://github.com/pytorch/pytorch/pull/65472))**\r\n\r\nAs part of cleaning up `TH` from the codebase, the `THCeilDiv` function has been removed. Instead, please use `at::ceil_div`, and include the corresponding `ATen/ceil_div.h` header\r\n\r\n### **Removed `THCudaCheck` (**[**#66391**](https://github.com/pytorch/pytorch/pull/66391)**)**\r\n\r\nYou can replace it with `C10_CUDA_CHECK`, which has been available since at least PyTorch 1.4, so just replacing is enough even if you support older versions\r\n\r\n### **Removed `THCudaMalloc()`, `THCudaFree()`,  `THCThrustAllocator.cuh` (**[**#65492**](https://github.com/pytorch/pytorch/pull/65492)**)**\r\n\r\nIf your extension is using `THCThrustAllocator.cuh`, please replace it with `ATen/cuda/ThrustAllocator.h` and corresponding APIs (see examples in this PR).\r\n\r\nThis PR also removes `THCudaMalloc/THCudaFree` calls. Please use `c10::cuda::CUDACachingAllocator::raw_alloc(size)/raw_delete(ptr)`, or, preferably, switch to `c10:cuda::CUDaCachingAllocator::allocate` which manages deallocation. Caching allocator APIs are available since PyTorch 1.2, so just replacing it is enough even if you support older versions of PyTorch.\r\n\r\n## Build\r\n\r\n### Stopped building shared library for AOT Compiler, `libaot_compiler.so` ([#66227](https://github.com/pytorch/pytorch/pull/66227))\r\n\r\nBuilding `aot_compiler.cpp` as a separate library is not necessary, as it\u2019s already included in `libtorch.so`.\r\nYou can update your build system to only dynamically link `libtorch.so`.\r\n\r\n## Mobile\r\n\r\n### Make `typing.Union` type unsupported for mobile builds ([#65556](https://github.com/pytorch/pytorch/pull/65556))\r\n\r\n`typing.Union` support was added for TorchScript in 1.10. It was removed specifically for mobile due to its lack of use and increase in binary size of PyTorch for Mobile builds.\r\n\r\n## Distributed\r\n\r\n### `torch.distributed.rpc`: Final Removal of ProcessGroup RPC backend ([#67363](https://github.com/pytorch/pytorch/pull/67363))\r\n\r\nProcessGroup RPC backend is deprecated. In 1.10, it threw an error to help users update their code, and, in 1.11, it is removed completely.\r\n\r\nThe backend type \u201cPROCESS_GROUP\u201d is now deprecated, e.g.\r\n`torch.distributed.rpc.init_rpc(\"worker0\", backend=\"PROCESS_GROUP\", rank=0, world_size=1)`\r\nand should be replaced with:\r\n`torch.distributed.rpc.init_rpc(\"worker0\", backend=\"TENSORPIPE\", rank=0, world_size=1)`\r\n\r\n## Quantization\r\n\r\n### Disabled the support for `getitem` in FX Graph Mode Quantization ([#66647](https://github.com/pytorch/pytorch/pull/66647))\r\n\r\n`getitem` used to be quantized in `FX Graph Mode Quantization`, and it is no longer quantized. This won\u2019t break any models but could result in a slight difference in numerics.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = torch.nn.Linear(5, 5)\r\n    def forward(self, x):\r\n        x = self.linear(x)\r\n        y = torch.stack([x], 0)\r\n        return y[0]\r\nm = M().eval()\r\nm = prepare_fx(m, {\"\": torch.ao.quantization.default_qconfig})\r\nm = convert_fx(m)\r\nprint(m)\r\n# prints\r\n# GraphModule(\r\n#   (linear): QuantizedLinear(in_features=5, out_features=5,\r\n#      scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\r\n# )\r\n# def forward(self, x):\r\n#     linear_input_scale_0 = self.linear_input_scale_0\r\n#     linear_input_zero_point_0 = self.linear_input_zero_point_0\r\n#     quantize_per_tensor = torch.quantize_per_tensor(x,\r\n#         linear_input_scale_0, linear_input_zero_point_0, torch.quint8)\r\n#     x = linear_input_scale_0 = linear_input_zero_point_0 = None\r\n#     linear = self.linear(quantize_per_tensor)\r\n#     quantize_per_tensor = None\r\n#     stack = torch.stack([linear], 0);  linear = None\r\n#     getitem = stack[0]; stack = None\r\n#     dequantize_2 = getitem.dequantize();  getitem = None\r\n#     return getitem\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = torch.nn.Linear(5, 5)\r\n    def forward(self, x):\r\n        x = self.linear(x)\r\n        y = torch.stack([x], 0)\r\n        return y[0]\r\nm = M().eval()\r\nm = prepare_fx(m, {\"\": torch.ao.quantization.default_qconfig})\r\nm = convert_fx(m)\r\nprint(m)\r\n# prints\r\n# GraphModule(\r\n#   (linear): QuantizedLinear(in_features=5, out_features=5, scale=1.0,\r\n                    zero_point=0, qscheme=torch.per_tensor_affine)\r\n# )\r\n# def forward(self, x):\r\n#     linear_input_scale_0 = self.linear_input_scale_0\r\n#     linear_input_zero_point_0 = self.linear_input_zero_point_0\r\n#     quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0,\r\n                     linear_input_zero_point_0, torch.quint8)\r\n#     x = linear_input_scale_0 = linear_input_zero_point_0 = None\r\n#     linear = self.linear(quantize_per_tensor);  quantize_per_tensor = None\r\n#     stack = torch.stack([linear], 0);  linear = None\r\n#     dequantize_2 = stack.dequantize();  stack = None\r\n#     getitem = dequantize_2[0];  dequantize_2 = None\r\n#     return getitem\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### **Users should now use `fuse_modules` for PTQ fusion and `fuse_modules_qat` for QAT fusion ([#69878](https://github.com/pytorch/pytorch/pull/69878), [#71956](https://github.com/pytorch/pytorch/pull/71956))**\r\n\r\nThere are two types of fusion supported by fuse_modules api: PTQ and QAT fusion. Previously we relied on `module.training` to decide which mode user wanted, but this was a misuse of the `training` attribute since that is not the intended purpose. This PR removes the dependency on `module.training` and uses separate APIs to make the fusion requested by the user explicit.\r\n\r\nPreviously, `fuse_module` used to support both cases and distinguished PTQ/QAT fusion based on `module.training`, but now `fuse_module` only supports the PTQ fusion. So, in the case when user wants to do QAT fusion, they need to change the call to `fuse_modules_qat`, instead of using `fuse_modules`, otherwise, they would silently get unwanted fusion results (PTQ fusion), or if the model is in training mode, it might result in error.\r\n\r\n**Note:** Currently it is still enforced that if the model is in eval mode, only PTQ fusion can be used; if the model is in training mode, then only QAT fusion can be used. In the future this constraint will be relaxed.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nfrom torch.ao.quantization import fuse_modules\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = torch.nn.Conv2d(3, 3, 3)\r\n        self.bn = torch.nn.BatchNorm2d(3)\r\n    def forward(self, x):\r\n        return self.bn(self.conv(x))\r\nm = M().train()\r\nm = fuse_modules(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\nm = M().eval()\r\nm = fuse_modules(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\n&lt;class 'torch.nn.intrinsic.modules.fused.ConvBn2d'&gt;\r\n&lt;class 'torch.nn.modules.conv.Conv2d'&gt;\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nfrom torch.ao.quantization import fuse_modules\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = torch.nn.Conv2d(3, 3, 3)\r\n        self.bn = torch.nn.BatchNorm2d(3)\r\n    def forward(self, x):\r\n        return self.bn(self.conv(x))\r\nm = M().train()\r\n# For Quantization Aware Training, use fuse_modules_qat()\r\nm = fuse_modules_qat(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\nm = M().eval()\r\nm = fuse_modules(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\n# Result (doesn't change):\r\n&lt;class 'torch.nn.intrinsic.modules.fused.ConvBn2d'&gt;\r\n&lt;class 'torch.nn.modules.conv.Conv2d'&gt;\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n## ONNX\r\n\r\n### Removed `f` arg from `onnx.export_to_pretty_string` ([#69546](https://github.com/pytorch/pytorch/pull/69546))\r\n\r\nThe arg has always been ignored. Simply remove it from your code.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export_to_pretty_string(model, inputs, \"file_name\")\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export_to_pretty_string(model, inputs)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `use_external_data_format` arg from `onnx.export` ([#67809](https://github.com/pytorch/pytorch/pull/67809))\r\n\r\nThe arg has been deprecated and ignored since 1.10. The external data format is now used automatically if and only if the exported file would exceed protocol buffer\u2019s file size limit. Simply remove it from your code.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name, use_external_data_format=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `example_outputs` arg from `torch.onnx.export` ([#67809](https://github.com/pytorch/pytorch/pull/67809))\r\n\r\nThe arg has been deprecated and ignored since 1.10. The provided model is instead executed once to produce example outputs. Simply remove it from your code.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name, exaple_outputs=(foo,))\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `enable_onnx_checker` arg from `onnx.export` ([#67276](https://github.com/pytorch/pytorch/pull/67276))\r\n\r\nThe arg has been deprecated and ignored since 1.10. The ONNX checker is always enabled. If it fails, `onnx.CheckerError` will be raised. Users can catch and ignore that exception.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name, enable_onnx_checker=False)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntry:\r\n    torch.onnx.export(model, inputs, f_name)\r\nexcept torch.onnx.CheckerError:\r\n    pass # ignore error\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Moved and renamed `onnx.utils.ONNXCheckerError` to `onnx.CheckerError` ([#66644](https://github.com/pytorch/pytorch/pull/66644))\r\n\r\nPreviously the documentation was incorrect and stated `ONNXCheckerError` was in the `onnx` module, so this moves the class to the originally intended module and brings the code in line with the documentation. The new name is shorter and less redundant with the module name.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nexcept torch.onnx.utils.ONNXCheckerError:\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nexcept torch.onnx.CheckerError:\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `_retain_param_name` arg from `onnx.export` ([#67276](https://github.com/pytorch/pytorch/pull/67276))\r\n\r\nThe arg has been deprecated and ignored since 1.10. Param names are now always retained. Simply remove it from your code. If you want to remove param names, you can do so by editing the exported ONNX model.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# NOTE: No way to get same behavior as _retain_param_name=False.\r\ntorch.onnx.export(model, inputs, f_name, _retain_param_name=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name)\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### Deprecated `x.T` on tensors of dimension other than 0 or 2 ([#64180](https://github.com/pytorch/pytorch/pull/64180))\r\n\r\n`x.T` only accepts tensors with 0 or 2 dimensions. Calling `x.T` on tensors with a different number of dimensions has been deprecated.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\na = torch.ones(2, 3, 4)\r\na.T.size()\r\n# torch.Size([4, 3, 2])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\na = torch.ones(2, 3, 4)\r\na.T.size()\r\n# UserWarning: The use of `x.T` on tensors of dimension other than 2\r\n# to reverse their shape is deprecated and it will throw an error in a future release.\r\n# Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))`\r\n# to reverse the dimensions of a tensor. (Triggered internally at \r\n# aten/src/ATen/native/TensorShape.cpp:2386.)\r\n# torch.Size([4, 3, 2])\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Quantization\r\n\r\n### `torch.ao.quantization.QConfigDynamic` is deprecated and going to be removed in next the release, please use `torch.ao.quantization.QConfig` instead ([#69875](https://github.com/pytorch/pytorch/pull/69875), [#69864](https://github.com/pytorch/pytorch/pull/69864))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nqconfig = torch.ao.quantization.QConfigDynamic(...)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nqconfig = torch.ao.quantization.QConfig(...)\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# New features\r\n\r\n## Python API\r\n\r\n* Added `set_deterministic_debug_mode` and `get_deterministic_debug_mode` ([#67778](https://github.com/pytorch/pytorch/pull/67778), [#66233](https://github.com/pytorch/pytorch/pull/66233))\r\n* Added n-dimensional Hermitian FFT: `torch.fft.ifftn` and `torch.fft.hfftn` ([#63890](https://github.com/pytorch/pytorch/pull/63890))\r\n* Added `Wishart` distribution to `torch.distributions` ([#70377](https://github.com/pytorch/pytorch/pull/70377))\r\n* Preliminary support for the [Python Array API](https://data-apis.org/array-api/latest/) standard has been added to the `torch` and `torch.linalg` modules. PyTorch implements over 90% of the operators defined by the Python Array API, including the `torch.from_dlpack` operation for improved DLPack support ([#60627](https://github.com/pytorch/pytorch/pull/60627))\r\n* Moved `torch.testing` from prototype to beta ([#69668](https://github.com/pytorch/pytorch/pull/69668))\r\n\r\n## Autograd\r\n\r\n* Added new `torch.utils.checkpoint` implementation that does not use reentrant autograd (can be toggled with the new `use_reentrant` flag) ([#69508](https://github.com/pytorch/pytorch/pull/69508))\r\n* Added `batched_grad` parameter to `autograd.grad` to allow batched gradient computation ([#65564](https://github.com/pytorch/pytorch/pull/65564))\r\n* Forward mode AD:\r\n    * Added support for most ops (and many of their backwards as well) ([#71026](https://github.com/pytorch/pytorch/pull/71026), [#69956](https://github.com/pytorch/pytorch/pull/69956), [#70355](https://github.com/pytorch/pytorch/pull/70355), [#71901](https://github.com/pytorch/pytorch/pull/71901), [#69908](https://github.com/pytorch/pytorch/pull/69908), [#69884](https://github.com/pytorch/pytorch/pull/69884), [#67837](https://github.com/pytorch/pytorch/pull/67837), [#68566](https://github.com/pytorch/pytorch/pull/68566), [#69661](https://github.com/pytorch/pytorch/pull/69661), [#69384](https://github.com/pytorch/pytorch/pull/69384), [#68631](https://github.com/pytorch/pytorch/pull/68631), [#70468](https://github.com/pytorch/pytorch/pull/70468), [#70460](https://github.com/pytorch/pytorch/pull/70460), [#67820](https://github.com/pytorch/pytorch/pull/67820), [#70460](https://github.com/pytorch/pytorch/pull/70460), [#65546](https://github.com/pytorch/pytorch/pull/65546), [#67043](https://github.com/pytorch/pytorch/pull/67043),  [#67268](https://github.com/pytorch/pytorch/pull/67268), [#67837](https://github.com/pytorch/pytorch/pull/67837), [#69727](https://github.com/pytorch/pytorch/pull/69727))\r\n        * *Check the following issue ([#71117](https://github.com/pytorch/pytorch/issues/71117)) to see the list of ops that do not yet support forward AD. Please comment there if you run into any ops that don\u2019t support forward AD that you want prioritized or are missing from that list.*\r\n    * Added `ctx.save_for_forward` function to `autograd.Function` ([#71569](https://github.com/pytorch/pytorch/pull/71569))\r\n    * `autograd.forward_ad.unpack_dual` returns a named tuple instead of plain tuple ([#68062](https://github.com/pytorch/pytorch/pull/68062), [#68628](https://github.com/pytorch/pytorch/pull/68628))\r\n* Linear algebra operation support:\r\n    * Added forward AD support for `torch.linalg.{eig, inverse, householder_product, qr}` and `torch.*_solve` ([#65546](https://github.com/pytorch/pytorch/pull/65546), [#67043](https://github.com/pytorch/pytorch/pull/67043), [#67268](https://github.com/pytorch/pytorch/pull/67268), [#67837](https://github.com/pytorch/pytorch/pull/67837))\r\n    * Added forward and backward AD support for `torch.linalg.lstsq` ([#65054](https://github.com/pytorch/pytorch/pull/65054)) \r\n    * Added support for a wider range of inputs for `linalg.pinv` ([#66092](https://github.com/pytorch/pytorch/pull/66092))\r\n\r\n## Build\r\n\r\n* Added FlexiBLAS build support ([#64815](https://github.com/pytorch/pytorch/pull/64815))\r\n* Added `IS_LINUX` and `IS_MACOS` global vars for cpp extensions building ([#69093](https://github.com/pytorch/pytorch/pull/69093))\r\n* Added ARC for iOS CMake builds ([#67884](https://github.com/pytorch/pytorch/pull/67884))\r\n* Added support for IBM z14/15 SIMD ([#66407](https://github.com/pytorch/pytorch/pull/66407))\r\n\r\n## Complex Numbers\r\n\r\n* Added complex number support to `Adagrad` and `Adadelta` optimizers ([#66671](https://github.com/pytorch/pytorch/pull/66671), [#66587](https://github.com/pytorch/pytorch/pull/66587))\r\n\r\n## Dataloader\r\n\r\n* TorchData library is going to provide modular data loading primitives for easily constructing flexible and performant data pipelines. Beta release will be provided after the release of PyTorch Core (https://github.com/pytorch/data)\r\n\r\n## LinAlg\r\n\r\n* Added an **experimental** flag that allows specifying a preferred linear algebra library (see the docs [here](https://pytorch.org/docs/master/backends.html?highlight=preferred_linalg_library#torch.backends.cuda.preferred_linalg_library)) ([#67980](https://github.com/pytorch/pytorch/pull/67980))\r\n* Added the `linalg.matrix_exp` operation (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.matrix_exp.html?highlight=matrix_exp#torch.linalg.matrix_exp)) ([#62715](https://github.com/pytorch/pytorch/pull/62715))\r\n* Added the `linalg.cross` operation (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.cross.html?highlight=linalg%20cross#torch.linalg.cross)) ([#63285](https://github.com/pytorch/pytorch/pull/63285))\r\n* Added the `linalg.diagonal` operation, an alias for torch.diagonal (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.diagonal.html?highlight=linalg%20diagonal#torch.linalg.diagonal)) ([#70599](https://github.com/pytorch/pytorch/pull/70599))\r\n* Added the `linalg.lu_factor` operation (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.lu_factor.html?highlight=lu_factor#torch.linalg.lu_factor)) ([#66933](https://github.com/pytorch/pytorch/pull/66933))\r\n\r\n## torch.nn\r\n\r\n* Added `torch.nn.utils.rnn.{unpack_sequence,unpad_sequence}` functions ([#66550](https://github.com/pytorch/pytorch/pull/66550))\r\n\r\n## Sparse\r\n\r\n* Added `torch.sparse.sampled_addmm` for CSR Tensors on GPU ([#68007](https://github.com/pytorch/pytorch/pull/68007))\r\n\r\n## CUDA\r\n\r\n* The Jiterator  - enables compiling rarely used CUDA kernels at runtime ([#69439](https://github.com/pytorch/pytorch/pull/69439))\r\n    * Low precision supported for jiterator ([#70157](https://github.com/pytorch/pytorch/pull/70157)) - enables runtime-compilation of ops on low precision tensors (half and bfloat16)\r\n    * Enable cpu scalar arguments for jiterator ([#69861](https://github.com/pytorch/pytorch/pull/69861)) - enables passing cpu scalars as an argument to the jit-compiled kernels at runtime\r\n    * The Cacherator ([#71350](https://github.com/pytorch/pytorch/pull/71350)) - caches the jit-compiled kernels on disk, so that they can be reused between different processes\r\n    * Added complex support for Jiterator, port sinc to Jiterator ([#71577](https://github.com/pytorch/pytorch/pull/71577))\r\n    * Jiterates `lcm`, `i0e`, `i1e`, `ndtri`, `efcx`, `digamma`, `trigamma`, `lgamma` ([#70663](https://github.com/pytorch/pytorch/pull/70663))\r\n    * Jiterates `exp2`, `erfc`, `erfinv` and `entr` ([#71295](https://github.com/pytorch/pytorch/pull/71295))\r\n    * Fixes jiterator cache macro include + updates CUDA note with cache variables ([#71452](https://github.com/pytorch/pytorch/pull/71452))\r\n    * Jiterates `polygamma` ([#71162](https://github.com/pytorch/pytorch/pull/71162))\r\n* Added cuSPARSE descriptors and updated CSR addmm ([#60838](https://github.com/pytorch/pytorch/pull/60838))\r\n* Sparse CSR CUDA: added `addmv_out` ([#61407](https://github.com/pytorch/pytorch/pull/61407))\r\n* Added nvidia-smi memory and utilization as native Python API ([#69104](https://github.com/pytorch/pytorch/pull/69104))\r\n\r\n## Vulkan\r\n\r\n* Added Vulkan support for several torch operators:\r\n    * `torch.cat` (support for concatenation along the height and channel dimensions for 4-D tensors)  ([#66669](https://github.com/pytorch/pytorch/pull/66669), [#66103](https://github.com/pytorch/pytorch/pull/66103), [#67207](https://github.com/pytorch/pytorch/pull/67207))\r\n    * `torch.nn``.ConvTranspose2d` ([#67104](https://github.com/pytorch/pytorch/pull/67104), [#67358](https://github.com/pytorch/pytorch/pull/67358))\r\n    * `torch.permute` ([#68274](https://github.com/pytorch/pytorch/pull/68274))\r\n    * Tensor indexing (`at::slice`) ([#69382](https://github.com/pytorch/pytorch/pull/69382))\r\n    * `torch.clone` ([#69551](https://github.com/pytorch/pytorch/pull/69551))\r\n* Added the `vulkan_perf_test` benchmark binary to benchmark Vulkan ops under various input conditions. ([#67230](https://github.com/pytorch/pytorch/pull/67230))\r\n\r\n## Mobile\r\n\r\n* Tracing Based Selective Build (PyTorch Mobile Build Size Reduction) is a new feature that reduces a mobile model\u2019s binary size by only including the operators that the model uses.\r\n    * Build tracer for tracing based workflow ([#66267](https://github.com/pytorch/pytorch/pull/66267))\r\n    * Used operator.yaml to build LibTorch library ([#66237](https://github.com/pytorch/pytorch/pull/66237))\r\n    * Unified tracer between internal and external ([#64152](https://github.com/pytorch/pytorch/pull/64152))\r\n    * Reorganized model tracer dependency ([#63421](https://github.com/pytorch/pytorch/pull/63421))\r\n    * Added support for the `bool` and `int` dtypes in the copy kernel by default when using Tracing Based Selective Build ([#69106](https://github.com/pytorch/pytorch/pull/69106), [#69297](https://github.com/pytorch/pytorch/pull/69297))\r\n    * Generic build features for selective build ([#67817](https://github.com/pytorch/pytorch/pull/67817))\r\n    * Made more classes selective ([#67397](https://github.com/pytorch/pytorch/pull/67397))\r\n    * Added custom classes to selective build and compatibility APIs ([#67004](https://github.com/pytorch/pytorch/pull/67004), [#66972](https://github.com/pytorch/pytorch/pull/66972), [#67340](https://github.com/pytorch/pytorch/pull/67340))\r\n\r\n## Distributed\r\n\r\n* `FullyShardedDataParallel`\r\n    * FSDP is a type of data-parallel training but unlike traditional data-parallel it shards model\u2019s parameters, gradients and optimizer states across data parallel workers and can optionally offload the sharded model parameters to the CPUs. This new API can help users to scale their large model training with minimal code change when switching from DDP to FSDP.  ([#63881](https://github.com/pytorch/pytorch/pull/63881), [#64964](https://github.com/pytorch/pytorch/pull/64964), [#66578](https://github.com/pytorch/pytorch/pull/66578), [#66904](https://github.com/pytorch/pytorch/pull/66904), [#66956](https://github.com/pytorch/pytorch/pull/66956), [#66957](https://github.com/pytorch/pytorch/pull/66957), [#67117](https://github.com/pytorch/pytorch/pull/67117), [#67292](https://github.com/pytorch/pytorch/pull/67292), [#67249](https://github.com/pytorch/pytorch/pull/67249), [#67135](https://github.com/pytorch/pytorch/pull/67135), [#67813](https://github.com/pytorch/pytorch/pull/67813), [#68308](https://github.com/pytorch/pytorch/pull/68308), [#68155](https://github.com/pytorch/pytorch/pull/68155), [#68417](https://github.com/pytorch/pytorch/pull/68417), [#68776](https://github.com/pytorch/pytorch/pull/68776), [#69356](https://github.com/pytorch/pytorch/pull/69356), [#69357](https://github.com/pytorch/pytorch/pull/69357), [#69358](https://github.com/pytorch/pytorch/pull/69358), [#70340](https://github.com/pytorch/pytorch/pull/70340), [#71803](https://github.com/pytorch/pytorch/pull/71803), [#71804](https://github.com/pytorch/pytorch/pull/71804), [#70341](https://github.com/pytorch/pytorch/pull/70341), [#70235](https://github.com/pytorch/pytorch/pull/70235), [#72084](https://github.com/pytorch/pytorch/pull/72084))\r\n* `DistributedDataParallel`\r\n    * Made static graph to be stable ([#71459](https://github.com/pytorch/pytorch/pull/71459), [#68413](https://github.com/pytorch/pytorch/pull/68413))\r\n    * Made LocalSGD beta, cleaned up some docs ([#71621](https://github.com/pytorch/pytorch/pull/71621))\r\n    * Support custom buffer reduction in DDP via hooks ([#64513](https://github.com/pytorch/pytorch/pull/64513))\r\n\r\n## TorchScript\r\n\r\n* Enabled running `torch.jit.freeze()` and `torch.jit.optimize_for_inference` on functions that are not forward ([#68668](https://github.com/pytorch/pytorch/pull/68668), [#69367](https://github.com/pytorch/pytorch/pull/69367))\r\n* Enabled `torch.jit.freeze` to work on for sparse COO tensors ([#69614](https://github.com/pytorch/pytorch/pull/69614))\r\n* Enabled `torch.jit.script()`, `torch.jit.freeze()` and serialization for tensors in Compressed Sparse Row (CSR) format ([#69555](https://github.com/pytorch/pytorch/pull/69555))\r\n* Allowed users to set the fusion strategy for `torch.jit.fuser` through the now public  `torch.jit.set_fusion_strategy` . ([#72937](https://github.com/pytorch/pytorch/pull/72937)) \r\n* Enabled Dynamic Shape Fusion For GPU & CPU, configurable via `torch.jit.set_fusion_strategy` ([#72036](https://github.com/pytorch/pytorch/pull/72036))\r\n\r\n## Quantization\r\n\r\n* Added bilinear quantized implementation of `torch.nn.functional.grid_sample` 2d operator ([#66879](https://github.com/pytorch/pytorch/pull/66879))\r\n* Added the `torch.quantize_per_tensor_dynamic` operator ([#68004](https://github.com/pytorch/pytorch/pull/68004))\r\n* Added Quantization Aware Training support for `torch.nn.Embedding` and `torch.nn.EmbeddingBag`\r\n    * Added basic EmbeddingBag QAT fakeQuant workflow ([#65443](https://github.com/pytorch/pytorch/pull/65443))\r\n    * Added support for quantization of Embedding{Bag} in dynamic quant APIs ([#65674](https://github.com/pytorch/pytorch/pull/65674))\r\n    * Eager mode QAT for Embeddings ([#66429](https://github.com/pytorch/pytorch/pull/66429))\r\n    * Add benchmarks for QAT Embedding+EmbeddingBag ([#66560](https://github.com/pytorch/pytorch/pull/66560))\r\n    * Supported Embedding QAT via FX API ([#69333](https://github.com/pytorch/pytorch/pull/69333))\r\n    * Add FX support for QAT EmbeddingBag ([#69334](https://github.com/pytorch/pytorch/pull/69334))\r\n* Added support for depthwise quantized `torch.nn.Conv3d` in qnnpack, for use in quantization\r\n    * Depthwise Conv3d Indirection Buffer Setup ([#69311](https://github.com/pytorch/pytorch/pull/69311))\r\n    * Depthwise Conv3d Weight Packing ([#69312](https://github.com/pytorch/pytorch/pull/69312))\r\n    * Depthwise Conv3d mp8x27 (per channel) Neon Kernel ([#69313](https://github.com/pytorch/pytorch/pull/69313))\r\n    * Depthwise Conv3d mp8x27 (per-channel) Sse2 Kernel ([#69314](https://github.com/pytorch/pytorch/pull/69314))\r\n    * Tightened Step Height for Indirection Buffers ([#70530](https://github.com/pytorch/pytorch/pull/70530))\r\n    * Enabled Depthwise Specific Conv3d Kernel for Kernel Size 3x3x3 ([#69315](https://github.com/pytorch/pytorch/pull/69315))\r\n    * Implemented 3d convolution in qnnpack ([#66350](https://github.com/pytorch/pytorch/pull/66350))\r\n\r\n## ONNX\r\n\r\n* Supports opset version 15 ([#67805](https://github.com/pytorch/pytorch/pull/67805))\r\n* Supports exporting `nn.Module` calls as ONNX local functions ([#66140](https://github.com/pytorch/pytorch/pull/66140), [#67803](https://github.com/pytorch/pytorch/pull/67803))\r\n* Supports for exporting new ops:\r\n    * `tanhshrink`, `hardshrink`, `softshrink` ([#68492](https://github.com/pytorch/pytorch/pull/68492))\r\n    * `__xor__` ([#64581](https://github.com/pytorch/pytorch/pull/64581))\r\n    * `isfinite` ([#64754](https://github.com/pytorch/pytorch/issues/64754))\r\n    * `log10` ([#64374](https://github.com/pytorch/pytorch/pull/64374))\r\n    * `diagonal` ([#66144](https://github.com/pytorch/pytorch/pull/66144))\r\n* Added BFloat16 type support ([#66788](https://github.com/pytorch/pytorch/pull/66788))\r\n* Supports exporting with Apex O2 ([#66700](https://github.com/pytorch/pytorch/pull/66700))\r\n\r\n## Infra (Releng)\r\n\r\n* Added support for ROCm 4.3.1 ([#65624](https://github.com/pytorch/pytorch/pull/65624))\r\n* Added support for ROCm 4.5.2 ([#71064](https://github.com/pytorch/pytorch/pull/71064))\r\n* Added support for CUDA 11.5 ([#69262](https://github.com/pytorch/pytorch/pull/69262))\r\n* Added support for CUDA enabled Bazel builds ([#66241](https://github.com/pytorch/pytorch/pull/66241))\r\n* Added support for Python 3.10 ([#71132](https://github.com/pytorch/pytorch/pull/71132), [#71419](https://github.com/pytorch/pytorch/pull/71419))\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n* NumPy compatibility:\r\n    * Improved `torch.searchsorted` to be more consistent with NumPy ([#66818](https://github.com/pytorch/pytorch/pull/66818))\r\n    * Added `torch.argwhere` to match NumPy ([#64257](https://github.com/pytorch/pytorch/pull/64257))\r\n    * Added an alias for `torch.special.softmax` ([#62251](https://github.com/pytorch/pytorch/pull/62251))\r\n* Improved `torch.Tensor.view(dtype)`: enable all dtype combinations ([#66493](https://github.com/pytorch/pytorch/pull/66493))\r\n* Improved `torch.diff` by adding support for n greater than 1 ([#67260](https://github.com/pytorch/pytorch/pull/67260))\r\n* Improved `torch.movedim` to handle scalar as no-op ([#69537](https://github.com/pytorch/pytorch/pull/69537))\r\n* Improved `cartesian_prod`: fixed a warning in the docs example ([#68753](https://github.com/pytorch/pytorch/pull/68753))\r\n* Improved error messages for `max_unpool{}d` operators ([#67328](https://github.com/pytorch/pytorch/pull/67328))\r\n* `torch.distributions`\r\n    * Implemented positive-semidefinite constraint in `torch.distributions` ([#71375](https://github.com/pytorch/pytorch/pull/71375))\r\n    * Implemented Entropy methods for Binomial and Multinomial distributions ([#67609](https://github.com/pytorch/pytorch/pull/67609))\r\n    * Implemented support for `non-negative` constraint in exponential distribution (allowing it to include zero). ([#67184](https://github.com/pytorch/pytorch/pull/67184))\r\n    * Implemented `kl divergence` between `normal` and `laplace` distribution. ([#68807](https://github.com/pytorch/pytorch/pull/68807))\r\n* Improved meta tensor support for operators:\r\n    * `max` ([#61449](https://github.com/pytorch/pytorch/pull/61449)) `min` ([#61450](https://github.com/pytorch/pytorch/pull/61450))  `tril`, `triu` ([#67055](https://github.com/pytorch/pytorch/pull/67055)) `mv` ([#67373](https://github.com/pytorch/pytorch/pull/67373)) `range`, `arange`, `linspace`, `logspace` ([#67032](https://github.com/pytorch/pytorch/pull/67032)) `lerp` ([#68924](https://github.com/pytorch/pytorch/pull/68924)) `smooth_l1_loss` ([#67404](https://github.com/pytorch/pytorch/pull/67404)) `fractional_max_pool2d_backward` ([#68245](https://github.com/pytorch/pytorch/pull/68245)) `linalg.lu_factor` ([#66934](https://github.com/pytorch/pytorch/pull/66934)) `fractional_maxpool3d`: port to structured kernel ([#70414](https://github.com/pytorch/pytorch/pull/70414))\r\n* Added support for `torch.Tensor.real` for real-valued tensors ([#71718](https://github.com/pytorch/pytorch/pull/71718))\r\n*  `torch.logaddexp, torch.logaddexp2, torch.remainder`: added BFloat16 support on CPU ([#63621](https://github.com/pytorch/pytorch/pull/63621))\r\n*  `torch.bucketize` and `searchsorted`: added Half precision support ([#67077](https://github.com/pytorch/pytorch/pull/67077))\r\n* Added new `torch.slice_scatter`,`torch.select_scatter`, `torch.diagonal_scatter` ops ([#64430](https://github.com/pytorch/pytorch/pull/64430))\r\n* Made `torch.scatter_reduce` a public API ([#68580](https://github.com/pytorch/pytorch/pull/68580), [#73125](https://github.com/pytorch/pytorch/pull/73125/files))\r\n\r\n## C++ API\r\n\r\n* Added C++ API and docs for `hfftn` ([#66127](https://github.com/pytorch/pytorch/pull/66127))\r\n* Added support for `MaybeOwned<IValue>` ([#68157](https://github.com/pytorch/pytorch/pull/68157))\r\n* Added `set_to_none` option for `zero_grad()` to C++ API ([#68801](https://github.com/pytorch/pytorch/pull/68801))\r\n* Added an environment variable, `TORCH_CPP_LOG_LEVEL`, that you can use to toggle the log level in the c10 library ([#71746](https://github.com/pytorch/pytorch/pull/71746))\r\n\r\n## Autograd\r\n\r\n* Added nesting support for `torch.autograd.graph.saved_tensor_hooks` ([#70932](https://github.com/pytorch/pytorch/pull/70932))\r\n* Delayed all warnings encountered during the backward pass until the end of backward execution ([#66235](https://github.com/pytorch/pytorch/pull/66235))\r\n* Added complex autograd support to `torch.{col2im,im2col}` ([#68199](https://github.com/pytorch/pytorch/pull/68199))\r\n* Added new reduce options and autograd support for `torch.scatter_reduce` ([#71788](https://github.com/pytorch/pytorch/pull/71788))\r\n* Added derivatives wrt the second argument for `torch.{remainder,fmod}` ([#69908](https://github.com/pytorch/pytorch/pull/69908))\r\n* Added new `strategy` flag to `autograd.functional.{Jacobian, Hessian}` to enable vectorized computation ([#67041](https://github.com/pytorch/pytorch/pull/67041), [#66292](https://github.com/pytorch/pytorch/pull/66292)) \r\n* Added `check_backward_ad` flag to `torch.autograd.gradcheck` to be able to skip backward mode AD checks ([#65040](https://github.com/pytorch/pytorch/pull/65040))\r\n* Relaxed forward AD layout check to allow primal and tangent stride to differ when their size is 1 ([#66294](https://github.com/pytorch/pytorch/pull/66294))\r\n\r\n## Build\r\n\r\n* Improved incremental build times of PyTorch core by removing a dependency on `native_functions.yaml` in many core files ([#64499](https://github.com/pytorch/pytorch/pull/64499), [#66914](https://github.com/pytorch/pytorch/pull/66914), [#64172](https://github.com/pytorch/pytorch/pull/64172), [#64171](https://github.com/pytorch/pytorch/pull/64171), [#66620](https://github.com/pytorch/pytorch/pull/66620), [#66793](https://github.com/pytorch/pytorch/pull/66793), [#66913](https://github.com/pytorch/pytorch/pull/66913), [#66794](https://github.com/pytorch/pytorch/pull/66794), [#64169](https://github.com/pytorch/pytorch/pull/64169), [#64173](https://github.com/pytorch/pytorch/pull/64173), [#64170](https://github.com/pytorch/pytorch/pull/64170), [#67735](https://github.com/pytorch/pytorch/pull/67735))\r\n* Enabled bazel build without glog and gflags ([#70850](https://github.com/pytorch/pytorch/pull/70850))\r\n* Added support for C++ frontend wrapper on Linux ([#69094](https://github.com/pytorch/pytorch/pull/69094))\r\n* Added support for dynamic codegen outputs in CMake ([#68246](https://github.com/pytorch/pytorch/pull/68246))\r\n* Max CMake version is now used by default with setup.py ([#69355](https://github.com/pytorch/pytorch/pull/69355))\r\n* Upgraded oneDNN to v2.3.3 and package oneDNN Graph API together ([#63748](https://github.com/pytorch/pytorch/pull/63748))\r\n* Code base should now be `-Wno-unused-variable` compliant ([#66041](https://github.com/pytorch/pytorch/pull/66041))\r\n* Added lazy import for `packaging` in `torch_version` ([#71345](https://github.com/pytorch/pytorch/pull/71345))\r\n\r\n## Dataloader\r\n\r\n* Support custom `Sequence` and `Mapping` for `utils.data.default_collate` ([#68779](https://github.com/pytorch/pytorch/pull/68779))\r\n* Allowed specifying `num_samples` to `RandomSampler` when `replacement` is `False` ([#71568](https://github.com/pytorch/pytorch/pull/71568))\r\n* Fixed the warning of shape inconsistency `utils.data.default_collate` ([#71065](https://github.com/pytorch/pytorch/pull/71065))\r\n\r\n## ForEach\r\n\r\n* Implemented `ForEach` L1 & L2 norm ([#62646](https://github.com/pytorch/pytorch/pull/62646))\r\n\r\n## LinAlg\r\n\r\n* The `linalg.matrix_rank` ([docs](https://pytorch.org/docs/master/generated/torch.linalg.matrix_rank.html?highlight=matrix_rank#torch.linalg.matrix_rank)) and `linalg.pinv` ([docs](https://pytorch.org/docs/master/generated/torch.linalg.pinv.html?highlight=pinv#torch.linalg.pinv)) operations now support specifying absolute and relative tolerances for better handling of singular values ([#63102](https://github.com/pytorch/pytorch/pull/63102))\r\n\r\n## torch.nn\r\n\r\n* Added `channels_last` support for `ChannelShuffle` ([#50247](https://github.com/pytorch/pytorch/pull/50247))\r\n* Added no-batch-dim support for `nn.{AdaptiveLogSoftmaxWithLoss, Bilinear, Conv*d, ConvTranspose*d, CrossEntropyLoss, CTCLoss, Fold, FractionalMaxPool3d, GaussianNLLLoss, GRU, GRUCell, InstanceNorm*d, LSTM, LSTMCell, MarginRankingLoss, MultiheadAttention, MultiLabelSoftMarginLoss, RNN, RNNCell, Transformer, TransformerDecoderLayer, TransformerEncoderLayer}` ([#69054](https://github.com/pytorch/pytorch/pull/69054), [#69539](https://github.com/pytorch/pytorch/pull/69539), [#70506](https://github.com/pytorch/pytorch/pull/70506), [#71055](https://github.com/pytorch/pytorch/pull/71055), [#70092](https://github.com/pytorch/pytorch/pull/70092), [#64909](https://github.com/pytorch/pytorch/pull/64909), [#69732](https://github.com/pytorch/pytorch/pull/69732), [#69783](https://github.com/pytorch/pytorch/pull/69783), [#70236](https://github.com/pytorch/pytorch/pull/70236), [#65323](https://github.com/pytorch/pytorch/pull/65323), [#71056](https://github.com/pytorch/pytorch/pull/71056), [#64975](https://github.com/pytorch/pytorch/pull/64975), [#67176](https://github.com/pytorch/pytorch/pull/67176), [#70590](https://github.com/pytorch/pytorch/pull/70590), [#65690](https://github.com/pytorch/pytorch/pull/65690), [#70977](https://github.com/pytorch/pytorch/pull/70977), [#70597](https://github.com/pytorch/pytorch/pull/70597), [#70322](https://github.com/pytorch/pytorch/pull/70322), [#69291](https://github.com/pytorch/pytorch/pull/69291))\r\n* Added `BFloat16` support on CPU to `nn.{AdaptiveAvgPool2d, AdaptiveMaxPool2d, AvgPool2d, MaxPool2d}` ([#56902](https://github.com/pytorch/pytorch/pull/56902), [#66929](https://github.com/pytorch/pytorch/pull/66929), [#66927](https://github.com/pytorch/pytorch/pull/66927), [#56903](https://github.com/pytorch/pytorch/pull/56903))\r\n* Added `maximize` support to `optim.{Adam, AdamW, SGD}` ([#68164](https://github.com/pytorch/pytorch/pull/68164), [#70146](https://github.com/pytorch/pytorch/pull/70146), [#67847](https://github.com/pytorch/pytorch/pull/67847), [#68733](https://github.com/pytorch/pytorch/pull/68733), [#71023](https://github.com/pytorch/pytorch/pull/71023))\r\n* `F.interpolate`: Add `nearest-exact` mode to fix off-by-one error in `nearest` mode ([#64501](https://github.com/pytorch/pytorch/pull/64501))\r\n* `F.interpolate`: Added support for anti-aliasing to bilinear and bicubic algorithms ([#70930](https://github.com/pytorch/pytorch/pull/70930), [#68819](https://github.com/pytorch/pytorch/pull/68819), [#65142](https://github.com/pytorch/pytorch/pull/65142), [#69318](https://github.com/pytorch/pytorch/pull/69318))\r\n* `F.interpolate`: Improved error message for invalid shapes ([#66417](https://github.com/pytorch/pytorch/pull/66417))\r\n* `nn.Conv*d`: Accepts 0-sized channel inputs ([#66256](https://github.com/pytorch/pytorch/pull/66256))\r\n* `nn.LogSigmoid`: Used `log1p` for improved precision ([#66441](https://github.com/pytorch/pytorch/pull/66441))\r\n* `nn.Module`: Added flag for removing duplicates from parameters ([#71542](https://github.com/pytorch/pytorch/pull/71542))\r\n* `nn.Module`: Added `register_module` alias for registering a sub-module ([#65174](https://github.com/pytorch/pytorch/pull/65174))\r\n* `nn.ModuleList`: Supported concatenation ([#70887](https://github.com/pytorch/pytorch/pull/70887))\r\n* `nn.MultiheadAttention`: Added flag to optionally average output attention weights across heads ([#70055](https://github.com/pytorch/pytorch/pull/70055))\r\n* `nn.ParameterDict`: Supported full set of `dict` methods ([#69403](https://github.com/pytorch/pytorch/pull/69403))\r\n* `nn.{RNN, GRU}`: Allowed `hidden_size` to be 0 ([#70556](https://github.com/pytorch/pytorch/pull/70556))\r\n* `nn.Sequential`: Added `append` method ([#71326](https://github.com/pytorch/pytorch/pull/71326))\r\n* `nn.Upsample`: Exposed `recompute_scale_factor` ([#66419](https://github.com/pytorch/pytorch/pull/66419))\r\n* `nn.ZeroPad2d`: Added `extra_repr` for printing purposes ([#69206](https://github.com/pytorch/pytorch/pull/69206))\r\n* `optim.{ChainedScheduler, SequentialLR}`: Added `optimizer` attribute ([#67406](https://github.com/pytorch/pytorch/pull/67406), [#69817](https://github.com/pytorch/pytorch/pull/69817))\r\n* `optim.swa_utils.AveragedModel`: Added `use_buffers` flag for averaging buffers in addition to parameters ([#65921](https://github.com/pytorch/pytorch/pull/65921), [#71763](https://github.com/pytorch/pytorch/pull/71763))\r\n\r\n## torch.fx\r\n\r\n* Improved the customizability of `fx.Graph`\u2019s code generation function, including support for setting a breakpoint in the generated code ([#67139](https://github.com/pytorch/pytorch/pull/67139))\r\n* Supported printing inplace operators in FX ([#71887](https://github.com/pytorch/pytorch/pull/71887))\r\n\r\n## Sparse\r\n\r\n* Add CSR support for several operators:\r\n    * `torch.triangular_solve`, `torch.addmv`, `torch.addmm`, `torch.add`  for all arguments on CPU [(#62180](https://github.com/pytorch/pytorch/pull/62180), [#61536](https://github.com/pytorch/pytorch/pull/61536), [#65606](https://github.com/pytorch/pytorch/pull/65606), [#64391](https://github.com/pytorch/pytorch/pull/64391))\r\n    * `torch.triangular_solve`, `torch.addmv`, `torch.addmm`, `torch.add`  for all arguments on GPU ([#61407](https://github.com/pytorch/pytorch/pull/61407), [#61858](https://github.com/pytorch/pytorch/pull/61858), [#63511](https://github.com/pytorch/pytorch/pull/63511), [#63948](https://github.com/pytorch/pytorch/pull/63948))\r\n    * zero-preserving unary functions ([#68123](https://github.com/pytorch/pytorch/pull/68123), [#69292](https://github.com/pytorch/pytorch/pull/69292))\r\n    * `torch.empty`, `torch.resize_`, `torch.copy_`, `torch.randn_like`, `torch.clone` ([#63509](https://github.com/pytorch/pytorch/pull/63509), [#63510](https://github.com/pytorch/pytorch/pull/63510), [#68083](https://github.com/pytorch/pytorch/pull/68083), [#70581](https://github.com/pytorch/pytorch/pull/70581))\r\n    * `transpose` ([#70582](https://github.com/pytorch/pytorch/pull/70582))\r\n* Added torch.sparse_coo Layout support to `zeros_like` ([#68108](https://github.com/pytorch/pytorch/pull/68108))\r\n* Added Half, BFloat16, and Complex dtype support for matrix multiplication of two COO Tensors on GPU ([#59980](https://github.com/pytorch/pytorch/pull/59980))\r\n* Added support for conversion of CSR to COO Tensor to `to_sparse` ([#66774](https://github.com/pytorch/pytorch/pull/66774))\r\n* Added support for empty COO Tensors to sparse.sum ([#71091](https://github.com/pytorch/pytorch/pull/71091))\r\n\r\n## AMD\r\n\r\n* Added sparse mappings for CUDA->HIP translation ([#67323](https://github.com/pytorch/pytorch/pull/67323))\r\n* Enabled frexp support for ROCm builds ([#67226](https://github.com/pytorch/pytorch/pull/67226))\r\n* Used hipCUB/rocPRIM scan algorithms for large index support ([#68487](https://github.com/pytorch/pytorch/pull/68487))\r\n\r\n## CUDA\r\n\r\n* Allows external CUDA streams to be set as current ([#66324](https://github.com/pytorch/pytorch/pull/66324))\r\n* Added an option to disable reduced precision reductions for FP16 GEMM ([#67946](https://github.com/pytorch/pytorch/pull/67946))\r\n* Improved CUDA memory usage of `nanmedian` result ([#68591](https://github.com/pytorch/pytorch/pull/68591))\r\n* Reduced number of `igamma` kernel instantiations ([#70666](https://github.com/pytorch/pytorch/pull/70666))\r\n* Reduced number of `compare` kernels by unifying them ([#69111](https://github.com/pytorch/pytorch/pull/69111))\r\n* Reduced number of `bernoulli` tensor tensor kernel instantiations ([#70169](https://github.com/pytorch/pytorch/pull/70169))\r\n* Used `cub::FutureValue` to simplify 64bit indexing split of cub scan ([#66711](https://github.com/pytorch/pytorch/pull/66711))\r\n* Added `hascuSOLVER` flag to Context ([#69825](https://github.com/pytorch/pytorch/pull/69825))\r\n* Improved error message from `CUDACachingAllocator` ([#69174](https://github.com/pytorch/pytorch/pull/69174))\r\n* Fixed `masked_softmax` perf for element_size is not 8 ([#70271](https://github.com/pytorch/pytorch/pull/70271))\r\n* Reduced binary size of `TensorCompare.cu` ([#68835](https://github.com/pytorch/pytorch/pull/68835))\r\n* Improved error message for `interpolation` ([#72066](https://github.com/pytorch/pytorch/pull/72066))\r\n* Doesn't compile `pow` kernels for non-existent case ([#70017](https://github.com/pytorch/pytorch/pull/70017))\r\n\r\n## Profiler\r\n\r\n* Added flop count formulas for `bmm` and `baddbmm` ([#66636](https://github.com/pytorch/pytorch/pull/66636))\r\n\r\n## Vulkan\r\n\r\n* Allowed Vulkan models to return multiple outputs by improving Vulkan tensor lifecycle management to release GPU resources when the tensor is destroyed, instead of being released at the end of every inference ([#66477](https://github.com/pytorch/pytorch/pull/66477), [#66478](https://github.com/pytorch/pytorch/pull/66478))\r\n* Enabled multiple Vulkan models to execute concurrently in parallel threads, by moving components of the Vulkan global context into thread local objects ([#67733](https://github.com/pytorch/pytorch/pull/67733), [#69576](https://github.com/pytorch/pytorch/pull/69576))\r\n\r\n## Mobile\r\n\r\n* Introduced multiple improvements for `NNAPI` \r\n    * Added converters for torchscript ops `quantized::mul` and `quantized::convtranspose2d` to converter (`torch.backends._nnapi.prepare.convert_model_to_nnapi`) ([#63913](https://github.com/pytorch/pytorch/pull/63913), [#63914](https://github.com/pytorch/pytorch/pull/63914))\r\n    * Supported `int32` and `qint16` type in Torchscript expressions ([#70197](https://github.com/pytorch/pytorch/pull/70197), [#70621](https://github.com/pytorch/pytorch/pull/70621))\r\n    * Supported runtime flexible shapes and return shapes ([#70334](https://github.com/pytorch/pytorch/pull/70334))\r\n* Improved Model Tracer Coverage and Selective Metal Ops ([#68134, #69492, #69328](https://github.com/pytorch/pytorch/pull/68134))\r\n* Introduced multiple improvements for `CoreML`\r\n    * Fixed error messages ([#67410](https://github.com/pytorch/pytorch/pull/67410))\r\n    * Assigned `computationUnit` to executor ([#67411](https://github.com/pytorch/pytorch/pull/67411))\r\n    * Cleaned up shape information from `TensorSpec` ([#67412](https://github.com/pytorch/pytorch/pull/67412))\r\n* Type Support in Mobile Lite Interpreter\r\n    * Extended `type_parser` to handle `NamedTuple` type ([#63130](https://github.com/pytorch/pytorch/pull/63130), [#62612](https://github.com/pytorch/pytorch/pull/62612))\r\n\r\n## Distributed\r\n\r\n* `torch.distributed`\r\n    * Improvements to error handling in `TCPStore\u2019`s socket implementation (#68225)\r\n    * Enabled `ncclAvg` for reductions ([#62835](https://github.com/pytorch/pytorch/pull/62835))\r\n    * Init dummy `NCCL` comms in constructor ([#65173](https://github.com/pytorch/pytorch/pull/65173), [#66393](https://github.com/pytorch/pytorch/pull/66393))\r\n    * Added pybind trampoline for `ProcessGroup` and `Work` ([#66338](https://github.com/pytorch/pytorch/pull/66338))\r\n    * Setup `c10d` extension Backend class attr the same way as builtin ones ([#66991](https://github.com/pytorch/pytorch/pull/66991))\r\n    * Added barrier to `ProcessGroup` trampoline ([#67236](https://github.com/pytorch/pytorch/pull/67236))\r\n    * Raised warning when calling collectives on non-member group objects ([#67639](https://github.com/pytorch/pytorch/pull/67639))\r\n    * Patched `bfloat16` support for NCCL ([#67843](https://github.com/pytorch/pytorch/pull/67843))\r\n    * Fixed `c10d` TCP store race condition with mutex ([#68499](https://github.com/pytorch/pytorch/pull/68499))\r\n    * Surfaced `ncclUniqueId` store broadcast error ([#68597](https://github.com/pytorch/pytorch/pull/68597))\r\n    * Checks for file existence before invoking cleanup logic in `FileStore` destructor ([#68603](https://github.com/pytorch/pytorch/pull/68603))\r\n    * Implemented gather primitive for `ProcessGroupNCCL` ([#66745](https://github.com/pytorch/pytorch/pull/66745))\r\n    * Implemented scatter primitive for `ProcessGroupNCCL` ([#70029](https://github.com/pytorch/pytorch/pull/70029))\r\n    * Enabled `gather_object` on `NCCL` ([#71623](https://github.com/pytorch/pytorch/pull/71623))\r\n    * Implemented `allreduce_coalesced` for `ProcessGroupNCCL` ([#62140](https://github.com/pytorch/pytorch/pull/62140))\r\n    * Set non-default backend names to lower case ([#69400](https://github.com/pytorch/pytorch/pull/69400))\r\n    * Added support for `deleteKey` for `FileStore` ([#69953](https://github.com/pytorch/pytorch/pull/69953))\r\n    * Fixed `TSAN` issue in `TCPStore` ([#69590](https://github.com/pytorch/pytorch/pull/69590))\r\n\r\n* `DistributedDataParallel`\r\n    * Refactored and removed `sync_params` ([#64514](https://github.com/pytorch/pytorch/pull/64514))\r\n    * Used `named_params` and `named_buffers` explicitly ([#65181](https://github.com/pytorch/pytorch/pull/65181))\r\n    * Allow await of custom buffer reduction in backward ([#64515](https://github.com/pytorch/pytorch/pull/64515))\r\n    * Profiling range for bucket copy ([#65769](https://github.com/pytorch/pytorch/pull/65769))\r\n    * Logs iteration in debug mode ([#65770](https://github.com/pytorch/pytorch/pull/65770))\r\n\r\n* `torch.distributed.rpc`\r\n    * Added a timeout argument to RPC shutdown() ([#65425](https://github.com/pytorch/pytorch/pull/65425))\r\n    * Released GIL during RPC shutdown. ([#69586](https://github.com/pytorch/pytorch/pull/69586))\r\n    * Updated RPC `shutdown()` logic to remove process group usage. ([#65946](https://github.com/pytorch/pytorch/pull/65946))\r\n    * Removal of Process Group dependency for TensorPipe Agent. ([#68128](https://github.com/pytorch/pytorch/pull/68128))\r\n\r\n* `torch.distributed.autograd`\r\n    * Made Kineto + distributed a warning rather than an error ([#71120](https://github.com/pytorch/pytorch/pull/71120))\r\n\r\n* `torch.distributed.elastic`\r\n    * Added ability to override sys.executable for `torch.distributed.run` ([#66179](https://github.com/pytorch/pytorch/pull/66179))\r\n\r\n## TorchScript\r\n\r\n* Several improvements to NVFuser, which is an optimization that speeds up all JIT graphs with a CUDA Tensors on Nvidia GPUs. This includes extending fusing support to normalization and reduction kernels, enabling multiple kernel launch for single `CudaFusionGroup`, and addition of a graph segmentation cache to the hierarchical caching system. ([#63745](https://github.com/pytorch/pytorch/pull/63745), [#65137](https://github.com/pytorch/pytorch/pull/65137), [#63745](https://github.com/pytorch/pytorch/pull/63745), [#65137](https://github.com/pytorch/pytorch/pull/65137))\r\n* Enabled `profile_ivalue` to convert dynamic scalar into compile time constants in NVFuser. (e.g. reduction axes). ([#63745,](https://github.com/pytorch/pytorch/pull/63745) [#65137](https://github.com/pytorch/pytorch/pull/65137))\r\n* Added support in `torch.jit.trace` for tracing already JITted subgraphs([#59949](https://github.com/pytorch/pytorch/pull/59949))\r\n* We now provide full types on graph inputs when tracing graphs that are already JITted([#67424](https://github.com/pytorch/pytorch/pull/67424))\r\n* `torch.jit.freeze` now can preserve attributes of submodules - previously, it was only possible to prevent inlining of attributes of the top level module.([#66102](https://github.com/pytorch/pytorch/pull/66102))\r\n* The peephole optimizer, which is used in `torch.jit.freeze` now coalesces consecutive calls to `torch.concat` into a single call ([#67000](https://github.com/pytorch/pytorch/pull/67000))\r\n* Added ability for Torch.JIT C dispatch to convert python `None` into an undefined Tensor([#67793](https://github.com/pytorch/pytorch/pull/67793))\r\n* `torch.jit.script` now recognizes union of scalars as a JIT NumberType ([#66591](https://github.com/pytorch/pytorch/pull/66591))\r\n* No longer adds a tensor in a returned list to the wildcard alias set in AliasDB, allowing for additional optimizations in JIT optimization passes. ([#71170](https://github.com/pytorch/pytorch/pull/71170))\r\n* In `torch.jit.optimize_for_inference`, there is a new graph pass to precompute transposes for linear layers. ([#65631](https://github.com/pytorch/pytorch/pull/65631), [68024](https://github.com/pytorch/pytorch/pull/68024))\r\n* In `torch.jit.freeze`, there is a new pass where we concat together multiple linear layers with same input Tensor (different weight/bias) ([#63198](https://github.com/pytorch/pytorch/pull/63198), #[68024](https://github.com/pytorch/pytorch/pull/68024))\r\n* Added support for normalizing `torch.Tensor.__rsub__` in `normalize_ops` JIT pass([#65014](https://github.com/pytorch/pytorch/pull/65014))\r\n\r\n## Quantization\r\n\r\n* Quantized op improvements\r\n    * `torch.ao.FakeQuantize` now supports `fp32/fp16` `zero_point`. ([#65836](https://github.com/pytorch/pytorch/pull/65836))\r\n    * `torch.ops.quantized.add` now supports broadcasting ([#66049](https://github.com/pytorch/pytorch/pull/66049))\r\n    * `torch.Tensor.dequantize` now supports fp16 + cuda ([#67234](https://github.com/pytorch/pytorch/pull/67234))\r\n    * Added quantized CPU support for `torch.nn.GELU` ([#69968](https://github.com/pytorch/pytorch/pull/69968))\r\n    * `torch.nn.quantized.functional.hardsigmoid` supports an `inplace` flag ([#65740](https://github.com/pytorch/pytorch/pull/65740))\r\n* Workflow improvements\r\n    * FX graph mode quantization: enable `torch.nn.Linear + torch.nn.BatchNorm1d` fusion for PTQ ([#66484](https://github.com/pytorch/pytorch/pull/66484))\r\n    * Added an option in `torch.ao.quantization.quantize_fx.convert_fx` to accept `qconfig_dict` to skip quantization ([#66878](https://github.com/pytorch/pytorch/pull/66878))\r\n    * Added `torch.nn.qat.dynamic.modules.Linear` module ([#67325](https://github.com/pytorch/pytorch/pull/67325))\r\n    * Added `torch.nn.ConvTranspose{n}d + torch.nn.BatchNorm{n}d` fusion support ([#70022](https://github.com/pytorch/pytorch/pull/70022))\r\n    * Extended `torch.ao.quantization.prepare_qat` with `allow_list` argument, to allow custom mapping and custom QAT module ([#65119](https://github.com/pytorch/pytorch/pull/65119))\r\n    * Added `torch.ao.quantization.default_replay_qconfig` which allows observer reuse for `torch.reshape` in FX graph mode quantization ([#69249](https://github.com/pytorch/pytorch/pull/69249))\r\n\r\n## ONNX\r\n\r\n* Set `ir_version` of the exported model based on `opset_version`. This increases the odds that the exported ONNX model will be usable. Before this change, we were setting the IR version to a hard-coded value which may be higher than what the model consumer supports. ([#67803](https://github.com/pytorch/pytorch/pull/67803))\r\n* Preserved op input names when op just passes through the input to the output ([#67275](https://github.com/pytorch/pytorch/pull/67275))\r\n* Shape inference improvements:\r\n    * Updated slice process shape to support rank only inference ([#66149](https://github.com/pytorch/pytorch/pull/66149))\r\n    * Represent symbolic shape as value ([#69545](https://github.com/pytorch/pytorch/pull/69545))\r\n* Included op type in exported models\u2019 input and output names ([#68976](https://github.com/pytorch/pytorch/pull/68976))\r\n* Supports Conv-BatchNorm fusion inside blocks ([#67272](https://github.com/pytorch/pytorch/pull/67272))\r\n* Exported `torch.reciprocal` to ONNX Reciprocal operator instead of `Div(1, x)` ([#67271](https://github.com/pytorch/pytorch/pull/67271))\r\n* Supports `beta!=1` in softplus ([#66146](https://github.com/pytorch/pytorch/pull/66146))\r\n* Added warning for inplace updates on `tensor.shape` in tracing mode ([#66142](https://github.com/pytorch/pytorch/pull/66142))\r\n* Supports `instance_norm` in training mode ([#64375](https://github.com/pytorch/pytorch/pull/64375))\r\n* Allow registration of custom symbolics for ops specifying aten namespace (i.e. `aten::foo` is allowed as well as \u201cfoo\u201d). ([#67810](https://github.com/pytorch/pytorch/pull/67810))\r\n* Allow registration of custom symbolics for `prim` namespace ([#66139](https://github.com/pytorch/pytorch/pull/66139))\r\n* Supports dynamic inputs for `OneHot`, bool for `Einsum` ([#66147](https://github.com/pytorch/pytorch/pull/66147))\r\n\r\n## Infra (Releng)\r\n\r\n* Build with BUILD_SPLIT_CUDA for all 11.X Windows builds ([#70899](https://github.com/pytorch/pytorch/pull/70899))\r\n\r\n## torch.package\r\n\r\n* Add ability to retrieve the dependency graph via `all_path` function([#65602](https://github.com/pytorch/pytorch/pull/65602))\r\n* Add support for pickle v4 ([#70642](https://github.com/pytorch/pytorch/pull/70642))\r\n* Add better testing support for Package Exporter ([#70641](https://github.com/pytorch/pytorch/pull/70641))\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n* Fixed scalar inputs for aliased binary ops {`multiply`, `subtract`, `divide`} ([#65937](https://github.com/pytorch/pytorch/pull/65937))\r\n* Fixed `torch.save` when saving storages that view same data with different type ([#66949](https://github.com/pytorch/pytorch/pull/66949))\r\n* Fixed `torch.save` error if storages are unallocated ([#68787](https://github.com/pytorch/pytorch/pull/68787))\r\n* Fixed `k` out-of-bounds in `torch.kthvalue` (cpu kernel) ([#68863](https://github.com/pytorch/pytorch/pull/68863))\r\n* Fixed `inference_mode` decorator: `with inference_mode(mode=False)` used to ignore the `mode` argument and always set inference mode. ([#68617](https://github.com/pytorch/pytorch/pull/68617))\r\n* Fixed `cdist_backward` in the case when `cdist` inputs are not contiguous ([#70016](https://github.com/pytorch/pytorch/pull/70016))\r\n* Fixed `cdist` error message typo ([#70178](https://github.com/pytorch/pytorch/pull/70178))\r\n* Fixed `scatter` for empty indexes ([#70662](https://github.com/pytorch/pytorch/pull/70662))\r\n* Fixed `torch.{unique, unique_consecutive}` out of bound ([#71540](https://github.com/pytorch/pytorch/pull/71540))\r\n* Fixed `torch.isin` in the case when inputs are non-contiguous on CPU ([#70659](https://github.com/pytorch/pytorch/pull/70659))\r\n* Fixed `hsplit vsplit dsplit` crash when section is 0 ([#69342](https://github.com/pytorch/pytorch/pull/69342))\r\n* Fixed: `torch.gradient` ignores dim argument when checking edge_order ([#67926](https://github.com/pytorch/pytorch/pull/67926))\r\n* Fixed: `TransformedDistribution.icdf` should perform validation *after* applying the inverse transformation rather than before. ([#71393](https://github.com/pytorch/pytorch/pull/71393))\r\n* Fixed `torch.all and torch.any` internal assert error with requires_grad=True ([#65714](https://github.com/pytorch/pytorch/pull/65714))\r\n* Fixed `torch.logsumexp` type promotion: promote integral inputs to floating for([#63393](https://github.com/pytorch/pytorch/pull/63393))\r\n\r\n## C++ API\r\n\r\n* Fixed libtorch `at::Tensor::print()` linking error ([#69615](https://github.com/pytorch/pytorch/pull/69615))\r\n* Avoided UB when indexing into size-0 tensors ([#65878](https://github.com/pytorch/pytorch/pull/65878))\r\n* Fixed an ICE when compiling PyTorch from source on MacOS with clang-1300 ([#65655](https://github.com/pytorch/pytorch/pull/65655))\r\n\r\n## Autograd\r\n\r\n* Fixed autocast state propagation in the `torch.utils.checkpoint` API ([#71169](https://github.com/pytorch/pytorch/pull/71169))\r\n* Fixed `torch.nn.functional.conv_transpose3d` backward when grad_out is non-contiguous ([#67829](https://github.com/pytorch/pytorch/pull/67829))\r\n* Forward mode AD:\r\n    * Fixed a case where forward AD in-place-over-view silently copies the view ([#67816](https://github.com/pytorch/pytorch/pull/67816))\r\n    * Fixed deadlock in forward AD for functions that return multiple outputs ([#67995](https://github.com/pytorch/pytorch/pull/67995))\r\n    * Fixed forward AD codegen for functions that have multiple formulas ([#68535](https://github.com/pytorch/pytorch/pull/68535))\r\n    * Fixed deadlock when forward and backward AD are used at the same time ([#67360](https://github.com/pytorch/pytorch/pull/67360))\r\n    * Fixed  `Tensor.copy_` forward AD to handle broadcasting ([#69592](https://github.com/pytorch/pytorch/pull/69592))\r\n    * Do not generate not_implemented error for forward AD when input with tangent passed to non-differentiable function ([#66926](https://github.com/pytorch/pytorch/pull/66926))\r\n* Fixed `autograd.Function` when non-Tensor argument precedes tensor argument ([#71530](https://github.com/pytorch/pytorch/pull/71530))\r\n* Fixed `autograd.Function` forward AD when forward is a no-op to no longer raise an internal error ([#71531](https://github.com/pytorch/pytorch/pull/71531))\r\n\r\n## Build\r\n\r\n* Stopped reporting CPU Capability as AVX512 on machines with AVX512 support but without AVX512 kernels ([#66703](https://github.com/pytorch/pytorch/pull/66703))\r\n* Disabled SVE when cross-compiling for M1 ([#67114](https://github.com/pytorch/pytorch/pull/67114))\r\n* Added failure if `pocketfft` is not found and `at_mkl` is not enabled ([#67909](https://github.com/pytorch/pytorch/pull/67909))\r\n* Fixed clang issues when compiling with `_GLIBCXX_USE_CXX11_ABI` ([#72081](https://github.com/pytorch/pytorch/pull/72081))\r\n\r\n## Complex Numbers\r\n\r\n* Fixed `torch.autograd.gradcheck` to generate valid inputs for forward AD computation for complex functions ([#68001](https://github.com/pytorch/pytorch/pull/68001))\r\n* Fixed `torch.Tensor.copy_` transpose path for tensors with conjugate or negative bit set ([#69026](https://github.com/pytorch/pytorch/pull/69026))\r\n* Fixed `torch.Tensor.copy_` behavior for the case when two conjugated or negated tensors of the same dtype (one or both of which are non-contiguous) are copied into each other ([#68963](https://github.com/pytorch/pytorch/pull/68963))\r\n\r\n## Dataloader\r\n\r\n* Made `ProcessException` picklable ([#70118](https://github.com/pytorch/pytorch/pull/70118))\r\n* Fixed persistent worker exiting before `pin_memory_thread` ([#71579](https://github.com/pytorch/pytorch/pull/71579))\r\n\r\n## torch.nn\r\n\r\n* `nn.AdaptiveAvgPool*d`: Throws an error for negative `output_size` ([#70488](https://github.com/pytorch/pytorch/pull/70488))\r\n* `nn.Conv1d`: Fixed for 1D convolution on MKL-DNN backend ([#68166](https://github.com/pytorch/pytorch/pull/68166))\r\n* `nn.CrossEntropyLoss`: Fixed for usage of `weight`, `ignore_index`, and `label_smoothing` together ([#69511](https://github.com/pytorch/pytorch/pull/69511))\r\n* `nn.Fold`: Checked that block height and width are positive ([#69048](https://github.com/pytorch/pytorch/pull/69048))\r\n* `nn.LayerNorm`: Fixed incorrect result on CUDA when `gamma` or `bias` are missing ([#69210](https://github.com/pytorch/pytorch/pull/69210))\r\n* `nn.LayerNorm`: Avoided overflow by doing computation in `float` for `half` ([#66920](https://github.com/pytorch/pytorch/pull/66920))\r\n* `nn.Module`: Throws a proper error message from `load_state_dict` for non-tensor values ([#70596](https://github.com/pytorch/pytorch/pull/70596))\r\n* `nn.ModuleList`: Fixed incorrect return type in `__getitem__` ([#69083](https://github.com/pytorch/pytorch/pull/69083))\r\n* `nn.MultiheadAttention`: Used query dtype for mask type ([#68077](https://github.com/pytorch/pytorch/pull/68077))\r\n* `nn.NLLLoss`: Fixed backward computation with negative weights ([#64572](https://github.com/pytorch/pytorch/pull/64572))\r\n* `nn.{RNN, GRU}`: Fixed RNN modules with input shapes containing-0 in CUDA ([#71696](https://github.com/pytorch/pytorch/pull/71696))\r\n* `nn.utils.rnn.pad_sequence`: Fix regression to support tuples for padding ([#72436](https://github.com/pytorch/pytorch/pull/72436))\r\n* `optim._LrScheduler`: Fixed print formatting ([#68338](https://github.com/pytorch/pytorch/pull/68338))\r\n* `optim.ChainedScheduler`: Fixed `get_last_lr()` ([#69112](https://github.com/pytorch/pytorch/pull/69112))\r\n* `optim.CosineAnnealingWarmRestarts`: Fixed ordering bug when `last_epoch > 0` ([#64758](https://github.com/pytorch/pytorch/pull/64758))\r\n* `optim.SequentialLR`: Updated `_last_lr` on step ([#70558](https://github.com/pytorch/pytorch/pull/70558))\r\n\r\n## torch.fx\r\n\r\n* Supported `torch.layout` as arg ([#66048](https://github.com/pytorch/pytorch/pull/66048))\r\n* Specified a default value when possible for placeholders created from `concrete_args` ([#59569](https://github.com/pytorch/pytorch/pull/59569))\r\n* Fixed issue where `GraphModule.delete_all_unused_submodules` deletes submodules from called leaf modules ([#66430](https://github.com/pytorch/pytorch/pull/66430))\r\n* Fixed `torch.fx.subgraph_rewriter.replace_pattern` mechanism so that multiple one-liner instances of the pattern are captured correctly ([#66442](https://github.com/pytorch/pytorch/pull/66442))\r\n* Fixed bug in graph matcher that caused certain nodes to be matched twice ([#69238](https://github.com/pytorch/pytorch/pull/69238))\r\n* Ensured node stack trace survives copying ([#69368](https://github.com/pytorch/pytorch/pull/69368))\r\n* Fixed `to_folder` not saving dtype ([#69983](https://github.com/pytorch/pytorch/pull/69983))\r\n* Added a `default_value` arg to `fx.Graph.placeholder` and fix `split_module` ([#71016](https://github.com/pytorch/pytorch/pull/71016))\r\n\r\n## Sparse\r\n\r\n* Fixed CSR storage access to throw when used ([#70072](https://github.com/pytorch/pytorch/pull/70072))\r\n* Fixed multiplication of 0-D sparse tensors ([#70749](https://github.com/pytorch/pytorch/pull/70749))\r\n* Fixed result dtype for neg if given sparse Tensor ([#68885](https://github.com/pytorch/pytorch/pull/68885))\r\n\r\n## CUDA\r\n\r\n* Fixed CUDA vs CPU consistency for index_put_ when accumulating ([#66790](https://github.com/pytorch/pytorch/pull/66790))\r\n* Fixed CUDA vs CPU consistency for index_put_ when accumulating (part 2) ([#67189](https://github.com/pytorch/pytorch/pull/67189))\r\n* Fixed error in warning about unsupported GPU ([#67900](https://github.com/pytorch/pytorch/pull/67900))\r\n* Disabled TF32 in `pinv_jvp` and `pinv_backward` ([#67948](https://github.com/pytorch/pytorch/pull/67948))\r\n* Fixed DLPack CUDA stream convention ([#67618](https://github.com/pytorch/pytorch/pull/67618))\r\n* Sets device guard in `_cudnn_impl` functions ([#70406](https://github.com/pytorch/pytorch/pull/70406))\r\n* Fixed `mem_get_info` when querying on a device other than the current device ([#69640](https://github.com/pytorch/pytorch/pull/69640))\r\n\r\n## Benchmark\r\n\r\n* Fixed divide-by-zero errors in `torch.utils.benchmark.Timer` ([#70050](https://github.com/pytorch/pytorch/pull/70050))\r\n\r\n## Dispatcher\r\n\r\n* Added explicit `OperatorHandle` destructor, so that the symbol shows up in windows builds ([#70033](https://github.com/pytorch/pytorch/pull/70033))\r\n\r\n## Profiler\r\n\r\n* Fixed race condition in profiler ([#65812](https://github.com/pytorch/pytorch/pull/65812))\r\n* Fixed TensorBoard memory profiling ([#71417](https://github.com/pytorch/pytorch/pull/71417))\r\n\r\n## Visualization\r\n\r\n* Fixed `torch.utils.tensorboard` parsing JIT graph incorrectly ([#65692](https://github.com/pytorch/pytorch/pull/65692))\r\n\r\n## Vulkan\r\n\r\n* Greatly reduced memory usage of the Vulkan backend by updating the configuration of the Vulkan Memory Allocator ([#69088](https://github.com/pytorch/pytorch/pull/69088))\r\n* Addressed several warnings raised by the Vulkan Validation layers:\r\n    * Updated all texture resources to have the same dimensionality ([#67647](https://github.com/pytorch/pytorch/pull/67647))\r\n    * Added image format qualifier to shader files ([#69330](https://github.com/pytorch/pytorch/pull/69330))\r\n    * Disabled SPIR-V compiler size optimization ([#69331](https://github.com/pytorch/pytorch/pull/69331))\r\n\r\n## Mobile\r\n\r\n* Fixed quantized logistic converter for `NNAPI` ([#70847](https://github.com/pytorch/pytorch/pull/70847))\r\n* Fixed potential crash if `MTLCreateSystemDefaultDevice` returns nil ([#66859](https://github.com/pytorch/pytorch/pull/66859))\r\n* Used full name to look for the promoted prim operator table ([#66081](https://github.com/pytorch/pytorch/pull/66081))\r\n* Fixed function name bug in mobile export ([#66915](https://github.com/pytorch/pytorch/pull/66915))\r\n* Fixed issues with `irange` not having a header included in `Metal` ([#66877](https://github.com/pytorch/pytorch/pull/66877))\r\n* Fixed backward compatibility issue for UnionType on mobile in `type_parser`. ([#71341](https://github.com/pytorch/pytorch/pull/71341))\r\n* Fixed forward flatbuffer type handling with dynamic type in `flatbuffer_loader`. ([#71500](https://github.com/pytorch/pytorch/pull/71500))\r\n* Fixed type equalities issue in `pytorch_jni_common` ([#71508](https://github.com/pytorch/pytorch/pull/71508))\r\n* Fixed missing properties to the executor in `CoreML` ([#67737](https://github.com/pytorch/pytorch/pull/67737/files))\r\n* Fixed memory computation when both constants and data tensors are present in model_dump ([#66006](https://github.com/pytorch/pytorch/pull/66006))\r\n* Ensured that function participating in bundled inputs have their \u201c__name__\" attribute set ([#65856](https://github.com/pytorch/pytorch/pull/65856))\r\n\r\n## Distributed\r\n\r\n* `torch.distributed`\r\n    * Fixed bug on empty `GLOO_SOCKET_IFNAME_ENV` ([#68933](https://github.com/pytorch/pytorch/pull/68933))\r\n* `DistributedDataParallel`\r\n    * Fixed \u201cCannot modify in-place due to DDPSink\u201d ([#66015](https://github.com/pytorch/pytorch/pull/66015))\r\n* `torch.distributed.elastic`\r\n    * Fixed scale down bug caused by calling `rdzv_handler.shutdown()` on premature agent failures ([#67749](https://github.com/pytorch/pytorch/pull/67749))\r\n\r\n## TorchScript\r\n\r\n* Fixed a race condition in the JIT interpreter when unpickling source ranges ([5525e9a591](https://github.com/pytorch/pytorch/commit/5525e9a5910a01b880f5f34827c43c29a1473775))\r\n* Fixed a ref counting loop for `CompilationUnit`, resulting in memory leaks when class objects were in JIT graphs. ([#65442](https://github.com/pytorch/pytorch/pull/65442))\r\n* Fixed bug where output type was discarded after calling SubgraphRewriter in C++ ([#65453](https://github.com/pytorch/pytorch/pull/65453))\r\n* Fixed bug where `torch.jit.optimize_for_inference` did not `torch.jit.freeze` a module when passed a a non-frozen module ([#71436](https://github.com/pytorch/pytorch/pull/71436))\r\n* Fixed bug where running module.forward() on a `torch.jit.freeze` ed  module ran the wrong graph ([#68316](https://github.com/pytorch/pytorch/pull/68316))\r\n* Fixed bug where alias analysis in the JIT optimizer was incorrect for the int[] version of `torch.split` , resulting in invalid optimizations in various JIT optimization passes ([#69745](https://github.com/pytorch/pytorch/pull/69745))\r\n* Fixed places where using `torch.autocast`  together with autodiff (module.backwards())  in a JIT graph had the wrong number of arguments and would error out. ([#67648](https://github.com/pytorch/pytorch/pull/67648))\r\n* Forbid propagating gradients through views in JIT graphs as currently it is broken ([#67732](https://github.com/pytorch/pytorch/pull/67732))\r\n* Fixed bug where graph input types were incorrect after running `torch.jit.trace` ([#68242](https://github.com/pytorch/pytorch/pull/68242))\r\n* Fixed case where BroadcastMKLDNN breaks the stack invariant by pushing more than 2 tensors to the stack for when `torch.jit.freeze` ops are converted to MKLDNN([#66628](https://github.com/pytorch/pytorch/pull/66628))\r\n* Raised error instead of segfaulting when passing None into torch.jit.Graph.create ([#68253](https://github.com/pytorch/pytorch/pull/68253))\r\n* Raised error instead of crashing when a JIT ScriptFunction is pickled with an incompatible Python `pickle` version.([#69807](https://github.com/pytorch/pytorch/pull/69807))\r\n* Fixed bug where `torch.jit.script` fails when comments in function has less indent than surrounding code ([#70227](https://github.com/pytorch/pytorch/pull/70227))\r\n* Fixed incorrect device type when torch.device is called inside scripted (`torch.jit.script`) code ([#69645](https://github.com/pytorch/pytorch/pull/69645))\r\n* Fixed warning: overloaded virtual function `torch::jit::Function::call` is only partially overridden in class `torch::jit::GraphFunction` ([4bf1be898d](https://github.com/pytorch/pytorch/commit/4bf1be898d))\r\n\r\n## Quantization\r\n\r\n* Fixed applying non-zero offset 1 to null pointer in `torch.nn.functional.interpolate` for quantized tensors ([#65570](https://github.com/pytorch/pytorch/pull/65570))\r\n* Doesn't assume bias is a keyword argument to `torch.nn.Conv{n}d` ([#61647](https://github.com/pytorch/pytorch/pull/61647), [#71426](https://github.com/pytorch/pytorch/pull/71426))\r\n* Made error message when trying to use `torch.quantize_per_tensor` on non floats more specific ([#66050](https://github.com/pytorch/pytorch/pull/66050))\r\n* Quantized `torch.nn.Embedding` conversion with unsupported dtype: make error message clearer ([#66051](https://github.com/pytorch/pytorch/pull/66051))\r\n* Fixed `torch.nn.qat.EmbeddingBag` from_float error message ([#66989](https://github.com/pytorch/pytorch/pull/66989))\r\n* Fixed bug enforcing quant_min <= zero_point <= quant_max for float zeropoint in `torch.nn.Embedding` QAT ([#68852](https://github.com/pytorch/pytorch/pull/68852))\r\n* Fixed scale+zp serialization of `torch.nn.quantized.BatchNorm{2|3}d` ([#70432](https://github.com/pytorch/pytorch/pull/70432))\r\n* Fixed `torch.nn.Dropout` in FX graph mode quantization ([#71043](https://github.com/pytorch/pytorch/pull/71043), [#71438](https://github.com/pytorch/pytorch/pull/71438))\r\n* Fixed `qconfig` setting for fused modules in FX graph mode quantization ([#71254](https://github.com/pytorch/pytorch/pull/71254))\r\n* Removed assumption number of rows is in 32 bit in fbgemm ([#69066](https://github.com/pytorch/pytorch/pull/69066))\r\n* Fixed `reduce_range` warning when using default observers ([#71027](https://github.com/pytorch/pytorch/pull/71027))\r\n\r\n## ONNX\r\n\r\n* Doesn\u2019t create invalid `index_select` op when constant folding through ONNX Gather with indices rank > 1. Fixes export of some uses of Embedding. ([#68493](https://github.com/pytorch/pytorch/pull/68493))\r\n* Shape inference:\r\n    * ConstantMap setters to update existing value instead of emplace, and fix default value of `keepdims` for Reduce ([#67812](https://github.com/pytorch/pytorch/pull/67812))\r\n    * Fixed memory leak ([#68210](https://github.com/pytorch/pytorch/pull/68210))\r\n    * Fixed reshape shape inference regression affecting LSTM ([#72532](https://github.com/pytorch/pytorch/pull/72532))\r\n* Fixed inplace `fill_` dtype export mismatch ([#64580](https://github.com/pytorch/pytorch/pull/64580))\r\n* Fixed `remainder` ([#64578](https://github.com/pytorch/pytorch/pull/64578))\r\n* Fixed `reciprocal` when input is not floating point ([#67808](https://github.com/pytorch/pytorch/pull/67808))\r\n* Fixed `new_full` and `full_like` for Python 3.9 ([#67806](https://github.com/pytorch/pytorch/pull/67806))\r\n* Fixed reduce ops on `binary_cross_entropy_with_logits` ([#67805](https://github.com/pytorch/pytorch/pull/67805))\r\n* Propagated node metadata across passes ([#45256](https://github.com/pytorch/pytorch/pull/45256))\r\n* Ensured outputs don\u2019t have the same name ([#66137](https://github.com/pytorch/pytorch/pull/66137))\r\n* Fixed `pad` with sequence inputs ([#64377](https://github.com/pytorch/pytorch/pull/64377))\r\n* Fixed `instance_norm` with `track_running_stats=True` ([#64375](https://github.com/pytorch/pytorch/pull/64375))\r\n* Fixed `all` and `any` with `dim` arg ([#67270](https://github.com/pytorch/pytorch/pull/67270))\r\n* Allows autograd functions (`prim::PythonOp`) to be exported with `OperatorExportTypes.ONNX_FALLTHROUGH` ([#67273](https://github.com/pytorch/pytorch/pull/67273))\r\n\r\n## torch.package\r\n\r\n* Prevent import race condition that leaves `torch.package.PackagePickler` with unwanted dispatch table entries. ([#71025](https://github.com/pytorch/pytorch/pull/71025))\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n* Speed up pickling for `torch.dtype` ([#65182](https://github.com/pytorch/pytorch/pull/65182))\r\n* Speed up `histogram`: avoid index_put_ overhead in histogram kernel's inner loop ([#67815](https://github.com/pytorch/pytorch/pull/67815))\r\n* Speed up `torch.topk` with sort for some cases ([#68632](https://github.com/pytorch/pytorch/pull/68632))\r\n* Speed up `torch.stack`: don't unsqueeze every stack arg if possible ([#70288](https://github.com/pytorch/pytorch/pull/70288))\r\n* Speed up `LayerNorm` 4-5% ([#71423](https://github.com/pytorch/pytorch/pull/71423))\r\n* Speed up structured kernels: fix some unnecessary refcount bumps ([#71140](https://github.com/pytorch/pytorch/pull/71140))\r\n* Speed up `indexing` functions: release GIL in a few places ([#71728](https://github.com/pytorch/pytorch/pull/71728))\r\n* Speed up `torch.empty` a bit: define check_sizes_nonnegative as inline ([#71640](https://github.com/pytorch/pytorch/pull/71640))\r\n* Speed up `XLA` tensor printing by reducing compilations ([#71147](https://github.com/pytorch/pytorch/pull/71147))\r\n\r\n## C++ API\r\n\r\n* Updated `c10::SmallVector` from LLVM ([#69110](https://github.com/pytorch/pytorch/pull/69110))\r\n* Reduced some framework overhead in `at::copy_()` ([#68950](https://github.com/pytorch/pytorch/pull/68950))\r\n* Reduced some overhead in `StorageImpl::set_data_ptr` ([#65432](https://github.com/pytorch/pytorch/pull/65432))\r\n* Improved `IValue` performance for tuples by inlining tuple storage ([#64066](https://github.com/pytorch/pytorch/pull/64066))\r\n\r\n## Autograd\r\n\r\n* Stopped materializing Tensors full of 0s in forward AD when possible ([#64837](https://github.com/pytorch/pytorch/pull/64837))\r\n* Rewrote the backward of `linalg.lu` and `linalg.lu_solve` to use `linalg_solve_triangular` ([#63569](https://github.com/pytorch/pytorch/pull/63569)) \r\n* Updated `nn.functional.grid_sample` backward to compute input gradient only if required ([#66069](https://github.com/pytorch/pytorch/pull/66069), [#66070](https://github.com/pytorch/pytorch/pull/66070))\r\n* Stopped erroneously saving the output of `torch.softplus` for backward ([#70296](https://github.com/pytorch/pytorch/pull/70296))\r\n\r\n## Complex Numbers\r\n\r\n* Release GIL when assigning to real or imaginary components of a complex tensor ([#71747](https://github.com/pytorch/pytorch/pull/71747))\r\n* Restored conjugate and negative bits of a tensor when calling `repeat_interleave`  ([#68523](https://github.com/pytorch/pytorch/pull/68523))\r\n\r\n## CUDA\r\n\r\n* Used a better hash table in `CUDACachingAllocator` ([#71667](https://github.com/pytorch/pytorch/pull/71667))\r\n*  `TopK` CUDA Optimization: used multiple block per slice ([#71081](https://github.com/pytorch/pytorch/pull/71081))\r\n* Removed sync in `Embedding` caused by `unique` ([#66091](https://github.com/pytorch/pytorch/pull/66091))\r\n* `EmbeddingBackward` exclusive_scan thrust->cub ([#66566](https://github.com/pytorch/pytorch/pull/66566))\r\n* `sort_out_cuda`: Used custom kernels to fill index tensors ([#66668](https://github.com/pytorch/pytorch/pull/66668))\r\n* `masked_scatter`: fuse mask count check into one kernel ([#66871](https://github.com/pytorch/pytorch/pull/66871))\r\n* Enabled better depthwise conv perf on cudnn 8.2+ ([#58749](https://github.com/pytorch/pytorch/pull/58749))\r\n* Improved native `layer_norm` forward perf ([#67977](https://github.com/pytorch/pytorch/pull/67977))\r\n* Improved native `layer_norm` backward perf ([#68238](https://github.com/pytorch/pytorch/pull/68238))\r\n* Fast path for size 0 GPU host malloc ([#68532](https://github.com/pytorch/pytorch/pull/68532))\r\n* Alternative implementation of CUDA pinned memory allocator focusing on multi-threaded scalability ([#69299](https://github.com/pytorch/pytorch/pull/69299))\r\n* Used legacy unrolled kernel for non-trivial offset calc cases ([#71710](https://github.com/pytorch/pytorch/pull/71710))\r\n* Removed `call_once` from `CUDACachingAllocator` ([#71668](https://github.com/pytorch/pytorch/pull/71668))\r\n* Reworked stat collection in `CUDACachingAllocator` ([#71669](https://github.com/pytorch/pytorch/pull/71669))\r\n* Fixed CUDA `LpNormFunctor` ([#70601](https://github.com/pytorch/pytorch/pull/70601))\r\n\r\n## Dispatcher\r\n\r\n* Made `c10::KernelFunction` struct smaller, which should reduce some memory usage by the dispatcher ([#65618](https://github.com/pytorch/pytorch/pull/65618))\r\n\r\n## torch.fx\r\n\r\n* Made `torch.fx.symbolic_trace` reuse buffers if they're the same ([#66211](https://github.com/pytorch/pytorch/pull/66211))\r\n\r\n## Profiler\r\n\r\n* Optimized profiler internals ([#68397](https://github.com/pytorch/pytorch/pull/68397), [#68411](https://github.com/pytorch/pytorch/pull/68411), [#69737](https://github.com/pytorch/pytorch/pull/69737), [#68412](https://github.com/pytorch/pytorch/pull/68412), [#70001](https://github.com/pytorch/pytorch/pull/70001), [#70002](https://github.com/pytorch/pytorch/pull/70002), [#70133](https://github.com/pytorch/pytorch/pull/70133))\r\n\r\n## Mobile\r\n\r\n* Reduced PyTorch Library startup time by 40% for mobile and edge deployments([#65735](https://github.com/pytorch/pytorch/pull/65735), [#65732](https://github.com/pytorch/pytorch/pull/65732), [#65939](https://github.com/pytorch/pytorch/pull/65939), [#66112](https://github.com/pytorch/pytorch/pull/66112), [#66064](https://github.com/pytorch/pytorch/pull/66064), [#66131](https://github.com/pytorch/pytorch/pull/66131))\r\n* Reduced PyTorch Library heap memory utilization by 40% for mobile and edge deployments([#65732](https://github.com/pytorch/pytorch/pull/65732), [#66112](https://github.com/pytorch/pytorch/pull/66112), [#66064](https://github.com/pytorch/pytorch/pull/66064), [#66131](https://github.com/pytorch/pytorch/pull/66131))\r\n* Improve efficiency of IValue and reduce overhead in code paths that use IValue and perform Type Parsing ([#65710](https://github.com/pytorch/pytorch/pull/65710), [#64278](https://github.com/pytorch/pytorch/pull/64278), [#66717](https://github.com/pytorch/pytorch/pull/66717), [#65381](https://github.com/pytorch/pytorch/pull/65381), [#66134](https://github.com/pytorch/pytorch/pull/66134), [#65951](https://github.com/pytorch/pytorch/pull/65951), [#70477](https://github.com/pytorch/pytorch/pull/70477))\r\n\r\n## TorchScript\r\n\r\n* Improved performance of autodiff on small JIT graphs ([#71666](https://github.com/pytorch/pytorch/pull/71666))\r\n* Enabled autocasting of tensors between fp16, bfloat 16 and fp32 in torchscript models ([#63939](https://github.com/pytorch/pytorch/pull/63939), [#67707](https://github.com/pytorch/pytorch/pull/67707))\r\n* Enables optimizations in more gradSumToSize cases in the JIT Autograd support([#63941](https://github.com/pytorch/pytorch/pull/63941))\r\n* In Unpickling a JIT graph, avoid reading file from a stream for 0 byte tensor storage([#67787](https://github.com/pytorch/pytorch/pull/67787))\r\n\r\n## Quantization\r\n\r\n* Sped up quantized `torch.nn.functional.interpolate` for channels last ([#66525](https://github.com/pytorch/pytorch/pull/66525))\r\n* Sped up `torch.nn.functional.upsample` for channels last ([#70903](https://github.com/pytorch/pytorch/pull/70903))\r\n* Parallelized computation in `torch.quantize_per_tensor_affine` and `torch.dequantize` ([#65845](https://github.com/pytorch/pytorch/pull/65845))\r\n\r\n# Documentation\r\n\r\n## Python API\r\n\r\n* Added docs for `torch.adjoint`. ([#68869](https://github.com/pytorch/pytorch/pull/68869))\r\n* Clarified difference in behavior of `empty_strided` and `as_strided` ([#64568](https://github.com/pytorch/pytorch/pull/64568))\r\n* Added some missing generated doc entries (`torch.select`, `torch.slice_scatter`, `torch.diagonal_scatter`, `torch.select_scatter`) ([#69030](https://github.com/pytorch/pytorch/pull/69030)),  `histogramdd`  ([#68273](https://github.com/pytorch/pytorch/pull/68273))\r\n* Typo and formatting fixes. `LinearLR`  ([#67840](https://github.com/pytorch/pytorch/pull/67840)), `torch.any` ([#65310](https://github.com/pytorch/pytorch/pull/65310), [#70187](https://github.com/pytorch/pytorch/pull/70187)), `torch.futures`  ([#70630](https://github.com/pytorch/pytorch/pull/70630)), jit docs ([#68557](https://github.com/pytorch/pytorch/pull/68557)), `Tensor.type`  ([#67019](https://github.com/pytorch/pytorch/pull/67019)), `torch.lobpcg` ([#71464](https://github.com/pytorch/pytorch/pull/71464)), `Tensor.triu()`, `Tensor.tril()`, `Tensor.ravel()`. ([#71057](https://github.com/pytorch/pytorch/pull/71057)), `torch.acosh` ([#66814](https://github.com/pytorch/pytorch/pull/66814)), ([#70439](https://github.com/pytorch/pytorch/pull/70439))\r\n* General Doc improvements for individual ops.  `torch.finfo` (mention `torch.bfloat16`) ([#68496](https://github.com/pytorch/pytorch/pull/68496)), `torch.quantile` interpolation kwarg ([#70637](https://github.com/pytorch/pytorch/pull/70637)), `from_dlpack` and `to_dlpack` ([#70437](https://github.com/pytorch/pytorch/pull/70437)), `set_printoptions` added examples ([#68324](https://github.com/pytorch/pytorch/pull/68324)), `index_add` ([#65806](https://github.com/pytorch/pytorch/pull/65806)), topk doc ([#65938](https://github.com/pytorch/pytorch/pull/65938)), `unique` ([#66132](https://github.com/pytorch/pytorch/pull/66132)), `chi2`  ([#67379](https://github.com/pytorch/pytorch/pull/67379)), `torch.histc` ([#64191](https://github.com/pytorch/pytorch/pull/64191)),  `empty` and `empty_like`  ([#68874](https://github.com/pytorch/pytorch/pull/68874)), `torch.cholesky_inverse` ([#69069](https://github.com/pytorch/pytorch/pull/69069)), `torch.dsplit`  ([#70557](https://github.com/pytorch/pytorch/pull/70557))\r\n* Changed README getting started link to explicit instructions ([#66828](https://github.com/pytorch/pytorch/pull/66828))\r\n* Modernized and clarified docs for `torch.tensor` and `torch.as_tensor` ([#63308](https://github.com/pytorch/pytorch/pull/63308))\r\n* Improved `torchhub` docs ([#69970](https://github.com/pytorch/pytorch/pull/69970))\r\n* Updated docs for `torch.Tensor.real` to indicate that it's supported for real tensors ([#71962](https://github.com/pytorch/pytorch/pull/71962))\r\n\r\n## C++ API\r\n\r\n* Fixed typos in ATen README ([#69170](https://github.com/pytorch/pytorch/pull/69170))\r\n* Mentioned `TORCH_SHOW_CPP_STACKTRACES` in `Contributing.md` docs ([#64052](https://github.com/pytorch/pytorch/pull/64052))\r\n* Updated link to C++ frontend examples ([#66095](https://github.com/pytorch/pytorch/pull/66095))\r\n* Added docs for Visual Studio extension ([#63944](https://github.com/pytorch/pytorch/pull/63944))\r\n* Added docs about an issue with compiling C++ extensions with CUDA 11.5 and Windows ([#73013](https://github.com/pytorch/pytorch/pull/73013))\r\n\r\n## Autograd\r\n\r\n* Updated docs for forward AD and make them public ([#71643](https://github.com/pytorch/pytorch/pull/71643), [#71159](https://github.com/pytorch/pytorch/pull/71159))\r\n* Updated \u201cExtending PyTorch\u201d doc to cover forward AD ([#66962](https://github.com/pytorch/pytorch/pull/66962))\r\n* Fixed broken code syntax in autograd.rst ([#69362](https://github.com/pytorch/pytorch/pull/69362))\r\n* Fixed incorrect variable in autograd docs ([#70884](https://github.com/pytorch/pytorch/pull/70884))\r\n* Fixed typo in `torch.autograd.Function` docs that prevented it from compiling ([#66754](https://github.com/pytorch/pytorch/pull/66754))\r\n\r\n## Dataloader\r\n\r\n* Added docstring for `default_collate` and `default_convert` ([#69862](https://github.com/pytorch/pytorch/pull/69862))\r\n* Updated the documentation for AMP with DataParallel ([#69218](https://github.com/pytorch/pytorch/pull/69218))\r\n\r\n## torch.nn\r\n\r\n* `F.binary_cross_entropy`: Updated examples to avoid deprecated calls ([#69816](https://github.com/pytorch/pytorch/pull/69816))\r\n* `F.linear`: Fixed shape docs to indicate no-batch-dim support ([#66884](https://github.com/pytorch/pytorch/pull/66884))\r\n* `F.max_pool*d`: Added functional docs ([#63264](https://github.com/pytorch/pytorch/pull/63264))\r\n* `F.multilabel_soft_margin_loss`: Added reduction args to signature ([#70420](https://github.com/pytorch/pytorch/pull/70420))\r\n* `nn.AdaptiveLogSoftmaxWithLoss`: Fixed typo in `log_prob` name ([#68926](https://github.com/pytorch/pytorch/pull/68926))\r\n* `nn.{BatchNorm1d, InstanceNorm1d}`: Fixed input shape notation inconsistencies ([#71371](https://github.com/pytorch/pytorch/pull/71371))\r\n* `nn.CrossEntropyLoss`: Corrected typo in formula for class probability targets ([#70220](https://github.com/pytorch/pytorch/pull/70220))\r\n* `nn.{ELU, Hardshrink, Hardsigmoid, MultiHeadAttention, Softplus, Tanh}`: Made first line of docstring readable for overview docs ([#70574](https://github.com/pytorch/pytorch/pull/70574), [#71012](https://github.com/pytorch/pytorch/pull/71012), [#70987](https://github.com/pytorch/pytorch/pull/70987), [#71100](https://github.com/pytorch/pytorch/pull/71100), [#70576](https://github.com/pytorch/pytorch/pull/70576), [#70577](https://github.com/pytorch/pytorch/pull/70577))\r\n* `nn.Flatten`: Simplified example code ([#67472](https://github.com/pytorch/pytorch/pull/67472))\r\n* `nn.{Hardsigmoid, Hardswish, Mish, RReLU, SiLU}`: Added activation function images ([#65415](https://github.com/pytorch/pytorch/pull/65415))\r\n* `nn.KLDivLoss`: Fixed rendering of `reduction` arg ([#66583](https://github.com/pytorch/pytorch/pull/66583))\r\n* `nn.KLDivLoss`: Rewrote docs to clarify math ([#67443](https://github.com/pytorch/pytorch/pull/67443))\r\n* `nn.MaxUnpool2d`: Changed misleading example to better demonstrate `output_size` usage ([#68936](https://github.com/pytorch/pytorch/pull/68936))\r\n* `nn.Module`: Added note describing required `super().__init__()` call ([#66909](https://github.com/pytorch/pytorch/pull/66909))\r\n* `nn.Module`: Changed `super()` usage to Python 3 syntax in example ([#65748](https://github.com/pytorch/pytorch/pull/65748))\r\n* `nn.Module`: Fixed formatting for `named_modules()` ([#70491](https://github.com/pytorch/pytorch/pull/70491))\r\n* `nn.NLLLoss`: Corrected default value for `reduce` ([#68426](https://github.com/pytorch/pytorch/pull/68426))\r\n* `nn.SmoothL1Loss`: Clarified equivalence with `nn.L1Loss` when `beta == 0` ([#70673](https://github.com/pytorch/pytorch/pull/70673))\r\n* `nn.{TransformerDecoderLayer, TransformerEncoderLayer}`: Clarified default `batch_first=False` dimension format ([#66574](https://github.com/pytorch/pytorch/pull/66574))\r\n* `nn.Upsample`: Indicated that `align_corners` takes effect in `bicubic` mode ([#66756](https://github.com/pytorch/pytorch/pull/66756))\r\n* `nn.utils.clip_grad_norm_`: Fixed rendering of `parameters` in `error_if_nonfinite` arg docs ([#69958](https://github.com/pytorch/pytorch/pull/69958))\r\n* `optim.Adam`: Fixed formatting ([#70387](https://github.com/pytorch/pytorch/pull/70387))\r\n* `optim.AdamW`: Fixed formula ([#68587](https://github.com/pytorch/pytorch/pull/68587))\r\n* `optim.RAdam`: Corrected default value of `lr` arg ([#69186](https://github.com/pytorch/pytorch/pull/69186))\r\n* Removed orphan from cuDNN persistent note ([#65160](https://github.com/pytorch/pytorch/pull/65160))\r\n* Updated link to tutorial on defining NN modules ([#65534](https://github.com/pytorch/pytorch/pull/65534))\r\n* `nn.{AvgPool1d, AdaptiveAvgPool3d, MultiMarginLoss, PairwiseDistance, TripletMarginLoss}, ``F.{conv3d, conv_transpose3d, fold, linear}`: Fix doc formatting regressions from no-batch-dim support ([#73014](https://github.com/pytorch/pytorch/pull/73014))\r\n\r\n## torch.fx\r\n\r\n* Fixed for retracing documentation which would break for n-ary operators ([#71599](https://github.com/pytorch/pytorch/pull/71599))\r\n* Updated `torch.fx.passes.split_module` docstring ([#65542](https://github.com/pytorch/pytorch/pull/65542))\r\n* Updated `fx.rst` example outputs ([#68043](https://github.com/pytorch/pytorch/pull/68043))\r\n* Added document gotcha about training flag ([#68915](https://github.com/pytorch/pytorch/pull/68915))\r\n* Defined `get_dot_``graph` to match documentation ([#70541](https://github.com/pytorch/pytorch/pull/70541))\r\n\r\n## Sparse\r\n\r\n* Updated sparse.rst to warn about _values() ([#71088](https://github.com/pytorch/pytorch/pull/71088))\r\n\r\n## CUDA\r\n\r\n* Updated Stream `wait` documentation to reference underlying `cudaStreamWaitEvent` call ([#67973](https://github.com/pytorch/pytorch/pull/67973))\r\n* Documented `torch.cuda.ExternalStream`, `torch.cuda.caching_allocator_alloc` and `torch.cuda.caching_allocator_delete` ([#70126](https://github.com/pytorch/pytorch/pull/70126))\r\n* Updated `CUDA Graphs` docs: Fixed `make_graphed_callables` example typos ([#69379](https://github.com/pytorch/pytorch/pull/69379))\r\n\r\n## Mobile\r\n\r\n* Added user facing documentation for tracing-based selective build mobile interpreter in Android and iOS ([#1709](https://github.com/pytorch/tutorials/pull/1709))\r\n* Added recipe for bundled inputs in TorchScript models ([#1524](https://github.com/pytorch/tutorials/pull/1524/files))\r\n\r\n## Distributed\r\n\r\n* `DistributedDataParallel`\r\n    * DDP doc fix ([#71363](https://github.com/pytorch/pytorch/pull/71363))\r\n    * Clarified how to check memory saving if using gradient_as_bucket_view ([#71483](https://github.com/pytorch/pytorch/pull/71483))\r\n\r\n* `torch.distributed`\r\n    * Updated distributed.rst to show that CUDA send/recv on GPU is supported ([#65601](https://github.com/pytorch/pytorch/pull/65601))\r\n    * Clarified checkpoint support ([#68827](https://github.com/pytorch/pytorch/pull/68827))\r\n    * Updated distributed.rst for ProcessGroup Extensions ([#71482](https://github.com/pytorch/pytorch/pull/71482))\r\n* `torch.distributed.elastic`\r\n    * Made --max_restarts explicit in the quickstart and runner docs ([#65838](https://github.com/pytorch/pytorch/pull/65838))\r\n* `torch.distributed.optim`\r\n    * Rendered `torch.distributed.optim` members ([#67885](https://github.com/pytorch/pytorch/pull/67885))\r\n* `torch.distributed.rpc`\r\n    * Deleted distributed optimizer section from RPC and add reference to namespace docs page ([#68068](https://github.com/pytorch/pytorch/pull/68068))\r\n\r\n## TorchScript\r\n\r\n* Added `typing.Union` to supported types in documentation ([#68435](https://github.com/pytorch/pytorch/pull/68435))\r\n* Added documentation to `torch.jit.is_tracing()` ([#67326](https://github.com/pytorch/pytorch/pull/67326))\r\n* Fixed typos in `jit_language_reference.rst` ([#68706](https://github.com/pytorch/pytorch/pull/68706))\r\n\r\n## Quantization\r\n\r\n* Added documentation for quantized model save/load instructions ([#69789](https://github.com/pytorch/pytorch/pull/69789))\r\n* Updated link to qnnpack in quantization doc. ([#66226](https://github.com/pytorch/pytorch/pull/66226))\r\n* Improved quantization API docs ([#66379](https://github.com/pytorch/pytorch/pull/66379))\r\n* Quantization docs: add pages for Numeric Suite (Eager and FX) ([#66380](https://github.com/pytorch/pytorch/pull/66380))\r\n* Documented the quantization custom module APIs ([#67449](https://github.com/pytorch/pytorch/pull/67449))\r\n* Improved quantization documentation ([#68907](https://github.com/pytorch/pytorch/pull/68907))\r\n\r\n## ONNX\r\n\r\n* Improved documentation of `operator_export_type` and `opset_version` args ([#69549](https://github.com/pytorch/pytorch/pull/69549))\r\n* Fixed documentation for `do_constant_folding` arg default ([#71348](https://github.com/pytorch/pytorch/pull/71348))\r\n* Documented `ExportTypes`, `CheckerError`, and `unregister_custom_op_symbolic` ([#68489](https://github.com/pytorch/pytorch/pull/68489))\r\n* Fixed link to ONNX Runtime custom op documentation ([#67944](https://github.com/pytorch/pytorch/pull/67944))\r\n* Added section \u201cDiscovering all unconvertible ATen ops at once\u201d ([#66143](https://github.com/pytorch/pytorch/pull/66143))\r\n* Fixed typos ([#66090](https://github.com/pytorch/pytorch/pull/66090))\r\n* Documented work-arounds for indexing export limitations, and improve error messages ([#64579](https://github.com/pytorch/pytorch/pull/64579))\r\n\r\n## torch.package\r\n\r\n* Add some docs describing how to debug `torch.package` dependencies ([#65704](https://github.com/pytorch/pytorch/pull/65704))\r\n\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.11.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.11.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.11.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/61381447", "release_id": 61381447, "date_created": "2022-03-08T15:46:44Z", "date_published": "2022-03-10T16:59:55Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/58138618", "tag": "v1.10.2", "name": "PyTorch 1.10.2 Release, small bug fix release", "author": {"name": "atalman", "type": "User"}, "description": "This release is meant to deploy additional fixes not included in 1.10.1 release:\r\n\r\n* fix pybind issue for get_autocast_cpu_dtype and get_autocast_gpu_dtype #66396\r\n* Remove fgrad_input from slow_conv2d #64280\r\n* fix formatting CIRCLE_TAG when building docs #67026\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.10.2", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.10.2", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.10.2", "url": "https://api.github.com/repos/pytorch/pytorch/releases/58138618", "release_id": 58138618, "date_created": "2021-12-14T17:24:18Z", "date_published": "2022-01-27T21:51:23Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/55404241", "tag": "v1.10.1", "name": "PyTorch 1.10.1 Release, small bug fix release", "author": {"name": "seemethere", "type": "User"}, "description": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n* torch.nn.cross_entropy silently incorrect in PyTorch 1.10 on CUDA on non-contiguous inputs #67167\r\n* channels_last significantly degrades accuracy #67239\r\n* Potential strict aliasing rule violation in bitwise_binary_op (on ARM/NEON) #66119\r\n* torch.get_autocast_cpu_dtype() returns a new dtype #65786\r\n* Conv2d grad bias gets wrong value for bfloat16 case  #68048\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/69100) should contain all relevant pull requests related to this release as well as links to related issues", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.10.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.10.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.10.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/55404241", "release_id": 55404241, "date_created": "2021-12-09T16:59:45Z", "date_published": "2021-12-15T22:27:43Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/51800581", "tag": "v1.10.0", "name": "PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and compiler improvements", "author": {"name": "albanD", "type": "User"}, "description": "# 1.10.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.10. This release is composed of over 3,400 commits since 1.9, made by 426 contributors. We want to sincerely thank our community for continuously improving PyTorch. \r\n\r\nPyTorch 1.10 updates are focused on improving training and performance of PyTorch, and developer usability. Highlights include:\r\n* CUDA Graphs APIs are integrated to reduce CPU overheads for CUDA workloads.\r\n* Several frontend APIs such as FX, `torch.special`, and `nn.Module` Parametrization, have moved from beta to stable.  \r\n* Support for automatic fusion in JIT Compiler expands to CPUs in addition to GPUs.\r\n* Android NNAPI support is now available in beta.\r\n\r\nYou can check the blogpost that shows the new features [here](https://pytorch.org/blog/pytorch-1.10-released/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### `torch.any`/`torch.all` behavior changed slightly to be more consistent for zero-dimension, `uint8` tensors. ([#64642](https://github.com/pytorch/pytorch/pull/64642))\r\n\r\nThese two functions match the behavior of NumPy, returning an output dtype of bool for all support dtypes, except for `uint8` (in which case they return a 1 or a 0, but with `uint8` dtype). In some cases with 0-dim tensor inputs, the returned `uint8` value could mistakenly take on a value > 1. This has now been fixed.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8))\r\ntensor(1, dtype=torch.uint8)\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8), dim=0)\r\ntensor(42, dtype=torch.uint8) # wrong, old behavior\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8))\r\ntensor(1, dtype=torch.uint8)\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8), dim=0)\r\ntensor(1, dtype=torch.uint8) # new, corrected and consistent behavior\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Remove deprecated `torch.{is,set}_deterministic` ([#62158](https://github.com/pytorch/pytorch/pull/62158))\r\n\r\nThis is the end of the deprecation cycle for both of these functions. You should be using `torch.use_deterministic_algorithms` and`torch.are_deterministic_algorithms_enabled` instead.\r\n\r\n## Complex Numbers\r\n\r\n### **Conjugate View: [`tensor.conj()`](https://pytorch.org/docs/1.10./generated/torch.conj.html) now returns a view tensor that aliases the same memory and has conjugate bit set ([#54987](https://github.com/pytorch/pytorch/pull/54987), [#60522](https://github.com/pytorch/pytorch/pull/60522), [#66082](https://github.com/pytorch/pytorch/pull/66082), [#63602](https://github.com/pytorch/pytorch/pull/63602)).** \r\n\r\nThis means that `.conj()` is now an O(1) operation and returns a tensor that views the same memory as `tensor` and has conjugate bit set. This notion of conjugate bit enables fusion of operations with conjugation which gives a lot of performance benefit for operations like matrix multiplication. All out-of-place operations will have the same behavior as before, but an in-place operation on a conjugated tensor will additionally modify the input tensor. \r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> y.add_(2)\r\n>>> print(x)\r\ntensor([1.+2.j])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> y.add_(2)\r\n>>> print(x)\r\ntensor([3.+2.j])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nNote: You can verify if the conj bit is set by calling `tensor.is_conj()`. The conjugation can be resolved, i.e., you can obtain a new tensor that doesn\u2019t share storage with the input tensor at any time by calling `conjugated_tensor.clone()` or `conjugated_tensor.resolve_conj()` .\r\n\r\nNote that these conjugated tensors behave differently from the corresponding numpy arrays obtained from `np.conj()` when an in-place operation is performed on them (similar to the example shown above).\r\n\r\n\r\n### **Negative View: `tensor.conj().neg()` returns a view tensor that aliases the same memory as both tensor and `tensor.conj()` and has a negative bit set ([#56058](https://github.com/pytorch/pytorch/pull/56058)).**\r\n\r\n`conjugated_tensor.neg()` continues to be an O(1) operation, but the returned tensor shares memory with both `tensor` and `conjugated_tensor`.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> z = y.imag\r\n>>> z.add_(2)\r\n>>> print(x)\r\ntensor([1.+2.j])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> z = y.imag\r\n>>> print(z.is_neg())\r\nTrue\r\n>>> z.add_(2)\r\n>>> print(x)\r\ntensor([1.-0.j])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `tensor.numpy()` now throws `RuntimeError` when called on a tensor with conjugate or negative bit set ([#61925](https://github.com/pytorch/pytorch/pull/61925)).\r\n\r\nBecause the notion of conjugate bit and negative bit doesn\u2019t exist outside of PyTorch, calling operations that return a Python object viewing the same memory as input like `.numpy()` would no longer work for tensors with conjugate or negative bit set.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj().imag\r\n>>> print(y.numpy())\r\n[2.]\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj().imag\r\n>>> print(y.numpy())\r\nRuntimeError: Can't call numpy() on Tensor that has negative\r\nbit set. Use tensor.resolve_neg().numpy() instead.\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Autograd\r\n\r\n### Raise `TypeError` instead of `RuntimeError` when assigning to a Tensor\u2019s grad field with wrong type ([#64876](https://github.com/pytorch/pytorch/pull/64876))\r\n\r\nSetting the `.grad` field with a non-None and non-Tensor object used to return a `RuntimeError` but it now properly returns a `TypeError`. If your code was catching this error, you should simply update it to catch a `TypeError` instead of a `RuntimeError`.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntry:\r\n    # Assigning an int to a Tensor's grad field\r\n    a.grad = 0\r\nexcept RuntimeError as e:\r\n    pass\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntry:\r\n   a.grad = 0\r\nexcept TypeError as e:\r\n    pass\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Raise error when inputs to `autograd.grad` are empty ([#52016](https://github.com/pytorch/pytorch/pull/52016))\r\n\r\nCalling `autograd.grad` with an empty list of inputs used to do the same as backward. To reduce confusion, it now raises the expected error. If you were relying on this, you can simply update your code as follows:\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ngrad = autograd.grad(out, tuple())\r\nassert grad == tuple()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nout.backward()\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Optional arguments to `autograd.gradcheck` and `autograd.gradgradcheck` are now kwarg-only ([#65290](https://github.com/pytorch/pytorch/pull/65290))\r\n\r\nThese two functions now have a significant number of optional arguments controlling what they do (i.e., `eps`, `atol`, `rtol`, `raise_exception`, etc.). To improve readability, we made these arguments kwarg-only. If you are passing these arguments to `autograd.gradcheck` or `autograd.gradgradcheck` as positional arguments, you can update your code as follows:\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.autograd.gradcheck(fn, x, 1e-6)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.autograd.gradcheck(fn, x, eps=1e-6)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### In-place detach (`detach_`) now errors for views that return multiple outputs ([#58285](https://github.com/pytorch/pytorch/pull/58285))\r\n\r\nThis change is finishing the deprecation cycle for the inplace-over-view logic. In particular, a few things that were warning are updated:\r\n\r\n    * `detach_` will now raise an error when invoked on any view created by `split`, `split_with_sizes`, or `chunk`. You should use the non-inplace `detach` instead.\r\n    * The error message for when an in-place operation (that is not detach) is performed on a view created by `split`, `split_with_size`, and `chunk` has been changed from \"This view is an output of a function...\" to \"This view is the output of a function...\".\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nb = a.split(1)[0]\r\nb.detach_()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nb = a.split(1)[0]\r\nc = b.detach()\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Fix saved variable unpacking version counter ([#60195](https://github.com/pytorch/pytorch/pull/60195))\r\n\r\nIn-place on the unpacked SavedVariables used to be ignored. They are now properly detected which can lead to errors saying that a variable needed for backward was modified in-place.\r\nThis is a valid error and the user should fix this by cloning the unpacked saved variable before using it.\r\n\r\nNo internal formula will trigger this, but it might be triggered by user custom `autograd.Function` if the backward modifies a saved Tensor inplace and you do multiple backwards. This used to silently return the wrong result and will now raise the expected error.\r\n\r\n## torch.nn\r\n\r\n### Added optional tensor arguments to `__torch_function__` handling checks ([#63967](https://github.com/pytorch/pytorch/pull/63967))\r\n\r\nThis fixes the `has_torch_function*()` checks throughout `torch.nn.functional` to correctly pass in optional tensor arguments; prior to this fix, `handle_torch_function()` was not called for these optional tensor arguments. Previously, passing a tensor-like object into a function that accepts an optional tensor might not trigger that object's `__torch_function__`. Now, the object's `__torch_function__` will be triggered as expected.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.nn.functional as F\r\nclass TestTensor(object):\r\n    def __init__(self, weight):\r\n        self.weight = weight\r\n    def __torch_function__(self, func, _, args=(), kwargs=None):\r\n        print(func)\r\n        print(func == F.group_norm)\r\n# Call F.group_norm with a custom Tensor as the non-optional arg 'features'\r\nfeatures = TestTensor(torch.randn(3,3))\r\nF.group_norm(features, 3)\r\n# ...prints \"group_norm\" and True\r\n# Call F.group_norm with a custom Tensor as the optional arg 'weight'\r\nfeatures = torch.randn(3,3)\r\nweight = TestTensor(torch.randn(3))\r\nF.group_norm(features, 3, weight=weight)\r\n# ...prints \"group_norm\" and False because weight's __torch_function__ is\r\n# called with func as torch.group_norm instead of F.group_norm\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.nn.functional as F\r\nclass TestTensor(object):\r\n    def __init__(self, weight):\r\n        self.weight = weight\r\n    def __torch_function__(self, func, _, args=(), kwargs=None):\r\n        print(func)\r\n        print(func == F.group_norm)\r\n# Call F.group_norm with a custom Tensor as the non-optional arg 'features'\r\nfeatures = TestTensor(torch.randn(3,3))\r\nF.group_norm(features, 3)\r\n# ...prints \"group_norm\" and True\r\n# Call F.group_norm with a custom Tensor as the optional arg 'weight'\r\nfeatures = torch.randn(3,3)\r\nweight = TestTensor(torch.randn(3))\r\nF.group_norm(features, 3, weight=weight)\r\n# ...prints \"group_norm\" and True\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## CUDA\r\n\r\n### Removed post-backward syncs on default stream ([#60421](https://github.com/pytorch/pytorch/pull/60421))\r\n\r\nCalls to backward() or grad() synced only the calling thread's default stream with autograd leaf streams at the end of backward. This made the following weird pattern safe:\r\n\r\n```python\r\nwith torch.cuda.stream(s):\r\n    # imagine forward used many streams, so backward leaf nodes may run on many streams\r\n    loss.backward()# no sync\r\nuse grads\r\n```\r\n\r\nbut a more benign-looking pattern was unsafe:\r\n\r\n```python\r\nwith torch.cuda.stream(s):\r\n    # imagine forward used a lot of streams, so backward leaf nodes may run on many streams\r\n    loss.backward()\r\n    # backward() syncs the default stream with all the leaf streams, but does not sync s with anything,\r\n    # so counterintuitively (even though we're in the same stream context as backward()!)\r\n    # it is NOT SAFE to use grads here, and there's no easy way to make it safe,\r\n    # unless you manually sync on all the streams you used in forward,\r\n    # or move \"use grads\" back to default stream outside the context.\r\n    use grads\r\n```\r\n\r\nNote: this change makes it so that backward() has [same user-facing stream semantics as any cuda op](https://pytorch.org/docs/master/notes/cuda.html#stream-semantics-of-backward-passes).** In other words, the weird pattern is unsafe, and the benign-looking pattern is safe. Implementation-wise, this meant backward() should sync its calling thread's current stream, not default stream, with the leaf streams. This PR  deletes syncs on the default stream. \r\n\r\n## torch.package\r\n\r\n* Removed verbose mode from PackageExporter ([#61145](https://github.com/pytorch/pytorch/pull/61145))\r\n    * PackageExporter is losing \u201cverbose\u201d mode argument as we have found it is not useful and sometimes confusing. See following examples on how to modify your code to accommodate this change.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nwith PackageExporter(buffer, verbose=False) as e:\r\n    e.intern(\"**\")\r\n    e.save_pickle(\"res\", \"mod1.pkl\", mod1)\r\n    e.save_pickle(\"res\", \"mod2.pkl\", mod2)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nwith PackageExporter(buffer) as e:\r\n    e.intern(\"**\")\r\n    e.save_pickle(\"res\", \"mod1.pkl\", mod1)\r\n    e.save_pickle(\"res\", \"mod2.pkl\", mod2)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n## Quantization\r\n\r\n### Added extra observer/fake_quant (the same observer/fake_quant instance as the input) for some operators in prepare_fx, e.g. maxpool, add_scalar and mul_scalar ([#61687](https://github.com/pytorch/pytorch/pull/61687), [#61859](https://github.com/pytorch/pytorch/pull/61859))\r\n\r\nPreviously the way we insert observers/fake_quants are specific to fbgemm/qnnpack backend, as we work on making FX Graph Mode Quantization extensible to custom backends, we are changing some behaviors for the fbgemm/qnnpack path as well. The above changes are adding extra observer/fake_quant to the output of some operators to make sure we model the quantized operator more accurately in quantization aware training, the comprehensive list of operators where the behavior changes are the following:\r\n\r\n* modules: torch.nn.MaxPool1d, torch.nn.MaxPool2d, torch.nn.MaxPool3d, torch.nn.Identity\r\n* torch functions: torch.nn.functional.max_pool1d, torch.nn.functional.max_pool2d, torch.nn.functional.max_pool3d, torch.chunk, torch.flatten, torch.transpose, torch.repeat_interleave, torch.sort, torch.squeeze, torch.stack, torch.unsqueeze, operator.getitem, \r\n* Tensor methods: chunk, contiguous, detach, detach_, numel, permute, repeat, repeat_interleave, reshape, resize_, shape, size, squeeze, squeeze_, transpose, unsqueeze, unsqueeze_, view\r\n* Tensor operations: add scalar and mul scalar (add/mul with a Tensor and a Scalar input)\r\n\r\n\r\nWe will show an example with torch.nn.MaxPool2d:\r\n\r\n```python\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.maxpool2d = torch.nn.MaxPool2d(kernel_size=3)\r\n\r\n    def forward(self, x):\r\n        x = self.maxpool2d(x)\r\n        return x\r\nm = M().eval()        \r\nm = prepare_fx(m, {\"\": torch.quantization.default_qconfig})\r\nprint(m.code)\r\n```\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ndef forward(self, x):\r\n    x_activation_post_process_0 = self.x_activation_post_process_0(x); x = None\r\n    maxpool2d = self.maxpool2d(x_activation_post_process_0); x_activation_post_process_0 = None\r\n    return maxpool2d\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ndef forward(self, x):\r\n    x_activation_post_process_0 = self.x_activation_post_process_0(x); x = None\r\n    maxpool2d = self.maxpool2d(x_activation_post_process_0); x_activation_post_process_0 = None\r\n    maxpool2d_activation_post_process_0 = self.maxpool2d_activation_post_process_0(maxpool2d); maxpool2d = None\r\n    return maxpool2d_activation_post_process_0\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nNote that `self.maxpool2d_activation_post_process_0` and `self.x_activation_post_process_0` will refer to the same observer/fake_quant instance, this is to simulate the numerics for the quantized maxpool implementation, where the output would reuse the quantization parameter of the input. Simple illustration with graph:\r\n\r\nBefore:\r\n\r\n```\r\nobserver_0 - maxpool - ...\r\n```\r\n\r\nAfter:\r\n\r\n```\r\nobserver_0 - maxpool - observer_0 (same observer instance as input observer) - ...\r\n```\r\n\r\n## ONNX\r\n\r\n### Removed `aten` arg from `torch.onnx.export()`. ([#62759](https://github.com/pytorch/pytorch/pull/62759))\r\n\r\nThe new `OperatorExportTypes.ONNX` removes the need for an explicit `aten` argument. If Pytorch was built with `-DPYTORCH_ONNX_CAFFE2_BUNDLE` the a `None` value means `OperatorExportTypes.ONNX_ATEN_FALLBACK`\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(..., aten=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(..., operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### Deprecate **`__torch_function__`** as a plain methods ([#64843](https://github.com/pytorch/pytorch/pull/64843))\r\n\r\nThe `__torch_function__` function used to create Tensor like objects did not have any constraint whether it should be a method, class method or static method.\r\n\r\nTo make it compatible with newer features on Tensor-like objects, we are deprecating setting it as a plain method. You can define it as a class method to get the current class and scan the argument list if you need an object that is an instance of this class.\r\n\r\n## Mobile\r\n\r\n### Removed API torch.utils.bundled_inputs.run_on_bundled_input ([#58344](https://github.com/pytorch/pytorch/pull/58344))\r\n\r\nThis API caused many issues and is not really necessary. The functionality (run model with bundled input) can be achieved by using `get_all_bundled_inputs`. For example:\r\n\r\n1.9.1:\r\n\r\n```python\r\nmodel.run_on_bundled_input(0)\r\n```\r\n\r\n1.10.0:\r\n\r\n```python\r\nmodel(*model.get_all_bundled_inputs()[0])\r\n```\r\n\r\n## Distributed\r\n\r\n### `torch.distributed.rpc`: Removed ProcessGroup RPC backend ([#62411](https://github.com/pytorch/pytorch/pull/62411) , [#62985](https://github.com/pytorch/pytorch/pull/62985))\r\n\r\nProcessGroup RPC backend has been deprecated and 1.9 was the last release which carried it. The default RPC backend is TensorPipe which is the recommended backend for RPC. Users who use `torch.distributed.rpc.BackendType.PROCESS_GROUP` will be given an error message to switch to `torch.distributed.rpc.BackendType.TENSORPIPE`.\r\n\r\n## ONNX\r\n\r\n### Removed following arguments in torch.onnx.export(): enable_onnx_checker, strip_doc_string, _retain_param_name  ([#64369](https://github.com/pytorch/pytorch/pull/64369), [#64371](https://github.com/pytorch/pytorch/pull/64371), [#64370](https://github.com/pytorch/pytorch/pull/64370))\r\n\r\n`enable_onnx_checker` argument is removed. ONNX checker will now always run by default. Users can catch exceptions to ignore raised failures. `strip_doc_string` has been rolled into the `verbose` arg in `torch.onnx.export()`. `_retain_param_name` argument has been removed in  `torch.onnx.export()` will default to `True` . There is no way to get the old behavior of `_retain_param_name=False`. Users should stop setting this arg.\r\n\r\n1.9.1:\r\n\r\n```\r\ntorch.onnx.export(..., enable_onnx_checker=False, strip_doc_string=False)\r\n```\r\n\r\n1.10.0:\r\n\r\n```\r\ntry:\r\n    torch.onnx.export(verbose=True)\r\nexcept torch.onnx.utils.ONNXCheckerError:\r\n   pass\r\n```\r\n\r\n## Infra (Releng)\r\n\r\n### Disable ParallelTBB ([#65092](https://github.com/pytorch/pytorch/pull/65092))\r\n\r\n`ParallelTBB` config/codepath is no longer actively tested by PyTorch CI and as result is subject to code/functionality degradation\r\n\r\n\r\n# New features\r\n\r\n## Python API\r\n\r\n* Added new functions:\r\n    *  `torch.isin()` ([#53125](https://github.com/pytorch/pytorch/pull/53125)), `torch.bitwise_{left/right}_shift`, `__rlshift__`, `__rrshift__` ([#59544](https://github.com/pytorch/pytorch/pull/59544)), `torch.Tensor.{__rand__, __ror__,__rxor__}` ([#59240](https://github.com/pytorch/pytorch/pull/59240)),  `torch.aminmax` ([#62401](https://github.com/pytorch/pytorch/pull/62401)),  `torch.new_ones` ([#58405](https://github.com/pytorch/pytorch/pull/58405))\r\n    * For numpy compatibility `torch.cov` ([#58311](https://github.com/pytorch/pytorch/pull/58311)), `torch.frombuffer` ([#59077](https://github.com/pytorch/pytorch/pull/59077)), `torch.corrcoef` ([#60420](https://github.com/pytorch/pytorch/pull/60420)), `torch.nanmean` ([#62671](https://github.com/pytorch/pytorch/pull/62671)), `torch.cumulative_trapezoid` ([#61615](https://github.com/pytorch/pytorch/pull/61615))\r\n* The [torch.special module](https://pytorch.org/docs/1.10.0/special.html?highlight=special) is now stable! This module, consistent with SciPy\u2019s special module, has 30 operations including the Hurwitz zeta function and various gamma functions.  ([#59623](https://github.com/pytorch/pytorch/pull/59623), [#56352](https://github.com/pytorch/pytorch/pull/56352), [#58126](https://github.com/pytorch/pytorch/pull/58126), [#59141](https://github.com/pytorch/pytorch/pull/59141), [#59143](https://github.com/pytorch/pytorch/pull/59143), [#58650](https://github.com/pytorch/pytorch/pull/58650), [#55878](https://github.com/pytorch/pytorch/pull/55878), [#58838](https://github.com/pytorch/pytorch/pull/58838), [#60512](https://github.com/pytorch/pytorch/pull/60512), [#60641](https://github.com/pytorch/pytorch/pull/60641), [#61633](https://github.com/pytorch/pytorch/pull/61633), [#60519](https://github.com/pytorch/pytorch/pull/60519), [#59691](https://github.com/pytorch/pytorch/pull/59691), [#58194](https://github.com/pytorch/pytorch/pull/58194))\r\n* Added support for slots and subclass magic getstate/setstate method for Tensor serialization ([#62745](https://github.com/pytorch/pytorch/pull/62745))\r\n* `torch.optim`:\r\n    * Added Nesterov Adam as `NAdam` ([#59009](https://github.com/pytorch/pytorch/pull/59009))\r\n    * Added `lr_scheduler.ChainedScheduler` ([#63491](https://github.com/pytorch/pytorch/pull/63491), [#63457](https://github.com/pytorch/pytorch/pull/63457), [#65034](https://github.com/pytorch/pytorch/pull/65034)))\r\n    * Added `lr_scheduler.SequentialLR` ([#64037](https://github.com/pytorch/pytorch/pull/64037), [#65035](https://github.com/pytorch/pytorch/pull/65035))\r\n    * Added `lr_scheduler.{ConstantLR,LinearLR}` ([#64395](https://github.com/pytorch/pytorch/pull/64395))\r\n* `torch.cpu.amp.autocast`: enable new API for CPU autocast ([#57386](https://github.com/pytorch/pytorch/pull/57386), [#63534](https://github.com/pytorch/pytorch/pull/63534))\r\n* Added `BFloat16` support for `torch.{cross, tril, triu, tril_indices, triu_indices, cumsum, cummax, cummin, median, kthvalue, nansum, nextafter, range, sinh, cosh, frexp, nan_to_num, sigmoid, sigmoid_backward, tanh_backward, addcmul, addcdiv, bucketize, bernoulli, dropout, fold, unfold, MaxPool2D, AdaptiveAvgPool2D, topk}` on CPU ([#62454](https://github.com/pytorch/pytorch/pull/62454), [#63307](https://github.com/pytorch/pytorch/pull/63307), [#55210](https://github.com/pytorch/pytorch/pull/55210), [#60074](https://github.com/pytorch/pytorch/pull/60074), [#61083](https://github.com/pytorch/pytorch/pull/61083), [#61829](https://github.com/pytorch/pytorch/pull/61829), [#55221](https://github.com/pytorch/pytorch/pull/55221),  [#61826](https://github.com/pytorch/pytorch/pull/61826), [#55588](https://github.com/pytorch/pytorch/pull/55588), [#56372](https://github.com/pytorch/pytorch/pull/56372), [#62880](https://github.com/pytorch/pytorch/pull/62880), [#55202](https://github.com/pytorch/pytorch/pull/55202), [#59547](https://github.com/pytorch/pytorch/pull/59547))\r\n* Added `BFloat16` support for  `torch.{ceil, floor, frac, round, trunc, sort, topk, aminmax, cumsum, logcumsumexp, cumprod, cummin, cummax}` on CUDA ([#57910](https://github.com/pytorch/pytorch/pull/57910), [#58196](https://github.com/pytorch/pytorch/pull/58196), [#59977](https://github.com/pytorch/pytorch/pull/59977), [#62767](https://github.com/pytorch/pytorch/pull/62767), [#57904](https://github.com/pytorch/pytorch/pull/57904)).\r\n* Added  `torch.cuda.is_bf16_supported` ([#63798](https://github.com/pytorch/pytorch/pull/63798))\r\n* Added zero rate to Poisson distribution ([#61511](https://github.com/pytorch/pytorch/pull/61511))\r\n* Added `torch.segment_reduce` ([#59951](https://github.com/pytorch/pytorch/pull/59951), [#60018](https://github.com/pytorch/pytorch/pull/60018), [#61141](https://github.com/pytorch/pytorch/pull/61141), [#61266](https://github.com/pytorch/pytorch/pull/61266), [#59521](https://github.com/pytorch/pytorch/pull/59521), [#60379](https://github.com/pytorch/pytorch/pull/60379), [#60379](https://github.com/pytorch/pytorch/pull/60379))\r\n* Added boolean support to `torch.isclose` ([#61271](https://github.com/pytorch/pytorch/pull/61271))\r\n* Added `torch.trapezoid` ([#61475](https://github.com/pytorch/pytorch/pull/61475)).\r\n* Added `torch.gradient` support for second order central differences (edge_order=2) ([#58165](https://github.com/pytorch/pytorch/pull/58165))\r\n* `torch.sigmoid`: CUDA support and complex autograd support ([#48647](https://github.com/pytorch/pytorch/pull/48647))\r\n* Added channels-last support for `torch.bilinear` and `torch.nn,MaxUnpool2d` ([#56322](https://github.com/pytorch/pytorch/pull/56322), [#49984](https://github.com/pytorch/pytorch/pull/49984))\r\n\r\n## Autograd\r\n\r\n* [Experimental] Forward mode AD:\r\n    * *NOTE: In addition to operators listed below, many simple ops are already supported. If you encounter an operator that does not have a forward-mode AD formula implemented, please file an issue. As a workaround, you can use custom `autograd.Function` to implement your own forward-mode-AD-supported operator.*\r\n    * Added forward-mode AD support for custom `autograd.Function` ([#64061](https://github.com/pytorch/pytorch/pull/64061), [#63434](https://github.com/pytorch/pytorch/pull/63434))\r\n    * Added forward-mode AD support for `torch.{acos, add, addbmm, addcdiv, addcmul, addmm, addmv, addr, angle, acosh, asinh, atanh, asin, atan, conj, baddbmm, bmm, cat, ceil, clamp, clamp_min, clamp_max, complex, copy_sign, cos, cosh, cross, cumprod, cumsum, cummax, cummin, deg2rad, div, dot, vdot, exp, exp2, expm1, expand, floor, frac, frexp, gather, hardswish, hstack, hypot, index_add_, index_copy_, index_put_, index_select, kthvalue, lerp, lgamma, digamma, polygamma, log, log10, log1p, log2, logaddexp, logaddexp2, xlogy, masked_fill_, masked_fill_, masked_scatter_, masked_select, max, maximum, fmax, mean, min, mininum, fmin, mm, mode, mul, lu, lu_solve, vstack}` ([#57768](https://github.com/pytorch/pytorch/pull/57768), [#57863](https://github.com/pytorch/pytorch/pull/57863) [#59711](https://github.com/pytorch/pytorch/pull/59711), [#64742](https://github.com/pytorch/pytorch/pull/64742))\r\n    * Added Forward AD support for the following element-wise and linear operators `torch.{mvlgamma, nan_to_num, permute, pow,  reciprocal, remainder, repeat, round, rsqrt, sigmoid, logit, sign, sgn, sin, sinc, sinh, sqrt, squeeze, sub, sum, t, flip, roll, rot90, take, tan, tanh, trace, transpose, tril, triu, trunc, unfold, unsqueeze, view, zero_, hardshrink} `([#59993](https://github.com/pytorch/pytorch/pull/59993))\r\n    * Added Forward AD support for `torch.special.`{`xlog1py, entr}` ([#59711](https://github.com/pytorch/pytorch/pull/59711), [#59993](https://github.com/pytorch/pytorch/pull/59993))\r\n    * Added forward AD support for `torch.linalg.{cholesky, cholesky_ex, eigh, inv, inv_ex, solve}`  ([#62160](https://github.com/pytorch/pytorch/pull/62160), [#64646](https://github.com/pytorch/pytorch/pull/64646), [#62163](https://github.com/pytorch/pytorch/pull/62163), [#62159](https://github.com/pytorch/pytorch/pull/62159))\r\n    * Added forward AD support for `torch.functional.leak_relu` ([#59993](https://github.com/pytorch/pytorch/pull/59993)) \r\n* Added saved tensor hooks to customize packing/unpacking behavior of tensors saved for backward ([#60685](https://github.com/pytorch/pytorch/pull/60685), [#60663](https://github.com/pytorch/pytorch/pull/60663), [#62564](https://github.com/pytorch/pytorch/pull/62564), [#60975](https://github.com/pytorch/pytorch/pull/60975), [#62909](https://github.com/pytorch/pytorch/pull/62909), [#62717](https://github.com/pytorch/pytorch/pull/62717))\r\n* Exposed raw saved tensors for custom `autograd.Function` to use with the saved tensor hooks ([#60551](https://github.com/pytorch/pytorch/pull/60551))\r\n* Added default saved tensor hooks ([#61834](https://github.com/pytorch/pytorch/pull/61834), [#62563](https://github.com/pytorch/pytorch/pull/62563), [#62361](https://github.com/pytorch/pytorch/pull/62361))\r\n* Added context manager using default saved tensor hooks to automatically move saved tensors on CPU and back ([#61928](https://github.com/pytorch/pytorch/pull/61928), [#62410](https://github.com/pytorch/pytorch/pull/62410))\r\n* Added C++ and python bindings for `.is_inference()` method ([#58729](https://github.com/pytorch/pytorch/pull/58729)) \r\n* `torch.lu_solve`: Implement support for backward AD ([#61681](https://github.com/pytorch/pytorch/pull/61681)).\r\n\r\n## torch.nn\r\n\r\n* Added new modules: `nn.{ReflectionPad3d, LazyInstanceNorm*d}` ([#59791](https://github.com/pytorch/pytorch/pull/59791), [#60837](https://github.com/pytorch/pytorch/pull/60837), [#61308](https://github.com/pytorch/pytorch/pull/61308), [#60982](https://github.com/pytorch/pytorch/pull/60982))\r\n* `nn.CrossEntropyLoss`: Added support for class probability targets ([#61044](https://github.com/pytorch/pytorch/pull/61044))\r\n* `nn.CrossEntropyLoss`: Added support for label smoothing ([#63122](https://github.com/pytorch/pytorch/pull/63122))\r\n* `nn.Module`: Added support for arbitrary objects in state_dicts via `get_extra_state()` / `set_extra_state()` ([#62976](https://github.com/pytorch/pytorch/pull/62976))\r\n* `nn.utils.skip_init()`: Added function to skip module parameter / buffer initialization ([#57555](https://github.com/pytorch/pytorch/pull/57555))\r\n\r\n## Profiler\r\n\r\n* Added profiler support for mobile ([#62419](https://github.com/pytorch/pytorch/pull/62419), [#62418](https://github.com/pytorch/pytorch/pull/62418), [#62417](https://github.com/pytorch/pytorch/pull/62417),[#62228](https://github.com/pytorch/pytorch/pull/62228),[#62191,](https://github.com/pytorch/pytorch/pull/62191)[#61792](https://github.com/pytorch/pytorch/pull/61792))\r\n* Ported Nvtx support to new profiler ([#61634](https://github.com/pytorch/pytorch/pull/61634))\r\n* Added Tensor core usage stats and recommendations in Tensorboard ([`#364`](https://github.com/pytorch/kineto/pull/364)[,](https://github.com/pytorch/kineto/pull/402/commits/e435a8f55fdbf2a2331931782404b9020eefa4ba)[`#368`](https://github.com/pytorch/kineto/pull/368)[,](https://github.com/pytorch/kineto/pull/402/commits/d3132ebc51faed586e6699e895fecc6b4d255334)[`#383`](https://github.com/pytorch/kineto/pull/383), [`#422`](https://github.com/pytorch/kineto/pull/422))\r\n\r\n## CUDA\r\n\r\n* Allow enabling warnings on CUDA synchronization ([#62092](https://github.com/pytorch/pytorch/pull/62092))\r\n* Added CUDA graph Prototype API and documentation ([#63269](https://github.com/pytorch/pytorch/pull/63269))\r\n* Make stream semantics of backward calls consistent with other cuda ops ([#57833](https://github.com/pytorch/pytorch/pull/57833), [#60230](https://github.com/pytorch/pytorch/pull/60230), [#60127](https://github.com/pytorch/pytorch/pull/60127))\r\n* Enabled autocast support for user-specified device and dtype ([#61002](https://github.com/pytorch/pytorch/pull/61002), [#63416](https://github.com/pytorch/pytorch/pull/63416))\r\n\r\n## C++ API\r\n\r\n* Added C++ API for meta functions. They are available in the `at::meta::` namespace ([#58570](https://github.com/pytorch/pytorch/pull/58570))\r\n* Exposed interface to set grain size on `cpu_kernel`, `cpu_kernel_vec` and `cpu_kernel_multiple_outputs` ([#58949](https://github.com/pytorch/pytorch/pull/58949))\r\n* Added `at::native::resize_bytes_cpu` to resize `Storage` in ATen ([#60324](https://github.com/pytorch/pytorch/pull/60324))\r\n* Added `transpose` to PackedTensorAccessor ([#61114](https://github.com/pytorch/pytorch/pull/61114))\r\n* Added `torch::linalg::qr` as the C++ API ([#60529](https://github.com/pytorch/pytorch/pull/60529))\r\n* Exposed `amin` and `amax` to aten symbols ([#61550](https://github.com/pytorch/pytorch/pull/61550))\r\n* Added support to invoke callable activation function for Transformer modules ([#62342](https://github.com/pytorch/pytorch/pull/62342))\r\n* Added support for `c10::optional` to compare with different but comparable types ([#62890](https://github.com/pytorch/pytorch/pull/62890))\r\n* Added a unified API `c10::util::check_env` to check environment variable ([#59052](https://github.com/pytorch/pytorch/pull/59052))\r\n\r\n## TorchScript\r\n\r\n* Added reference semantics to TorchScript classes ([#44324](https://github.com/pytorch/pytorch/pull/44324)) \r\n* Conservatively moved all suitable prim ops from full-jit to mobile, and make them selective. ([#58353](https://github.com/pytorch/pytorch/pull/58353)) \r\n* Added change to predicate uses of RPC APIs on `torch.distributed.rpc.is_available()` ([#58887](https://github.com/pytorch/pytorch/pull/58887)) \r\n* Added a phase to perform inplace<->functional conversion for activation operators ([#57477](https://github.com/pytorch/pytorch/pull/57477)) \r\n* Enabled Profile-Directed Typing in `torch.jit.script` ([#62420](https://github.com/pytorch/pytorch/pull/62420)) \r\n* Introduced enhancement for smart serialization for operator schemas with out arg ([#63096](https://github.com/pytorch/pytorch/pull/63096))\r\n* Added a pass to transform better handle concatenation ops ([#59881](https://github.com/pytorch/pytorch/pull/59881)) \r\n* Added a new operator for concat that takes in variadic parameters ([#59880](https://github.com/pytorch/pytorch/pull/59880)) \r\n* Added support for union in TorchScript ([#64234](https://github.com/pytorch/pytorch/pull/64234)) \r\n\r\n## torch.package\r\n\r\n* Added basic tooling to enable users to see what is inside of a PackageExporter ([#61147](https://github.com/pytorch/pytorch/pull/61147))\r\n* Added hasattr to `torch::deploy` C++ API ([#62669](https://github.com/pytorch/pytorch/pull/62669))\r\n* Added support to re-save a PackageImporter module ([#65101](https://github.com/pytorch/pytorch/pull/65101))\r\n* Added support to make frozen symbol name customizable in `torch::deploy`. ([#63817](https://github.com/pytorch/pytorch/pull/63817))\r\n\r\n## Mobile\r\n\r\n* Enabled kineto profiler on mobile via EdgeKinetoProfiler ([#62419](https://github.com/pytorch/pytorch/pull/62419))\r\n* Added support of loading lite interpreter module from assets in Android ([#61609](https://github.com/pytorch/pytorch/pull/61609))\r\n* Enabled tracing based selective build ([#63421,](https://github.com/pytorch/pytorch/pull/63421) [#64087](https://github.com/pytorch/pytorch/pull/64087), [#66237,](https://github.com/pytorch/pytorch/pull/66237) [#66395](https://github.com/pytorch/pytorch/pull/66395))\r\n    * built tracer in OSS  ([#64087](https://github.com/pytorch/pytorch/pull/64087))\r\n    * used operator.yaml to build libtorch library ([#66237)](https://github.com/pytorch/pytorch/pull/66237)\r\n    * Built tracer and enabled tracing-based build with tracer output  ([#66395](https://github.com/pytorch/pytorch/pull/66395))\r\n* NNAPI\r\n    * Android NNAPI delegate implementation of runtime initialization (compilation) and execution ([#62272](https://github.com/pytorch/pytorch/pull/62272))\r\n    * Added `aten::{avgpool2d,softmax,to,div,flatten,detach,slice,log_softmax,conv2d_transpose}` to NNAPI converter ([#58538](https://github.com/pytorch/pytorch/pull/58538), [#58539](https://github.com/pytorch/pytorch/pull/58539), [#58540](https://github.com/pytorch/pytorch/pull/58540), [#58541](https://github.com/pytorch/pytorch/pull/58541), [#60885](https://github.com/pytorch/pytorch/pull/60885), [#58543](https://github.com/pytorch/pytorch/pull/58543), [#59364](https://github.com/pytorch/pytorch/pull/59364), [#61378](https://github.com/pytorch/pytorch/pull/61378), [#59529](https://github.com/pytorch/pytorch/pull/59529)\r\n    * Added Int32 support for NNAPI ([#59365](https://github.com/pytorch/pytorch/pull/59365))\r\n    * Made nnapi `aten::{conv2d,linear,cat,flatten}` converter accept flexible batch ([#61021](https://github.com/pytorch/pytorch/pull/61021), [#61022](https://github.com/pytorch/pytorch/pull/61022), [76c0f223d3](https://github.com/pytorch/pytorch/commit/76c0f223d3), [#61024](https://github.com/pytorch/pytorch/pull/61024))\r\n    * Added option to specify custom NNAPI serializer ([#61025](https://github.com/pytorch/pytorch/pull/61025))\r\n    * Made Android NNAPI preprocess to accept both single Tensor inputs and Tensor List inputs ([#61752](https://github.com/pytorch/pytorch/pull/61752))\r\n    * Added a few improvements in NNAPI delegation ([#63489](https://github.com/pytorch/pytorch/pull/63489))\r\n    * Added support const values in binary ops ([2d58f3f56d](https://github.com/pytorch/pytorch/commit/2d58f3f56d))\r\n* Added unary/binary ops necessary and more shape functions for mobilenet ([#56828](https://github.com/pytorch/pytorch/pull/56828), [#58932](https://github.com/pytorch/pytorch/pull/58932))\r\n* Added `aten::{hardswish,tanh,clamp}` for iOS Metal ([#64588](https://github.com/pytorch/pytorch/pull/64588), [#61383](https://github.com/pytorch/pytorch/pull/61383))\r\n* Added CoreML support ([#64521](https://github.com/pytorch/pytorch/pull/64521), [#64522](https://github.com/pytorch/pytorch/pull/64522), [#64523](https://github.com/pytorch/pytorch/pull/64523))\r\n* Added compatibility API ([#61477](https://github.com/pytorch/pytorch/pull/61477), [#57501](https://github.com/pytorch/pytorch/pull/57501))\r\n* Added support operators with default argument in front of out argument ([#63651](https://github.com/pytorch/pytorch/pull/63651), [#63540](https://github.com/pytorch/pytorch/pull/63540))\r\n\r\n## Distributed\r\n\r\n`DistributedDataParallel`\r\n\r\n* Local SGD and variants for DDP communication optimization ([#60303](https://github.com/pytorch/pytorch/pull/60303), [#60320](https://github.com/pytorch/pytorch/pull/60320), [#60632](https://github.com/pytorch/pytorch/pull/60632), [#60891](https://github.com/pytorch/pytorch/pull/60891), [#61206](https://github.com/pytorch/pytorch/pull/61206), [#61207](https://github.com/pytorch/pytorch/pull/61207), [#62105](https://github.com/pytorch/pytorch/pull/62105), [#62111](https://github.com/pytorch/pytorch/pull/62111), [#62131](https://github.com/pytorch/pytorch/pull/62131), [#62132](https://github.com/pytorch/pytorch/pull/62132), [#62392](https://github.com/pytorch/pytorch/pull/62392), [#63277](https://github.com/pytorch/pytorch/pull/63277), [#63340](https://github.com/pytorch/pytorch/pull/63340), [#64885](https://github.com/pytorch/pytorch/pull/64885), [#65197](https://github.com/pytorch/pytorch/pull/65197))\r\n* Provided a noop hook for performance debugging ([#64344](https://github.com/pytorch/pytorch/pull/64344), [#64352](https://github.com/pytorch/pytorch/pull/64352))\r\n* Implemented BF16 allreduce gradient communication hook ([#63260](https://github.com/pytorch/pytorch/pull/63260))\r\n* Allowed retrieval of model parameters in communication hook ([#61637](https://github.com/pytorch/pytorch/pull/61637))\r\n\r\n`torch.distributed`\r\n\r\n* Added a function to create new subgroups of a given size ([#59111](https://github.com/pytorch/pytorch/pull/59111))\r\n* Introduced a new torchrun entry point for elastic ([#64049](https://github.com/pytorch/pytorch/pull/64049))\r\n\r\n## torch.fx\r\n\r\n* Added APIs to mutate specific args/kwargs ([#58571](https://github.com/pytorch/pytorch/pull/58571))\r\n* Introduced EngineHolder for serializing and running TRT Engines with PyTorch ([06399d441d](https://github.com/pytorch/pytorch/commit/06399d441d))\r\n* Introduced `__fx_create_arg__` dunder method for controlling custom classes are handled as node args ([#61780](https://github.com/pytorch/pytorch/pull/61780))\r\n* Added `autowrap_functions` kwarg to Tracer ([#62106](https://github.com/pytorch/pytorch/pull/62106))\r\n* Gradual typing\r\n    * Added type annotation field to nodes ([#60621](https://github.com/pytorch/pytorch/pull/60621))\r\n    * Added experimental gradual typechecker ([#60805](https://github.com/pytorch/pytorch/pull/60805))\r\n    * Extended all experimental type-checking operations to support `conv2d`, `BatchNorm2D`,  `ReLU`, `maxpool2D`, `AdaptiveAvgPooling2D`, `flatten` ([#61093](https://github.com/pytorch/pytorch/pull/61093), [#61012](https://github.com/pytorch/pytorch/pull/61012), [#61150](https://github.com/pytorch/pytorch/pull/61150), [#61188](https://github.com/pytorch/pytorch/pull/61188), [#61239](https://github.com/pytorch/pytorch/pull/61239), [#61265](https://github.com/pytorch/pytorch/pull/61265))\r\n    * Added experimental refinement types and unification for symbolic shape inference ([#61776](https://github.com/pytorch/pytorch/pull/61776))\r\n    * Changed output node handling for typechecker to deal with tuples ([#62582](https://github.com/pytorch/pytorch/pull/62582))\r\n    * Added handle of `get_attr` operations in typechecker ([#62682](https://github.com/pytorch/pytorch/pull/62682))\r\n    * Added equality constraints for some acc operations for symbolic inference ([#63689](https://github.com/pytorch/pytorch/pull/63689))\r\n    * Added inference for algebraic expressions ([#63822](https://github.com/pytorch/pytorch/pull/63822))\r\n* Provided function interface for `remove_duplicate_output_args` ([#65134](https://github.com/pytorch/pytorch/pull/65134))\r\n* Introduced helper function to generate an unique name for an attr in a module ([#64970](https://github.com/pytorch/pytorch/pull/64970))\r\n\r\n## ONNX\r\n\r\n* Added support for ONNX op set 14 ([#59486](https://github.com/pytorch/pytorch/pull/59486))\r\n* Added support for GRU RNNs with packed input in scripting mode ([#58691](https://github.com/pytorch/pytorch/pull/58691))\r\n* Enhanced shape inference ([#64585](https://github.com/pytorch/pytorch/pull/64585))\r\n* Added support for `torch.{linspace, new_ones, nn.LSTMCell, bernoulli, dot, nn.utils.spectral_norm,bernoulli, distributions.normal.Normal, roll}` ([#58854](https://github.com/pytorch/pytorch/pull/58854), [#59255](https://github.com/pytorch/pytorch/pull/59255), [#62757](https://github.com/pytorch/pytorch/pull/62757), [#62765](https://github.com/pytorch/pytorch/pull/62765), [#59536,](https://github.com/pytorch/pytorch/pull/59536)[#61560,](https://github.com/pytorch/pytorch/pull/61560)[#58697](https://github.com/pytorch/pytorch/pull/58697))\r\n\r\n## Infra (Releng)\r\n\r\n* Default Linux/Windows testing workflows were migrated to GitHub Actions. PyTorch Probot has been extended to support new set of rerun command with new set of labels that one can use to opt in and opt out of certain types of CI. More information can be found on [Continuous Integration](https://github.com/pytorch/pytorch/wiki/Continuous-Integration#user-guide) wiki page\r\n* Overall statistics and health of PyTorch CI/CD system can be viewed at [https://metrics.pytorch.org](https://metrics.pytorch.org/) ([#65157](https://github.com/pytorch/pytorch/pull/65157), [#61389](https://github.com/pytorch/pytorch/pull/61389), [#62217](https://github.com/pytorch/pytorch/pull/62217), [#64948](https://github.com/pytorch/pytorch/pull/64948), [#60026](https://github.com/pytorch/pytorch/pull/60026), [#61071](https://github.com/pytorch/pytorch/pull/61071), [#64303](https://github.com/pytorch/pytorch/pull/64303))\r\n* Improved mechanism for disabling tests via issues. Creating an issue which title begins with \u201cDISABLED\u201d followed by the test name will disable the test in question for all platforms, which could be refined by explicitly specifying list of platforms in the issue body. Comment from @pytorch-probot would indicate that issue format was recognized by the CI system and test is now disabled. Closing the issue re-enabled the specified test in CI. Disabled tests will be temporarily re-enabled while running CI for PR marked as fixing it ([#61427](https://github.com/pytorch/pytorch/pull/61427))\r\n* New documentation preview and new artifacts frontend. Using [https://hud.pytorch.org](https://hud.pytorch.org/), one can get an overview of PR/commit CI status, download build artifacts as well as read documentation associated with this build. See [Using HUD](https://github.com/pytorch/pytorch/wiki/Using-hud.pytorch.org) wiki page for more information ([#60711](https://github.com/pytorch/pytorch/pull/60711),  [#60792](https://github.com/pytorch/pytorch/pull/60792), [#60893](https://github.com/pytorch/pytorch/pull/60893))\r\n\r\n## Misc\r\n\r\n* Added support for `torch.fft.` operators on ARM-based platforms using pocket FFT ([#60976](https://github.com/pytorch/pytorch/pull/60976), [#62222](https://github.com/pytorch/pytorch/pull/62222), [#63714](https://github.com/pytorch/pytorch/pull/63714))\r\n* `torch.einsum`: added support for the \u201csublist\u201d format ([#56625](https://github.com/pytorch/pytorch/pull/56625))\r\n* `torch.linalg.det`: added support for complex autograd ([#58195](https://github.com/pytorch/pytorch/pull/58195))\r\n* Added autograd support for `Tensor.to_sparse` ([#58413](https://github.com/pytorch/pytorch/pull/58413))\r\n* Added more CUDA support for CSR layout: constructors ([#59010](https://github.com/pytorch/pytorch/pull/59010)), sparse_to_dense/add_sparse_csr ([#59011](https://github.com/pytorch/pytorch/pull/59011)), addmm/matvec ([#59012](https://github.com/pytorch/pytorch/pull/59012))\r\n* Vulkan: Added support for `max_pool2d`, `tanh`, `hardshrink`, `log_softmax`, `leaky_relu`, `softmax` ([#58806](https://github.com/pytorch/pytorch/pull/58806), [#60695](https://github.com/pytorch/pytorch/pull/60695), [#62870](https://github.com/pytorch/pytorch/pull/62870), [#63193](https://github.com/pytorch/pytorch/pull/63193), [#62239](https://github.com/pytorch/pytorch/pull/62239))\r\n* Enabled local run of clang-tidy and clang-format lint workflows ([#61121](https://github.com/pytorch/pytorch/pull/61121), [#61797](https://github.com/pytorch/pytorch/pull/61797), [#60745](https://github.com/pytorch/pytorch/pull/60745))\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n* Added clearer stack trace for `torch.floor_divide` deprecation warning ([#64034](https://github.com/pytorch/pytorch/pull/64034))\r\n* Use cascade-summation algorithm to improve `torch.nansum` accuracy ([#61082](https://github.com/pytorch/pytorch/pull/61082))\r\n* `torch.i0`: now promote integer inputs to float ([#52735](https://github.com/pytorch/pytorch/pull/52735))\r\n*  `torch.kthvalue:` added change to adjust output dim size for numpy compatibility ([#59214](https://github.com/pytorch/pytorch/pull/59214))\r\n* Added reduce variants for `torch.scatter` operation. ([#57015](https://github.com/pytorch/pytorch/pull/57015))\r\n* Added support for quantized tensors in `torch.testing.assert_close` ([#58926](https://github.com/pytorch/pytorch/pull/58926))\r\n* Improved error message for invalid value input to Distribution methods ([#61056](https://github.com/pytorch/pytorch/pull/61056))\r\n* `torch.isclose` upcast to most precise dtype within their category before the comparison ([#60536](https://github.com/pytorch/pytorch/pull/60536))\r\n* Added change to cast `alpha` to `acc_type` for `torch.add` and `torch.sub` ([#60227](https://github.com/pytorch/pytorch/pull/60227))\r\n* Fixed dimension in the error message for CUDA `torch.cat` shape check and removed unnecessary offending index information ([#64556](https://github.com/pytorch/pytorch/pull/64556)).\r\n* Improved DLPack support ([#57110](https://github.com/pytorch/pytorch/pull/57110)).\r\n* Added change to raise an error when empty index tensor is passed to `torch.gather` ([#65006](https://github.com/pytorch/pytorch/pull/65006)).\r\n* Added change to store `float64` in `tensorboard` instead of `float32` ([#59435](https://github.com/pytorch/pytorch/pull/59435)).\r\n* Added `use_strict_trace` to tensorboard `add_graph` method ([#63120](https://github.com/pytorch/pytorch/pull/63120)).\r\n* Add option to skip GH validation for `torch.hub` ([#62139](https://github.com/pytorch/pytorch/pull/62139))\r\n* Added a new kwarg `output_size` to `tensor.repeat_interleave`([#58881](https://github.com/pytorch/pytorch/pull/58881))\r\n* Add support for `torch.isclose` ([#63571](https://github.com/pytorch/pytorch/pull/63571))\r\n* Make the behavior of `torch.{testting.assert_close,is_close}` consistent with numpy ([#63841](https://github.com/pytorch/pytorch/pull/63841))\r\n\r\n## Autograd\r\n\r\n* Added warning about memory leak when `.backward()` is called with `create_graph=True` ([#59412](https://github.com/pytorch/pytorch/pull/59412))\r\n* Added warning when accessing `Tensor::grad()` on a non-leaf Tensor in the C++ API ([#59362](https://github.com/pytorch/pytorch/pull/59362))\r\n* Fixed error message formatting in `grad_output` creation for `.backward()` and `autograd.grad()` ([#59532](https://github.com/pytorch/pytorch/pull/59532))\r\n* Added change to raise `NotImplementedError` for forward and backward-mode AD formulas that are not implemented ([#59482](https://github.com/pytorch/pytorch/pull/59482), [#59483](https://github.com/pytorch/pytorch/pull/59483))\r\n* Reduced memory usage for `torch.relu` for common use cases ([#63089](https://github.com/pytorch/pytorch/pull/63089))\r\n* Added support for non-leaf inputs for `autograd.backward()` function `inputs` argument ([#60521](https://github.com/pytorch/pytorch/pull/60521))\r\n* Improved error message when a tensor with `requires_grad=True`  is passed to a non-differentiable function ([#60610](https://github.com/pytorch/pytorch/pull/60610))\r\n* Made `binary_cross_entropy` differentiable w.r.t. `target` ([#59447](https://github.com/pytorch/pytorch/pull/59447))\r\n\r\n## torch.nn\r\n\r\n* Added support for inputs with no batch dimensions for `nn.{AdaptiveAvgPool*d, AdaptiveMaxPool*d, AvgPool*d, CosineEmbeddingLoss, Dropout, FractionalMaxPool2d, Linear, LPPool1d, MaxPool*d, MaxUnpool*d, NLLLoss, PairwiseDistance, ReflectionPad*d, ReplicationPad*d, TripletMarginLoss, ZeroPad*d}`, most other loss modules, and all activation modules ([#61264](https://github.com/pytorch/pytorch/pull/61264), [#61847](https://github.com/pytorch/pytorch/pull/61847), [#61860](https://github.com/pytorch/pytorch/pull/61860), [#64590](https://github.com/pytorch/pytorch/pull/64590), [#61911](https://github.com/pytorch/pytorch/pull/61911), [#62490](https://github.com/pytorch/pytorch/pull/62490), [#60992](https://github.com/pytorch/pytorch/pull/60992), [#62190](https://github.com/pytorch/pytorch/pull/62190), [#62206](https://github.com/pytorch/pytorch/pull/62206), [#61984](https://github.com/pytorch/pytorch/pull/61984), [#61310](https://github.com/pytorch/pytorch/pull/61310), [#62651](https://github.com/pytorch/pytorch/pull/62651), [#64882](https://github.com/pytorch/pytorch/pull/64882), [#62183](https://github.com/pytorch/pytorch/pull/62183), [#61060](https://github.com/pytorch/pytorch/pull/61060), [#61262](https://github.com/pytorch/pytorch/pull/61262), [#62729](https://github.com/pytorch/pytorch/pull/62729), [#61300](https://github.com/pytorch/pytorch/pull/61300), [#61461](https://github.com/pytorch/pytorch/pull/61461), [#62726](https://github.com/pytorch/pytorch/pull/62726))\r\n* Added support for inputs with 0 batch size for `nn.{AdaptiveAvgPool*d, AdaptiveMaxPool*d, Bilinear, FractionalMaxPool*d, LocalResponseNorm, MaxPool*d, MaxUnpool*d, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer}` ([#62025](https://github.com/pytorch/pytorch/pull/62025), [#62088](https://github.com/pytorch/pytorch/pull/62088), [#47106](https://github.com/pytorch/pytorch/pull/47106), [#62083](https://github.com/pytorch/pytorch/pull/62083), [#62801](https://github.com/pytorch/pytorch/pull/62801), [#64082](https://github.com/pytorch/pytorch/pull/64082), [#62800](https://github.com/pytorch/pytorch/pull/62800))\r\n* Parametrization: Added support for nested parametrizations, parametrizations depending on several inputs, resizing of parametrized tensors, and the orthogonal parametrization ([#65167](https://github.com/pytorch/pytorch/pull/65167), [#60530](https://github.com/pytorch/pytorch/pull/60530), [#60418](https://github.com/pytorch/pytorch/pull/60418), [#62089](https://github.com/pytorch/pytorch/pull/62089))\r\n* `nn.AvgPool2d`: Added `channels_last` support on CPU ([#58725](https://github.com/pytorch/pytorch/pull/58725))\r\n* `nn.BatchNorm`: Use `resize_output` and `empty` instead of `empty_like` to improve flexibility in output memory format choice ([#63084](https://github.com/pytorch/pytorch/pull/63084))\r\n* `nn.Bilinear`: Added support for non-contiguous tensor inputs ([#38409](https://github.com/pytorch/pytorch/pull/38409))\r\n* `nn.GELU`: Added support for fp32/bfloat16 in CPU path using mkldnn implementation ([#58525](https://github.com/pytorch/pytorch/pull/58525))\r\n* `nn.GroupNorm`: Improved numerical stability by using the Welford algorithm and cascade summation ([#54921](https://github.com/pytorch/pytorch/pull/54921))\r\n* `nn.LayerNorm`: Improved numerical stability by using the Welford algorithm and pairwise sums ([#59987](https://github.com/pytorch/pytorch/pull/59987))\r\n* `nn.NLLLoss`: Added support for target of dtype `byte` ([#60308](https://github.com/pytorch/pytorch/pull/60308), [#60650](https://github.com/pytorch/pytorch/pull/60650))\r\n* `nn.SmoothL1Loss`: Added support for integral target within the backward pass ([#61112](https://github.com/pytorch/pytorch/pull/61112))\r\n* `nn.Transformer`: Added configurable pre/post LayerNorm placement ([#60593](https://github.com/pytorch/pytorch/pull/60593), [#61692](https://github.com/pytorch/pytorch/pull/61692))\r\n* Added check to verify non-zero sequence length for `nn.{RNN, LSTM, GRU}` ([#60269](https://github.com/pytorch/pytorch/pull/60269))\r\n* Added support for bfloat16 in CPU path to `nn.{LeakyReLU, RReLU}` ([#61514](https://github.com/pytorch/pytorch/pull/61514))\r\n* Added support for `channels_last` memory format in `nn.{AdaptiveMaxPool2d, GroupNorm}` ([#48920](https://github.com/pytorch/pytorch/pull/48920), [#49821](https://github.com/pytorch/pytorch/pull/49821))\r\n* Added callable activation function support to `nn.{MultiheadAttention, Transformer, TransformerDecoderLayer, TransformerEncoderLayer}` ([#61355](https://github.com/pytorch/pytorch/pull/61355), [#62342](https://github.com/pytorch/pytorch/pull/62342))\r\n\r\n## Profiler\r\n\r\n* Changed `profiler.profile` argument `with_flops`  when set to `True` to report total FLOPs rather than FLOP/s, and support more operators ([#62779](https://github.com/pytorch/pytorch/pull/62779), [#61895](https://github.com/pytorch/pytorch/pull/61895))\r\n* Improved memory profiling and Tensorboard memory view, enabling better understanding of memory usage by showing active memory allocations at various points of your program run as well as a memory usage trend chart.  ([#61282](https://github.com/pytorch/pytorch/pull/61282), [`#361`](https://github.com/pytorch/kineto/pull/361), [`#404`](https://github.com/pytorch/kineto/pull/404)[,](https://github.com/pytorch/kineto/pull/435/commits/36f069ad8f819255f5b575782e99b0c4573a6d0f)[`#416`](https://github.com/pytorch/kineto/pull/416)[,](https://github.com/pytorch/kineto/pull/435/commits/d6d28b719270b1ceb10fca1003cfb77a11e18c79)[`#421`](https://github.com/pytorch/kineto/pull/421))\r\n* Added flow arrows between ops in the forward pass and the corresponding ops in the backward pass in the trace view ([#62553](https://github.com/pytorch/pytorch/pull/62553), [#372](https://github.com/pytorch/kineto/pull/372))\r\n* Increased profiling coverage of backward pass ([#63619](https://github.com/pytorch/pytorch/pull/63619))\r\n* Made threads and GPU streams appear in a consistent sorted order in the trace view ([#399](https://github.com/pytorch/kineto/pull/399))\r\n* Added shapes and reg usage to the GPU kernel view ([`#351`](https://github.com/pytorch/kineto/pull/351)[)](https://github.com/pytorch/kineto/pull/402/commits/eed895ba7ce521deb457dee4678d7a6c8a4a7bd6)\r\n\r\n## Dataloader\r\n\r\n* Properly delegated indices called by `Subset` to dataset ([#59513](https://github.com/pytorch/pytorch/pull/59513))\r\n* Removed the restriction that input datasets in `ConcatDataset` must be `Sized` ([#64114](https://github.com/pytorch/pytorch/pull/64114))\r\n* Allowed annotation of `IterableDataset` to accept keyword-only arguments and `abc` class ([#58450](https://github.com/pytorch/pytorch/pull/58450))\r\n* Changed annotation of `DataLoader` to accept non-integer `Sampler` as input([#63500](https://github.com/pytorch/pytorch/pull/63500))\r\n\r\n## CUDA\r\n\r\n* Include function name in the error message for inputs being on different devices ([#58502](https://github.com/pytorch/pytorch/pull/58502))\r\n* Fix MAGMA initialization ([#58521](https://github.com/pytorch/pytorch/pull/58521))\r\n* Updated NCCL to 2.10 ([#62276](https://github.com/pytorch/pytorch/pull/62276))\r\n* Added deterministic path for `torch.scatter_add` for 1D tensors ([#58761](https://github.com/pytorch/pytorch/pull/58761))\r\n* Added CUDA support for mean reduction ([#59543](https://github.com/pytorch/pytorch/pull/59543))\r\n* Add missing CUDA kernel launch check ([#60114](https://github.com/pytorch/pytorch/pull/60114))\r\n* Improved CUDA extension building error/warning messages ([#59665](https://github.com/pytorch/pytorch/pull/59665), [#60592](https://github.com/pytorch/pytorch/pull/60592))\r\n* Added change to compute CUDA reduction buffer size in elements ([#63969](https://github.com/pytorch/pytorch/pull/63969))\r\n\r\n## TorchScript\r\n\r\n* Added change to simplify pass on arithmetic expressions for integers. ([#61444](https://github.com/pytorch/pytorch/pull/61444)) \r\n* Set future's error to current exception as is when `--torch_jit_enable_rethrow_caught_exception=true` ([#63348](https://github.com/pytorch/pytorch/pull/63348)) \r\n* Improved TorchScript module getattr() to be same as python class getattr() method ([#61599](https://github.com/pytorch/pytorch/pull/61599)) \r\n* Improved slicing for scripted version of `torch.nn.ModuleList` to support arbitrary step size ([#58361](https://github.com/pytorch/pytorch/pull/58361)) \r\n* Added parsing logic for `Tuple[()]` annotation ([#58340](https://github.com/pytorch/pytorch/pull/58340)) \r\n* Changed list striding kernel implementation to handle optional integers ([#58536](https://github.com/pytorch/pytorch/pull/58536)) \r\n* Added support for `torch.nn.Parameter` type for Profile-Directed-Typing ([#59249](https://github.com/pytorch/pytorch/pull/59249)) \r\n* Added change to annotate NoneType as Optional[type] ([#60383](https://github.com/pytorch/pytorch/pull/60383)) \r\n* Added support for default values on NamedTuple fields ([#54682](https://github.com/pytorch/pytorch/pull/54682)) \r\n* Improved JIT support for `torch.einsum` ([#59265](https://github.com/pytorch/pytorch/pull/59265)) \r\n* Added change to allow for heterogenous List and Dict values + Improve container typing algorithm ([#57137](https://github.com/pytorch/pytorch/pull/57137)) \r\n* Added support for eager mode use of `torch.jit.isinstance` with multiple types ([#60465](https://github.com/pytorch/pytorch/pull/60465)) \r\n* Allowed uncompiled strings as input to `checkScriptRaisesRegex` ([#63901](https://github.com/pytorch/pytorch/pull/63901))\r\n* Introduced more robust check of whether a class is defined in torch ([#64083](https://github.com/pytorch/pytorch/pull/64083)) \r\n* Added change to preserve types during empty container assignment ([#58911](https://github.com/pytorch/pytorch/pull/58911)) \r\n* Made JIT not assume that the device is CUDA. ([#54238](https://github.com/pytorch/pytorch/pull/54238)) \r\n* Updated `optimize_for_mobile` to preserve nodes\u2019 debug information ([#63106](https://github.com/pytorch/pytorch/pull/63106)) \r\n* Added support for device as Dict key ([#65079](https://github.com/pytorch/pytorch/pull/65079))  \r\n* Added support for Python C extension modules in `torch::deploy` ([#58117](https://github.com/pytorch/pytorch/pull/58117)) \r\n* Added a flag to suppress stacktrace in exception messages([#63073](https://github.com/pytorch/pytorch/pull/63073)) \r\n* Added API to change logging levels for JIT ([#58821](https://github.com/pytorch/pytorch/pull/58821)) \r\n* Provided API to preserve source range and callstack information during graph rewrite ([#58300](https://github.com/pytorch/pytorch/pull/58300)) \r\n* Re-enabled BatchNorm autodiff  ([#57321](https://github.com/pytorch/pytorch/pull/57321)) \r\n* Extracted element-wise ops supported by JIT fuser into a separate list ([#59579](https://github.com/pytorch/pytorch/pull/59579)) \r\n* Reworked requires_grad on DifferentiableGraphOp ([#57575](https://github.com/pytorch/pytorch/pull/57575)) \r\n\r\n## torch.package\r\n\r\n* Unified three categories of dependency handling error (broken, denied, unhandled) into a single \"error\" field in the node, with optional context ([#58572](https://github.com/pytorch/pytorch/pull/58572))\r\n* Renamed MockZipReader into DirectoryReader ([#59107](https://github.com/pytorch/pytorch/pull/59107))\r\n* Added change to silently skip cases where the __**import__** statement cannot be parsed ([#61148](https://github.com/pytorch/pytorch/pull/61148))\r\n* Make torch::deploy work with or without cuda ([#58493](https://github.com/pytorch/pytorch/pull/58493))\r\n\r\n## Mobile\r\n\r\n* Added check to ensure op name does not contain open parenthesis ([#58687](https://github.com/pytorch/pytorch/pull/58687))\r\n* Added handles and symbolicate exception callstack thrown from backend ([#55462](https://github.com/pytorch/pytorch/pull/55462), [#57441](https://github.com/pytorch/pytorch/pull/57441), [#57481](https://github.com/pytorch/pytorch/pull/57481))\r\n* Enabled implicit operator versioning via number of arguments ([#58852](https://github.com/pytorch/pytorch/pull/58852))\r\n* Cleaned up unused APIs and improve debugging experience for iOS GPU ([#60280](https://github.com/pytorch/pytorch/pull/60280), [#60281,](https://github.com/pytorch/pytorch/pull/60281)[#60282](https://github.com/pytorch/pytorch/pull/60282))\r\n* Added debug information to track memory allocation exception for Metal ([#59112](https://github.com/pytorch/pytorch/pull/59112))\r\n* Added print of IValue type name in error message for Android ([#64602](https://github.com/pytorch/pytorch/pull/64602))\r\n* Added print of error message when failing to load model file ([#63404](https://github.com/pytorch/pytorch/pull/63404))\r\n* Introduced multiple improvements in `torch.utils.model_dump` APIs: \r\n    * Make stdout argument for main kwarg-only ([#60699](https://github.com/pytorch/pytorch/pull/60699))\r\n    * Implement \"Hider\" properly ([#57654](https://github.com/pytorch/pytorch/pull/57654))\r\n    * Handle `torch.device` objects ([#57656](https://github.com/pytorch/pytorch/pull/57656))\r\n    * Handle dict rendering ([#57657](https://github.com/pytorch/pytorch/pull/57657))\r\n    * Add a section that summarizes tensor memory usage ([#57658](https://github.com/pytorch/pytorch/pull/57658))\r\n    * Handle invalid UTF-8 in pickles ([#57661](https://github.com/pytorch/pytorch/pull/57661))\r\n\r\n## Quantization\r\n\r\n* Added out variant for int8 `quantized::linear` ([#58282](https://github.com/pytorch/pytorch/pull/58282)) and `quantized::embedding_bag_byte_prepack` ([#64081](https://github.com/pytorch/pytorch/pull/64081))\r\n* FX graph mode quantization: improve `qconfig_dict` argument handling ([#59605](https://github.com/pytorch/pytorch/pull/59605), [#58566](https://github.com/pytorch/pytorch/pull/58566))\r\n* Added support to embedding trained in FP16 ([#60736](https://github.com/pytorch/pytorch/pull/60736))\r\n* Added support for `torch.index_select` on quantized tensors ([#61406](https://github.com/pytorch/pytorch/pull/61406))\r\n* Added a new fused MovingAvg Obs + FakeQuant operator ([#61570](https://github.com/pytorch/pytorch/pull/61570), [#61589](https://github.com/pytorch/pytorch/pull/61589), [#61691](https://github.com/pytorch/pytorch/pull/61691), [#62346](https://github.com/pytorch/pytorch/pull/62346), [#62863](https://github.com/pytorch/pytorch/pull/62863), [#62702](https://github.com/pytorch/pytorch/pull/62702), [#63043](https://github.com/pytorch/pytorch/pull/63043), [#64829](https://github.com/pytorch/pytorch/pull/64829))\r\n* Added support for dynamic linear + relu fusion (INT8) ([#63799](https://github.com/pytorch/pytorch/pull/63799),[#63826](https://github.com/pytorch/pytorch/pull/63826))\r\n* Enabled JIT tracing on quantizable LSTM ([#64438](https://github.com/pytorch/pytorch/pull/64438))\r\n\r\n## Distributed\r\n\r\n`DistributedDataParallel`\r\n\r\n* Added error logging to DDP logging API ([#59281](https://github.com/pytorch/pytorch/pull/59281), [#59284](https://github.com/pytorch/pytorch/pull/59284), [#59351,](https://github.com/pytorch/pytorch/pull/59351)[#65023](https://github.com/pytorch/pytorch/pull/65023))\r\n* Added `NCCL_ASYNC_ERROR_HANDLING` environment variable to control NCCL error handling ([#59109](https://github.com/pytorch/pytorch/pull/59109))\r\n* Communication hook APIs to always return single tensor ([#62074](https://github.com/pytorch/pytorch/pull/62074), [#62389](https://github.com/pytorch/pytorch/pull/62389), [#62457](https://github.com/pytorch/pytorch/pull/62457))\r\n* Added DDP bucket sizes in DDP logging API ([#62229](https://github.com/pytorch/pytorch/pull/62229), [#62232](https://github.com/pytorch/pytorch/pull/62232), [#62231](https://github.com/pytorch/pytorch/pull/62231), [#62625](https://github.com/pytorch/pytorch/pull/62625), \r\n* Improved rebuilding buckets logic  ([#62279](https://github.com/pytorch/pytorch/pull/62279), [#58097](https://github.com/pytorch/pytorch/pull/58097))\r\n* Allowed DDP uneven inputs work with communication hooks ([#61017](https://github.com/pytorch/pytorch/pull/61017), [#61018](https://github.com/pytorch/pytorch/pull/61018), [#61019](https://github.com/pytorch/pytorch/pull/61019), [#61020](https://github.com/pytorch/pytorch/pull/61020))\r\n* Added logging if graph is static at end of training ([#61871](https://github.com/pytorch/pytorch/pull/61871))\r\n* Added logging of unused param names under DETAIL debug mode. ([#62209](https://github.com/pytorch/pytorch/pull/62209))\r\n* Allowed tuning of first bucket in DDP ([#62748](https://github.com/pytorch/pytorch/pull/62748))\r\n* Added gradient ready order, host-side timestamps, and bucket indices to DDP logging ([#62751](https://github.com/pytorch/pytorch/pull/62751), [#62770](https://github.com/pytorch/pytorch/pull/62770))\r\n* Added a debug check in C++ fp16 gradient hook ([#63379](https://github.com/pytorch/pytorch/pull/63379))\r\n* Added a fallback to use `mul` and `copy_` instead of `mul`\u2019s `out=` variant when gradient tensor requires grad in DDP ([#63831](https://github.com/pytorch/pytorch/pull/63831))\r\n* Used `Tensor.set_` instead of directory assigning data in model averaging ([#63895](https://github.com/pytorch/pytorch/pull/63895))\r\n* Added more iterations for DDP logging ([#64071](https://github.com/pytorch/pytorch/pull/64071),  [#64411](https://github.com/pytorch/pytorch/pull/64411))\r\n\r\n`torch.distributed`\r\n\r\n* Introduced ProcessGroup wrapper and use it in debug mode([#58224](https://github.com/pytorch/pytorch/pull/58224), [#58281](https://github.com/pytorch/pytorch/pull/58281), [#60237](https://github.com/pytorch/pytorch/pull/60237))\r\n* Made a small change for `torch.distributed` launcher ([#59152](https://github.com/pytorch/pytorch/pull/59152))\r\n* Added complex number support for all_to_all/scatter ([#61299](https://github.com/pytorch/pytorch/pull/61299))\r\n* Made gloo communication profiling more accurate ([#61342](https://github.com/pytorch/pytorch/pull/61342))\r\n* Used generator instead of list to save memory in scatter ([#62516](https://github.com/pytorch/pytorch/pull/62516))\r\n* Provided failure reason from ProcessGroup when aborting NCCL communicator ([#64241](https://github.com/pytorch/pytorch/pull/64241))\r\n* Introduced error raised when capturing uncapturable NCCL in CUDA graphs. ([#64440](https://github.com/pytorch/pytorch/pull/64440))\r\n* Added Single-Machine Model Parallel Support to `torch.distributed.optim.ZeroRedundancyOptimizer` ([#61370](https://github.com/pytorch/pytorch/pull/61370))\r\n\r\n`torch.distributed.nn.RemoteModule`\r\n\r\n* Supported creating a RemoteModule by RRef ([#59242](https://github.com/pytorch/pytorch/pull/59242))\r\n* Supported switching RemoteModule between train/eval ([#59026](https://github.com/pytorch/pytorch/pull/59026))\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Added minor logging and error formatting improvements ([#63214](https://github.com/pytorch/pytorch/pull/63214),  [#62823](https://github.com/pytorch/pytorch/pull/62823))\r\n* Improved process termination logic ([#61602](https://github.com/pytorch/pytorch/pull/61602))\r\n* Added fqdn hostname to error printout ([#66662](https://github.com/pytorch/pytorch/pull/66662/))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Fix RPC initialization to avoid shutdown timeout ([#59801](https://github.com/pytorch/pytorch/pull/59801))\r\n* Supported RRefs that contain `threading.Locks` ([#57943](https://github.com/pytorch/pytorch/pull/57943)), `torch.cuda.Event` ([#61354](https://github.com/pytorch/pytorch/pull/61354))\r\n* Updated rpc tensorpipe logic for sparse tensors ([#64575](https://github.com/pytorch/pytorch/pull/64575))\r\n* Added rpc sparse tensor fix ([#59609](https://github.com/pytorch/pytorch/pull/59609), [#62794](https://github.com/pytorch/pytorch/pull/62794))\r\n* Added change to ensure that future completion doesn't swallow exception. ([#61094](https://github.com/pytorch/pytorch/pull/61094))\r\n* Set streams when invoking UDFs ([#59210](https://github.com/pytorch/pytorch/pull/59210))\r\n* Set and propagate devices in RRef completion Future ([#59211](https://github.com/pytorch/pytorch/pull/59211))\r\n* Made TensorPipe agent use streams from Future when sending response ([#59212](https://github.com/pytorch/pytorch/pull/59212))\r\n* Added change to leverage TensorPipe's automatic SHM address selection ([#63028](https://github.com/pytorch/pytorch/pull/63028))\r\n* Made Future store Storages instead of references to DataPtrs ([#60470](https://github.com/pytorch/pytorch/pull/60470), [#60943](https://github.com/pytorch/pytorch/pull/60943))\r\n* Added change to avoid re-doing CUDA stream sync in OwnerRRef ([#57355](https://github.com/pytorch/pytorch/pull/57355))\r\n\r\n`torch.distributed.Store`\r\n\r\n* Enhanced connect timeout error message ([#61390](https://github.com/pytorch/pytorch/pull/61390))\r\n* Added minor fixes in c10d for Windows ([#62953](https://github.com/pytorch/pytorch/pull/62953))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* Supported non-tensor inputs in pipeline parallel API ([#55441](https://github.com/pytorch/pytorch/pull/55441), [#57226](https://github.com/pytorch/pytorch/pull/57226), [#57325](https://github.com/pytorch/pytorch/pull/57325))\r\n* Added a `WithDevice` wrapper to specify device execution for a module. ([#65190](https://github.com/pytorch/pytorch/pull/65190))\r\n\r\n## torch.fx\r\n\r\n* Added users of a node to the serialized JSON ([#59357](https://github.com/pytorch/pytorch/pull/59357))\r\n* Added requires_grad to TensorMetadata ([#60972](https://github.com/pytorch/pytorch/pull/60972))\r\n* Added change to swap out Python's AnnAssign with an Assign node where the annotation function is called ([#60622](https://github.com/pytorch/pytorch/pull/60622))\r\n* Added type annotations for the `torch.nn.Module` constructor ([#61334](https://github.com/pytorch/pytorch/pull/61334))\r\n* Enabled `torch.deploy` for GraphModules with non-torch dependencies ([#61680](https://github.com/pytorch/pytorch/pull/61680))\r\n* Added change to allow FX tracer to trace control flow (if/while) statements when parameter shapes are in the conditionals ([#61820](https://github.com/pytorch/pytorch/pull/61820))\r\n* Added `torch.memory_format` as a BaseArgumentType ([#62593](https://github.com/pytorch/pytorch/pull/62593))\r\n* Added backwards compatibility guarantees for 1.10 ([#63888](https://github.com/pytorch/pytorch/pull/63888))\r\n    * Renamed reduce functions back to their old, public names ([#64324](https://github.com/pytorch/pytorch/pull/64324))\r\n    * Added change to ensure BC coverage for all of `torch.fx` passes ([#65081](https://github.com/pytorch/pytorch/pull/65081))\r\n* Add `__matmul__` to the magic methods for FX tracing ([#64512](https://github.com/pytorch/pytorch/pull/64512))\r\n\r\n## Composability\r\n\r\n* Added meta tensor support for `torch.{any, all, fmax, fmin, remainder, glu, argmax, argmin, avg_pool3d_backward, isposinf, isneginf, fmod, fmin, signbit, slow_conv_transpose2d, nll_loss_backward, cumprod, aminmax, addcmul, addcdiv, gather, hardshrink_backward, softshrink_backward, hardshrink, gelu, gelu_backward, avg_pool2d, avg_pool2d_backward, avg_pool3d, reflection_pad1d_backward, all, any, silu_backward, sgn, softplus, leaky_relu_backward, hardsigmoid_backward, elu_backward, eq, xlogy, ne, lt, gt, le, ge, sigmoid_backward, tanh_backward, logit_backward, bitwise_or, bitwise_xor, bitwise_and, nll_loss_forward, log_softmax, log_softmax_backward_data, prod, norm, sum.dim_IntList, clamp}` ([#64642](https://github.com/pytorch/pytorch/pull/64642), [#58458,](https://github.com/pytorch/pytorch/pull/58458)[#58732](https://github.com/pytorch/pytorch/pull/58732), [#61800](https://github.com/pytorch/pytorch/pull/61800), [#60363](https://github.com/pytorch/pytorch/pull/60363), [#60364](https://github.com/pytorch/pytorch/pull/60364), [#59084](https://github.com/pytorch/pytorch/pull/59084), [#60633](https://github.com/pytorch/pytorch/pull/60633), [#60809](https://github.com/pytorch/pytorch/pull/60809), [#60810](https://github.com/pytorch/pytorch/pull/60810), [#57936](https://github.com/pytorch/pytorch/pull/57936), [#55503](https://github.com/pytorch/pytorch/pull/55503), [#62144](https://github.com/pytorch/pytorch/pull/62144), [#61899](https://github.com/pytorch/pytorch/pull/61899), [#62401](https://github.com/pytorch/pytorch/pull/62401), [#62318](https://github.com/pytorch/pytorch/pull/62318), [#62319](https://github.com/pytorch/pytorch/pull/62319), [#63312](https://github.com/pytorch/pytorch/pull/63312), [#58662](https://github.com/pytorch/pytorch/pull/58662), [#58663](https://github.com/pytorch/pytorch/pull/58663), [#58664](https://github.com/pytorch/pytorch/pull/58664), [#58665](https://github.com/pytorch/pytorch/pull/58665), [#58987](https://github.com/pytorch/pytorch/pull/58987), [#59082](https://github.com/pytorch/pytorch/pull/59082), [#59083](https://github.com/pytorch/pytorch/pull/59083), [#59103](https://github.com/pytorch/pytorch/pull/59103), [#60360](https://github.com/pytorch/pytorch/pull/60360), [#60361](https://github.com/pytorch/pytorch/pull/60361), [#58661](https://github.com/pytorch/pytorch/pull/58661), [#58197](https://github.com/pytorch/pytorch/pull/58197), [#58482](https://github.com/pytorch/pytorch/pull/58482), [#58483](https://github.com/pytorch/pytorch/pull/58483), [#58484](https://github.com/pytorch/pytorch/pull/58484), [#58660](https://github.com/pytorch/pytorch/pull/58660), [#60177](https://github.com/pytorch/pytorch/pull/60177), [#60814](https://github.com/pytorch/pytorch/pull/60814), [#60942](https://github.com/pytorch/pytorch/pull/60942), [#60815](https://github.com/pytorch/pytorch/pull/60815), [#60816](https://github.com/pytorch/pytorch/pull/60816), [#60817](https://github.com/pytorch/pytorch/pull/60817), [#60811](https://github.com/pytorch/pytorch/pull/60811), [#60812](https://github.com/pytorch/pytorch/pull/60812), [#60813](https://github.com/pytorch/pytorch/pull/60813), [#61443](https://github.com/pytorch/pytorch/pull/61443), [#57374](https://github.com/pytorch/pytorch/pull/57374), [#62372](https://github.com/pytorch/pytorch/pull/62372), [#62024](https://github.com/pytorch/pytorch/pull/62024), [#62711](https://github.com/pytorch/pytorch/pull/62711), [#61642](https://github.com/pytorch/pytorch/pull/61642), [#61361](https://github.com/pytorch/pytorch/pull/61361))\r\n* PyObject preservation: Previously, tensors in python that no longer had any python-side references (but still had references in C++, e.g. if it\u2019s saved for autograd) would get deallocated, and we would create a new Python object to replace it next time it passes from C++ to Python. We now preserve the PyObject as long as there are any references on either the python or C++ side. This ensures that any metadata on the original python object is preserved. For example, tensor subclasses that were saved for autograd now get properly preserved. ([#56017](https://github.com/pytorch/pytorch/pull/56017))\r\n\r\n## Build_Frontend\r\n\r\n* Added a new include directory in BLIS search path ([#58166](https://github.com/pytorch/pytorch/pull/58166))\r\n* Added print to show full Python version in `torch.utils.collect_env` ([#59632](https://github.com/pytorch/pytorch/pull/59632))\r\n* Added change to respect `CMAKE_PREFIX_PATH` choice set by caller ([#61904](https://github.com/pytorch/pytorch/pull/61904))\r\n* Dropped incremental linking on Windows when REL_WITH_DEB_INFO=1. ([#64892](https://github.com/pytorch/pytorch/pull/64892))\r\n* Enabled kineto build for ROCm platform ([#58401](https://github.com/pytorch/pytorch/pull/58401))\r\n* Added support to system-provided Intel TBB ([#61934](https://github.com/pytorch/pytorch/pull/61934))\r\n* Added Pytorch build support with [Newlib](https://en.wikipedia.org/wiki/Newlib) c library ([#60345](https://github.com/pytorch/pytorch/pull/60345), [#60052](https://github.com/pytorch/pytorch/pull/60052))\r\n* Imrpove `torch.__version__` comparisons ([#61556](https://github.com/pytorch/pytorch/pull/61556), [#64565](https://github.com/pytorch/pytorch/pull/64565), [#63848](https://github.com/pytorch/pytorch/pull/63848))\r\n* CMake: added optional precompiled header support ([#61940](https://github.com/pytorch/pytorch/pull/61940))\r\n* Removed unnecessary Ubuntu version checks ([#61738](https://github.com/pytorch/pytorch/pull/61738))\r\n* Added GPU support to `bazel` builds ([#63604](https://github.com/pytorch/pytorch/pull/63604))\r\n\r\n## Infra (Releng)\r\n\r\n* Improved automated test sharding. ([#59727](https://github.com/pytorch/pytorch/pull/59727), [#60206](https://github.com/pytorch/pytorch/pull/60206))\r\n* Added change to strictly type everything in .github and tools ([#59117](https://github.com/pytorch/pytorch/pull/59117))\r\n* Upgraded Windows CI Python to 3.8 ([#59729](https://github.com/pytorch/pytorch/pull/59729)) and CUDA to 10.2 ([#65080](https://github.com/pytorch/pytorch/pull/65080))\r\n* Made change to use expecttest from PyPI ([#60658](https://github.com/pytorch/pytorch/pull/60658), [#63320](https://github.com/pytorch/pytorch/pull/63320))\r\n* Added option to run specified tests option to run_test.py ([#59649](https://github.com/pytorch/pytorch/pull/59649))\r\n* Enabled Metal in PyTorch MacOS/iOS nightly builds ([#63718](https://github.com/pytorch/pytorch/pull/63718), [#65075](https://github.com/pytorch/pytorch/pull/65075))\r\n* Added retries to flaky CI steps. ([#65013](https://github.com/pytorch/pytorch/pull/65013), [#65104](https://github.com/pytorch/pytorch/pull/65104), [#64120](https://github.com/pytorch/pytorch/pull/64120), [#60216](https://github.com/pytorch/pytorch/pull/60216), [#63319](https://github.com/pytorch/pytorch/pull/63319))\r\n* Allowed Docker build on macOS ([#60375](https://github.com/pytorch/pytorch/pull/60375))\r\n\r\n## Misc\r\n\r\n* Added support for MIOpen channel last convolution ([#63617](https://github.com/pytorch/pytorch/pull/63617))\r\n* Enabled kernel asserts on rocm ([#49624](https://github.com/pytorch/pytorch/pull/49624))\r\n* Added bool, float16, bfloat16 and complex support for to_dense for CSR sparse Tensors ([#60657](https://github.com/pytorch/pytorch/pull/60657))\r\n* Added complex dtype support for matrix multiplication of two COO sparse Tensors on CPU ([#59554](https://github.com/pytorch/pytorch/pull/59554))\r\n* Added the \u201cupper\u201d kwarg to `torch.linalg.cholesky` ([#62434](https://github.com/pytorch/pytorch/pull/62434))\r\n* Improved error message in ONNX when attempting to export dict modification ([#58696](https://github.com/pytorch/pytorch/pull/58696))\r\n* Migrated `THAllocator` to `MapAllocator` in ATen ([#60325](https://github.com/pytorch/pytorch/pull/60325))\r\n* Converted input type of `TensorOptions.device_index` from `int16_t` to to `c10::DeviceIndex` ([#60412](https://github.com/pytorch/pytorch/pull/60412))\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n* Added fix to recognize transposed dense tensors as a form of partial overlap ([#59014](https://github.com/pytorch/pytorch/pull/59014))\r\n* Fixed `torch.polygamma` incorrect behavior at infinites when n>=1 ([#61641](https://github.com/pytorch/pytorch/pull/61641))\r\n* Fixed for non-contiguous inputs for `torch.{sort,topk}` on CUDA ([#63029](https://github.com/pytorch/pytorch/pull/63029)), `torch.tensor_split` indices([#63390](https://github.com/pytorch/pytorch/pull/63390))\r\n* Fixed legacy constructor `torch.Tensor`  when given a scalar Tensor ([#58885](https://github.com/pytorch/pytorch/pull/58885))\r\n* Added change to not wrap `Tensor.{grad,_base}` by default for Tensor-like objects([#60464](https://github.com/pytorch/pytorch/pull/60464))\r\n* Fixed `torch.angle` on aarch64 ([#59832](https://github.com/pytorch/pytorch/pull/59832))\r\n* Fixed specialized convolution kernel on arm64 ([#60460](https://github.com/pytorch/pytorch/pull/60460))\r\n* `torch.normal`: fixed RuntimeError when standard deviation named arg is torch.empty [(#66524](https://github.com/pytorch/pytorch/pull/66524/))\r\n* Fixed random sampling on SGX platforms ([#60368](https://github.com/pytorch/pytorch/pull/60368))\r\n* Fixed testing when Scipy is not available ([#61699](https://github.com/pytorch/pytorch/pull/61699))\r\n* Fixed `torch.Tensor.copy_` when using large inputs and broadcasting ([#64425](https://github.com/pytorch/pytorch/pull/64425))\r\n* Fixed broadcasting behavior for `torch.trapezoid` ([#64054](https://github.com/pytorch/pytorch/pull/64054)).\r\n* Fixed dtype check of comparison ops ([#64267](https://github.com/pytorch/pytorch/pull/64267)).\r\n* Fixed `torch.median` crash on empty tensor ([#61698](https://github.com/pytorch/pytorch/pull/61698))\r\n* Fixed missing lazy initialization in `torch.get_num_threads` ([#64486](https://github.com/pytorch/pytorch/pull/64486))\r\n* Fixed check for empty named dims list to `torch.flatten` ([#61953](https://github.com/pytorch/pytorch/pull/61953))\r\n* Fixed `torch.hub.{list,help}` functions for Windows ([#63773](https://github.com/pytorch/pytorch/pull/63773))\r\n* Fixed `torch.{istft,rfft}` errors for special inputs ([#63469](https://github.com/pytorch/pytorch/pull/63469), [#63327](https://github.com/pytorch/pytorch/pull/63327))\r\n* Fixed type annotation\r\n    * `optim.lr_scheduler.CosineAnnealingWarmRestart` ([#61106](https://github.com/pytorch/pytorch/pull/61106))\r\n    * Fixed type annotation of `torch.hub.load` ([#63755](https://github.com/pytorch/pytorch/pull/63755))\r\n* `x[index] = value` no longer results in a RuntimeError if `x` and `value` are different devices.\r\n    ([#61612](https://github.com/pytorch/pytorch/pull/61612))\r\n* Fixed crash while creating new tensor if NumPy is not available ([#66433](https://github.com/pytorch/pytorch/pull/66433))\r\n* Handle exceptions from THPModule_setQEngine ([#60073](https://github.com/pytorch/pytorch/pull/60073))\r\n* Fixed `torch.Tensor.cauchy_` on CUDA for inf values ([#60186](https://github.com/pytorch/pytorch/pull/60186))\r\n\r\n## Autograd\r\n\r\n* `torch.{signbit,isin}` no longer raise an error when passed a tensor that requires grad ([#62529](https://github.com/pytorch/pytorch/pull/62529))\r\n* Fixed sub-gradient for `torch.a{max,min}` ([#59669](https://github.com/pytorch/pytorch/pull/59669))\r\n* Fixed segfaults when a tensor hook removes itself ([#61250](https://github.com/pytorch/pytorch/pull/61250))\r\n* Fixed double backward for `binary_cross_entropy` loss function when `reduction=sum`. ([#59479](https://github.com/pytorch/pytorch/pull/59479))\r\n* Made sure that TLS (grad mode, inference mode, dispatcher state, etc) are properly set in hooks being called during the backward pass ([#60067](https://github.com/pytorch/pytorch/pull/60067))\r\n\r\n## torch.nn\r\n\r\n* `nn.AdaptiveAvgPool2d`: Correctly dispatch to CUDA implementation ([#61851](https://github.com/pytorch/pytorch/pull/61851))\r\n* `nn.AdaptiveAvgPool3d`: Fixed gradient computation ([#60630](https://github.com/pytorch/pytorch/pull/60630))\r\n* `nn.BatchNorm`: Fixed mixed precision usage when `affine=False` ([#61962](https://github.com/pytorch/pytorch/pull/61962))\r\n* `nn.BatchNorm2d`: Fixed issue when input is non-contiguous ([#63392](https://github.com/pytorch/pytorch/pull/63392))\r\n* Fixed `batch_norm()` to preserve output memory layout based on input ([#62773](https://github.com/pytorch/pytorch/pull/62773))\r\n* `nn.MaxPool2d`: Use `channels_last` memory format for output and indices when input is channels_last ([#61245](https://github.com/pytorch/pytorch/pull/61245))\r\n* `nn.Module`: Fixed full backward hook when grad is disabled ([#65335](https://github.com/pytorch/pytorch/pull/65335))\r\n* `nn.Module`: Fixed `get_buffer()` to check buffers by name instead of value ([#61429](https://github.com/pytorch/pytorch/pull/61429))\r\n* `nn.Module`: Fixed pre-forward hooks for Lazy modules ([#60517](https://github.com/pytorch/pytorch/pull/60517))\r\n* `nn.Softmax`: Improve numerical stability by subtracting max value in vectorized CPU implementation ([#63132](https://github.com/pytorch/pytorch/pull/63132))\r\n* `F.cosine_similarity`: Fixed type promotion behavior and added input validation checks ([#62054](https://github.com/pytorch/pytorch/pull/62054), [#66191](https://github.com/pytorch/pytorch/pull/66191), [#62912](https://github.com/pytorch/pytorch/pull/62912), [#58559](https://github.com/pytorch/pytorch/pull/58559))\r\n* `F.embedding`: Added check to validate that weights are 2D ([#59314](https://github.com/pytorch/pytorch/pull/59314))\r\n* `F.interpolate`: Fixed output for edge case of single pixel without align_corners ([#61166](https://github.com/pytorch/pytorch/pull/61166))\r\n* `F.nll_loss`: Fixed regression for gradient computation ([#64203](https://github.com/pytorch/pytorch/pull/64203))\r\n* `F.pad`: Fixed type of default pad value to be floating point ([#62095](https://github.com/pytorch/pytorch/pull/62095))\r\n* Fixed issues with printing `torch._ops.ops.{atan, quantized}` modules ([#62447](https://github.com/pytorch/pytorch/pull/62447))\r\n* Fixed `torch.nn.utils.parametrizations.spectral_norm` so that it can be used twice in the same forward pass ([#62293](https://github.com/pytorch/pytorch/pull/62293))\r\n* Disabled cuDNN persistent RNN on A30 to avoid exceptions from hard-to-detect edge cases ([#59830](https://github.com/pytorch/pytorch/pull/59830))\r\n\r\n## Dataloader\r\n\r\n* Fixed `IterableFecher` to stop fetching data after `StopIterator` ([#59313](https://github.com/pytorch/pytorch/pull/59313))\r\n* Fixed `ExceptionWrapper` to re-raise Exception with multiple args ([#58131](https://github.com/pytorch/pytorch/pull/58131))\r\n\r\n## AMD\r\n\r\n* Fix ROCm compilation by properly marking c++ functions as CPU only ([#62628](https://github.com/pytorch/pytorch/pull/62628))\r\n* Fixed `torch.{i1,i1e}` ROCm failure: mark array as const so that it is available for host and device ([#59187](https://github.com/pytorch/pytorch/pull/59187))\r\n\r\n## CUDA\r\n\r\n* Fixed to not use deprecated data accessor in IndexKernel.cu ([#62268](https://github.com/pytorch/pytorch/pull/62268))\r\n* Fixed sign comparison ([#62194](https://github.com/pytorch/pytorch/pull/62194), [#62483](https://github.com/pytorch/pytorch/pull/62483))\r\n* Fixed `torch.manual_seed{_all}` memory leak ([#62534](https://github.com/pytorch/pytorch/pull/62534))\r\n* Fixed CUDA_KERNEL_ASSERT ambiguous symbol in NDEBUG mode ([#62527](https://github.com/pytorch/pytorch/pull/62527))\r\n* Changed to use long index type for `torch.index_add` deterministic implementation ([#59254](https://github.com/pytorch/pytorch/pull/59254))\r\n* Fixed illegal memory access on NHWC BN kernel ([#59981](https://github.com/pytorch/pytorch/pull/59981))\r\n* Fixed typo in Normalization.cu ([#62515](https://github.com/pytorch/pytorch/pull/62515))\r\n* Added change to ignore and clear errors related to cuda not being ready yet ([#61554](https://github.com/pytorch/pytorch/pull/61554))\r\n* Fixed segmentation fault due to access to destroyed global IPC variable([#56141](https://github.com/pytorch/pytorch/pull/56141))\r\n* Fixed reduction launch config ([#64304](https://github.com/pytorch/pytorch/pull/64304))\r\n* Fixed typo embedding_renorm_ cuda implementation ([#64542](https://github.com/pytorch/pytorch/pull/64542))\r\n* Added missing kernel checks ([#60635](https://github.com/pytorch/pytorch/pull/60635))\r\n* CUDA graphs: made sure graph mempool malloc counter pairs with frees for all allocations ([#61567](https://github.com/pytorch/pytorch/pull/61567))\r\n* Fix bug where some kernels would not properly call cuda lazy initialization ([#61882](https://github.com/pytorch/pytorch/pull/61882))\r\n* Added check for contiguous to dispatch to NHWC CUDA template ([#62839](https://github.com/pytorch/pytorch/pull/62839))\r\n* Moved grid_sampler to autocast promote list ([#58618](https://github.com/pytorch/pytorch/pull/58618))\r\n* Added check for memory overlap in sort for large input sizes ([#58327](https://github.com/pytorch/pytorch/pull/58327))\r\n\r\n## C++ API\r\n\r\n* Fixed `map` function for `vec256` to accept const pointer to function ([#59957](https://github.com/pytorch/pytorch/pull/59957))\r\n* Added `supports_as_strided` method to `Device` and fixed indices of `to_sparse()` contiguous on all devices ([#59370](https://github.com/pytorch/pytorch/pull/59370))\r\n* Removed redundant bitwise-and op in MT19937RNGEngine ([#63219](https://github.com/pytorch/pytorch/pull/63219))\r\n* Fixed subprocess encoding for cpp extension on Windows ([#63756](https://github.com/pytorch/pytorch/pull/63756))\r\n* Define the SYCL device version `__assert_fail` when the NDEBUG defined. ([#58906](https://github.com/pytorch/pytorch/pull/58906))\r\n\r\n## TorchScript\r\n\r\n* Fixed inconsistency between Python and JIT power operation ([#62842](https://github.com/pytorch/pytorch/pull/62842))\r\n* Added change to convert `__constants__` attribute in model to a set to be consistent ([#60003](https://github.com/pytorch/pytorch/pull/60003)) \r\n* Added change to Ignore unsupported attribute checker pass for `torch.jit.trace` ([#60200](https://github.com/pytorch/pytorch/pull/60200)) \r\n* Fixed missing element types and shapes when `torch.autograd.Function` has multiple tensor outputs ([#57966](https://github.com/pytorch/pytorch/pull/57966))\r\n* Fixed `Tensor.to` schema to reflect that the output may alias input ([#60001](https://github.com/pytorch/pytorch/pull/60001))  \r\n* Added change to turn off layer norm in jit symbolic differentiation ([#63816](https://github.com/pytorch/pytorch/pull/63816)) \r\n* Fixed name conflict by using a more specific prefix for lowered module name. ([#61007](https://github.com/pytorch/pytorch/pull/61007)) \r\n* Added change to allow disabling cache in autocast (automatic mixed precision) ([#63552](https://github.com/pytorch/pytorch/pull/63552)) \r\n* Fixed concat optimization to handle cases when input list is mutated after cat using AliasDb ([#60774](https://github.com/pytorch/pytorch/pull/60774)) \r\n* Fixed symbolic derivative of hardswish ([#59405](https://github.com/pytorch/pytorch/pull/59405)) \r\n\r\n## torch.package\r\n\r\n* Fixed a bug when using `importlib.resources.path` for python <3.8.8 ([#58718](https://github.com/pytorch/pytorch/pull/58718))\r\n* Fixed bugs when using `os` and `os.path` ([#60276](https://github.com/pytorch/pytorch/pull/60276))\r\n* Fixed storage serialization collision when saving a `ScriptModule` and then saving a `Tensor` owned by it. ([#61806](https://github.com/pytorch/pytorch/pull/61806))\r\n* Fixed use-after-free during autograd shutdown ([#64620](https://github.com/pytorch/pytorch/pull/64620))\r\n* Fixed non-determinism in naming scheme of serialized storages in export code paths and ABA ABA storage identity problem during serialization for `torch.package` ([#59735](https://github.com/pytorch/pytorch/pull/59735))\r\n* Fixed GIL issue when acquiring multiple sessions. ([#58584](https://github.com/pytorch/pytorch/pull/58584))\r\n\r\n## Mobile\r\n\r\n* Fixed Nnapi backend dangling pointer bug ([#63092](https://github.com/pytorch/pytorch/pull/63092))\r\n* Fixed missing constants archive in torchscript model after backport ([#58892](https://github.com/pytorch/pytorch/pull/58892))\r\n* Fixed type hints in optimize_for_mobile to be consistent with the default([#59282](https://github.com/pytorch/pytorch/pull/59282))\r\n* Fixed xnnpack hardswish memory issue ([#59577](https://github.com/pytorch/pytorch/pull/59577), [#61622](https://github.com/pytorch/pytorch/pull/61622))\r\n* Fixed the issue that model_dump didn\u2019t work with delegate models ([#61043](https://github.com/pytorch/pytorch/pull/61043))\r\n* Fixed concat shaders didn\u2019t work for certain iOS devices ([#61074](https://github.com/pytorch/pytorch/pull/61074))\r\n* Fixed the Metal `torch.clamp` shader function for x86_64 ([#63062](https://github.com/pytorch/pytorch/pull/63062))\r\n* Fixed callstack pointer serialization bug ([#63576](https://github.com/pytorch/pytorch/pull/63576))\r\n* Fixed model loading error for Vulkan backend in Java API ([#63402](https://github.com/pytorch/pytorch/pull/63402))\r\n* Fixed the issue that sub modules with same names are not serialized correctly in bytecode format ([#61933](https://github.com/pytorch/pytorch/pull/61933))\r\n\r\n## Quantization\r\n\r\n* Fixed crash when model outputs dicts or lists ([#58416](https://github.com/pytorch/pytorch/pull/58416))\r\n* QAT: Fixed the runtime run `cannot resize variables that require grad` ([#57068](https://github.com/pytorch/pytorch/pull/57068))\r\n* Fixed support for custom module ([#59041](https://github.com/pytorch/pytorch/pull/59041))\r\n* Fixed the \"tensors to be on the same device\" error in HistogramObserver ([#59234](https://github.com/pytorch/pytorch/pull/59234))\r\n* Fixed dimension for output of batchnorm 1d ([#59264](https://github.com/pytorch/pytorch/pull/59264))\r\n* Fixed quantized mean operator in QNNPACK backend ([#59761](https://github.com/pytorch/pytorch/pull/59761))\r\n* Fixed a bug in .to for qtensors so scale/zp move too ([#61576](https://github.com/pytorch/pytorch/pull/61576))\r\n* Fixed quantized Conv1d module parameters ([#62356](https://github.com/pytorch/pytorch/pull/62356))\r\n* Fixed quantization for tuple arguments ([#63376](https://github.com/pytorch/pytorch/pull/63376))\r\n* Fixed fuse qconfig comparison ([#63384](https://github.com/pytorch/pytorch/pull/63384))\r\n* Fixed the conversion of the quantizable RNN ([#63879](https://github.com/pytorch/pytorch/pull/63879))\r\n* Fixed quantization for sub_scalar ([#64603](https://github.com/pytorch/pytorch/pull/64603))\r\n* Fixed a bug for sub ([#65109](https://github.com/pytorch/pytorch/pull/65109))\r\n* Add change to ensure qconfig works for QAT with multiple modules ([#63343](https://github.com/pytorch/pytorch/pull/63343))\r\n\r\n## Distributed\r\n\r\n`DistributedDataParallel`\r\n\r\n* Fixed Pipe + DDP for unused parameters, static graph ([#60118](https://github.com/pytorch/pytorch/pull/60118))\r\n* Fixed case where new tensors with no grad_fn are returned in DDP forward. ([#60882](https://github.com/pytorch/pytorch/pull/60882))\r\n* Re-enabled the optimization of fusing copy and division when no comm hook is specified for both dense and sparse tensors ([#61379](https://github.com/pytorch/pytorch/pull/61379), [#61814](https://github.com/pytorch/pytorch/pull/61814))\r\n* Fixed fp16 C++ DDP gradient communication hook ([#63375](https://github.com/pytorch/pytorch/pull/63375))\r\n* Added change to ensure buffers are broadcasted properly when they are reassigned in module ([#64776](https://github.com/pytorch/pytorch/pull/64776))\r\n* Fixed GradBucket.is_last() logic ([#63768](https://github.com/pytorch/pytorch/pull/63768))\r\n\r\n\r\n`torch.distributed.Store`\r\n\r\n* torch.distributed and RPC cannot both be initialized with the same host:port pair ([#58328](https://github.com/pytorch/pytorch/pull/58328), [#58329](https://github.com/pytorch/pytorch/pull/58329), [#58330](https://github.com/pytorch/pytorch/pull/58330), [#58331](https://github.com/pytorch/pytorch/pull/58331))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Added change to run dist_autograd backward RPCs on appropriate CUDA streams. ([#60606](https://github.com/pytorch/pytorch/pull/60606))\r\n* Fixed race condition in TensorPipe agent ([#58753](https://github.com/pytorch/pytorch/pull/58753))\r\n* Fixed issue when some gradients are None for distributed optimizers ([#62249](https://github.com/pytorch/pytorch/pull/62249))\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Added change to ensure rendezvous timeout does not get overwritten ([#61471](https://github.com/pytorch/pytorch/pull/61471))\r\n* Fixed the edge case when no node is alive ([#59663](https://github.com/pytorch/pytorch/pull/59663))\r\n* Added change to cast timestamp type to int ([#59712](https://github.com/pytorch/pytorch/pull/59712))\r\n* Added properly formatted traceback on error ([#65041](https://github.com/pytorch/pytorch/pull/65041))\r\n\r\n`torch.distributed.autograd`\r\n\r\n* Updated GraphTask::owner_ in a single thread for DistEngine. ([#58625](https://github.com/pytorch/pytorch/pull/58625))\r\n* Introduced the deadlock fix ([#61588](https://github.com/pytorch/pytorch/pull/61588), [#61593](https://github.com/pytorch/pytorch/pull/61593))\r\n\r\n`torch.distributed`\r\n\r\n* Fixed the slowdown of _object_to_tensor since 1.9 (#65721) ([#65721](https://github.com/pytorch/pytorch/pull/65721))\r\n\r\n## torch.fx\r\n\r\n* Fixed retracing wrapped functions ([#58061](https://github.com/pytorch/pytorch/pull/58061))\r\n* Added override for call_function so that wrapped functions stay wrapped ([#60057](https://github.com/pytorch/pytorch/pull/60057))\r\n* Added fix to retain node.meta after normalizing args ([#60449](https://github.com/pytorch/pytorch/pull/60449))\r\n* Added change to skip the output nodes but process possible nodes after it, when creating a single partition  ([#60370](https://github.com/pytorch/pytorch/pull/60370))\r\n* Fixed fx patch module name ([#61062](https://github.com/pytorch/pytorch/pull/61062))\r\n* Fixed graph `copy.deepcopy` to propagate output type ([#61747](https://github.com/pytorch/pytorch/pull/61747))\r\n* Added change to allow starter nodes to depend on `get_attr` node ([#62234](https://github.com/pytorch/pytorch/pull/62234))\r\n* Added change to prevent implicit submodule inlining when submodule is a GraphModule ([#62436](https://github.com/pytorch/pytorch/pull/62436))\r\n* Added change to persist `tracer_cls` on `fx.Graph` when deep copying ([#63353](https://github.com/pytorch/pytorch/pull/63353))\r\n* Fixed GraphModule deepcopy to use deepcopied graph ([#63090](https://github.com/pytorch/pytorch/pull/63090))\r\n* Fixed constant folding for attrs in submodule hierarchies ([#64342](https://github.com/pytorch/pytorch/pull/64342))\r\n* Fixed some const fold cases with deep model hierarchy ([#64945](https://github.com/pytorch/pytorch/pull/64945))\r\n* Fixed tracing of bitwise and/or ([#65196](https://github.com/pytorch/pytorch/pull/65196))\r\n\r\n## ONNX\r\n\r\n* Added shape type inference fixes for control flow ([#60248](https://github.com/pytorch/pytorch/pull/60248))\r\n* Fixed sum export with attribute `keepdims` ([#60245](https://github.com/pytorch/pytorch/pull/60245))\r\n* Fixed shape inference for large model ([#60244](https://github.com/pytorch/pytorch/pull/60244))\r\n* Fixed split export in op set 13 ([#57605](https://github.com/pytorch/pytorch/pull/57605))\r\n* Fixed control-flow shape inference with contrib op ([#62762](https://github.com/pytorch/pytorch/pull/62762))\r\n* Updated `instance_norm2d` export to handle `track_running_stats=True` ([#58690](https://github.com/pytorch/pytorch/pull/58690))\r\n* Fixed the issue of converting empty list to sequence([#61558](https://github.com/pytorch/pytorch/pull/61558))\r\n* Fixed sum could not be exported for empty tensor ([#59537](https://github.com/pytorch/pytorch/pull/59537))\r\n* Fixed an issue that optimizations might adjust graph inputs unexpectedly ([#62763](https://github.com/pytorch/pytorch/pull/62763))\r\n\r\n## Vulkan\r\n\r\n* Fixed an issue where comparing equivalent descriptors would evaluate to `false` ([#60199](https://github.com/pytorch/pytorch/pull/60199))\r\n* Fixed asserts in Vulkan JIT passes to actually throw an exception ([#61495](https://github.com/pytorch/pytorch/pull/61495))\r\n\r\n## Performance_as_a_product\r\n\r\n* Added fix to ensure number of thread utilities are initialized before getting the number of threads ([#60185](https://github.com/pytorch/pytorch/pull/60185))\r\n* Added fix to ensure thread id is valid in nested parallel regions ([#60183](https://github.com/pytorch/pytorch/pull/60183))\r\n* Fixed parallel tbb build ([#60532](https://github.com/pytorch/pytorch/pull/60532))\r\n* Added change to make flags in the pytorch managed thread pool atomic. ([#58457](https://github.com/pytorch/pytorch/pull/58457))\r\n* Set mkl thread locally ([#62891](https://github.com/pytorch/pytorch/pull/62891))\r\n\r\n## Composability\r\n\r\n* Added a fix to ensure that the C++ API\u2019s that skip the dispatcher (such as `at::cpu::{op}` and `at::cuda::{op}` get external linkage, so they can be used outside of libtorch ([#58569](https://github.com/pytorch/pytorch/pull/58569))\r\n* Fixed bug where shared memory tensor file names can collide ([#60978](https://github.com/pytorch/pytorch/pull/60978))\r\n\r\n## Build_Frontend\r\n\r\n* Fixed binary building without python ([#66031](https://github.com/pytorch/pytorch/pull/66031))\r\n* Fixed Windows ninja builds when MAX_JOBS is specified ([#65444](https://github.com/pytorch/pytorch/pull/65444))\r\n* Skipped Bfloat16 support when building for VSX ([#61630](https://github.com/pytorch/pytorch/pull/61630))\r\n* Made change to use python3 alias in Makefile ([#58786](https://github.com/pytorch/pytorch/pull/58786))\r\n* Made change to use `pybind11` from `third_party` folder by default ([#58951](https://github.com/pytorch/pytorch/pull/58951))\r\n* Made change to ensure FindLAPACK finds the same BLAS library ([#49647](https://github.com/pytorch/pytorch/pull/49647))\r\n* Improved Python package detection in `torch.utils.collect_env` ([#63321](https://github.com/pytorch/pytorch/pull/63321))\r\n* Skipped SVE acceleration on M1 machine ([#58785](https://github.com/pytorch/pytorch/pull/58785))\r\n* Made `SciPy` dependency optional in PyTorch unary operators tests ([#59304](https://github.com/pytorch/pytorch/pull/59304))\r\n* Fixed error-handling when Python executable can not be found ([#61230](https://github.com/pytorch/pytorch/pull/61230))\r\n* Fixed `setup.py` re-run incremental build logic on Windows ([#59689](https://github.com/pytorch/pytorch/pull/59689))\r\n* Reduced binary size for CUDA-split build by establishing correct linking order ([#58287](https://github.com/pytorch/pytorch/pull/58287))\r\n* Fixed  `torch.utils.cpp_extension` behavior when older setuptools are used ([#61484](https://github.com/pytorch/pytorch/pull/61484))\r\n\r\n## Infra (Releng)\r\n\r\n* Fixed windows ci squid env ([#62353](https://github.com/pytorch/pytorch/pull/62353))\r\n* Introduced CI dependency pinning: ([#64922](https://github.com/pytorch/pytorch/pull/64922), [#65017](https://github.com/pytorch/pytorch/pull/65017))\r\n* Fixed breakpad build and add to more images ([#59236](https://github.com/pytorch/pytorch/pull/59236))\r\n* Updated certificate trust chain CI to depend on the linked commits ([#65934](https://github.com/pytorch/pytorch/pull/65934), [#66004](https://github.com/pytorch/pytorch/pull/66004))\r\n\r\n## LinAlg_Frontend\r\n\r\n* Fixed an issue where the \u201cinfo\u201d tensor returned by `torch.linalg.inv_ex` could sometimes be on the wrong device ([#59223](https://github.com/pytorch/pytorch/pull/59223))\r\n* Fixed an issue where `torch.linalg.norm` could return tensors with the wrong shape in some edge cases ([#60273](https://github.com/pytorch/pytorch/pull/60273))\r\n* Fixed an issue where `torch.linalg.svd` could return tensors with the wrong shape in some edge cases ([#62022](https://github.com/pytorch/pytorch/pull/62022))\r\n* Fixed an issue where `torch.matmul` would throw an error when attempting to multiply certain empty tensors ([#63359](https://github.com/pytorch/pytorch/pull/63359))\r\n\r\n## Sparse_Frontend\r\n\r\n* Fixed dtype inference in sparse_csr_tensor_ctor ([#58631](https://github.com/pytorch/pytorch/pull/58631))\r\n* Fixed addmm failure for CSR Tensors when MKL is not available ([#58768](https://github.com/pytorch/pytorch/pull/58768))\r\n* Fixed overflow of numel for sparse COO tensors after calling coalesce ([#57492](https://github.com/pytorch/pytorch/pull/57492))\r\n* Fixed multiplication of 0-dim Tensor and COO sparse Tensor and improved Error message for multiplication of dense and sparse COO tensor ([#61723](https://github.com/pytorch/pytorch/pull/61723))\r\n* Fixed internal assert error for CSR tensors crow_/col_indices methods in Debug build ([#63176](https://github.com/pytorch/pytorch/pull/63176))\r\n* Fixed support of torch.conj for zero-dimensional sparse COO Tensors ([#59553](https://github.com/pytorch/pytorch/pull/59553))\r\n\r\n## Misc\r\n\r\n* Added change to increase warmup for better steady state measurements. ([#58801](https://github.com/pytorch/pytorch/pull/58801))\r\n* Fixed bad use of channels last kernel in sync batch norm backward ([#64100](https://github.com/pytorch/pytorch/pull/64100))\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n* `torch.special.{'i0', 'i0e', 'i1', 'i1e'}:` converted floating-point constants to input type in Bessel functions ([#59416](https://github.com/pytorch/pytorch/pull/59416))\r\n* Added change to speed up `torch.unique_consecutive()` ([#64835](https://github.com/pytorch/pytorch/pull/64835))\r\n* Made sure all graphs tests call `torch.cuda.empty_cache()` before capture to fix flaky tests ([#59233](https://github.com/pytorch/pytorch/pull/59233))\r\n* `torch.flip` : improved performance via TensorIterator ([#59509](https://github.com/pytorch/pytorch/pull/59509))\r\n* Added change to parallelize `torch.gelu` via tensoriterator ([#58950](https://github.com/pytorch/pytorch/pull/58950))\r\n* `torch.sum`: added change to accumulate 16-bit float sums in 32-bit accumulators for improved precision and performance ([#60387](https://github.com/pytorch/pytorch/pull/60387))\r\n* Added fast path for conjugated tensors for  `torch.`{`dot, vdot, mm, addmm, bmm, baddbmm}` ([#62915](https://github.com/pytorch/pytorch/pull/62915), [#59380](https://github.com/pytorch/pytorch/pull/59380))\r\n\r\n## Autograd\r\n\r\n* Faster `torch.cum{sum,prod}` backward formulas ([#60642](https://github.com/pytorch/pytorch/pull/60642))\r\n* Reduced overhead from `reshape` call if the tensor already has the right shape ([#61466](https://github.com/pytorch/pytorch/pull/61466))\r\n* Added change to speed up saving variables for backward ([#59837](https://github.com/pytorch/pytorch/pull/59837), [#61927](https://github.com/pytorch/pytorch/pull/61927))\r\n* Reduced number of TLS access when deciding if an op needs to be tracked by autograd or not ([#60740](https://github.com/pytorch/pytorch/pull/60740))\r\n* Improved code that detect when it is valid to re-use existing Tensors during the backward pass ([#59817](https://github.com/pytorch/pytorch/pull/59817))\r\n\r\n## torch.nn\r\n\r\n* `nn.utils.clip_grad_norm_`: Removed device syncs ([#61042](https://github.com/pytorch/pytorch/pull/61042))\r\n* `nn.BatchNorm2d`: Optimized performance for `channels_last` on CPU ([#59286](https://github.com/pytorch/pytorch/pull/59286))\r\n* `nn.Softmax`: Vectorized softmax calculation for the non-last-dimension case ([#59195](https://github.com/pytorch/pytorch/pull/59195), [#60371](https://github.com/pytorch/pytorch/pull/60371))\r\n* `nn.Transformer`: Faster `generate_square_subsequent_mask` ([#60631](https://github.com/pytorch/pytorch/pull/60631))\r\n\r\n## CUDA\r\n\r\n* Updated launch bounds for trilinear 3d ([#59999](https://github.com/pytorch/pytorch/pull/59999))\r\n* Migrated Embedding thrust sort to cub sort ([#62495](https://github.com/pytorch/pytorch/pull/62495))\r\n* Make `unique` call in embedding use cub instead of thrust ([#63042](https://github.com/pytorch/pytorch/pull/63042))\r\n* Migrated masked_scatter to use cub instead of thrust ([#56750](https://github.com/pytorch/pytorch/pull/56750))\r\n* Reverted D28547564: [pytorch][PR] masked_scatter thrust\u2192cub ([9e261de630](https://github.com/pytorch/pytorch/commit/9e261de630))\r\n* Make sort in EmbeddingBag use cub instead of thrust ([#64498](https://github.com/pytorch/pytorch/pull/64498))\r\n* Migrated Embedding thrust sort to cub sort ([#63806](https://github.com/pytorch/pytorch/pull/63806))\r\n* Removed cat, equal, and stack from autocast promote list ([#59497](https://github.com/pytorch/pytorch/pull/59497))\r\n* Add cublas and cusolver paths for LU solve ([#59148](https://github.com/pytorch/pytorch/pull/59148))\r\n* Fixed launch bounds for gathertopk kernel ([#60314](https://github.com/pytorch/pytorch/pull/60314))\r\n* Changed launch bounds, unrolled for loop for grid sampler 2d fwd and bwd ([#60405](https://github.com/pytorch/pytorch/pull/60405))\r\n* Changed launch bound to fix col2im kernel ([#60315](https://github.com/pytorch/pytorch/pull/60315))\r\n* Fixed launch bounds for grid sampler 3d ([#60385](https://github.com/pytorch/pytorch/pull/60385))\r\n* CUDA graphs: added change to not sync between replays for CUDA driver version 11.4+ ([#61063](https://github.com/pytorch/pytorch/pull/61063))\r\n* Changed launch bounds for upsample_linear1d fwd, bwd from 1024 to 512 ([#61307](https://github.com/pytorch/pytorch/pull/61307))\r\n* Added change to reduce max_num_threads for complex double ops in reduce_kernel ([#61438](https://github.com/pytorch/pytorch/pull/61438))\r\n* Added change to use `fastAtomicAdd` in EmbeddingBag (mode \"max\") backward ([#63298](https://github.com/pytorch/pytorch/pull/63298))\r\n* Added change to use multi-dimensional cuFFT transforms to improve FFT performance ([#61203](https://github.com/pytorch/pytorch/pull/61203))\r\n* `F.avg_pool3d` CUDA backward: use fast atomic adds ([#63387](https://github.com/pytorch/pytorch/pull/63387))\r\n* Add cuSOLVER path for LU factorization in CUDA. ([#56887](https://github.com/pytorch/pytorch/pull/56887))\r\n* Reverted launch bounds change in topK that induced a regression in perf ([#63431](https://github.com/pytorch/pytorch/pull/63431))\r\n* Added change to bring back old algorithm for sorting on small number of segments ([#64127](https://github.com/pytorch/pytorch/pull/64127))\r\n\r\n## Mobile\r\n\r\n* Added change to use channel-last to transform the weights for Metal ([#59113](https://github.com/pytorch/pytorch/pull/59113))\r\n* Implemented RoIAlign in Metal shaders using Sampler ([#56075](https://github.com/pytorch/pytorch/pull/56075))\r\n* Added cache operator lambda during model loading ([#61996](https://github.com/pytorch/pytorch/pull/61996))\r\n* Added Operator Call De-dup at TorchScript Serialization Level ([#64269](https://github.com/pytorch/pytorch/pull/64269))\r\n* Added change to speed up model loading by 1directly calling the C file API from FileAdapter ([#61997](https://github.com/pytorch/pytorch/pull/61997))\r\n* Moved from input ivalues in ByteCodeDeserializer ([#64029](https://github.com/pytorch/pytorch/pull/64029))\r\n* Fixed MobileDebugInfo vector copy ([#64030](https://github.com/pytorch/pytorch/pull/64030))\r\n* Added change to gate tls_local_dispatch_key_set off on iOS too ([#64753](https://github.com/pytorch/pytorch/pull/64753))\r\n* Added change to not store multiple kernels per key on mobile ([#64447](https://github.com/pytorch/pytorch/pull/64447))\r\n* Added OpCode cache in ByteCodeDeserializer ([#64110](https://github.com/pytorch/pytorch/pull/64110))\r\n* Reduced mobile model size by reusing constant and bump bytecode to v5 ([#59722](https://github.com/pytorch/pytorch/pull/59722))\r\n\r\n## Distributed\r\n\r\n* `torch.distributed:` replaced all_gather with more efficient collective api _all_gather_base ([#57769](https://github.com/pytorch/pytorch/pull/57769))\r\n* `torch.distributed.optim.ZeroRedundancyOptimizer: `Sorted params by size (decreasing) ([#59586](https://github.com/pytorch/pytorch/pull/59586))\r\n\r\n## Vulkan\r\n\r\n* Improved the performance of pointwise convolutions by having each shader invocation calculate a 4x4 output tile  ([#60760](https://github.com/pytorch/pytorch/pull/60760))\r\n* Implemented a simple scheme to set the local work group size adaptively ([#61170](https://github.com/pytorch/pytorch/pull/61170))\r\n\r\n## Performance_as_a_product\r\n\r\n* TensorIterator: added change to reduce serial_for_each static overhead ([#58909](https://github.com/pytorch/pytorch/pull/58909))\r\n* Added change to avoid using `std::regex` for device string parsing ([#63204](https://github.com/pytorch/pytorch/pull/63204))\r\n\r\n## Composability\r\n\r\n* Introduced some perf improvements for reduction ops ([#58655](https://github.com/pytorch/pytorch/pull/58655))\r\n* Added optimization to some internal representations of sizes ([#59333](https://github.com/pytorch/pytorch/pull/59333))\r\n* Reduced the number of tensor refcount bumps in many existing kernels ([#58303](https://github.com/pytorch/pytorch/pull/58303), [#59827](https://github.com/pytorch/pytorch/pull/59827), [#58273](https://github.com/pytorch/pytorch/pull/58273), [#58272](https://github.com/pytorch/pytorch/pull/58272), [#58276](https://github.com/pytorch/pytorch/pull/58276), [#58277](https://github.com/pytorch/pytorch/pull/58277), [#58279](https://github.com/pytorch/pytorch/pull/58279), [#60546](https://github.com/pytorch/pytorch/pull/60546), [#58280](https://github.com/pytorch/pytorch/pull/58280))\r\n* Added micro-optimizations to improve the time it takes to load pytorch ([#64784](https://github.com/pytorch/pytorch/pull/64784), [#64820](https://github.com/pytorch/pytorch/pull/64820), [#64821](https://github.com/pytorch/pytorch/pull/64821), [#64822](https://github.com/pytorch/pytorch/pull/64822), [#64838](https://github.com/pytorch/pytorch/pull/64838), [#64678](https://github.com/pytorch/pytorch/pull/64678), [#64682](https://github.com/pytorch/pytorch/pull/64682), [#64670](https://github.com/pytorch/pytorch/pull/64670))\r\n\r\n## Build_Frontend\r\n\r\n* Compiled BatchLinearAlgebra CUDA integration routines with host compiler ([#64146](https://github.com/pytorch/pytorch/pull/64146))\r\n* Sped-up compilation by splitting autogenerated files into smaller ones ([#62186](https://github.com/pytorch/pytorch/pull/62186))\r\n* Allowed [ninja-build](https://ninja-build.org/) to dynamically pick best parallel build option ([#64733](https://github.com/pytorch/pytorch/pull/64733), [#65162](https://github.com/pytorch/pytorch/pull/65162))\r\n\r\n## Infra (Releng)\r\n\r\n* .github: upload /download large artifacts to s3 ([#58506](https://github.com/pytorch/pytorch/pull/58506))\r\n* Made change to only run mem leak check on master ([#60023](https://github.com/pytorch/pytorch/pull/60023))\r\n* Enabled parallel clang-tidy on ec2 runner ([#60870](https://github.com/pytorch/pytorch/pull/60870))\r\n* Made change to skip magma library installation for Windows CPU builds ([#59619](https://github.com/pytorch/pytorch/pull/59619))\r\n\r\n## Sparse_Frontend\r\n\r\n* Sped up conversion of COO to CSR Tensor `to_sparse_csr` by writing custom CPU/GPU kernels ([#61340](https://github.com/pytorch/pytorch/pull/61340), [#61838](https://github.com/pytorch/pytorch/pull/61838))\r\n* Slightly sped up calculation of number of dense entries for sparse softmax via `c10::multiply_integers`  for COO Tensors ([#60872](https://github.com/pytorch/pytorch/pull/60872))\r\n* Slightly sped up sparse softmax for COO Tensors by improve usage of `std::vector` ([#60873](https://github.com/pytorch/pytorch/pull/60873))\r\n* Sped up index_select for sparse COO Tensor ([#63008](https://github.com/pytorch/pytorch/pull/63008))\r\n\r\n## Misc\r\n\r\n* Greatly reduced the post-processing time of the profiler ([#60432](https://github.com/pytorch/pytorch/pull/60432))\r\n* Saved some little memory in `default_collate` ([#61424](https://github.com/pytorch/pytorch/pull/61424))\r\n* Added new ops to the operator microbenchmark: `gelu`, `bmm`, `mm`, `einsum`, `log1p` ([#59334](https://github.com/pytorch/pytorch/pull/59334), [#59595](https://github.com/pytorch/pytorch/pull/59595), [#63654](https://github.com/pytorch/pytorch/pull/63654), [#64647](https://github.com/pytorch/pytorch/pull/64647), [#64032](https://github.com/pytorch/pytorch/pull/64032), [#64205](https://github.com/pytorch/pytorch/pull/64205))\r\n* Added AVX512 support in ATen & remove AVX support ([#61903](https://github.com/pytorch/pytorch/pull/61903))\r\n\r\n\r\nYou can also find the dev specific and documentation related changes in the forum post [here](https://dev-discuss.pytorch.org/t/pytorch-1-10-dev-release-notes/379)", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.10.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.10.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.10.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/51800581", "release_id": 51800581, "date_created": "2021-10-15T01:35:23Z", "date_published": "2021-10-21T15:49:53Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/50056192", "tag": "v1.9.1", "name": "Small bug fix release", "author": {"name": "malfet", "type": "User"}, "description": "# PyTorch 1.9.1 Release Notes\r\n\r\n* Improvements\r\n* Bug Fixes\r\n* Documentation\r\n\r\n# Improvements\r\n\r\n* Stop warning on `.names()` access in `max_pool2d` #60059\r\n* Remove Caffe2 thread-pool leak warning #60318\r\n* Add option to skip GitHub tag validation for `torch.hub.load` #62139\r\n* Use `log.warning` in `torch.distributed.run`  to print OMP_NUM_THREADS warning #63953\r\n* TorchElastic: Pretty print the failure message captured by @record #64036\r\n* `torch.distribtued.run` to set `nproc_per_node` to 1 by default #61552\r\n* Remove experimental API warning  from `torch.distributed.elastic.utils.store` #60807\r\n* Deprecate  `use_env` in `torch.distributed.run` #59409\r\n* Better engineering changes for torch.distributed launcher #59152\r\n\r\n# Bug fixes\r\n\r\n## Distributed / TorchElastic\r\n\r\n* Make init_method=tcp:// compatible with `torch.distributed.run` #63910\r\n* Fix default parameters (number of restarts, log level, number of processes per node)  that regressed with the transition from `torch.distributed.launch` and `torch.distributed.run` and clarify the documentation accordingly #61294\r\n\r\n## Hub\r\n\r\n* Fix HTTP/403 error when calling `torch.hub.load` for TorchVision models #62072\r\n\r\n## Misc\r\n\r\n* `torch.mm` to check input matrix sizes shapes #61394\r\n\r\n# Documentation\r\n\r\n* Fix broken link in elastic launch doc #62378\r\n* Fix typo in `torch.distribtued.run` warning message #61127\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.9.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.9.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.9.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/50056192", "release_id": 50056192, "date_created": "2021-09-14T04:13:35Z", "date_published": "2021-09-22T12:58:15Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/47996900", "tag": "v1.8.2", "name": "LTS 1.8.2, Wrap cub in its own namespace", "author": {"name": "seemethere", "type": "User"}, "description": "# **PyTorch 1.8.2 Release Notes** \r\n\r\n* Highlights\r\n* Bug Fixes\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.8.2. This is the first release we are making as part of the [Pytorch Enterprise Support Program](https://pytorch.org/enterprise-support-program). This release includes a bug fix requested by a customer in an LTS branch. \r\nWe'd like to thank Microsoft for their support and work on this release.\r\n\r\n# Bug Fixes\r\n* Wrap cub in its own namespace ([#55292](https://github.com/pytorch/pytorch/pull/55292)) ([#61605](https://github.com/pytorch/pytorch/pull/61605))", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.8.2", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.8.2", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.8.2", "url": "https://api.github.com/repos/pytorch/pytorch/releases/47996900", "release_id": 47996900, "date_created": "2021-07-23T18:17:46Z", "date_published": "2021-08-17T18:33:00Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/43927405", "tag": "v1.9.0", "name": "PyTorch 1.9 Release, including Torch.Linalg and Mobile Interpreter", "author": {"name": "anjali411", "type": "User"}, "description": "# **PyTorch 1.9 Release Notes** \r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* Deprecations\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. Highlights include:\r\n\r\n* Major improvements to support scientific computing, including torch.linalg, torch.special, and Complex Autograd\r\n* Major improvements in on-device binary size with Mobile Interpreter\r\n* Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core\r\n* Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support\r\n* New APIs to optimize performance and packaging for model inference deployment \r\n* Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler\r\n\r\nWe\u2019d like to thank the community for their support and work on this latest release. We\u2019d especially like to thank Quansight and Microsoft for their contributions.\r\n\r\nYou can find more details on all the highlighted features in the [_PyTorch 1.9 Release blogpost_](https://pytorch.org/blog/pytorch-1.9-released/). \r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n* **`torch.divide` with `rounding_mode='floor'` now returns infinity when a non-zero number is divided by zero (**[**#56893**](https://github.com/pytorch/pytorch/pull/56893)**).**\r\nThis fixes the `rounding_mode='floor'` behavior to return the same non-finite values as other rounding modes when there is a division by zero. Previously it would always result in a NaN value, but a non-zero number divided by zero should return +/- infinity in IEEE floating point arithmetic. Note this does not effect `torch.floor_divide` or the floor division operator, which currently use `rounding_mode='trunc'` (and are also deprecated for that reason).\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([-1.0, 0.0, 1.0])\r\n>>> b = torch.tensor([0.0])\r\n>>> torch.divide(a, b, rounding_mode='floor')\r\ntensor([nan, nan, nan])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([-1.0, 0.0, 1.0])\r\n>>> b = torch.tensor([0.0])\r\n>>> torch.divide(a, b, rounding_mode='floor')\r\ntensor([-inf, nan, inf])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n        \r\n\r\n* **Legacy tensor constructors and `Tensor.new` no longer support passing both `Tensor` and `device` as inputs  ([#58108](https://github.com/pytorch/pytorch/pull/58108)).**\r\nThis fixes a bug in which 1-element integer tensors were misinterpreted as specifying tensor size, yielding an uninitialized tensor. As noted in the error message, use the new-style `torch.tensor(...)` or `torch.as_tensor(...)` to copy or alias an existing tensor. If you want to create an uninitialized tensor, use `torch.empty(...)`. \r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([1])\r\n>>> torch.LongTensor(a, device='cpu') # uninitialized\r\ntensor([7022349217739848992])\r\n>>> a.new(a, device='cpu')\r\ntensor([4294967295]) # uninitialized\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([1])\r\n>>> torch.LongTensor(a, device='cpu')\r\nRuntimeError: Legacy tensor constructor of the form torch.Tensor(tensor, device=device) is\r\nnot supported. Use torch.tensor(...) or torch.as_tensor(...) instead.\r\n>>> a.new(a, device='cpu')\r\nRuntimeError: Legacy tensor new of the form tensor.new(tensor, device=device) is not\r\nsupported. Use torch.as_tensor(...) instead.\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>        \r\n        \r\n* **`torch.divide` with `rounding_mode='true'` is replaced with `rounding_mode=None` ([#51988](https://github.com/pytorch/pytorch/pull/51988)).**\r\n`torch.divide`'s undocumented `rounding_mode='true'` option has been removed, and instead `rounding_mode=None` should be passed to indicate no rounding should take place. This is equivalent to omitting the argument entirely.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a, b = torch.full((2,), 4.2), torch.full((2,), 2)\r\n>>> torch.divide(a, b, rounding_mode='true')\r\ntensor([2.1000, 2.1000])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a, b = torch.full((2,), 4.2), torch.full((2,), 2)\r\n>>> torch.divide(a, b, rounding_mode=None) # equivalent to  torch.divide(a, b, rounding_mode='true') from the prior release\r\ntensor([2.1000, 2.1000])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n* **`import torch.tensor as tensor` is no longer supported ([#53424](https://github.com/pytorch/pytorch/pull/53424)).**\r\nInstead, use `from torch import tensor`\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch.tensor as tensor\r\n>>> torch.tensor(1.)\r\ntensor(1.)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch.tensor as tensor\r\nModuleNotFoundError: No module named 'torch.tensor'\r\n>>> from torch import tensor\r\n>>> tensor(1.)\r\ntensor(1.)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n* **binary release: `numpy` is no longer a required dependency**\r\nIf you require `numpy` (and don't already have it installed) you will need to install it separately.\r\n\r\n\r\n## Autograd\r\n\r\n* **`torch.autograd.gradcheck.get_numerical_jacobian` and `torch.autograd.gradcheck.get_analytical_jacobian` no longer support functions that return complex valued output as well as any other values of `grad_out` not equal to 1** ([#55692](https://github.com/pytorch/pytorch/pull/55692)).\r\nThis change is a part of a refactor of `gradcheck`\u2019s internals. Note that `gradcheck` itself still supports functions with complex output. This new restriction only applies to calls to the two internal helper functions. As a workaround, you can wrap your functions to return either the real or imaginary component of its output before calling these functions. Additionally these internal helpers no longer accept any other value except 1 for `grad_out` for any input function. Note that these helper functions are also being deprecated in this release.\r\n \r\n1.8.1:\r\n```python\r\nget_numerical_jacobian(torch.complex, (a, b), grad_out=2.0)\r\n```\r\n\r\n1.9.0:\r\n```python\r\n      def wrapped(fn):\r\n            def wrapper(*input):\r\n                return torch.real(fn(*input))\r\n            return wrapper\r\n        \r\n        get_numerical_jacobian(wrapped(torch.complex), (a, b), grad_out=1.0)\r\n```\r\n\r\n* **`torch.autograd.gradcheck` now throws `GradcheckError`** ([#55656](https://github.com/pytorch/pytorch/pull/55656)).\r\nThis change is a part of a refactor of `gradcheck`\u2019s internals. All errors that are able to be silenced by `raise_exception=False` now raise `GradcheckError` (which inherits from `RuntimeError`). If you explicitly check that the type of the error is `RuntimeError` you'll need to update your code to check for `GradcheckError` instead. Otherwise if you use something like `except` or `isinstance`, no changes are necessary.\r\n\r\n1.8.1:\r\n```python\r\n# An example of a situation that will now return GradcheckError instead of\r\n# RuntimeError is when there is a jacobian mismatch, which can happen\r\n# for example when you forget to specify float64 for your inputs.\r\ntry:\r\n    torch.autograd.gradcheck(torch.sin, (torch.ones(1, requires_grad=True),))\r\nexcept RuntimeError as e:\r\n    assert type(e) is RuntimeError # explicitly check type -> NEEDS UPDATE\r\n```\r\n\r\n1.9.0:\r\n```python\r\ntry:\r\n    torch.autograd.gradcheck(torch.sin, (torch.ones(1, requires_grad=True),)\r\nexcept RuntimeError as e:\r\n   # GradcheckError inherits from RuntimeError so you can still catch this\r\n   # with RuntimeError (No change necessary!)\r\n   \r\n   # BUT, if you explicitly check type...\r\n   assert type(e) is torch.autograd.GradcheckError\r\n```\r\n\r\n* **Finished deprecation cycle for in-place view error checks** ([#56093](https://github.com/pytorch/pytorch/pull/56093)).\r\n In-place modification of views will now raise an error if that view was created by a custom function or a function that returns multiple views, or if the view was created in no-grad mode. Modifying in-place a view created in the situations above are error-prone and have been deprecated since v1.5.0. Doing these in-place modifications are now forbidden. For more information on how to work around this, see the related sections the release notes linked below:\r\n    * [v1.5.0](https://github.com/pytorch/pytorch/releases?after=v1.5.1) (view created in custom autograd function, view created in no-grad block)\r\n    * [v1.7.0](https://github.com/pytorch/pytorch/releases?after=v1.8.0-rc3) (section on `split` and `chunk`, i.e., functions that return multiple views).\r\n\r\n## torch.nn\r\n\r\n* **Fixed regression for `nn.MultiheadAttention` to now apply bias flag to both in and out projection layers** ([#52537](https://github.com/pytorch/pytorch/pull/52537)).\r\nIn PyTorch 1.6, a regression was introduced that caused the `bias` flag of `nn.MultiheadAttention` only to apply to the input projection layer. This caused the output projection layer to always include a `bias` parameter, even with `bias=False` specified. The regression is now fixed in PyTorch 1.9, making the `bias` flag correctly apply to both the input and output projection layers. This fix is BC-breaking for the `bias=False` case as it will now result in no `bias` parameter for the output projection layer.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>v1.6 - v1.8.1:</th><th>pre 1.6 & 1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> mha = torch.nn.MultiheadAttention(4, 2, bias=False)\r\n>>> print(mha.out_proj.bias)\r\nParameter containing:\r\ntensor([0., 0., 0., 0.], requires_grad=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> mha = torch.nn.MultiheadAttention(4, 2, bias=False)\r\n>>> print(mha.out_proj.bias)\r\nNone\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n* **Updated `nn.Module` to fire full backward hooks even when no input requires grad** ([#56693](https://github.com/pytorch/pytorch/pull/56693)).\r\nPrior to this release, full backward hooks were not fired when no input requires gradients. This has been changed so that full backward hooks will always fire during the backward pass, regardless of whether or not any input requires gradients. If you are using full backward hooks, be aware that they may fire more frequently than pre-1.9 due to this change.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1:</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> m = torch.nn.Linear(2, 3)\r\n>>> def hook(mod, grad_input, grad_output):\r\n>>> print('hook called:', grad_input, grad_output)\r\n>>> m.register_full_backward_hook(hook)\r\n>>> input_no_grad = torch.rand(1, 2, requires_grad=False)\r\n>>> m(input_no_grad).sum().backward()\r\n>>> input_grad = torch.rand(1, 2, requires_grad=True)\r\n>>> m(input_grad).sum().backward()\r\nhook called: (tensor([[0.1478, 0.6517]]),) (tensor([[1., 1., 1.]]),)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> m = torch.nn.Linear(2, 3)\r\n>>> def hook(mod, grad_input, grad_output):\r\n>>> print('hook called:', grad_input, grad_output)\r\n>>> m.register_full_backward_hook(hook)\r\n>>> input_no_grad = torch.rand(1, 2, requires_grad=False)\r\n>>> m(input_no_grad).sum().backward()\r\nhook called: (None,) (tensor([[1., 1., 1.]]),)\r\n>>> input_grad = torch.rand(1, 2, requires_grad=True)\r\n>>> m(input_grad).sum().backward()\r\nhook called: (tensor([[0.1478, 0.6517]]),) (tensor([[1., 1., 1.]]),)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Dataloader\r\n\r\n* **Add Numpy seeding to worker of DataLoader** ([#56488](https://github.com/pytorch/pytorch/pull/56488)).\r\n`DataLoader` with `num_workers > 0` will now set independent random seed for NumPy random functions on each worker by default. So, users now won\u2019t be required to set random seed for NumPy using `worker_init_fn` to force NumPy random operations deterministic and independent across `DataLoader` workers. This PR won\u2019t affect users who have already set random seed for NumPy random functions using `worker_init_fn`.\r\n```python \r\n        # dataset returns numpy.random.randint(1, 10000) \r\n        ctx = mp.get_context('fork')\r\n        gen = torch.Generator().manual_seed(0)\r\n        dl = DataLoader(dataset, batch_size=2, num_workers=2, multiprocessing_context=ctx, generator=gen)\r\n        for epoch in range(2):\r\n            print(\"=\" * 4, \"Epoch\", epoch, \"=\" * 4)\r\n            for batch in dl:\r\n                print(batch)\r\n```\r\n        \r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1:</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# When using fork, each worker has same random seed for NumPy random functions at each epoch.\r\n========== Epoch 0 ==========\r\ntensor([[ 0, 340],\r\n[ 1, 7512]])\r\ntensor([[ 2, 340],\r\n[ 3, 7512]])\r\n========== Epoch 1 ==========\r\ntensor([[ 0, 340],\r\n[ 1, 7512]])\r\ntensor([[ 2, 340],\r\n[ 3, 7512]])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# Random seeds for NumPy are different across `DataLoader` workers in each epoch.\r\n========== Epoch 0 ==========\r\ntensor([[ 0, 8715],\r\n[ 1, 5555]])\r\ntensor([[ 2, 6379],\r\n[ 3, 1432]])\r\n========== Epoch 1 ==========\r\ntensor([[ 0, 1374],\r\n[ 1, 996]])\r\ntensor([[ 2, 143],\r\n[ 3, 3507]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n* **Added static type checking enforce for DataPipe** ([#54020](https://github.com/pytorch/pytorch/pull/54020)).\r\n\r\nA new attribute named `type` has been introduced for `IterableDataset` using the typing annotation at each class declaration. By adding this attribute, we are able to extend `IterableDataset` to have type inference and lazy initialization to incorporate the new DataLoader architecture. But, several BC-breaking restrictions are introduced due to this feature.\r\n\r\n1.8.1:\r\n```python\r\n# Users can use string to bypass the invalid type annotation without any error. \r\n# And, incorrect type annotations attached to `__iter__` function are ignored.\r\n```\r\n\r\n1.9.0:\r\n```python\r\n# The following scenario will now raise different Exceptions\r\n# 1) The type annotation is required to be valid now. Previous workaround\r\n# like using string to  represent the invalid type annotation is not supported now.\r\n\r\n# Raises Exception from the evaluation `eval(\"invalid_type\", globals, locals)`\r\nclass DS(IterableDataset[\"invalid_type\"]):  \r\n     ...\r\n# Raises TypeError if the return type of __iter__ is not an Iterator\r\nclass DS(IterableDataset[str]):\r\n    def __iter__(self) -> str:\r\n      ...\r\n# Raise TypeError if the return type of __iter__ is of the form Iterator[X], \r\n# but the argument type X is not a subtype of the IterableDataset.type attribute.\r\nclass DS(IterableDataset[str]):\r\n    def __iter__(self) -> Iterator[int]:\r\n       ...\r\n\r\n#  IterableDatset now has a metaclass, which will conflict with\r\n#  existing user-defined metaclasses on IterableDatasets\r\nclass DS(IterableDataset[str], metaclass=MyMeta): \r\n    ...\r\n```\r\n\r\n\r\n## Meta API\r\n\r\n* **Given Tensor a non-trivial (for now) metaclass _TensorMeta** ([#56147](https://github.com/pytorch/pytorch/pull/56147)).\r\nTensor now has a non-trivial metaclass. This shouldn't be user observable, as Tensor already inherits from a C defined class (and is thus incompatible with other typical metaclasses), but there may be unanticipated interactions with other language features in Python. This PR changes the metaclass of torch.tensor. I.e. `type(type(torch.tensor([1])))` now prints `<class 'torch._C._TensorMeta'>` (used to be `<class 'type'>`)\r\n\r\n## C++ API\r\n\r\n* **Changed in-place resize functions to return const Tensor&** ([#55351](https://github.com/pytorch/pytorch/pull/55351)).\r\nThe C++ signature for `resize_`, `resize_as_`, `resize_as_sparse_`, `sparse_resize_`, and `sparse_resize_and_clear_` has changed to return a `const Tensor&` instead of a `Tensor&`. This may break users\u2019 TORCH_LIBRARY operators that called these functions but returned a non-const `Tensor&`. Ideally, users can change their operators to also consume and return `const Tensor&`, but simply casting the result of the changed function with `const_cast<Tensor&>` is also an option.\r\n\r\n1.8.1:\r\n```cpp\r\nconst at::Tensor a = at::randn({2, 2});\r\nconst at::Tensor b = at::ones({1, 4}, at::kInt);\r\nat::Tensor& out = at::resize_as_(a, b); # success\r\n```\r\n\r\n1.9.0:\r\n```cpp\r\nconst at::Tensor b = at::ones({1, 4}, at::kInt);\r\nat::Tensor& out = at::resize_as_(a, b); \r\n# error: binding value of type 'const at::Tensor' to reference to type 'at::Tensor' drops 'const' qualifier\r\nconst at::Tensor& out = at::resize_as_(a, b); # Success\r\n```\r\n\r\n* **Some ATen Reduction Ops as well as `kron_out` now throw an error when an undefined tensor is passed as input for `out` argument** ([#53218](https://github.com/pytorch/pytorch/pull/53218), [#53640](https://github.com/pytorch/pytorch/pull/53640)).\r\n    * C++ API for the reductions ops like `sum_out`, `nansum_out`, `prod_out`, `std_var_out` have been changed to require users allocating result Tensor before calling these ops. The C++ API `allocate_reduction_result` has changed to `resize_reduction_result` to disallow allocating result Tensor in these reduction ops.\r\n    * The following code can be compiled, but will raise a `c10::Error` when executed. This code compiled and executed successfully in the prior release.\r\n```cpp\r\nat::Tensor out;  # Undefined Tensor\r\nconst at::Tensor a = at::randn({2, 2});\r\nat::IntArrayRef dim = {1};\r\nat::sum_out(out, a, dim);\r\n# c10::Error: Expected a Tensor of type Variable but found an undefined Tensor for argument #4 'out'\r\n```\r\n\r\n* **The C++ API utility functions `expand_inplace` and `expand_outplace` now return `c10::MaybeOwned<Tensor>` instead of `std::tuple<Tensor>`** ([#55065](https://github.com/pytorch/pytorch/pull/55065), [#55245](https://github.com/pytorch/pytorch/pull/55245)). \r\nThe rationale for this change is to avoid unnecessary Tensor creation, thus improving performance. Functions in ExpandUtils return `c10::MaybeOwned<Tensor>` because expansion may not actually be needed, in which case we can improve efficiency by returning `c10::MaybeOwned<Tensor>::borrowed(to_expand)`. However, this means that you need to be careful: the returned `c10::MaybeOwned<Tensor> `must not outlive the original `Tensor` object that `to_expand` referred to! The deleted rvalue reference overloads of these functions help with this by preventing trivial use of a temporary resulting from a function call, but it is still possible to make a mistake. \r\n\r\n## TorchScript\r\n\r\n* **Added recursive scripting for class type module attributes** ([#55124](https://github.com/pytorch/pytorch/pull/55124)).\r\n    * This change is BC-breaking because it will result in class type module attributes being scripted when a module instance is scripted. In previous versions, such attributes were ignored unless their class type was also marked with `@torch.jit.script`. This new feature attempts to script the type, and falls back to the old behaviour of marking the class type attribute as \"failed\" if scripting fails. However, if the class definition does not have type annotations, the definition of the scripted class can different from users might expect (see code sample). If needed, users can explicitly disable the scripting of a class type attribute by adding its name to the `__jit_ignored_attributes__` class attribute of the module being scripted.\r\n\r\n1.8.1:\r\n```python\r\nclass MyClass:\r\n    def __init__(self, a):\r\n        self.attr = a\r\n        \r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        self.attr = MyClass(4)\r\n        \r\nsm = torch.jit.script(MyModule())\r\n```\r\n\r\n1.9.0:\r\n```python\r\nclass MyClass:\r\n    def __init__(self, a):\r\n        self.attr = a\r\n        \r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        self.attr = MyClass(4)\r\n \r\n# RuntimeError: Could not cast attribute 'attr' to type Tensor: Unable to cast Python instance of type <class 'int'> to C++ type 'at::Tensor'         \r\nsm = torch.jit.script(MyModule()) \r\n```\r\n\r\nThis error occurs because `MyClass` is automatically scripted, but `self.attr` is inferred to be a `Tensor` instead of an `int` because `a` is not annotated. To fix this, annotate `a` with the right type `int`, or mark `attr` as an attribute that should be ignored by the scripting process and not recursively processed:\r\n```python\r\n       class MyModule(torch.nn.Module):\r\n            __jit_ignored_attributes__ = [\"attr\"]\r\n        \r\n            def __init__(self):\r\n                self.attr = MyClass(4)\r\n```\r\n                \r\n\r\n## Quantization\r\n\r\n*  **`torch.quantization.quantize_fx.convert_fx`\u2019s `debug` argument has been changed to `is_reference` ([#52179](https://github.com/pytorch/pytorch/pull/52179)).**\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1:</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch.quantization.quantize_fx as quantize_fx\r\n>>> m = quantize_fx.convert_fx(m, debug=True)\r\n(Runs successfully)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> m = quantize_fx.convert_fx(m, is_reference=True) # Runs successfully\r\n>>> m = quantize_fx.convert_fx(m, debug=True)\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nTypeError: convert_fx() got an unexpected keyword argument 'debug'\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n* **`torch.cat` is now quantized to `torch.cat` instead of `torch.ops.quantized.cat` ([#54924](https://github.com/pytorch/pytorch/pull/54924)).**\r\nPreviously, we produced torch.ops.quantize.cat which took inputs, dequantized them\r\n        and requantized them with new qparams. This behavior has been changed to produce `torch.cat` directly. [torch.cat](http://torch.cat/) uses the same observer/fake_quant instance for all inputs and output, assumes all inputs are sharing the same qparam, and produces a quantized Tensor with\r\n        the same qparam as all inputs. Using torch.cat is expected to be more efficient since it does not introduce extra quant/dequant.\r\n    * Version 1.8.1: `torch.cat` was quantized to  `torch.ops.quantized.cat.`\r\n    * Version 1.9: `torch.cat` is quantized to `torch.cat` (`torch.cat` works on both floating point and quantized Tensor).\r\n\r\n## Distributed\r\n\r\n* **`DistributedDataParallel`: Removed support for inter-process device replication in DDP ([#54454](https://github.com/pytorch/pytorch/pull/54454),  [#54825](https://github.com/pytorch/pytorch/pull/54825), [#54826](https://github.com/pytorch/pytorch/pull/54826), [#55212](https://github.com/pytorch/pytorch/pull/55212), [#55253](https://github.com/pytorch/pytorch/pull/55253)`, `[`#55353`](https://github.com/pytorch/pytorch/pull/55353)).**\r\n`DistributedDataParallel` now errors out when users attempt to use it in single-process multi-device mode, where a module is replicated across more than one device in a single process. This mode had been previously deprecated and is now removed. Use cases should switch to spawning a single process for each device that is used in replication, which is the performant way to use `DistributedDataParallel` and supports a variety of newly developed features.\r\n\r\n1.8.1:\r\n```python\r\n>>> # Assume the below is ran on 2 ranks in a distributed setting.\r\n>>> rank_to_devices = { 0: [0, 1], 1: [2, 3] }\r\n>>> # Each rank replicates model across 2 GPUs.\r\n>>> model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n        model,\r\n        device_ids=rank_to_devices[rank]\r\n    )\r\n>>> # No error is raised, but below warning is produced.\r\n>>> UserWarning: Single-Process Multi-GPU is not the recommended mode for DDP. In this mode, each DDP instance operates on multiple devices and creates multiple module replicas within one process. The overhead of scatter/gather and GIL contention in every forward pass can slow down training. Please consider using one DDP instance per device or per module replica by explicitly setting device_ids or CUDA_VISIBLE_DEVICES.\r\n```\r\n\r\n1.9.0:\r\n```python\r\n>>> # Assume the below is ran on 2 ranks in a distributed setting.\r\n>>> rank_to_devices = { 0: [0, 1], 1: [2, 3] }\r\n>>> # Each rank replicates model across 2 GPUs.\r\n>>> model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n        model,\r\n        device_ids=rank_to_devices[rank]\r\n    )\r\n>>> # Single process multi-GPU mode now produces an error on initialization.\r\n>>> ValueError: device_ids can only be None or contain a single element.\r\n```\r\n\r\n* **`torch.distributed.elastic`: Replaced `torch.distributed.launch` with `torch.distributed.elastic_launch` ([#56037](https://github.com/pytorch/pytorch/pull/56037)`, `[`#56214`](https://github.com/pytorch/pytorch/pull/56214)).**\r\n        * --logdir \u2192 \u2014log_dir. The stdout and stderr log dir arg name and destination changed. The file destination changed from `$logdir/node_{}_local_rank_{}_stdout` to  `$log_dir/$rank/stdout.log`. If users used the `\u2014logdir` introduced in 1.8 pytorch version, they need to use` \u2014log_dir` parameter now.\r\n\r\n1.8.1:\r\n```python\r\n#!/bin/bash\r\n# Assumes training script train.py exists.\r\npython -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=\"29500\" --logdir test_logdir train.py\r\n# Logs are written to $logdir/node_{}_local_rank_{}_stdout\r\n```\r\n1.9.0:\r\n```python\r\n#!/bin/bash\r\n# Assumes training script train.py exists.\r\npython -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=\"29500\" --log_dir test_logdir train.py\r\n# Logs are written to $log_dir/$rank/stdout.log\r\n```\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n* **`torch.floor_divide` has been deprecated in favor of `torch.div(..., rounding_mode=\u2018floor\u2019)` ([#50281](https://github.com/pytorch/pytorch/pull/50281)).**\r\n    * `torch.floor_divide` incorrectly divides then truncates (rounds towards zero) instead of dividing then flooring (rounds \u201cdown\u201d). Use the `rounding_mode` argument of `torch.div` to indicate if you\u2019d like to continue performing truncation division or floor division, instead, since `torch.floor_divide` will be removed in a future PyTorch release.\r\n* **Older linear algebra operations have been deprecated in favor of their new linalg module counterparts. Namely:**\r\n    *  `torch.{cholesky, qr, symeig, chain_matmul, solve, eig, matrix_rank, lstsq}` have been deprecated in favor of `torch.linalg.{cholesky, qr, symeig, chain_matmul, solve, eig, matrix_rank, lstsq}` ([#57725,](https://github.com/pytorch/pytorch/pull/57725)[#57745](https://github.com/pytorch/pytorch/pull/57745), [#57732,](https://github.com/pytorch/pytorch/pull/57732)[#53453](https://github.com/pytorch/pytorch/pull/53453), [#57741](https://github.com/pytorch/pytorch/pull/57741), [#57727](https://github.com/pytorch/pytorch/pull/57727), [#57734](https://github.com/pytorch/pytorch/pull/57734), [#57743](https://github.com/pytorch/pytorch/pull/57743)).\r\n    * `torch.norm` has been deprecated in favor of the new linalg module norm functions: `torch.linalg.vector_norm`, `torch.linalg.matrix_norm`, and `torch.linalg.norm` ([#57986](https://github.com/pytorch/pytorch/pull/57986)).\r\n    * Aliased `torch.det`, `torch.slogdet`, `torch.matrix_power`, `torch.inverse`, and `torch.pinverse` to their linalg module counterparts ([#57821](https://github.com/pytorch/pytorch/pull/57821)).\r\n\r\n## Autograd\r\n\r\n* **[cpp] Renamed `AutoNonVariableTypeMode` to `AutoDispatchBelowAutograd` and added a warning. ([#56422](https://github.com/pytorch/pytorch/pull/56422))** \r\n`AutoNonVariableTypeMode` is deprecated and will be removed in 1.10 release. For kernel implementations,  please use `AutoDispatchBelowAutograd` instead. Check out more details on how to migrate your kernel [here](https://pytorch.org/cppdocs/notes/inference_mode.html#migration-guide-from-autononvariabletypemode). If you are looking for a user-facing API to enable running your inference-only workload, please use `c10::InferenceMode`. Using `AutoDispatchBelowAutogradMode` in user code is under risk of producing silently wrong result for some edge cases.\r\n    \r\n1.8.1:\r\n```cpp\r\n{\r\n  at::AutoNonVariableTypeMode guard(true);\r\n}\r\n```\r\n\r\n1.9.0:\r\n```\r\n{\r\n  c10::AutoDispatchBelowAutograd guard(true); // for kernel implementations\r\n  // c10::InferenceMode guard(true); --> consider inference mode if you are looking for a user-facing API\r\n\r\n}\r\n```\r\n\r\n* **Removed logic for old style custom autograd `Function` ([#57357](https://github.com/pytorch/pytorch/pull/57357)).**\r\nInstantiating a custom autograd function is now deprecated and will raise a warning. Users should call `.apply()` on the class itself because it is a static method.\r\n\r\n1.8.1:\r\n```python\r\n        # Instantiating custom function will raise a warning in 1.9\r\n        Func().apply\r\n```\r\n\r\n1.9.0:\r\n```python\r\n        # You should directly call the `apply` (classmethod) on the class\r\n        Func.apply\r\n```\r\n\r\n* **Deprecated `get_analytical_jacobian` and `get_numerical_jacobian` ([#54378](https://github.com/pytorch/pytorch/pull/54378), [#54049](https://github.com/pytorch/pytorch/pull/54049)).**\r\n`torch.autograd.gradcheck.get_analytical_jacobian`  and `torch.autograd.gradcheck.get_numerical_jacobian` are internal-facing functions that are not a part of our public API. We\u2019ve refactored some PyTorch internals to work without it and will\r\n        remove it in a future release. For gradient checking purposes, please use `torch.autograd.gradcheck`. \r\n\r\n## C++ API\r\n\r\n* **Removed the redundant `linalg_` prefix from `torch::linalg::linalg_det` and `torch::linalg::linalg_norm` C++ API ([#57464](https://github.com/pytorch/pytorch/pull/57464)).**\r\nC++ code that used to call `torch::linalg::{linalg_det, linalg_norm}` should be updated to call `torch::linalg::{det, norm}`\r\n\r\n## Distributed\r\n\r\n* **`torch.distributed.rpc`: Added a warning message to retire ProcessGroup RPC backend ([#55616](https://github.com/pytorch/pytorch/pull/55616))**\r\n    * ProcessGroup RPC backend is being deprecated and 1.9 is the last release which will carry it. The default RPC backend is TensorPipe which is the recommended backend to use over ProcessGroup.\r\n\r\n# \r\n\r\n# New features\r\n\r\n### Python API\r\n\r\n* Added BFloat16 support for `torch.{ceil, floor, frac, round, trunc, lerp, roll, diag, logaddexp, logaddexp2, nan_to_num, exp2, expm1, rsqrt, erfc, atan2, hypot}` on CUDA ([#57910](https://github.com/pytorch/pytorch/pull/57910), [#57907](https://github.com/pytorch/pytorch/pull/57907), [#57916](https://github.com/pytorch/pytorch/pull/57916), [#57908](https://github.com/pytorch/pytorch/pull/57908), [#58063](https://github.com/pytorch/pytorch/pull/58063), [#57913](https://github.com/pytorch/pytorch/pull/57913), [#57905](https://github.com/pytorch/pytorch/pull/57905)).\r\n* Added `torch.pow()` for `torch.{float16, BFloat16}` on CPU ([#55280](https://github.com/pytorch/pytorch/pull/55280)).\r\n* Added `torch.{index_select, argmax, argmin, min, max, amin, amax}` for `torch.{float16, BFloat16}` ([#53898](https://github.com/pytorch/pytorch/pull/53898), [#52582](https://github.com/pytorch/pytorch/pull/52582), [#51244](https://github.com/pytorch/pytorch/pull/51244), [#52579](https://github.com/pytorch/pytorch/pull/52579)).\r\n* Added `torch.dot` for `BFloat16` on CUDA ([#57903](https://github.com/pytorch/pytorch/pull/57903)).\r\n* Added support for tensor inputs for `min` and `max` arguments in `torch.clamp` ([#52695](https://github.com/pytorch/pytorch/pull/52695), [#56367](https://github.com/pytorch/pytorch/pull/56367)).\r\n* Added a new `torch.special` namespace similar to `scipy.special` ([#52296](https://github.com/pytorch/pytorch/pull/52296)).\r\n    * Added special.{`entr` ([#53500](https://github.com/pytorch/pytorch/pull/53500)),  `xlog1py` ([#55138](https://github.com/pytorch/pytorch/pull/55138)), `i0e` ([#54409](https://github.com/pytorch/pytorch/pull/54409)), `erfc`, `erfinv` ([#53260](https://github.com/pytorch/pytorch/pull/53260))}.\r\n    * Added aliases for `special.{expm1, exp2}` ([#54670](https://github.com/pytorch/pytorch/pull/54670)).\r\n    * Added aliases for `special.{sigmoid, logit}` ([#54759](https://github.com/pytorch/pytorch/pull/54759)).\r\n* Added the following new operators in PyTorch similar to those in NumPy:\r\n    * `torch.gradient` ([#54617](https://github.com/pytorch/pytorch/pull/54617))\r\n    * `torch.{hsplit, vsplit, dsplit}` ([#53536](https://github.com/pytorch/pytorch/pull/53536))\r\n    * `torch.positive` ([#55891](https://github.com/pytorch/pytorch/pull/55891))\r\n    * `torch.frexp` ([#51097](https://github.com/pytorch/pytorch/pull/51097))\r\n    * `torch.take_along_dim` ([#52833](https://github.com/pytorch/pytorch/pull/52833))\r\n* Added a new keyword argument `alpha` to `torch.index_add` ([#54176](https://github.com/pytorch/pytorch/pull/54176)).\r\n* Added `torch.assert_async` ([#53086](https://github.com/pytorch/pytorch/pull/53086))\r\n* Added a new keyword argument `interpolation` to `torch.quantile` ([#49267](https://github.com/pytorch/pytorch/pull/49267)).\r\n* Add correction parameter to std/var ([#50903](https://github.com/pytorch/pytorch/pull/50903))\r\n* Added overloads for `torch.{std, var, std_mean, var_mean}` with a correction argument specifying the difference between the sample size and number of degrees of freedom. \r\n* Add support for integer type for `torch.`{`logit, rad2deg, deg2rad, polygamma}` ([#52028](https://github.com/pytorch/pytorch/pull/52028), [#51853,](https://github.com/pytorch/pytorch/pull/51853)[#57462](https://github.com/pytorch/pytorch/pull/57462))\r\n* Added support for stable sort algorithm on CPU by a new kwarg `stable` ([#51790](https://github.com/pytorch/pytorch/pull/51790)).\r\n* The `torch.linalg` module, analogous to NumPy\u2019s linalg module but with several additional functions, is stable! Added `torch.linalg.{multi_dot, lstsq, vector_norm, matrix_norm, matrix_power, det, eig, eigvals, svdvals, cholesky_ex, inv_ex}` ([#51807](https://github.com/pytorch/pytorch/pull/51807), [#49093](https://github.com/pytorch/pytorch/pull/49093), [#51099](https://github.com/pytorch/pytorch/pull/51099), [#57127](https://github.com/pytorch/pytorch/pull/57127), [#52608](https://github.com/pytorch/pytorch/pull/52608), [#53119](https://github.com/pytorch/pytorch/pull/53119), [#52491](https://github.com/pytorch/pytorch/pull/52491), [#56684](https://github.com/pytorch/pytorch/pull/56684), [#56724](https://github.com/pytorch/pytorch/pull/56724), [#58039](https://github.com/pytorch/pytorch/pull/58039)).\r\n* Added a new `device=meta` API ([#53143](https://github.com/pytorch/pytorch/pull/53143))\r\n    * \u201cmeta\u201d is a new device, like CPU/CUDA, that doesn\u2019t allocate any memory for data. Operators that are passed meta tensor inputs will perform shape inference, without running the actually kernel computation. For example, `torch.ones(2, device='meta') + torch.ones(1, 2, device='meta')` will return a new meta tensor of size `[1, 2]` (performing broadcasting), without allocating memory or running an actual kernel.\r\n    * `device=meta` API is implemented for `upsample_linear1d`([#51917](https://github.com/pytorch/pytorch/pull/51917)), `upsample_bilinear2d` and `upsample_bicubic2d` ([#52012](https://github.com/pytorch/pytorch/pull/52012)), `upsample_nearest3d` ([#52065](https://github.com/pytorch/pytorch/pull/52065)), `sin`([#52277](https://github.com/pytorch/pytorch/pull/52277)), `mul`([#52692](https://github.com/pytorch/pytorch/pull/52692)), `pow`([#53669](https://github.com/pytorch/pytorch/pull/53669)), `sub`([#53679](https://github.com/pytorch/pytorch/pull/53679)), `div`([#53680](https://github.com/pytorch/pytorch/pull/53680)), `copysign`([#55040](https://github.com/pytorch/pytorch/pull/55040)), `atan2`([#55130](https://github.com/pytorch/pytorch/pull/55130)), `sinh`([#55538](https://github.com/pytorch/pytorch/pull/55538)), `acosh`([#55540](https://github.com/pytorch/pytorch/pull/55540)), `cosh`([#55563](https://github.com/pytorch/pytorch/pull/55563)), `cos` ([#55564](https://github.com/pytorch/pytorch/pull/55564)), `replication_padding1d` ([#55481](https://github.com/pytorch/pytorch/pull/55481)), `replication_padding3d` ([#55499](https://github.com/pytorch/pytorch/pull/55499)), `replication_pad1d_backward` ([#55537](https://github.com/pytorch/pytorch/pull/55537)), `fractional_max_pool2d` ([#55581](https://github.com/pytorch/pytorch/pull/55581)), `reflection_pad1d` ([#55531](https://github.com/pytorch/pytorch/pull/55531)), `replication_pad2d` ([#55511](https://github.com/pytorch/pytorch/pull/55511)), `addmv` ([#55746](https://github.com/pytorch/pytorch/pull/55746)), all unary float functions ([#56082](https://github.com/pytorch/pytorch/pull/56082)), `adaptive_max_pool2d`([#56317](https://github.com/pytorch/pytorch/pull/56317)), `adaptive_max_pool3d` ([#56320](https://github.com/pytorch/pytorch/pull/56320)), all non-float unary operators (and `rsqrt`) ([#56151](https://github.com/pytorch/pytorch/pull/56151)), `adaptive_max_pool2d_backward` ([#56799](https://github.com/pytorch/pytorch/pull/56799)), `adaptive_max_pool3d_backward` ([#56800](https://github.com/pytorch/pytorch/pull/56800)), `neg`([#57212](https://github.com/pytorch/pytorch/pull/57212)), `max_pool2d_with_indices`([#56459](https://github.com/pytorch/pytorch/pull/56459)), `trunc` ([#57350](https://github.com/pytorch/pytorch/pull/57350)), `floor` ([#57587](https://github.com/pytorch/pytorch/pull/57587)), `sign` ([#57588](https://github.com/pytorch/pytorch/pull/57588)), `ceil` ([#57589](https://github.com/pytorch/pytorch/pull/57589)), `gcd` ([#57624](https://github.com/pytorch/pytorch/pull/57624)), `nextafter` ([#57625](https://github.com/pytorch/pytorch/pull/57625)), `igamma` and `igammac`([#57626](https://github.com/pytorch/pytorch/pull/57626)), `hypot`([#57627](https://github.com/pytorch/pytorch/pull/57627)), `lcm` ([#57628](https://github.com/pytorch/pytorch/pull/57628)), `logaddexp` and `logaddexp2` ([#57629](https://github.com/pytorch/pytorch/pull/57629)), `maximum` and `minimum` ([#57630](https://github.com/pytorch/pytorch/pull/57630)), `topk` ([#57790](https://github.com/pytorch/pytorch/pull/57790)), `max_pool2d_with_indices_backward` ([#57797](https://github.com/pytorch/pytorch/pull/57797)), `threshold` ([#57810](https://github.com/pytorch/pytorch/pull/57810)), `addmm` ([#57417](https://github.com/pytorch/pytorch/pull/57417)), `heaviside` ([#57933](https://github.com/pytorch/pytorch/pull/57933)), `elu`([#57619](https://github.com/pytorch/pytorch/pull/57619)), `softplus` ([#57620](https://github.com/pytorch/pytorch/pull/57620)), `leaky_relu` ([#57621](https://github.com/pytorch/pytorch/pull/57621)), `hardsigmoid` ([#57622](https://github.com/pytorch/pytorch/pull/57622)), `softshrink` ([#57623](https://github.com/pytorch/pytorch/pull/57623)), `silu` ([#58050](https://github.com/pytorch/pytorch/pull/58050)), `empty_strided` ([#53397](https://github.com/pytorch/pytorch/pull/53397)), non-composite in-place operators ([#54901](https://github.com/pytorch/pytorch/pull/54901))\r\n\r\n### Complex Numbers\r\n\r\n* Added complex autograd support for `torch.{masked_fill, polar, cumsum, lerp, prod, rsub, unfold, symeig, index_copy}` ([#52483](https://github.com/pytorch/pytorch/pull/52483), [#52488](https://github.com/pytorch/pytorch/pull/52488), [#53240](https://github.com/pytorch/pytorch/pull/53240), [#53689](https://github.com/pytorch/pytorch/pull/53689), [#48125](https://github.com/pytorch/pytorch/pull/48125), [#53702](https://github.com/pytorch/pytorch/pull/53702), [#52999](https://github.com/pytorch/pytorch/pull/52999), [#55085](https://github.com/pytorch/pytorch/pull/55085), [#52203](https://github.com/pytorch/pytorch/pull/52203)).\r\n* Added complex support for torch.lerp ([#54129](https://github.com/pytorch/pytorch/pull/54129)) and torch.sigmoid ([#55975](https://github.com/pytorch/pytorch/pull/55975)) on CUDA. \r\n* Added complex support for `torch.index_copy` and `torch.{take}` and `torch.Tensor.put_` on both CPU and CUDA ([#52203](https://github.com/pytorch/pytorch/pull/52203), [#53356](https://github.com/pytorch/pytorch/pull/53356)).\r\n* Added complex support to TorchScript.\r\n    * Added logic to teach TorchScript frontend to parse complex literals, and complex lists. ([#52881](https://github.com/pytorch/pytorch/pull/52881)).\r\n    * Added TorchScript support for:\r\n        *  complex constructor and `torch.{add, mul, sub, as_tensor}` ([#52881](https://github.com/pytorch/pytorch/pull/52881)).\r\n        * `cmath` unary ops: `cmath.{phase, log, log10, sqrt, exp, sin, cos, tan, asin, acos, atan, sinh, cosh, tanh, asinh, acosh, atanh}` ([#54089](https://github.com/pytorch/pytorch/pull/54089)).\r\n        * `cmath.`{`infj, nanj}` ([#54328](https://github.com/pytorch/pytorch/pull/54328)).\r\n        *  `cmath.{isinf, isnan, isfinite, rect}` ([#54541](https://github.com/pytorch/pytorch/pull/54541)).\r\n        * real and imag tensor attributes (`tensor.real/imag`) ([#54692](https://github.com/pytorch/pytorch/pull/54692)).\r\n    * Fixed `test_variant_consistency_jit_addmm` for complex types ([#54917](https://github.com/pytorch/pytorch/pull/54917), [#57129](https://github.com/pytorch/pytorch/pull/57129)).\r\n* Added initial operator support for sparse complex tensors ([#57125](https://github.com/pytorch/pytorch/pull/57125)).\r\n    * Added complex support for `torch.{sparse_coo_tensor, coalesce, to_dense, to_sparse, sparse_add, sspaddmm, saddmm}`.\r\n* Added `torch.Tensor.{cfloat, cdouble}` functions ([#58137](https://github.com/pytorch/pytorch/pull/58137)).\r\n* Added complex support for all reductions for `torch.{std, var}` to return a real valued output tensor for complex inputs ([#58066](https://github.com/pytorch/pytorch/pull/58066)) .\r\n* Updated autograd formulas for many linear algebra operations support complex tensors:\r\n    * `eig`: faster and with complex support ([#52875](https://github.com/pytorch/pytorch/pull/52875))\r\n    * `lu`: more numerically stable and with complex support. ([#53994](https://github.com/pytorch/pytorch/pull/53994))\r\n\r\n### torch.nn\r\n\r\n* New `torch.nn` modules: `nn.LazyBatchNorm*d` ([#51862](https://github.com/pytorch/pytorch/pull/51862)), `nn.HuberLoss` ([#50553](https://github.com/pytorch/pytorch/pull/50553)), `nn.Mish` ([#58375](https://github.com/pytorch/pytorch/issues/58375)).\r\n* New parametrization functionality ([#33344](https://github.com/pytorch/pytorch/pull/33344), [#58142](https://github.com/pytorch/pytorch/pull/58142), [#55456](https://github.com/pytorch/pytorch/pull/55456), [#57784](https://github.com/pytorch/pytorch/pull/57784)).\r\n* `nn.Conv*d`: Added `padding='same'` mode for non-strided convolutions ([#45667](https://github.com/pytorch/pytorch/pull/45667)).\r\n* `nn.EmbeddingBag`: Added `padding_idx` support ([#49237](https://github.com/pytorch/pytorch/pull/49237), [#56065](https://github.com/pytorch/pytorch/pull/56065), [#56618](https://github.com/pytorch/pytorch/pull/56618)).\r\n* Added mish activation function ([#58648](https://github.com/pytorch/pytorch/pull/58648)).\r\n* [memory format] Added channels last support for `MaxPool2d` ([#56361](https://github.com/pytorch/pytorch/pull/56361)).\r\n* Added the option to build PyTorch with DNNL + AMD BLIS path ([#54953](https://github.com/pytorch/pytorch/pull/54953)).\r\n\r\n### Profiler\r\n\r\n* Added `skip_first` parameter to the default schedule ([#58025](https://github.com/pytorch/pytorch/pull/58025)).\r\n* Added support for trace metadata ([#56575](https://github.com/pytorch/pytorch/pull/56575)).\r\n* Added `gzip` format support for chrome tracing ([#56554](https://github.com/pytorch/pytorch/pull/56554)).\r\n* Added `sequenceNr` and `fwdThreadId` to the trace ([#57182](https://github.com/pytorch/pytorch/pull/57182)).\r\n* Enabled Kineto in CPU builds ([#53174](https://github.com/pytorch/pytorch/pull/53174)).\r\n\r\n### Autograd\r\n\r\n* Added new inference mode both in C++ ([](https://github.com/pytorch/pytorch/pull/58045)[#54403](https://github.com/pytorch/pytorch/pull/54403), [#53343](https://github.com/pytorch/pytorch/pull/53343)) and python ([#58045](https://github.com/pytorch/pytorch/pull/58045), [#57480](https://github.com/pytorch/pytorch/pull/57480)).\r\n* Added `fast_mode` argument to `autograd.gradcheck` ([#54480](https://github.com/pytorch/pytorch/pull/54480)).\r\n* Added support for non-Tensor inputs and outputs to `torch.utils.checkpoint` functions ([#52422](https://github.com/pytorch/pytorch/pull/52422)).\r\n\r\n### Dataloader\r\n\r\n* Implemented `FilterIterDataPipe` ([#51783](https://github.com/pytorch/pytorch/pull/51783)).\r\n* Added context manager for runtime type validation ([#55936](https://github.com/pytorch/pytorch/pull/55936)).\r\n* Added typing enforcement for `DataPipe` at construct-time ([#54066](https://github.com/pytorch/pytorch/pull/54066)).\r\n* Added typing Enforcement for `DataPipe` at runtime ([#54544](https://github.com/pytorch/pytorch/pull/54544)).\r\n* Implemented `issubtype` for `DataLoader` type hints ([#54299](https://github.com/pytorch/pytorch/pull/54299)).\r\n* Added type hint for SequentialSampler ([#56374](https://github.com/pytorch/pytorch/pull/56374)).\r\n* Added `ConcatDataPipe` ([#53301](https://github.com/pytorch/pytorch/pull/53301)).\r\n* Introduced deterministic context to `DataLoader` ([#53271](https://github.com/pytorch/pytorch/pull/53271)).\r\n* Added `ZipIterDataPipe` ([#53554](https://github.com/pytorch/pytorch/pull/53554)).\r\n* Added switch to guaranteed determinism & add option to non_deterministic ([#53532](https://github.com/pytorch/pytorch/pull/53532)).\r\n* Added `TransformsIterDataPipe` ([#52604](https://github.com/pytorch/pytorch/pull/52604)).\r\n* Renamed Callable to `MapIterDataPipe` ([#51879](https://github.com/pytorch/pytorch/pull/51879)).\r\n\r\n### CUDA\r\n\r\n* Added the following new features to CUDA Graphs:\r\n    *  Private mempools ([#54038](https://github.com/pytorch/pytorch/pull/54038))\r\n    * Support for RNN capture when cuDNN dropout is used ([#57373](https://github.com/pytorch/pytorch/pull/57373)).\r\n* Added support for `'max'` reduction for `torch.segment_reduce` ([#56704](https://github.com/pytorch/pytorch/pull/56704)).\r\n* Added support for CUDA allocator to handle multiple streams seamlessly ([#55860](https://github.com/pytorch/pytorch/pull/55860)).\r\n\r\n### C++ API\r\n\r\n* Added `torch::nn::functional::huber_loss` ([#50553](https://github.com/pytorch/pytorch/pull/50553)).\r\n* Added learning rate schedulers to C++ API ([#52268](https://github.com/pytorch/pytorch/pull/52268)).\r\n* Added `padding='same'` mode to `torch::conv{1,2,3}d` ([#45667](https://github.com/pytorch/pytorch/pull/45667)).\r\n* Added `padding_idx` argument to `EmbeddingBag` ([#49237](https://github.com/pytorch/pytorch/pull/49237)).\r\n* Added mish activation function ([#58648](https://github.com/pytorch/pytorch/pull/58648)) ([#58940](https://github.com/pytorch/pytorch/pull/58940)).\r\n\r\n### TorchScript\r\n\r\n* Added reductions to NNC python bindings ([#52492](https://github.com/pytorch/pytorch/pull/52492)).\r\n* Added Python bindings for ExternalCalls. ([#52905](https://github.com/pytorch/pytorch/pull/52905)).\r\n* Added an API to reorder multiple loops ([#55568](https://github.com/pytorch/pytorch/pull/55568)).\r\n* Added NNC support for `pow` on CPU ([#56308](https://github.com/pytorch/pytorch/pull/56308)).\r\n* Enabled horizontal fusion of all loops ([#56324](https://github.com/pytorch/pytorch/pull/56324)).\r\n* Added an API for Buffer Compression ([#55853](https://github.com/pytorch/pytorch/pull/55853)).\r\n* Added API to distribute loops ([#53865](https://github.com/pytorch/pytorch/pull/53865)).\r\n* Added `matmul` for NNC lowering/unified dtypes ([#56456](https://github.com/pytorch/pytorch/pull/56456)).\r\n* Added a method to compute `conv` without bias ([#57512](https://github.com/pytorch/pytorch/pull/57512)).\r\n* Added support for computing `conv` with dynamic shapes ([#57514](https://github.com/pytorch/pytorch/pull/57514)).\r\n* Added NNC lowerings for `t`/`transpose`/`permute`/`expand` ([#57426](https://github.com/pytorch/pytorch/pull/57426)).\r\n* Updated external functions for mobile build ([#56850](https://github.com/pytorch/pytorch/pull/56850)).\r\n* Added `GELU` To NNC ([#57753](https://github.com/pytorch/pytorch/pull/57753)).\r\n* Implemented `GELU` Backward ([#58249](https://github.com/pytorch/pytorch/pull/58249)).\r\n* Added a mobile NNC backend skeleton ([#56852](https://github.com/pytorch/pytorch/pull/56852)).\r\n* Added support for `torch.type` ([#51904](https://github.com/pytorch/pytorch/pull/51904))\r\n* Added `dict()` constructor ([#51934](https://github.com/pytorch/pytorch/pull/51934)).\r\n* Added a new `torch::deploy` to manage multiple python interpreters in a single\r\n    process to deploy PyTorch models packaged with torch.package ([#51754](https://github.com/pytorch/pytorch/pull/51754)).\r\n* Reintroduced static dispatch ([#51957](https://github.com/pytorch/pytorch/pull/51957)).\r\n* Added TS support for `torch.any` ([#52360](https://github.com/pytorch/pytorch/pull/52360)).\r\n* Added a demo backend with compiler ([#52603](https://github.com/pytorch/pytorch/pull/52603)).\r\n* Added MKLDNN fuser ([#51600](https://github.com/pytorch/pytorch/pull/51600)).\r\n* Added a context manager for hiding source ranges ([#53188](https://github.com/pytorch/pytorch/pull/53188)).\r\n* Implemented `embedding_bag` for SR ([#52429](https://github.com/pytorch/pytorch/pull/52429)).\r\n* Allowed the use of `AliasDb` in Python ([#51336](https://github.com/pytorch/pytorch/pull/51336)).\r\n* Added support for `DictConstruct` ([#54438](https://github.com/pytorch/pytorch/pull/54438))\r\n* Added `sliceHead`/`sliceTail` APIs with short parameter list ([#55115](https://github.com/pytorch/pytorch/pull/55115)).\r\n* Added logic to infer argument types in TorchScript ([#56832](https://github.com/pytorch/pytorch/pull/56832)).\r\n* Added support for  custom Python classes in `CUDAFuture` ([#56516](https://github.com/pytorch/pytorch/pull/56516)).\r\n* Added a concat optimization pass ([#55474](https://github.com/pytorch/pytorch/pull/55474)).\r\n* Added initial support for PEP-585 types ([#57363](https://github.com/pytorch/pytorch/pull/57363)).\r\n* Added logic to infer types for arguments of methods not invoked directly by `MonkeyType` ([#57202](https://github.com/pytorch/pytorch/pull/57202)).\r\n* Added support for `torch.jit.ignore` as a context manager ([#55172](https://github.com/pytorch/pytorch/pull/55172)).\r\n* Implemented `hardswish`/`hardsigmoid `on MKLDNN tensors ([#55218](https://github.com/pytorch/pytorch/pull/55218)).\r\n* Added `model_dump` tool for model inspection ([#56868](https://github.com/pytorch/pytorch/pull/56868))\r\n* Added static method support for TorchBind ([#51177](https://github.com/pytorch/pytorch/pull/51177))\r\n* Added TS support for `pow` ([#52374](https://github.com/pytorch/pytorch/pull/52374))\r\n* Added support for default argument values to `TorchBind` ([#51253](https://github.com/pytorch/pytorch/pull/51253)).\r\n* Added support for AST rewriting for submodules ([#52297](https://github.com/pytorch/pytorch/pull/52297)).\r\n* Added `optimize_for_inference` API ([#58193](https://github.com/pytorch/pytorch/pull/58193)).\r\n* Registered `aten::index_out` ([#51742](https://github.com/pytorch/pytorch/pull/51742)).\r\n* Added `PYTORCH_TENSOREXPR_DONT_FUSE` env variable to disable fusion on specified operators ([#55650](https://github.com/pytorch/pytorch/pull/55650)).\r\n\r\n### torch.package\r\n\r\n* Allow TorchScript models to be contained in the package format ([#54891,](https://github.com/pytorch/pytorch/pull/54891)[#56299,](https://github.com/pytorch/pytorch/pull/56299)[#54893](https://github.com/pytorch/pytorch/pull/54893), [#57573](https://github.com/pytorch/pytorch/pull/57573), [#54894](https://github.com/pytorch/pytorch/pull/54894), [#57678](https://github.com/pytorch/pytorch/pull/57678)).\r\n\r\n### Mobile\r\n\r\n* Added 8x1 block sparse kernels for ARM and AArch64 ([#51118](https://github.com/pytorch/pytorch/pull/51118), [#51119](https://github.com/pytorch/pytorch/pull/51119), [#51120](https://github.com/pytorch/pytorch/pull/51120)).\r\n* Made NNAPI converter handle binary ops combining NHWC+NCHW in some cases ([#48812](https://github.com/pytorch/pytorch/pull/48812)).\r\n* Improved support for multiple inputs and outputs in NNAPI ([#54697](https://github.com/pytorch/pytorch/pull/54697)).\r\n* Added flexible size support for NNAPI ([#54701](https://github.com/pytorch/pytorch/pull/54701)).\r\n* Added new ops for Metal (concat, mul/sub/div, transpose, view, reshape, mean, chunk, reflection_pad2d) ( [#53950](https://github.com/pytorch/pytorch/pull/53950), [#54107](https://github.com/pytorch/pytorch/pull/54107), [#54522](https://github.com/pytorch/pytorch/pull/54522), [#56073](https://github.com/pytorch/pytorch/pull/56073), [#56074](https://github.com/pytorch/pytorch/pull/56074), [#58263](https://github.com/pytorch/pytorch/pull/58263)).\r\n* Added python binding to use mobile cpu allocator ([#52323](https://github.com/pytorch/pytorch/pull/52323)).\r\n* Added lightweight RandomSampler for mobile ([#58201](https://github.com/pytorch/pytorch/pull/58201)).\r\n* Added support for:\r\n    * new ops to NNAPI converter (size, unsqueeze, cat, mean) ([#52026](https://github.com/pytorch/pytorch/pull/52026), [#48811](https://github.com/pytorch/pytorch/pull/48811)).\r\n    * multi-dimension tensors in Metal via MPSImage ([#54106](https://github.com/pytorch/pytorch/pull/54106)).\r\n    * multiple output tensors in Metal ([#56072](https://github.com/pytorch/pytorch/pull/56072)).\r\n    * methods other than forward in optimize_for_mobile  ([#53314](https://github.com/pytorch/pytorch/pull/53314)).\r\n    * ChannelsLast in TensorImageUtils on Android ([#48990](https://github.com/pytorch/pytorch/pull/48990)).\r\n    * loading \u201cextra files\u201d in Java/Android ([#55644](https://github.com/pytorch/pytorch/pull/55644)).\r\n    * loading \u201cextra files\u201d in Lite interpreter ([#52635](https://github.com/pytorch/pytorch/pull/52635)).\r\n    * querying bytecode version in Lite interpreter and bytecode models ([#56948](https://github.com/pytorch/pytorch/pull/56948), [#56948](https://github.com/pytorch/pytorch/pull/56948)).\r\n    * exporting some older bytecode versions for Lite interpreter ([#56802](https://github.com/pytorch/pytorch/pull/56802)).\r\n    * querying available ops ([#57570](https://github.com/pytorch/pytorch/pull/57570)).\r\n* Added SqueezeNet to PyTorch Playground (71d0b5632b).\r\n* Added libtorch lite build ([#51419](https://github.com/pytorch/pytorch/pull/51419)).\r\n\r\n### Distributed\r\n\r\n* `torch.distributed.Store`\r\n    * Added `compare_set` op ([#51815](https://github.com/pytorch/pytorch/pull/51815)).\r\n    * Added new `watchKey` method to register callbacks on a key ([#56217](https://github.com/pytorch/pytorch/pull/56217)).\r\n* `torch.distributed.rpc`\r\n    * Allowed passing `cpu` to CUDA RPC device maps ([#57019](https://github.com/pytorch/pytorch/pull/57019)).\r\n    *  Add a new `devices` argument to TensorPipe options to specify set of devices for TensorPipe ([#56405](https://github.com/pytorch/pytorch/pull/56405))\r\n* `DistributedDataParallel`\r\n    * Adds a flag to ddp `join` context manager that enables throwing an error across all ranks when this flag is specified ([#56755](https://github.com/pytorch/pytorch/pull/56755))\r\n    * Enable static graph training in DDP ([#55248](https://github.com/pytorch/pytorch/pull/55248), [#54995](https://github.com/pytorch/pytorch/pull/54995))\r\n    * Log unused parameter names in DDP when crashing due to unused parameters ([#55075](https://github.com/pytorch/pytorch/pull/55075))\r\n    * Introduce `torch.distributed.algorithms.default_hooks.fp16_compress_wrapper` wrapper that can be combined with other communication hooks ([#53808](https://github.com/pytorch/pytorch/pull/53808))\r\n    * Support loading a non-DP/DDP model from a DP/DDP state_dict ([#53224](https://github.com/pytorch/pytorch/pull/53224))\r\n    * Enhanced logging in DDP for performance metrics ([#52957](https://github.com/pytorch/pytorch/pull/52957), [#53145](https://github.com/pytorch/pytorch/pull/53145), [#54647](https://github.com/pytorch/pytorch/pull/54647))\r\n* `torch.distributed`\r\n    * Support `work.result` API for MPI backend ([#57168](https://github.com/pytorch/pytorch/pull/57168))\r\n    * Support `work.result` for `ProcessGroupGloo::AsyncWork` objects ([#57565](https://github.com/pytorch/pytorch/pull/57565))\r\n    * Support `work.get_future()` API for ProcessGroupMPI and ProcessGroupGloo [(](https://github.com/pytorch/pytorch/pull/57818)[#57818,](https://github.com/pytorch/pytorch/pull/57818)[#57214](https://github.com/pytorch/pytorch/pull/57214))\r\n    * New` torch.distributed.monitored_barrier` API (Gloo-only) ([#53773](https://github.com/pytorch/pytorch/pull/53773), [#53787](https://github.com/pytorch/pytorch/pull/53787), [#55009](https://github.com/pytorch/pytorch/pull/55009), [#55010](https://github.com/pytorch/pytorch/pull/55010), [#55197](https://github.com/pytorch/pytorch/pull/55197), [#55265](https://github.com/pytorch/pytorch/pull/55265), [#55989](https://github.com/pytorch/pytorch/pull/55989), [#55990](https://github.com/pytorch/pytorch/pull/55990))\r\n    * Allow passing `options` field to process group initialization APIs ([#53662](https://github.com/pytorch/pytorch/pull/53662), [#54090](https://github.com/pytorch/pytorch/pull/54090), [#53663](https://github.com/pytorch/pytorch/pull/53663))\r\n    * Enable profiling for distributed collectives ([#51822](https://github.com/pytorch/pytorch/pull/51822), , [#52004](https://github.com/pytorch/pytorch/pull/52004), [#52031](https://github.com/pytorch/pytorch/pull/52031), [#52949](https://github.com/pytorch/pytorch/pull/52949), [#55204](https://github.com/pytorch/pytorch/pull/55204), [#56412](https://github.com/pytorch/pytorch/pull/56412), [#56216](https://github.com/pytorch/pytorch/pull/56216), [#56427](https://github.com/pytorch/pytorch/pull/56427))\r\n    * Allow user to specify `TORCH_DISTRIBUTED_DEBUG `environment variable ([#52481](https://github.com/pytorch/pytorch/pull/52481))\r\n    * Added `compareSet` method for `torch.distributed.{HashStore, FileStore}` ([#53803](https://github.com/pytorch/pytorch/pull/53803)).\r\n* Added new `torch.distributed.elastic `module that upstreams `pytorch/elastic`\r\n    * Introduce RendezvousSettings ([#56537](https://github.com/pytorch/pytorch/pull/56537))\r\n    * Introduce a new from_backend static constructor for DynamicRendezvousHandler ([#57150](https://github.com/pytorch/pytorch/pull/57150))\r\n    * Introduce the implementation of DynamicRendezvousHandler ([#57151](https://github.com/pytorch/pytorch/pull/57151))\r\n    * add support for the new error file format ([#57084](https://github.com/pytorch/pytorch/pull/57084))\r\n    * Introduce the delay utility function ([#56533](https://github.com/pytorch/pytorch/pull/56533))\r\n    *  Make torchelastic launcher compatible with the caffe2.distributed.launch ([#55687](https://github.com/pytorch/pytorch/pull/55687))\r\n    * Introduce `PeriodicTimer` ([#55919](https://github.com/pytorch/pytorch/pull/55919))\r\n    * Introduce `DynamicRendezvousHandler` and `RendezvousBackend`. ([#55635](https://github.com/pytorch/pytorch/pull/55635))\r\n    * Introduce `C10dRendezvousBackend`. ([#55636](https://github.com/pytorch/pytorch/pull/55636))\r\n    * Introduce `EtcdRendezvousBackend`. ([#55637](https://github.com/pytorch/pytorch/pull/55637))\r\n    * Added `torch.distributed.elastic.launchers.api`, `torch.distributed.elastic.metrics`, `torch.distributed.events`, `torch.distributed.rendezvous`, `torch.distributed.elastic.agent` modules ([#55471](https://github.com/pytorch/pytorch/pull/55471), [#53870](https://github.com/pytorch/pytorch/pull/53870), [#53574](https://github.com/pytorch/pytorch/pull/53574), [#53760](https://github.com/pytorch/pytorch/pull/53760), [#53172](https://github.com/pytorch/pytorch/pull/53172), [#54343](https://github.com/pytorch/pytorch/pull/54343))\r\n    * Upstreamed timer and multiprocessing classes to `torch.distribute.elastic.timer` and `torch.distributed.elastic.multiprocessing` ([#53574](https://github.com/pytorch/pytorch/pull/53574))\r\n* `torch.distributed.nn.RemoteModule`: Enable RemoteModule to directly send GPU tensors over the wire on TensorPipe RPC backend if a device map is provided ([#57288](https://github.com/pytorch/pytorch/pull/57288))\r\n* `torch.distributed.optim`: \r\n    * Allow `torch.optim.Adamax`  to be used as a TorchScript functional optimizer in RPC ([#55833](https://github.com/pytorch/pytorch/pull/55833))\r\n    * Allow `torch.optim.Rprop` to be used as a TorchScript functional optimizer in RPC ([#55834](https://github.com/pytorch/pytorch/pull/55834))\r\n\r\n### torch.fx\r\n\r\n* Added `torch.fx.Node.format_node()` ([#51737](https://github.com/pytorch/pytorch/pull/51737)).\r\n* Added a `Transformer` to normalize args/kwargs of `torch.nn.functional` calls into only kwargs ([#51816](https://github.com/pytorch/pytorch/pull/51816)).\r\n* Added submodule manipulation APIs on `GraphModule` ([#52358](https://github.com/pytorch/pytorch/pull/52358)).\r\n* Added `Graph.eliminate_dead_code` ([#52658](https://github.com/pytorch/pytorch/pull/52658)).\r\n* Added a function to retrieve `inspect.Signature` instances for PyTorch operations ([#53830](https://github.com/pytorch/pytorch/pull/53830)).\r\n* Experimental type annotation pass using Python signatures ([#53831](https://github.com/pytorch/pytorch/pull/53831)).\r\n* Added a transformer to normalize `torch` namespace operations ([#53832](https://github.com/pytorch/pytorch/pull/53832)).\r\n* Extended `NormalizeArgs` to work on `torch` namespace operations ([#54236](https://github.com/pytorch/pytorch/pull/54236)).\r\n* Added FX `optimize_for_inference` for Intel CPUs ([#53805](https://github.com/pytorch/pytorch/pull/53805), [#58293](https://github.com/pytorch/pytorch/pull/58293)).\r\n* Added a metadata dict to `Node` and switch shape-prop to use that ([#54926](https://github.com/pytorch/pytorch/pull/54926)).\r\n* Added C-level monkey patching of `torch.randn` to capture it during tracing ([#54060](https://github.com/pytorch/pytorch/pull/54060)).\r\n* Added a new API replace_input_with to `Node` ([#55887](https://github.com/pytorch/pytorch/pull/55887)).\r\n* Added net splitter and net minimizer utilities ([#56201](https://github.com/pytorch/pytorch/pull/56201)).\r\n* Added PyTree support to FX through `concrete_args` ([#55888](https://github.com/pytorch/pytorch/pull/55888)).\r\n* Added support for proxy-able classes ([#56737](https://github.com/pytorch/pytorch/pull/56737)).\r\n\r\n### ONNX\r\n\r\n* Support onnxifi interface for set/get options ([#52388](https://github.com/pytorch/pytorch/pull/52388)).\r\n* Support --onnxifi_min_ops in AOT flow ([#52380](https://github.com/pytorch/pytorch/pull/52380)).\r\n* Redesign onnx pass to enable shape type dependent pattern conversion - cont ([#51795)](https://github.com/pytorch/pytorch/pull/51795) ([#53304)](https://github.com/pytorch/pytorch/pull/53304).\r\n* Support inplace operations on inplace indexing ([#52063)](https://github.com/pytorch/pytorch/pull/52063) ([#53306](https://github.com/pytorch/pytorch/pull/53306)).\r\n* Symbolic shape inference ([#51481](https://github.com/pytorch/pytorch/pull/51481)) ([#53307](https://github.com/pytorch/pytorch/pull/53307)).\r\n* Support repeat_interleave symbolic ([#52855](https://github.com/pytorch/pytorch/pull/52855)) ([#53312](https://github.com/pytorch/pytorch/pull/53312)).\r\n* Support primitive type input/outputs and attributes ([#53550](https://github.com/pytorch/pytorch/pull/53550)) ([#54864](https://github.com/pytorch/pytorch/pull/54864)).\r\n* Support outer export to onnx ([#53603](https://github.com/pytorch/pytorch/pull/53603)) ([#54869](https://github.com/pytorch/pytorch/pull/54869)).\r\n* Support hardsigmoid symbolic in opset 9 #49649 ([#54193](https://github.com/pytorch/pytorch/pull/54193)).\r\n* Support support for hann_window operator ([#54587](https://github.com/pytorch/pytorch/pull/54587)) ([#56163](https://github.com/pytorch/pytorch/pull/56163)).\r\n* Enable tensordot symbolic function ([#55654](https://github.com/pytorch/pytorch/pull/55654)) ([#56166](https://github.com/pytorch/pytorch/pull/56166)).\r\n* Support for prim::min ([#55259](https://github.com/pytorch/pytorch/pull/55259)) ([#56168](https://github.com/pytorch/pytorch/pull/56168)).\r\n* Support mv op ([#55470](https://github.com/pytorch/pytorch/pull/55470)) ([#56169](https://github.com/pytorch/pytorch/pull/56169)).\r\n* Support .item() export & NumberType to tensor conversion ([#55697](https://github.com/pytorch/pytorch/pull/55697)) ([#57594](https://github.com/pytorch/pytorch/pull/57594)).\r\n* Support a new operator for fill_() function ([#56859](https://github.com/pytorch/pytorch/pull/56859)) ([#57596](https://github.com/pytorch/pytorch/pull/57596)).\r\n* Support index_add_ function ([#56867](https://github.com/pytorch/pytorch/pull/56867)) ([#57830](https://github.com/pytorch/pytorch/pull/57830)).\r\n* Support tensor.to(device) ([#56857](https://github.com/pytorch/pytorch/pull/56857)) ([#57599](https://github.com/pytorch/pytorch/pull/57599)).\r\n* Support registering custom export for prim::PythonOp from torch.autograd.Function ([#55630](https://github.com/pytorch/pytorch/pull/55630)) ([#57600](https://github.com/pytorch/pytorch/pull/57600)).\r\n\r\n### Vulkan\r\n\r\n* Added the `hardswish` and `hardsigmoid` activation functions ([#53362](https://github.com/pytorch/pytorch/pull/53362)).\r\n* Added the `reflection_pad2d` op ([#53604](https://github.com/pytorch/pytorch/pull/53604)).\r\n* Added an implementation of Winograd convolutions ([#54639](https://github.com/pytorch/pytorch/pull/54639)).\r\n* Added the `sigmoid` activation function ([#57867](https://github.com/pytorch/pytorch/pull/57867)).\r\n\r\n### Misc\r\n\r\n* Android packages are now published to maven central ([#53568](https://github.com/pytorch/pytorch/pull/53568)).\r\n* Kineto is now supported on Windows ([#56323](https://github.com/pytorch/pytorch/pull/56323)).\r\n* Added a Gloo `TCP_TLS `transport ([#56442](https://github.com/pytorch/pytorch/pull/56442)).\r\n* Add ability to collect minidumps after the crash ([#59236](https://github.com/pytorch/pytorch/pull/59236)).\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Added nondeterministic alert for `index_put_` when `accumulate=False` ([#55827](https://github.com/pytorch/pytorch/pull/55827)).\r\n* Added deterministic path for `torch.index_add` on CUDA ([#56521](https://github.com/pytorch/pytorch/pull/56521)).\r\n* Added deterministic path for `torch.index_copy` on CPU ([#56900](https://github.com/pytorch/pytorch/pull/56900)).\r\n* Removed beta warning for use_deterministic_algorithms ([#58074](https://github.com/pytorch/pytorch/pull/58074))\r\n* Updated `torch.Tensor.unflatten` to be able to infer size value in `sizes` from -1 ([#51955](https://github.com/pytorch/pytorch/pull/51955)).\r\n* Added a safe cast and copy for `out=` input tensor for `torch.tensordot` ([#56286](https://github.com/pytorch/pytorch/pull/56286)).\r\n* Added cross-device check for `out` and `input` tensors for `torch.cat` ([#53004](https://github.com/pytorch/pytorch/pull/53004)).\r\n* Modified the order of asserts to correct the error message when nan appears in `torch.multinomial` on CUDA ([#53288](https://github.com/pytorch/pytorch/pull/53288)).\r\n* Converted a few more checks for unsupported device to raise `NotImplementedError` ([#53610](https://github.com/pytorch/pytorch/pull/53610)).\r\n* Made shared cache thread-safe for `torch.multiprocessing` ([#53750](https://github.com/pytorch/pytorch/pull/53750)).\r\n* Added support for `torch.int32` indices in `torch.repeat_interleave` ([#55102](https://github.com/pytorch/pytorch/pull/55102)).\r\n* Added a check to give a clear error message when a binary function is called for  non-complex inputs with complex valued alpha ([#54964](https://github.com/pytorch/pytorch/pull/54964)).\r\n* Propagate error message from `torch_shm_manager` when running `torch.multiprocessing` ([#57307](https://github.com/pytorch/pytorch/pull/57307), [#57310](https://github.com/pytorch/pytorch/pull/57310)).\r\n* Enabled deterministic path for `index_copy_cud`a with index_put ([#58144](https://github.com/pytorch/pytorch/pull/58144)).\r\n* Added support for uppercase letters in `torch.einsum` ([#56475](https://github.com/pytorch/pytorch/pull/56475)).\r\n* Added CUDA support for `torch.orgqr` ([#51348](https://github.com/pytorch/pytorch/pull/51348)) and  `torch.ormqr` ([#57316](https://github.com/pytorch/pytorch/pull/57316)).\r\n* Added support for batched as well as complex inputs for `torch.geqrf` on both CPU and CUDA ([#56249](https://github.com/pytorch/pytorch/pull/56249), [#56251](https://github.com/pytorch/pytorch/pull/56251)).\r\n\r\n### Complex Numbers\r\n\r\n* Fixed `torch.{linspace, logspace}` to correctly infer complex type and return a complex tensor when the `start` and (or) `end` values are complex numbers, and the `dtype` value is `None`  ([#38875](https://github.com/pytorch/pytorch/pull/38875)).\r\n\r\n### Autograd\r\n\r\n* Added support for single tensor in `inputs` argument for `.backward()` ([#53827](https://github.com/pytorch/pytorch/pull/53827)).\r\n* Added support for C++ optional arguments in autograd custom functions ([#54270](https://github.com/pytorch/pytorch/pull/54270)).\r\n* Added autograd support to `torch.orgqr` ([#52637](https://github.com/pytorch/pytorch/pull/52637)), `torch.segment_reduce` ([#56792](https://github.com/pytorch/pytorch/pull/56792)).\r\n* Added deterministic backward for `torch.gather` for `dim=1` ([#55573](https://github.com/pytorch/pytorch/pull/55573)).\r\n* Make detach return an alias even under inference mode ([#59633](https://github.com/pytorch/pytorch/pull/59633)).\r\n\r\n### torch.nn\r\n\r\n* Add 3D depthwise separable convolution ([#51027](https://github.com/pytorch/pytorch/pull/51027))\r\n* Make bias in lazy modules lazy and avoid creating empty tensors ([#52212](https://github.com/pytorch/pytorch/pull/52212)).\r\n* BFloat16: enable prepacked weights's inference ([#48922](https://github.com/pytorch/pytorch/pull/48922)).\r\n* Enable mkldnn conv2d backward to support mkldnn tensor input ([#48994](https://github.com/pytorch/pytorch/pull/48994)).\r\n* Add OneDNN pooling backward ([#49454](https://github.com/pytorch/pytorch/pull/49454)).\r\n* Add 64bit indexing support for softmax ([#52713](https://github.com/pytorch/pytorch/pull/52713)).\r\n* `nn.init._calculate_fan_in_and_fan_out`: Support usage with `__torch_function__` ([#53522](https://github.com/pytorch/pytorch/pull/53522)).\r\n* `nn.Transformer` / `nn.MultiheadAttention`: Add `batch_first` argument ([#55285](https://github.com/pytorch/pytorch/pull/55285)).\r\n* `nn.Transformer`: Add `layer_norm_eps` arg ([#54494](https://github.com/pytorch/pytorch/pull/54494)).\r\n* `nn.AvgPool2d`: Add channels_last support on CPU ([#48918](https://github.com/pytorch/pytorch/pull/48918)).\r\n* `clip_grad_norm_`: Add `error_if_nonfinite` flag ([#53843](https://github.com/pytorch/pytorch/pull/53843), [#55169](https://github.com/pytorch/pytorch/pull/55169)).\r\n* `Module.train`: Raise nicer error when called with invalid modes ([#58247](https://github.com/pytorch/pytorch/pull/58247)).\r\n* `nn.Linear`: Support 0 `in_features` ([#56505](https://github.com/pytorch/pytorch/pull/56505)).\r\n* `nn.EmbeddingBag`: Support mix of int32 and int64 offsets/indices ([#55189](https://github.com/pytorch/pytorch/pull/55189)).\r\n* `xnnpack::linear`: Handle 1D input ([#54986](https://github.com/pytorch/pytorch/pull/54986)).\r\n* `nn.Module`: Add `allow_duplicate` flag to `named_modules()` ([#54812](https://github.com/pytorch/pytorch/pull/54812)).\r\n* `nn.Module`: Add `to_empty()` function for moving to a device without copying storage ([#56610](https://github.com/pytorch/pytorch/pull/56610)).\r\n* Make `pad_sequence` callable from C++ API ([#57868](https://github.com/pytorch/pytorch/pull/57868)).\r\n\r\n### Dataloader\r\n\r\n* Added `generate_state` for NumPy seeding ([#56797](https://github.com/pytorch/pytorch/pull/56797)).\r\n* Modified construct_time_validation to argument_validation ([#55836](https://github.com/pytorch/pytorch/pull/55836)).\r\n* Added mode to `LoadFilesFromDisk` ([#57056](https://github.com/pytorch/pytorch/pull/57056)).\r\n* Added the ability to override *reduce_ex* function of `DataPipe` ([#52858](https://github.com/pytorch/pytorch/pull/52858)).\r\n* Added lambda support to `MapIterDataPipe` ([#52856](https://github.com/pytorch/pytorch/pull/52856)).\r\n* Added functional way of stacking DataPipes ([#52885](https://github.com/pytorch/pytorch/pull/52885)).\r\n\r\n### C++ API\r\n\r\n* Suppressed unsigned comparison warning ([#52653](https://github.com/pytorch/pytorch/pull/52653)).\r\n* Fixed constexpr **host** warning ([#52702](https://github.com/pytorch/pytorch/pull/52702)).\r\n* Introduced a fluent API to construct tensors from external data ([#54530](https://github.com/pytorch/pytorch/pull/54530)).\r\n\r\n### AMD\r\n\r\n* Allow PYTORCH_ROCM_ARCH in cpp_extension ([#54341](https://github.com/pytorch/pytorch/pull/54341)).\r\n* Added support for `torch.half` dtype RNNs with MIOpen ([#52475](https://github.com/pytorch/pytorch/pull/52475)).\r\n* Added support for the new `hiprtc` precompiler feature ([#54350](https://github.com/pytorch/pytorch/pull/54350)).\r\n* Improved reliability of `hipfft` and `rocfft` detection for ROCm build ([#53408](https://github.com/pytorch/pytorch/pull/53408)).\r\n\r\n### CUDA\r\n\r\n* Improved warning message when old GPU is detected ([#56621](https://github.com/pytorch/pytorch/pull/56621))\r\n* Made `torch.cuda.amp.GradScaler` scale updates in-place for better composability with graph capture ([#55562](https://github.com/pytorch/pytorch/pull/55562)).\r\n* Add `USE_MAGMA` build flag ([#55994](https://github.com/pytorch/pytorch/pull/55994)).\r\n* Change link order for BUILD_SPLIT_CUDA option ([#58437](https://github.com/pytorch/pytorch/pull/58437)).\r\n* Improve CUDA-11.X binary builds ([#58459](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F58459&h=AT1iY5lImKBK5tsZFPG9Ub57qOFMix4DslZFPlHNwT13OJnRq6Tvh_HehGQ-k4GF2bUDNhIQHBS578V8RQ-Sk2YYUv4Cys6KDIZPsunP7HzrwcYtfEZnPczt41cqT0KEIuvTSa1ZiOZ4SvvQV9F2xiYZ4cCJ7WIl2URyvg)).\r\n* Move CUDA async warning to suffix ([#59467](https://github.com/pytorch/pytorch/pull/59467)).\r\n\r\n### torch.fx\r\n\r\n* Make `torch.fx.map_arg` require a callable ([#51907](https://github.com/pytorch/pytorch/pull/51907)).\r\n* Generalize dict key check in `torch.fx.Tracer.create_arg` ([#51927](https://github.com/pytorch/pytorch/pull/51927)).\r\n* Customize traceback for calls to symbolically-traced code ([#51648](https://github.com/pytorch/pytorch/pull/51648)).\r\n* Allow `Transformer` to accept output result that is not Proxy ([#52473](https://github.com/pytorch/pytorch/pull/52473)).\r\n* Make `TracerBase._find_user_frame` private ([#53654](https://github.com/pytorch/pytorch/pull/53654)).\r\n* Improve buffer registration during `GraphModule` init ([#53444](https://github.com/pytorch/pytorch/pull/53444)).\r\n* Garbage collect values in `Interpreter` ([#54726](https://github.com/pytorch/pytorch/pull/54726)).\r\n* Improve placeholder matching in subgraph rewriter ([#54958](https://github.com/pytorch/pytorch/pull/54958)).\r\n* Record `stride` on `Node` during `ShapeProp` pass ([#55108](https://github.com/pytorch/pytorch/pull/55108)).\r\n* Record `memory_format` on `Node` during `ShapeProp` pass ([#55815](https://github.com/pytorch/pytorch/pull/55815)).\r\n* Put tensor metadata into a `NamedTuple` in `ShapeProp` ([#55930](https://github.com/pytorch/pytorch/pull/55930)).\r\n* Preserve node meta info in `split_module` ([#56212](https://github.com/pytorch/pytorch/pull/56212)).\r\n* Make `shape_prop` handle targets with aggregate outputs ([#56221](https://github.com/pytorch/pytorch/pull/56221)).\r\n* Make arg normalization a method on `Node` and not a pass (also augment tests to be exhaustive) ([#55992](https://github.com/pytorch/pytorch/pull/55992)).\r\n* Allow for args to be left as args in NormalizeArgs ([#55995](https://github.com/pytorch/pytorch/pull/55995)).\r\n* Maintain submodule references during subgraph rewriting ([#55463](https://github.com/pytorch/pytorch/pull/55463)).\r\n* Changes in order to move `PythonKey` out of tree ([#57427](https://github.com/pytorch/pytorch/pull/57427)).\r\n* Handle cases in `GraphDrawer` when shape, type or stride are not present ([#57845](https://github.com/pytorch/pytorch/pull/57845)).\r\n* Handle the case when output consumes `get_attr` directly in `split_by_tags` ([#57844](https://github.com/pytorch/pytorch/pull/57844)).\r\n* Let submodules be collected as `args/kwargs`  in symbolic tracing([#57840](https://github.com/pytorch/pytorch/pull/57840)).\r\n\r\n### Profiler\r\n\r\n* Expanded Kineto platform support ([#56323](https://github.com/pytorch/pytorch/pull/56323)).\r\n* Added profiler fallback ([#57612](https://github.com/pytorch/pytorch/pull/57612)).\r\n* Added CUDA event fallback ([#58133](https://github.com/pytorch/pytorch/pull/58133)).\r\n\r\n### TorchScript\r\n\r\n* Added a flag to enable CPU fusion in benchmarks ([#48612](https://github.com/pytorch/pytorch/pull/48612)).\r\n* Updated fusion to handle loops that have the same bounds as expressions ([#55997](https://github.com/pytorch/pytorch/pull/55997)).\r\n* Updated normalization transformation to be in-place ([#56158](https://github.com/pytorch/pytorch/pull/56158)).\r\n* Added check to only lower float `conv2d`s ([#56289](https://github.com/pytorch/pytorch/pull/56289)).\r\n* Added more python bindings for loopnest ([#56213](https://github.com/pytorch/pytorch/pull/56213)).\r\n* Updated `fuseLoops` API to return bool flag and not throw any exceptions ([#56353](https://github.com/pytorch/pytorch/pull/56353)).\r\n* Added `unroll` and `flatten` APIs which do not require return stmt pointer ([#56420](https://github.com/pytorch/pytorch/pull/56420)).\r\n* Updated `Buf` on mutation instead of creating a new one ([#57513](https://github.com/pytorch/pytorch/pull/57513)).\r\n* Updated `flatten` transformation to be in-place ([#56629](https://github.com/pytorch/pytorch/pull/56629)).\r\n* Added missing python bindings for NNC Stmts ([#55570](https://github.com/pytorch/pytorch/pull/55570)).\r\n* Allowed backend preprocessing to take place outside of the backend interface ([#51757](https://github.com/pytorch/pytorch/pull/51757))\r\n* Added an error message for the case when `with` item is not an object ([#52335](https://github.com/pytorch/pytorch/pull/52335)).\r\n* Enabled `ModuleList` non-literal indexing ([#53410](https://github.com/pytorch/pytorch/pull/53410)).\r\n* Added recursive scripting for class type module attributes ([#55124](https://github.com/pytorch/pytorch/pull/55124)).\r\n* Added support for `mypy` ignore annotation with particular rule specified ([#51675](https://github.com/pytorch/pytorch/pull/51675)).\r\n* Added support for comparing two bool variables ([#51844](https://github.com/pytorch/pytorch/pull/51844)).\r\n* Added MKLDNN GELU function ([#53615](https://github.com/pytorch/pytorch/pull/53615)).\r\n* Added `hardtanh(0,6)` to the set of MKLDNN fusible ops for mobilenetv2 ([#56203](https://github.com/pytorch/pytorch/pull/56203)).\r\n* Captured argument names for traced functions and modules ([#51775](https://github.com/pytorch/pytorch/pull/51775)).\r\n* Improved `has_bf16_support` ([#57408](https://github.com/pytorch/pytorch/pull/57408)).\r\n* Walk Python AST to check for unsupported attribute type annotations ([#51805](https://github.com/pytorch/pytorch/pull/51805)).\r\n* Added `out` version for sum ([#52225](https://github.com/pytorch/pytorch/pull/52225))\r\n* Added logic to trace `torch.nn.Linear` as `aten::linear` ([#51897](https://github.com/pytorch/pytorch/pull/51897)).\r\n* Made `is_tracing` scriptable ([#49853](https://github.com/pytorch/pytorch/pull/49853)).\r\n* Added support for builtin `sum` ([#52188](https://github.com/pytorch/pytorch/pull/52188)).\r\n* Fused `clip_ranges` and `gather_ranges` ([#52461](https://github.com/pytorch/pytorch/pull/52461)).\r\n* Added support for features from `to_backend` for the Lite Interpreter ([#52870](https://github.com/pytorch/pytorch/pull/52870)).\r\n* Added a filter to remove mutation ([#51923](https://github.com/pytorch/pytorch/pull/51923)).\r\n* Added logic functionalize ops which to be included in MKLDNN group ([#51924](https://github.com/pytorch/pytorch/pull/51924))\r\n* Extended subgraph utils to cover merging a node following a subgraph ([#52513](https://github.com/pytorch/pytorch/pull/52513))\r\n* Included max pool in fusion groups ([#52613](https://github.com/pytorch/pytorch/pull/52613)).\r\n* Registered both TupleConstruct and ListConstruct as out variants ([#52684](https://github.com/pytorch/pytorch/pull/52684)).\r\n* Added Alias analysis to Memory Management/Planning ([#50060](https://github.com/pytorch/pytorch/pull/50060)).\r\n* Included max pool in fusion groups ([#52613](https://github.com/pytorch/pytorch/pull/52613)).\r\n* Added property binding in TorchBind ([#50670](https://github.com/pytorch/pytorch/pull/50670)).\r\n* Registered `pow` out variant ([#52454](https://github.com/pytorch/pytorch/pull/52454)).\r\n* Made `torch.load()` aware of import path changes ([#53139](https://github.com/pytorch/pytorch/pull/53139)).\r\n* Added `aten::to` copy out variant ([#52343](https://github.com/pytorch/pytorch/pull/52343)).\r\n* Added more variants to `create_empty_from` ([#53333](https://github.com/pytorch/pytorch/pull/53333)).\r\n* Added support for parsing Ellipsis in JIT frontend ([#53576](https://github.com/pytorch/pytorch/pull/53576)).\r\n* Added a bool `is_available()` method to the backend contract ([#53068](https://github.com/pytorch/pytorch/pull/53068)).\r\n* Added parallel support for the LLVM backend. ([#53243](https://github.com/pytorch/pytorch/pull/53243)) / Resubmit: Add parallel support for the LLVM backend. ([#54122](https://github.com/pytorch/pytorch/pull/54122)).\r\n* Rewrote `functional.tensordot` to be TorchScript-able ([#53672](https://github.com/pytorch/pytorch/pull/53672)).\r\n* Added python bindings for missing loop transformations in `LoopNest` ([#54355](https://github.com/pytorch/pytorch/pull/54355)).\r\n* Added support for list insertion for mutation removal ([#54271](https://github.com/pytorch/pytorch/pull/54271)).\r\n* Added support for  `torch.bfloat16` in the fuser ([#54571](https://github.com/pytorch/pytorch/pull/54571)).\r\n* Added some functions for manipulating MKLDNN tensors to TORCH_API ([#56954](https://github.com/pytorch/pytorch/pull/56954)).\r\n* Merged CUDA Streams and Events ([#53902](https://github.com/pytorch/pytorch/pull/53902)).\r\n* Added python bindings for `TensorExprKernel` ([#54450](https://github.com/pytorch/pytorch/pull/54450)).\r\n* Added support for dtype-specific tensor subclasses (e.g. LongTensor) ([#54817](https://github.com/pytorch/pytorch/pull/54817)).\r\n* Added support for tuple `add` operator ([#52292](https://github.com/pytorch/pytorch/pull/52292)).\r\n* Disambiguated error message for working with not fully refined tuple types ([#55745](https://github.com/pytorch/pytorch/pull/55745)).\r\n* Allowed unpacking tuple and assigning unpacked values to SELECT-type expressions ([#55268](https://github.com/pytorch/pytorch/pull/55268)).\r\n* Made NoneType `annotation_str` emit `NoneType` instead of `None` ([#54746](https://github.com/pytorch/pytorch/pull/54746)).\r\n* Added CUDA device synchronization support in JIT ([#55469](https://github.com/pytorch/pytorch/pull/55469)).\r\n* Added `optimize_graph_output_memory` flag ([#55811](https://github.com/pytorch/pytorch/pull/55811)).\r\n* Added support for refinement for `torch.jit.Future` ([#56148](https://github.com/pytorch/pytorch/pull/56148)).\r\n* Added implicit conversion from null tensor to `NoneType `([#55823](https://github.com/pytorch/pytorch/pull/55823)).\r\n* Added `aten::matmul`s to TE fuser ([#54605](https://github.com/pytorch/pytorch/pull/54605)).\r\n* Put explicit error message on class attribute accesses ([#55723](https://github.com/pytorch/pytorch/pull/55723)).\r\n* Added support for constant tensors in tensorexpr kernel ([#56319](https://github.com/pytorch/pytorch/pull/56319)).\r\n* Added native support for `aten::getitem` ([#55310](https://github.com/pytorch/pytorch/pull/55310)).\r\n* Added stricter check for function schemas with varargs ([#56509](https://github.com/pytorch/pytorch/pull/56509)).\r\n* Added graceful failure handling of DataPtr extraction in CUDAFuture ([#56511](https://github.com/pytorch/pytorch/pull/56511)).\r\n* Enabled forward/backward compatibility in TS mobile ([#56079](https://github.com/pytorch/pytorch/pull/56079)).\r\n* Added binding for `aten::clamp_min_out` ([#56635](https://github.com/pytorch/pytorch/pull/56635)), `aten::argmin_out` ([#56638](https://github.com/pytorch/pytorch/pull/56638)), and `aten::norm_out` ([#56636](https://github.com/pytorch/pytorch/pull/56636)).\r\n* Enhanced error message for `Future.setErrorIfNeeded` ([#56631](https://github.com/pytorch/pytorch/pull/56631)).\r\n* Added type inference support for `nn.Module `methods using PDT ([#57165](https://github.com/pytorch/pytorch/pull/57165)).\r\n* Disabled conv-add-relu fusion for cuDNN7 when model uses `torch.float16` ([#56579](https://github.com/pytorch/pytorch/pull/56579)).\r\n* Enabled conv-add-relu fusion as a part of frozen graph optimization ([#56580](https://github.com/pytorch/pytorch/pull/56580)).\r\n* Reduced inline autodiff threshold to enable the capture of smaller fusions ([#57062](https://github.com/pytorch/pytorch/pull/57062)).\r\n* Added static runtime support for `aten::matmul` ([#57291](https://github.com/pytorch/pytorch/pull/57291)).\r\n* Added `device()` method to `c10::Event` ([#57293](https://github.com/pytorch/pytorch/pull/57293)).\r\n* Added support for normalization of `is` op ([#57862](https://github.com/pytorch/pytorch/pull/57862)).\r\n* Enabled `cat` without conditionals iff CPU ([#58026](https://github.com/pytorch/pytorch/pull/58026)).\r\n* Added `LowerSimpleTuples` for freeze tuples ([#57915](https://github.com/pytorch/pytorch/pull/57915)).\r\n* Added support for striding for list slicing ([#49352](https://github.com/pytorch/pytorch/pull/49352)).\r\n* Wrapped `torch::deploy` API functions in safe rethrow macros ([#58192](https://github.com/pytorch/pytorch/pull/58192)).\r\n* Added binding for `aten::div_out` ([#56653](https://github.com/pytorch/pytorch/pull/56653))\r\n* Added binding for `aten::sub_out` ([#56656](https://github.com/pytorch/pytorch/pull/56656)).\r\n* Supported `clamp.Tensor `([#58191](https://github.com/pytorch/pytorch/pull/58191)).\r\n* Added an out version for `aten::repeat` ([#57683](https://github.com/pytorch/pytorch/pull/57683)).\r\n* Added default arguments to CUDA stream and events ([#53025](https://github.com/pytorch/pytorch/pull/53025)).\r\n* Added support for linear in MKLDNN fusion ([#51484](https://github.com/pytorch/pytorch/pull/51484)).\r\n* Handled MKLDNN broadcasting in MKLDNN fuser ([#51736](https://github.com/pytorch/pytorch/pull/51736)).\r\n* Added 0-dim support for binary MKLDNN ops ([#51921](https://github.com/pytorch/pytorch/pull/51921)).\r\n* Added OneDNN relu backward and reshape backward ([#49455](https://github.com/pytorch/pytorch/pull/49455)).\r\n* Added OneDNN batch_norm backward ([#50460](https://github.com/pytorch/pytorch/pull/50460)).\r\n* Added support for `hardshrink` ([#57749](https://github.com/pytorch/pytorch/pull/57749)).\r\n* Added non mutator bundled inputs method ([#58408](https://github.com/pytorch/pytorch/pull/58408)).\r\n* Added support to compare devices ([#53045](https://github.com/pytorch/pytorch/pull/53045)).\r\n* Added support for `memory_arg` in `aten::clone` ([#58100](https://github.com/pytorch/pytorch/pull/58100)).\r\n* Implemented `aten::cat` without conditionals ([#53128](https://github.com/pytorch/pytorch/pull/53128)).\r\n* Added external function bindings ([#53420](https://github.com/pytorch/pytorch/pull/53420)).\r\n* Added out variant of `sigrid_transforms_torch_bind` and `ListUnpack` ([#54761](https://github.com/pytorch/pytorch/pull/54761)).\r\n\r\n### torch.package\r\n\r\n* Added a reliable method for determining if a file is part of Python\u2019s standard library  ([#51694](https://github.com/pytorch/pytorch/pull/51694)).\r\n* Made package code more composable with other parts of PyTorch (package GraphModule, load non-code files from package) ([#51674](https://github.com/pytorch/pytorch/pull/51674), [#51976](https://github.com/pytorch/pytorch/pull/51976)).\r\n* Improved debugging facilities (allow_empty flag, zip file viewer, deny instruction, dependency tracing, query if object is from a package)  ([#53232,](https://github.com/pytorch/pytorch/pull/53232)[#53233](https://github.com/pytorch/pytorch/pull/53233), [#52176](https://github.com/pytorch/pytorch/pull/52176), [#55167](https://github.com/pytorch/pytorch/pull/55167), [#56190](https://github.com/pytorch/pytorch/pull/56190), [#56238](https://github.com/pytorch/pytorch/pull/56238), [#56729](https://github.com/pytorch/pytorch/pull/56729)).\r\n* Allow save_module to accept module as arg ([#55996](https://github.com/pytorch/pytorch/pull/55996)).\r\n* Follow dependencies created by `__import__` calls ([#55153](https://github.com/pytorch/pytorch/pull/55153)).\r\n* Added hooks to exporters\u2019 mock and extern calls to take action when a module is matched ([#58000](https://github.com/pytorch/pytorch/pull/58000))\r\n* Turn the default behavior of packaging into an \u2018intern\u2019 action so that it can be ordered with repeat to mock, extern, and deny actions ([#57341](https://github.com/pytorch/pytorch/pull/57341)).\r\n\r\n### Quantization\r\n\r\n* Added support for keeping output quantized for list and dict ([#56391](https://github.com/pytorch/pytorch/pull/56391)).\r\n* Added `torch.float16` and `torch.float64` support to `fake_quantize_per_channel` ([#56894](https://github.com/pytorch/pytorch/pull/56894)).\r\n* Support preserving attributes in deepcopy of observed/quantized graphmodule ([#56550](https://github.com/pytorch/pytorch/pull/56550)).\r\n* Added support for packed params in state_dict ([#51639](https://github.com/pytorch/pytorch/pull/51639)).\r\n* Added support for fusing `Conv3d + BatchNorm3d + ReLU` operations ([#50003](https://github.com/pytorch/pytorch/pull/50003)).\r\n* Change back to `multiple_outputs_gpu_kernel` for learnable fake per-channel quantization ([#52017](https://github.com/pytorch/pytorch/pull/52017)).\r\n* Added `torch.float16` and `torch.float32` support to `fake_quantize_per_tensor` ([#52612](https://github.com/pytorch/pytorch/pull/52612)).\r\n* Support batched embeddings for 8 Bit embedding bag quantization ([#55343](https://github.com/pytorch/pytorch/pull/55343)).\r\n* Expose nbins and ratio for `quantized::embedding_bag_4bit_prepack` ([#50398](https://github.com/pytorch/pytorch/pull/50398)).\r\n\r\n### Mobile\r\n\r\n* Removed caching of inflated bundled inputs ([#55181](https://github.com/pytorch/pytorch/pull/55181)).\r\n* Improved exception reporting for Lite interpreter ([#54284](https://github.com/pytorch/pytorch/pull/54284), [#55062](https://github.com/pytorch/pytorch/pull/55062), [#55252](https://github.com/pytorch/pytorch/pull/55252)).\r\n* Improved forward/backward compatibility in Lite interpreter when adding new optional arguments to ops ([#56845](https://github.com/pytorch/pytorch/pull/56845)).\r\n* Added model size to logged metadata when loading a Lite interpreter model ([#53578](https://github.com/pytorch/pytorch/pull/53578)).\r\n* Benchmarking binary speed_benchmark_torch now supports Lite interpreter ([#55402](https://github.com/pytorch/pytorch/pull/55402)).\r\n\r\n### Distributed\r\n\r\n`torch.distributed.Store`\r\n\r\n* Update `compare_set` for other Store implementations to be the same as `TCPStore`. ([#57175](https://github.com/pytorch/pytorch/pull/57175))\r\n* `torch.distributed.Store`: Expose C++ `compare_set` API to python. ([#57191](https://github.com/pytorch/pytorch/pull/57191))\r\n* `torch.distributed.Store`: Add `timeout`, `host`, `port` to TCPStore\u2019s python API as accessors. ([#52784](https://github.com/pytorch/pytorch/pull/52784))\r\n* Allow `world_size` and `is_master` to be optional when constructing TCPStore. ([#51809](https://github.com/pytorch/pytorch/pull/51809))\r\n* Add `wait_for_worker` param to `TCPStore`\u2019s Python API([#52888](https://github.com/pytorch/pytorch/pull/52888))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Allow `RRef` to be created with a specified set of CUDA devices ([#57085](https://github.com/pytorch/pytorch/pull/57085))\r\n* Correctness fixes for CUDA support in RPC framework ([#54024](https://github.com/pytorch/pytorch/pull/54024), )\r\n* Refactor RPC agent to use `Store` to collect and verify name ([#53209](https://github.com/pytorch/pytorch/pull/53209), [#53202](https://github.com/pytorch/pytorch/pull/53202))\r\n\r\n`DistributedDataParallel`\r\n\r\n* Make unused parameter search show up in profiler output ([#57376](https://github.com/pytorch/pytorch/pull/57376))\r\n* Update DDP communication hooks to divide by world size before all_reduce to avoid overflow ([#57410](https://github.com/pytorch/pytorch/pull/57410))\r\n* Stabilize `torch.distributed.GradBucket` interface for gradient compression ([#53010](https://github.com/pytorch/pytorch/pull/53010), [#53098](https://github.com/pytorch/pytorch/pull/53098), [#53102](https://github.com/pytorch/pytorch/pull/53102), [#53009](https://github.com/pytorch/pytorch/pull/53009), [#53099](https://github.com/pytorch/pytorch/pull/53099))\r\n* Skip CPU to GPU input copy if input is already on the right device. ([#55624](https://github.com/pytorch/pytorch/pull/55624))\r\n* Record forward pass of `DistributedDataParallel` and `DataParallel` in profiler.([#55578](https://github.com/pytorch/pytorch/pull/55578))\r\n*  Make `orthogonalization_epsilon` flag configurable in `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState` ([#55738](https://github.com/pytorch/pytorch/pull/55738))\r\n* Set default value of `start_powerSGD_iter` to 1K iterations in \r\n    `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook. `([#55272](https://github.com/pytorch/pytorch/pull/55272))\r\n* Add a minimum compression rate threshold parameter for `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook`   ([#52541](https://github.com/pytorch/pytorch/pull/52541))\r\n* Report compression rate for batched PowerSGD hook ([#55103](https://github.com/pytorch/pytorch/pull/55103))\r\n* Enable gradient compression hook testing on ROCm ([#52403](https://github.com/pytorch/pytorch/pull/52403))\r\n* Enhance warning for unused parameters in `DistributedDataParallel`. ([#52385](https://github.com/pytorch/pytorch/pull/52385))\r\n* Enhance error messages when crashing with unused parameters in `DistributedDataParallel`. ([#52391](https://github.com/pytorch/pytorch/pull/52391))\r\n\r\n`torch.distributed`\r\n\r\n* Add rank information on NCCL communicator abort ([#57974](https://github.com/pytorch/pytorch/pull/57974))\r\n* Enhance exception logging in NCCL ([#54557](https://github.com/pytorch/pytorch/pull/54557), [#54558](https://github.com/pytorch/pytorch/pull/54558), [#54117](https://github.com/pytorch/pytorch/pull/54117))\r\n\r\n`torch.distributed.nn.RemoteModule`\r\n\r\n* Create a separate remote module template when moving CPU tensors to a cuda device is not enabled ([#57413](https://github.com/pytorch/pytorch/pull/57413))\r\n* Allow passing `RemoteModule` as an argument over RPC ([#57695](https://github.com/pytorch/pytorch/pull/57695), [#58345](https://github.com/pytorch/pytorch/pull/58345))\r\n* Support async instantiation of RemoteModule ([#58052](https://github.com/pytorch/pytorch/pull/58052))\r\n* Place inputs on the appropriate devices in `RemoteModule` ([#56943](https://github.com/pytorch/pytorch/pull/56943))\r\n\r\n`torch.futures.Future`\r\n\r\n* Enable `torch.futures.Future` to be created with CUDA support ([#56517](https://github.com/pytorch/pytorch/pull/56517)) \r\n* `torch.futures`: Improve error propagation when using `then` API ([#54475](https://github.com/pytorch/pytorch/pull/54475))\r\n\r\n`torch.nn.SyncBatchNorm`\r\n\r\n* Migrate `apex.parallel.SyncBatchNorm` `channels_last` to PyTorch implementation ([#46906](https://github.com/pytorch/pytorch/pull/46906))\r\n* Fix `SyncBatchNorm`\u2019s forward pass to handle optional weight ([#54568](https://github.com/pytorch/pytorch/pull/54568))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* `torch.distributed.pipeline`: Merge pipeline partitions that are on the same device. ([#55973](https://github.com/pytorch/pytorch/pull/55973))\r\n\r\nAdded new `torch.distributed.elastic `module that upstreams `pytorch/elastic`\r\n\r\n* Rename `torch.distributed.elastic_launch` to `torch.distributed.run` ([#56831](https://github.com/pytorch/pytorch/pull/56831))\r\n* make process failure init error non-fatal ([#56739](https://github.com/pytorch/pytorch/pull/56739))\r\n* Reorder type definitions in dynamic_rendezvous.py ([#56534](https://github.com/pytorch/pytorch/pull/56534))\r\n* Revise the rendezvous handler registry logic. ([#55466](https://github.com/pytorch/pytorch/pull/55466))\r\n* Set error code in reply file when child process is terminated by signals. (f665a7f8a1)\r\n* Make sure torchelastic mp wait for queue to be drained before finishing the process ([#55412](https://github.com/pytorch/pytorch/pull/55412))\r\n* Revise the rendezvous exception types. ([#54803](https://github.com/pytorch/pytorch/pull/54803))\r\n* Expose a `stderr` parameter in `EtcdServer`. ([#54805](https://github.com/pytorch/pytorch/pull/54805))\r\n* Improve the implementation of the utility functions and add their unit tests. ([#54804](https://github.com/pytorch/pytorch/pull/54804))\r\n* Improve the implementation of `RendezvousParameters` and add its unit tests. ([#54807](https://github.com/pytorch/pytorch/pull/54807))\r\n\r\n`torch.distributed.optim.ZeroRedundancyOptimizer`\r\n\r\n* Add an option for buckets to be views of tensors and consolidate public interface ([#52987](https://github.com/pytorch/pytorch/pull/52987))\r\n* Make state dict for ZeroRedundancyOptimizer world size independent ([#52960](https://github.com/pytorch/pytorch/pull/52960))\r\n\r\nCombine backtrace print into one string to avoid interleaving ([#56961](https://github.com/pytorch/pytorch/pull/56961)).\r\nRaise exception rather than crash if GLOO_DEVICE_TRANSPORT is set to unknown value ([#58518](https://github.com/pytorch/pytorch/issues/58518)).\r\n\r\n### ONNX\r\n\r\n* Updated fuseLogSoftmaxNllLoss function to handle autocasting ([#51729](https://github.com/pytorch/pytorch/pull/51729)) ([#52349](https://github.com/pytorch/pytorch/pull/52349)).\r\n* Added support for sequence of tensor mutations in blocks ([#51577](https://github.com/pytorch/pytorch/pull/51577)) ([#52347](https://github.com/pytorch/pytorch/pull/52347)).\r\n* Updated LayerNorm symbolic to handle autocasting ([#52199](https://github.com/pytorch/pytorch/pull/52199)) ([#52350](https://github.com/pytorch/pytorch/pull/52350)).\r\n* Restored fast path in `OnnxifiOp::adjustOutputBatchSize` ([#52498](https://github.com/pytorch/pytorch/pull/52498)).\r\n* Improved index_put symbolic to handle singular Bool updates ([#53690](https://github.com/pytorch/pytorch/pull/53690)) ([#54863](https://github.com/pytorch/pytorch/pull/54863)).\r\n* Replaced decomposeLinear pre process pass with a symbolic ([#53077](https://github.com/pytorch/pytorch/pull/53077)) ([#54866](https://github.com/pytorch/pytorch/pull/54866)).\r\n* Improved assign input shape for tuple inputs & primitive type inputs ([#54112](https://github.com/pytorch/pytorch/pull/54112)) ([#56164](https://github.com/pytorch/pytorch/pull/56164)).\r\n* Updated repeat_interleave symbolic ([#54312](https://github.com/pytorch/pytorch/pull/54312)) ([#56165](https://github.com/pytorch/pytorch/pull/56165)).\r\n* Enabled `word_language_model` GRU and LSTM scripting ([#54310](https://github.com/pytorch/pytorch/pull/54310)) ([#56170](https://github.com/pytorch/pytorch/pull/56170)).\r\n* Added standardOps match more input type in ORT ([#53813](https://github.com/pytorch/pytorch/pull/53813)) ([#56172](https://github.com/pytorch/pytorch/pull/56172)).\r\n* Redesigned in-place conversion ([#55033](https://github.com/pytorch/pytorch/pull/55033)) ([#56173](https://github.com/pytorch/pytorch/pull/56173)).\r\n* Handled PackedParams inputs for _propagate_and_assign_input_shapes ([#56449](https://github.com/pytorch/pytorch/pull/56449)) ([#57079](https://github.com/pytorch/pytorch/pull/57079)).\r\n* Added a warning for the case when *len* is used to calculate tensor shape ([#55151](https://github.com/pytorch/pytorch/pull/55151)) ([#57595](https://github.com/pytorch/pytorch/pull/57595)).\r\n* Added special post processing for `onnx::Cast` and `onnx::ConstantOfShape` shape type inference ([#55962](https://github.com/pytorch/pytorch/pull/55962)) ([#57597](https://github.com/pytorch/pytorch/pull/57597)).\r\n* Handled NoneType in Assign Output Shapes ([#54623](https://github.com/pytorch/pytorch/pull/54623)) ([#57602](https://github.com/pytorch/pytorch/pull/57602)).\r\n* ListUnpack on dynamic tensor list ([#56592](https://github.com/pytorch/pytorch/pull/56592)) ([#57603](https://github.com/pytorch/pytorch/pull/57603)).\r\n* Handled mixed mask, index input for index_put ([#57604](https://github.com/pytorch/pytorch/pull/57604)).\r\n* Handled incorrect format for example_outputs ([#55802](https://github.com/pytorch/pytorch/pull/55802)) ([#57829](https://github.com/pytorch/pytorch/pull/57829)).\r\n* Enabled several script unit tests using new jit passes ([#51722](https://github.com/pytorch/pytorch/pull/51722)) ([#53309](https://github.com/pytorch/pytorch/pull/53309)).\r\n\r\n### Vulkan\r\n\r\n* Enabled broadcasting for arithmetic ops (add, sub, mul, and div) ([#52842](https://github.com/pytorch/pytorch/pull/52842)).\r\n* Reduced size of compiled shaders by using the `-Os` flag when calling `glslc` ([#57199](https://github.com/pytorch/pytorch/pull/57199)).\r\n* The vulkan optimization JIT pass now adds an `optimized_for_vulkan` attribute to the model ([#56414](https://github.com/pytorch/pytorch/pull/56414)).\r\n\r\n### Benchmark\r\n\r\n* Quality of life improvements to Timer ([#53294](https://github.com/pytorch/pytorch/pull/53294))\r\n* Add repeats to Timer.collect_callgrind(...) ([#54484](https://github.com/pytorch/pytorch/pull/54484))\r\n\r\n### Misc\r\n\r\n* Auto-detect ccache to speed up developer builds ([#49389](https://github.com/pytorch/pytorch/pull/49389)).\r\n* Catch and ignore tracebacks for compilation errors ([#55986](https://github.com/pytorch/pytorch/pull/55986)).\r\n* Register DefaultBackend implementations for functional/inplace structured operators ([#53037](https://github.com/pytorch/pytorch/pull/53037)).\r\n* Improved support for oneDNN on AArch64 when building from src ([#55913](https://github.com/pytorch/pytorch/pull/55913)).\r\n\r\n\r\n# Bug fixes\r\n\r\n### Python API\r\n\r\n* Updated `torch.lerp`  to make `weights` tensor broadcast-able ([#52319](https://github.com/pytorch/pytorch/pull/52319)).\r\n* Fixed print for negative torch.int8 tensors on ARM64 ([#52616](https://github.com/pytorch/pytorch/pull/52616)).\r\n* Fixed type annotation for `as_tuple` to clearly determine what `torch.nonzero` will resolve to ([#51635](https://github.com/pytorch/pytorch/pull/51635)).\r\n* Fixed `torch.logcumsumexp` to correctly handle infs and nans ([#52947](https://github.com/pytorch/pytorch/pull/52947)).\r\n* Fixed `torch.topk` for k=0 on CUDA by skipping the kernel launch in this case ([#58086](https://github.com/pytorch/pytorch/pull/58086)).\r\n* Fixed a bug for optimizers to have the hyper parameters be still defined when all parameters have no grad ([#52944](https://github.com/pytorch/pytorch/pull/52944)).\r\n* Fixed type promotion issue for `torch.pow` ([#54085](https://github.com/pytorch/pytorch/pull/54085)).\r\n* Fixed `torch.min()` and `torch.max()` to work on a non-empty dimension for tensors with 0 elements ([#52565](https://github.com/pytorch/pytorch/pull/52565)).\r\n* Fixed the upper bound computation for `torch.randperm` ([#56967](https://github.com/pytorch/pytorch/pull/56967)).\r\n* Allowed `std=0` in `torch.normal`, and added checks to consistently error out if `std<0` ([#51317](https://github.com/pytorch/pytorch/pull/51317))\r\n* Fixed  `torch.index_fill` to output 0-dim tensor for a 0-dim input tensor ([#52209](https://github.com/pytorch/pytorch/pull/52209)).\r\n* Fixed mul_() to correctly work for Mkldnn tensors ([#51758](https://github.com/pytorch/pytorch/pull/51758)).\r\n* Fixed temp file/bind race condition in torch_shm_manager for `torch.multiprocessing` ([#57309](https://github.com/pytorch/pytorch/pull/57309)).\r\n* Fixed tempfile address binding in torch_shm_manager to be destructed correctly for `torch.multiprocessing` ([#57566](https://github.com/pytorch/pytorch/pull/57566)).\r\n* Fixed `torch.multinomial` to never select an element with 0 weight for `torch.half` (already works correctly for other datatypes) ([#53480](https://github.com/pytorch/pytorch/pull/53480)).\r\n* Fixed a bug in `assertRaises` `NotImplemented` handling when no exception is thrown ([#54126](https://github.com/pytorch/pytorch/pull/54126)).\r\n* Fixed override for `__iter__` ([#54702](https://github.com/pytorch/pytorch/pull/54702)).\r\n* Fixed segmentation fault for `torch.floor_divide` when compiling on ARM64 ([#55608](https://github.com/pytorch/pytorch/pull/55608)).\r\n* Fixed `torch.digamma`\u2019s inconsistency with SciPy\u2019s digamma ([#56689](https://github.com/pytorch/pytorch/pull/56689)).\r\n* Fixed `torch.cat` to return correct result for non-contiguous tensors ([#57177](https://github.com/pytorch/pytorch/pull/57177)).\r\n* Fixed distributions for `torch.distributions.log_prob` which don't properly honor `validate_args=False` ([#53600](https://github.com/pytorch/pytorch/pull/53600)).\r\n* De-prioritized `Dimname` and `DimnameList` in python overload resolution ([#51350](https://github.com/pytorch/pytorch/pull/51350)).\r\n* Fixed the handling of scalar and zero dimensional inputs as well to `torch.take()` and `torch.Tensor.put_` on both CPU and CUDA ([#53356](https://github.com/pytorch/pytorch/pull/53356)).\r\n* Fixed a bug to not rebuild extensions for every import ([#56015](https://github.com/pytorch/pytorch/pull/56015)).\r\n* Fixed error message for `torch.as_strided` ([#53198](https://github.com/pytorch/pytorch/pull/53198)).\r\n* Added correct handling for tensor allocation for large tensors when using `torch.resize` on CUDA ([#52672](https://github.com/pytorch/pytorch/pull/52672)).\r\n* Fixed an illegal memory access that could happen when computing the inverse of a batch of matrices on CUDA ([#53064](https://github.com/pytorch/pytorch/pull/53064)).\r\n* Fixed a bug where `torch.sparse.addmm` would compute the wrong results for CUDA inputs when beta was not zero or one ([#56160](https://github.com/pytorch/pytorch/pull/56160)).\r\n* Fixed a bug where `torch.sparse.sparse_coo_tensor`\u2019s gradient could be calculated incorrectly ([#50361](https://github.com/pytorch/pytorch/pull/50361)).\r\n* `pow`: Fixed a bug caused for mixed cpu/cuda input tensors ([#53669](https://github.com/pytorch/pytorch/pull/53669)).\r\n* `sub`: Fixed a `sub.Scalar` bug ([#53679](https://github.com/pytorch/pytorch/pull/53679)).\r\n* Fixed `torch.unique` for discontiguous inputs ([#59003](https://github.com/pytorch/pytorch/pull/59003)).\r\n* Fixed `torch.randperm` on CUDA ([#59352](https://github.com/pytorch/pytorch/pull/59352)).\r\n* Fix `torch.reciprocal` for `torch.float32` on ARMv8 ([#59361](https://github.com/pytorch/pytorch/pull/59361)).\r\n* Disable overloading of std::max & std::min for inputs of different types, which could cause accuracy loss ([#55638](https://github.com/pytorch/pytorch/pull/55638))\r\n\r\n### Complex Numbers\r\n\r\n* Added custom implementation for `sqrt` and `acos` to be used if `libc++` is used to reduce numerical error for edge cases. ([#52018](https://github.com/pytorch/pytorch/pull/52018), [#54820](https://github.com/pytorch/pytorch/pull/54820), [#52287](https://github.com/pytorch/pytorch/pull/52287)).\r\n\r\n### Autograd\r\n\r\n* Fixed\r\n    * `torch.autograd.gradgradcheck` when outputs are independent of the inputs ([#58049](https://github.com/pytorch/pytorch/pull/58049)).\r\n    * `torch.utils.checkpoint` to behave properly when an error happens during forward ([#51746](https://github.com/pytorch/pytorch/pull/51746)).\r\n    * autograd\u2019s graph discovery when output is a leaf that requires gradients ([#51940](https://github.com/pytorch/pytorch/pull/51940)).\r\n    * some cases where `torch.autograd.gradcheck` did not return the correct value when `raise_exception=False`  ([#53916](https://github.com/pytorch/pytorch/pull/53916)) .\r\n    * thread local state not being properly propagated for some operations during the backward pass ([#56174](https://github.com/pytorch/pytorch/pull/56174)).\r\n    * `torch.index_fill_` formula to support duplicate indices ([#57101](https://github.com/pytorch/pytorch/pull/57101)).\r\n    * derivative of `torch.sinc` around `x=0` ([#56763](https://github.com/pytorch/pytorch/pull/56763), [#56986](https://github.com/pytorch/pytorch/pull/56986)).\r\n    * `torch.cdist` backward formula to correctly support broadcasting ([#56605](https://github.com/pytorch/pytorch/pull/56605)) and empty inputs ([#56606](https://github.com/pytorch/pytorch/pull/56606)).\r\n    * view creation metadata for functions that return multiple views in `no_grad` or inference mode. ([#57842](https://github.com/pytorch/pytorch/pull/57842)).\r\n    * `autograd.functional.*` functions to work in no_grad mode ([#47543](https://github.com/pytorch/pytorch/pull/47543)).\r\n    * rare deadlocks on exit due to autograd worker threads ([#53170](https://github.com/pytorch/pytorch/pull/53170)).\r\n\r\n### torch.nn\r\n\r\n* `nn.AdaptiveAveragePooling`: Fix crash for integral inputs ([#51443](https://github.com/pytorch/pytorch/pull/51443)).\r\n* `F.normalize`: Fix to make it properly scriptable ([#51909](https://github.com/pytorch/pytorch/pull/51909)).\r\n* `nn.parallel.scatter_gather.gather`: Fix to handle `NamedTuple`s and moving output to CPU ([#51104](https://github.com/pytorch/pytorch/pull/51104)).\r\n* `fractional_max_pool{2/3}d` : Fix segfaults for incorrect `kernel_size` and `output_size` ([#51626](https://github.com/pytorch/pytorch/pull/51626)).\r\n* `nn.CosineEmbeddingLoss`: Validate target has correct shape ([#53110](https://github.com/pytorch/pytorch/pull/53110)).\r\n* Fix multiprocessing serialization for integer parameters on CUDA ([#56529](https://github.com/pytorch/pytorch/pull/56529)).\r\n* `nn.Softplus`: Fix backwards computation by comparing `input` against `beta * threshold` ([#56484](https://github.com/pytorch/pytorch/pull/56484)).\r\n* `addmm_`: Add check to disallow resizing the input tensor for the in-place variation on CPU ([#56452](https://github.com/pytorch/pytorch/pull/56452)).\r\n* `nn.InstanceNorm*d`: Fix to perform correct input size check ([#56659](https://github.com/pytorch/pytorch/pull/56659)).\r\n* `nn.CTCLoss`: Fix backward pass regression on cuDNN ([#56639](https://github.com/pytorch/pytorch/pull/56639)).\r\n* `nn.ConvTranspose*d`: Fix regression that broke padding with a list of values ([#54911](https://github.com/pytorch/pytorch/pull/54911)).\r\n* `F.max_pool3d`: Fix illegal memory access for large inputs on CUDA by doing multiplication in `int64` ([#52828](https://github.com/pytorch/pytorch/pull/52828)).\r\n* `F.embedding`: Support `__torch_function__` ([#54478](https://github.com/pytorch/pytorch/pull/54478)).\r\n* `nn.ChannelShuffle`: Remove `NamedTensor` warnings ([#55911](https://github.com/pytorch/pytorch/pull/55911)).\r\n* `mkldnn_linear`: Fix incorrect results for non-contiguous inputs ([#51713](https://github.com/pytorch/pytorch/pull/51713)).\r\n* `nn.ModuleList` / `nn.ModuleDict`: Raise `NotImplementedError` for `forward()` ([#48785](https://github.com/pytorch/pytorch/pull/48785)).\r\n* Change `maybe_resize_storage_cpu` `new_size` arg to unsigned ([#52671](https://github.com/pytorch/pytorch/pull/52671)).\r\n* `nn.LSTM`: Fix regression that broke loading older serialized modules ([#57558](https://github.com/pytorch/pytorch/pull/57558)).\r\n* `F.reflection_pad2d`: Fix CUDA launch error ([#56451](https://github.com/pytorch/pytorch/pull/56451)).\r\n* Fix wrong detection of depthwise convolution on neon ([#55794](https://github.com/pytorch/pytorch/pull/55794)).\r\n* Re-enable fast winograd convolution on IOS ([#56021](https://github.com/pytorch/pytorch/pull/56021)).\r\n* `gaussian_nll_loss`: Fix incorrect `reduction=\u2018none\u2019` behavior ([#56469](https://github.com/pytorch/pytorch/pull/56469)).\r\n* Fix misaligned access #56325 ([#56403](https://github.com/pytorch/pytorch/pull/56403)).\r\n* Use native CTC loss for target length 256 ([#53557](https://github.com/pytorch/pytorch/pull/53557)).\r\n* `register_full_backward_hook`: Fix crash when first argument doesn't require a gradient ([#57945](https://github.com/pytorch/pytorch/pull/57945)).\r\n* Remove asserts of Tensor type and ignore mypy checks to support `__torch_function__` usage ([#57458](https://github.com/pytorch/pytorch/pull/57458)).\r\n* Handle stride > 1 with im2col in CUDA thnn conv2d ([#54080](https://github.com/pytorch/pytorch/pull/54080)).\r\n* Add device id to ConvolutionParams ([#50892](https://github.com/pytorch/pytorch/pull/50892)).\r\n* Enabling OneDNN for group convolution ([#54890](https://github.com/pytorch/pytorch/pull/54890)).\r\n* `nn.AdaptiveAveragePooling3d`: Add `AccumulateType` for CUDA ([#53607](https://github.com/pytorch/pytorch/pull/53607)).\r\n* Do not use depthwise3x3 conv in grad mode for ARM ([#56889](https://github.com/pytorch/pytorch/pull/56889)).\r\n* Fix type annotations for `state_dict()` override ([#55704](https://github.com/pytorch/pytorch/pull/55704)).\r\n* Pass contiguous weight to NNPACK convolution ([#56569](https://github.com/pytorch/pytorch/pull/56569)).\r\n* `nn.EmbeddingBag`: Mark backward as non-deterministic for max mode rather than all reducing modes ([#55574](https://github.com/pytorch/pytorch/pull/55574)).\r\n* `nn.EmbeddingBag`: Initialize `bag_size` output with zeros to make it deterministic ([#56661](https://github.com/pytorch/pytorch/pull/56661)).\r\n* `nn.EmbeddingBag`: Support the empty bag case on CPU ([#57446](https://github.com/pytorch/pytorch/pull/57446)).\r\n* Fix `nn.MHA` + `quantized` scriptability ([#58727](https://github.com/pytorch/pytorch/pull/58727)).\r\n* Fixes cuDNN performance on A100 ([#58287](https://github.com/pytorch/pytorch/pull/58287), [#59721](https://github.com/pytorch/pytorch/pull/59721), [#59744](https://github.com/pytorch/pytorch/pull/59744), [#59802](https://github.com/pytorch/pytorch/pull/59802)).\r\n\r\n### Dataloader\r\n\r\n* Fixed type hints of the callable DataLoader arguments ([#52924](https://github.com/pytorch/pytorch/pull/52924)).\r\n* Added a keyword arg to meta and support `abc` for typing ([#58450](https://github.com/pytorch/pytorch/pull/58450)).\r\n* Fixed a bug to use `generator` instead of `self.generator` in the `RandomSampler` ([#52956](https://github.com/pytorch/pytorch/pull/52956)).\r\n\r\n### C++ API\r\n\r\n* Fixed the lifetime of `PyTensorType` ([#51649](https://github.com/pytorch/pytorch/pull/51649)).\r\n* Fixed linker failure with ambiguous namespaces ([#45736](https://github.com/pytorch/pytorch/pull/45736)).\r\n* Fix Scalar output formatting ([#53229](https://github.com/pytorch/pytorch/pull/53229))\r\n* Fix printing of optional string arguments in schemas ([#55196](https://github.com/pytorch/pytorch/pull/55196))\r\n\r\n### AMD\r\n\r\n* Fixed `hipfft` transform type error ([#53411](https://github.com/pytorch/pytorch/pull/53411)).\r\n* Load only hipfft for ROCm > 4.1 ([#54349](https://github.com/pytorch/pytorch/pull/54349)).\r\n\r\n### CUDA\r\n\r\n* Added `torch.scatter_add` to `torch.cuda.amp` promote list ([#52133](https://github.com/pytorch/pytorch/pull/52133)).\r\n* Fixed segfault in distributed process group due to IPC ([#53080](https://github.com/pytorch/pytorch/pull/53080)).\r\n* Fixed multinomial CUDA misalignment and non-deterministic behavior ([#55364](https://github.com/pytorch/pytorch/pull/55364)).\r\n* Replaced raw cudaMalloc in `torch.sparse` code ([#57083](https://github.com/pytorch/pytorch/pull/57083)).\r\n* [CUDA graphs] Added proper sync after replay ([#57556](https://github.com/pytorch/pytorch/pull/57556)).\r\n* Fixed NVRTC versioning for CUDA 11.X (X>=3), CUDA 12 and later ([#57204](https://github.com/pytorch/pytorch/pull/57204)).\r\n* Fixed a correctness issue of CUDA channels-last `nn.SyncBatchNorm` ([#57077](https://github.com/pytorch/pytorch/pull/57077)).\r\n* Fixed CUDA caching allocator when trying to allocate ~2^64 memory ([#57571](https://github.com/pytorch/pytorch/pull/57571)).\r\n* Fixed raw_deleter() bug with PYTORCH_NO_CUDA_MEMORY_CACHING=1 ([#54775](https://github.com/pytorch/pytorch/pull/54775)).\r\n* Fixed undefined symbol for CUDA 11.1 Windows ([#52506](https://github.com/pytorch/pytorch/pull/52506)).\r\n* Automatically set BUILD_SPLIT_CUDA for cpp extensions ([#52503](https://github.com/pytorch/pytorch/pull/52503)).\r\n* Adds grid_sampler to the list of operations that can autocast `torch.float32` ([#58679](https://github.com/pytorch/pytorch/pull/58679)).\r\n\r\n### Dispatcher\r\n\r\n* Fix boxing/unboxing for `Scalar` bool values ([#53228](https://github.com/pytorch/pytorch/pull/53228))\r\n* Fix inaccurate dispatch table for `fill_` ([#53611](https://github.com/pytorch/pytorch/pull/53611))\r\n* Fix inaccurate dispatch tables ([#54127](https://github.com/pytorch/pytorch/pull/54127))\r\n* Fix issue with dispatch key: `AutogradXPU` ([#56336](https://github.com/pytorch/pytorch/pull/56336))\r\n* Modify `DispatchKeyExtractor` to also work for optional Tensors ([#58283](https://github.com/pytorch/pytorch/pull/58283))\r\n* Extract dispatch keys from optional Tensors (unboxed) ([#58296](https://github.com/pytorch/pytorch/pull/58296))\r\n\r\n### torch.fx\r\n\r\n* Preserve leaf modules in `Transformer` ([#51998](https://github.com/pytorch/pytorch/pull/51998)).\r\n* Fix tuple type annotations in FX codebase ([#52010](https://github.com/pytorch/pytorch/pull/52010)).\r\n* Fix type correctness on `GraphModule.graph` ([#54305](https://github.com/pytorch/pytorch/pull/54305)).\r\n* Remove `forward` from `forward.__globals__` to facilitate retracing ([#54011](https://github.com/pytorch/pytorch/pull/54011)).\r\n* Fix `ScriptMethod` dispatch on `__torch_function__` ([#56103](https://github.com/pytorch/pytorch/pull/56103)).\r\n* Fix `type_matches` for `Optional[List[int]]` arguments to make `NormalizeArgs` more permissive ([#56790](https://github.com/pytorch/pytorch/pull/56790)).\r\n* Fix `NormalizeArgs` issues with lists of tensors ([#57004](https://github.com/pytorch/pytorch/pull/57004)).\r\n* Changed parametric type error in `NormalizeArgs` to a warning ([#57183](https://github.com/pytorch/pytorch/pull/57183)).\r\n* Make `NormalizeArgs` not save output node in the `node_map` ([#58058](https://github.com/pytorch/pytorch/pull/58058)).\r\n\r\n### Profiler\r\n\r\n* Fixed intermittent CUDA activity flush issue (https://github.com/pytorch/kineto/pull/95).\r\n* Handled empty trace ([#58013](https://github.com/pytorch/pytorch/pull/58013)).\r\n* Added cuda synchronization points ([#56651](https://github.com/pytorch/pytorch/pull/56651)).\r\n* Removed usage of onEachDevice from legacy profiler ([#54125](https://github.com/pytorch/pytorch/pull/54125)).\r\n* Fixed double printing of FLOPs ([#56974](https://github.com/pytorch/pytorch/pull/56974)).\r\n\r\n### TorchScript\r\n\r\n* Fixed `jit.trace` mishandling of InterfaceType ([#53052](https://github.com/pytorch/pytorch/pull/53052)).\r\n* Made `reshape`/`flatten` deterministic ([#54353](https://github.com/pytorch/pytorch/pull/54353)).\r\n* Added logic to use `is_buffer` in `BufferPolicy::valid` ([#49588](https://github.com/pytorch/pytorch/pull/49588)).\r\n* Updated NNC to sanitize input names ([#52786](https://github.com/pytorch/pytorch/pull/52786)).\r\n* Handled ExternalCalls in LoadStore analysis and Inliner ([#52628](https://github.com/pytorch/pytorch/pull/52628)).\r\n* Fixed output restriding of size-1 dimensions ([#58256](https://github.com/pytorch/pytorch/pull/58256)).\r\n* Handled non literal constant bounds in Unroll ([#53029](https://github.com/pytorch/pytorch/pull/53029)).\r\n* Fixed a case where inlining wouldn't work because dim-size was 1 ([#53254](https://github.com/pytorch/pytorch/pull/53254)).\r\n* Removed cached argv from LLVMCodeGen to fix race condition ([#54286](https://github.com/pytorch/pytorch/pull/54286)).\r\n* Lowered scalar constants as doubles/longs ([#54824](https://github.com/pytorch/pytorch/pull/54824)).\r\n* Added a check to not try to vectorize kernels that use float16 ([#55970](https://github.com/pytorch/pytorch/pull/55970)).\r\n* Added a check to not fuse `torch.float16` on CPU ([#56119](https://github.com/pytorch/pytorch/pull/56119)).\r\n* Fixed `float->bool` conversion on CPU ([#57798](https://github.com/pytorch/pytorch/pull/57798)).\r\n* Fixed handling of the arguments of `aten::to` ([#58028](https://github.com/pytorch/pytorch/pull/58028)).\r\n* Don\u2019t error on 0-dim in convolution ([#51922](https://github.com/pytorch/pytorch/pull/51922)).\r\n* Allow `__exit__` to have a return value ([#52336](https://github.com/pytorch/pytorch/pull/52336)).\r\n* Added metacompile of ternary if ([#51789](https://github.com/pytorch/pytorch/pull/51789)).\r\n* Keep alive graph when creating iterators from it ([#51951](https://github.com/pytorch/pytorch/pull/51951)).\r\n* Fixed return value of `IValue::to` for Tensor/String ([#51463](https://github.com/pytorch/pytorch/pull/51463)).\r\n* Added function to check for memory leak ([#52342](https://github.com/pytorch/pytorch/pull/52342)).\r\n* Ignore user annotated ignored attributes ([#52367](https://github.com/pytorch/pytorch/pull/52367)).\r\n* Fixed `jit.trace` mishandling of InterfaceType ([#53052](https://github.com/pytorch/pytorch/pull/53052)).\r\n* Fixed tracing support for TorchBind ([#52884](https://github.com/pytorch/pytorch/pull/52884)).\r\n* Use correct warning type for tracer warnings ([#53460](https://github.com/pytorch/pytorch/pull/53460)).\r\n* Removed the assumption that `forward` exists in freeze_module ([#52918](https://github.com/pytorch/pytorch/pull/52918)).\r\n* Removed notion of \"level\" from `Module::dump_to_str` ([#52539](https://github.com/pytorch/pytorch/pull/52539)).\r\n* Made `IValue::toTensor()` inline-able ([#53213](https://github.com/pytorch/pytorch/pull/53213)).\r\n* Consider `normal_` as a special operation in the remove mutation pass ([#52175](https://github.com/pytorch/pytorch/pull/52175)).\r\n* Updated `set_stream` API to change the device ([#53741](https://github.com/pytorch/pytorch/pull/53741)).\r\n* Only run `ReplaceWithCopy` pass when `enable_out_variant` is true ([#54111](https://github.com/pytorch/pytorch/pull/54111)).\r\n* Disable dfusion group that is not supported by XPU device ([#54239](https://github.com/pytorch/pytorch/pull/54239)).\r\n* Don\u2019t require same-sized `src`/`dest` in `reshape_copy` ([#54467](https://github.com/pytorch/pytorch/pull/54467)).\r\n* Fixed `TupleType.annotation_str` to conform to `typing` module syntax for empty tuple type ([#54641](https://github.com/pytorch/pytorch/pull/54641)).\r\n* Made NoneType `annotation_str` emit `NoneType` instead of `None` ([#54642](https://github.com/pytorch/pytorch/pull/54642)).\r\n* Made sure the copy version of the op exists in `ReplaceWithCopy` ([#55337](https://github.com/pytorch/pytorch/pull/55337)).\r\n* Included `conv3d` in `conv-add-relu` fusion ([#54772](https://github.com/pytorch/pytorch/pull/54772)).\r\n* Added `cond-add-relu` matching pattern to cover in-place ops ([#55458](https://github.com/pytorch/pytorch/pull/55458)).\r\n* Fixed `TupleType.annotation_str` to conform to `typing` module syntax for empty tuple type ([#54745](https://github.com/pytorch/pytorch/pull/54745)).\r\n* Fixed `Optional[Tensor]` type in autodiff ([#55565](https://github.com/pytorch/pytorch/pull/55565)).\r\n* Raise TypeErrors when `IValue::getSubValues` fails ([#56510](https://github.com/pytorch/pytorch/pull/56510)).\r\n* Fixed num args for `to_copy` ([#56441](https://github.com/pytorch/pytorch/pull/56441))\r\n* Fixed error in JIT CUDA on ROCm ([#55243](https://github.com/pytorch/pytorch/pull/55243)).\r\n* Fixed a bug in `emitUse` to drop all values that are marked as drop ([#56652](https://github.com/pytorch/pytorch/pull/56652)).\r\n* Fixed default dtype for `randperm` and `triu`/`tril_indices` inside TorchScript ([#57105](https://github.com/pytorch/pytorch/pull/57105)).\r\n* Don't allow create() on singleton types ([#56807](https://github.com/pytorch/pytorch/pull/56807)).\r\n* Fix GIL mutithreading issue exposed by `torch::jit::toIValue()` ([#57688](https://github.com/pytorch/pytorch/pull/57688)).\r\n* Fold `NaiveSyncBatchNorm` when folding batch norm ([#57823](https://github.com/pytorch/pytorch/pull/57823)).\r\n* Fix UB in `LoopNest::distribute` ([#57883](https://github.com/pytorch/pytorch/pull/57883)).\r\n* Fix a condition when we use a native depthwise `conv2d` lowering ([#57906](https://github.com/pytorch/pytorch/pull/57906)).\r\n* Ensure `torch.save()` has deterministic output ([#57536](https://github.com/pytorch/pytorch/pull/57536))\r\n* Fixed `hasattr` support type ([#57950](https://github.com/pytorch/pytorch/pull/57950))\r\n* Return nullptr if the number of input args doesn't match ([#58018](https://github.com/pytorch/pytorch/pull/58018)).\r\n* Added fix for missing ops `aten::sorted.str` ([#58339](https://github.com/pytorch/pytorch/pull/58339)).\r\n* Fixed deadlock in `Future` due to lock inversion with GIL ([#58382](https://github.com/pytorch/pytorch/pull/58382)).\r\n* Added logic to prevent lock inversions with GIL in `Future` ([#58391](https://github.com/pytorch/pytorch/pull/58391)).\r\n* Fixed `MKLDNN_add` in-place behavior ([#51687](https://github.com/pytorch/pytorch/pull/51687)).\r\n* Use MKLDNN copy for `copy_ when` self and src are MKLDNN layout ([#54248](https://github.com/pytorch/pytorch/pull/54248)) .\r\n* Fixed default to align with documentation in `fuser.py` ([#53457](https://github.com/pytorch/pytorch/pull/53457)).\r\n* Fixed upcoming changes that are part of ROCm 4.2 and affect PyTorch JIT ([#57400](https://github.com/pytorch/pytorch/pull/57400)).\r\n* Fix for improper mobile and torch.package serialization ([#59642](https://github.com/pytorch/pytorch/pull/59642)).\r\n\r\n### torch.package\r\n\r\n* Add cpython as a dependency for torch_python_obj ([#56740](https://github.com/pytorch/pytorch/pull/56740)).\r\n* Catch exceptions where dependency resolution gets invalid imports ([#58573](https://github.com/pytorch/pytorch/pull/58573)).\r\n* Simplifications to broken dependency handling ([#58572](https://github.com/pytorch/pytorch/pull/58572)).\r\n\r\n### Quantization\r\n\r\n* Fixed conv packed param serialization in `state_dict` ([#52787](https://github.com/pytorch/pytorch/pull/52787)).\r\n* Fixed `torch.float16` dynamic quant for functional linear ([#52369](https://github.com/pytorch/pytorch/pull/52369)).\r\n* Fixed prepacking for `F.conv1d` ([#55311](https://github.com/pytorch/pytorch/pull/55311)).\r\n* MHA tensor assignment fix ([#53031](https://github.com/pytorch/pytorch/pull/53031)).\r\n* Fixed `conv` transpose with `qconfig == None` ([#52844](https://github.com/pytorch/pytorch/pull/52844)).\r\n* Quant norm layers: move scale + zp to buffers ([#52861](https://github.com/pytorch/pytorch/pull/52861)).\r\n* Handled the case when observed node has no users ([#53210](https://github.com/pytorch/pytorch/pull/53210)).\r\n* Only insert observers for fixed qparam ops ([#53330](https://github.com/pytorch/pytorch/pull/53330)).\r\n* Fixed a condition check for `CopyNode` ([#53585](https://github.com/pytorch/pytorch/pull/53585)).\r\n* Fix for `x.ndim` followed by `sub` ([#53120](https://github.com/pytorch/pytorch/pull/53120)).\r\n* Fixed using size of quant layer in `torch._assert` ([#53187](https://github.com/pytorch/pytorch/pull/53187)).\r\n* Fixed fx quant for `quant_layer -> stack -> sum` ([#53196](https://github.com/pytorch/pytorch/pull/53196)).\r\n* Fixed `deepcopy` on quantized `ConvNd` ([#56154](https://github.com/pytorch/pytorch/pull/56154))\r\n* Fixed `getitem` for unmatched nodes ([#57173](https://github.com/pytorch/pytorch/pull/57173)).\r\n* Made quantizeable MHA work with `torch.jit.script` ([#57774](https://github.com/pytorch/pytorch/pull/57774)).\r\n* Fixed `quantize_per_tensor` on CUDA ([#57703](https://github.com/pytorch/pytorch/pull/57703)).\r\n* Fixed a bug to handle bias in rowwise quantization of FC ([#58022](https://github.com/pytorch/pytorch/pull/58022)).\r\n* Skipped inserting observer for boolean Tensors ([#57375](https://github.com/pytorch/pytorch/pull/57375)).\r\n* Fixed `torch.float16` reference patterns for linear ([#55727](https://github.com/pytorch/pytorch/pull/55727)).\r\n* FX Quant:\r\n    * Fixed edge case with copynode after user function ([#55710](https://github.com/pytorch/pytorch/pull/55710)).\r\n    * Fixed subtle bug in BinaryOpQuantizeHanlder logic in matching ([#56294](https://github.com/pytorch/pytorch/pull/56294)).\r\n    * Fixed bug with fusion patterns and disabling quantization ([#54654](https://github.com/pytorch/pytorch/pull/54654)).\r\n* Fixed overflow issue in quantized instance_norm/layer_norm/group_norm ([#54872](https://github.com/pytorch/pytorch/pull/54872)).\r\n* Fixed zero_point rounding for _fake_quantize_learnable_per_channel_affine ([#52290](https://github.com/pytorch/pytorch/pull/52290)).\r\n* Bug fix to update requantization and zp parameters of input ([#52797](https://github.com/pytorch/pytorch/pull/52797)).\r\n* Fix embedding bag bug accessing unaligned memory ([#53300](https://github.com/pytorch/pytorch/pull/53300)).\r\n* Fix out variant for 4bit embedding bag ([#55096](https://github.com/pytorch/pytorch/pull/55096)).\r\n* Avoid tensor refcount bumps on embedding bag ([#55023](https://github.com/pytorch/pytorch/pull/55023)).\r\n\r\n### Mobile\r\n\r\n* Fixed some bugs in the implementation of various functions on iOS GPU:\r\n    * `max_pool_2d` when padding is used ([#52431](https://github.com/pytorch/pytorch/pull/52431)).\r\n    * `softmax` ([#54519](https://github.com/pytorch/pytorch/pull/54519)).\r\n    * binary element-wise ops to handle inputs with different number of dimensions ([#58262](https://github.com/pytorch/pytorch/pull/58262)).\r\n* Removed duplication of constant tensors in model when using Lite interpreter ([#58182](https://github.com/pytorch/pytorch/pull/58182), [#56002](https://github.com/pytorch/pytorch/pull/56002)).\r\n* Banned mutating operators in mobile GPU models ([#56070](https://github.com/pytorch/pytorch/pull/56070)).\r\n* Use lite interpreter as default and bump model version ([#58630](https://github.com/pytorch/pytorch/pull/58630))\r\n\r\n### Distributed\r\n\r\n`torch.distributed.Store`\r\n\r\n* Fix flag specifying whether there is more data for `TCPStore` delete key ([#53886](https://github.com/pytorch/pytorch/pull/53886))\r\n* Properly enforce timeout for `PrefixStore`. ([#53928](https://github.com/pytorch/pytorch/pull/53928))\r\n* Fix `TCPStore` `wait` hang when key is previously set ([#53860](https://github.com/pytorch/pytorch/pull/53860))\r\n* Properly order `TCPStore`\u2019s `compare_set` parameters in Python API ([#52696](https://github.com/pytorch/pytorch/pull/52696))\r\n* Fix resource leak bug in TCPStore constructor ([#52860](https://github.com/pytorch/pytorch/pull/52860))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Several fixes for CUDA support in the RPC framework ([#57926](https://github.com/pytorch/pytorch/pull/57926), [#57432](https://github.com/pytorch/pytorch/pull/57432), [#57394](https://github.com/pytorch/pytorch/pull/57394), [#57443](https://github.com/pytorch/pytorch/pull/57443), [#57487](https://github.com/pytorch/pytorch/pull/57487), [#58384](https://github.com/pytorch/pytorch/pull/58384), [#51820](https://github.com/pytorch/pytorch/pull/51820), [#57792](https://github.com/pytorch/pytorch/pull/57792), [#56895](https://github.com/pytorch/pytorch/pull/56895), [#54932](https://github.com/pytorch/pytorch/pull/54932))\r\n* Fix possible reference cycle by passing reference to parent future in RPC callbacks ([#57635](https://github.com/pytorch/pytorch/pull/57635))\r\n* Fix RPC `get_worker_info` for rank 0 ([#52804](https://github.com/pytorch/pytorch/pull/52804))\r\n* Fix crash when TensorPipe agent tries to double-set errors. ([#52837](https://github.com/pytorch/pytorch/pull/52837))\r\n\r\n`torch.distributed`\r\n\r\n* Fix path handling on Windows during rendezvous process ([#57000](https://github.com/pytorch/pytorch/pull/57000))\r\n* Fix and re-enable `ProcessGroupMPITest` ([#56709](https://github.com/pytorch/pytorch/pull/56709))\r\n\r\n`DistributedDataParallel`\r\n\r\n*  Correct the usage of min_compression_rate in gradient compression communication hooks ([#52979](https://github.com/pytorch/pytorch/pull/52979))\r\n* Fix mapping of parameter to parameter names when certain parameters don\u2019t require gradient ([#57771](https://github.com/pytorch/pytorch/pull/57771))\r\n* Skip rebuild buckets in `DistributedDataParallel` when running under `no_grad` mode. ([#54159](https://github.com/pytorch/pytorch/pull/54159))\r\n* Fix a race condition in `DistributedDataParallel` when all parameters are used but running with `find_unused_parameters=True`. ([#53160](https://github.com/pytorch/pytorch/pull/53160))\r\n* In `DistributedDataParallel`, pass in `process_group` argument into `dist.get_rank` calls ([#53793](https://github.com/pytorch/pytorch/pull/53793))\r\n* Fix `DistributedDataParallel`\u2019s process for verifying model consistency during initialization. ([#52887](https://github.com/pytorch/pytorch/pull/52887))\r\n\r\n`torch.distributed`\r\n\r\n* Check vector boundaries in `torch::cuda::scatter` ([#53057](https://github.com/pytorch/pytorch/pull/53057))\r\n* Release GIL before destructing ProcessGroup classes ([#56381](https://github.com/pytorch/pytorch/pull/56381))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* Fix hang in `pipeline` destructor by removing `join_workers` ([#53433](https://github.com/pytorch/pytorch/pull/53433))\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Resolve bug around incorrect rendezvous handler resolution ([#56386](https://github.com/pytorch/pytorch/pull/56386))\r\n\r\n`torch.nn.SyncBatchNorm`\r\n\r\n* Ensure `SyncBatchNorm` behaves like a regular `BatchNorm` layer in eval mode. ([#56982](https://github.com/pytorch/pytorch/pull/56982))\r\n\r\n`torch.distributed.optim.ZeroRedundancyOptimizer`\r\n\r\n* Typing fixes([#53165](https://github.com/pytorch/pytorch/pull/53165))\r\n\r\nFix monitored_barrier with wait_all_ranks ([#58702](https://github.com/pytorch/pytorch/pull/58702)).\r\n\r\n### ONNX\r\n\r\n* Removed the last Cast in pow symbolic_opset9 ([#52646](https://github.com/pytorch/pytorch/pull/52646)) ([#53305](https://github.com/pytorch/pytorch/pull/53305)).\r\n* Fixed export of `copy_` operator ([#53046](https://github.com/pytorch/pytorch/pull/53046)) ([#53310](https://github.com/pytorch/pytorch/pull/53310)) ([#51938](https://github.com/pytorch/pytorch/pull/51938)) ([#54870](https://github.com/pytorch/pytorch/pull/54870)).\r\n* Fixed export of embedding with `padding_idx` ([#53053](https://github.com/pytorch/pytorch/pull/53053)) ([#53530](https://github.com/pytorch/pytorch/pull/53530)).\r\n* Fixed onnx warning message ([#54371](https://github.com/pytorch/pytorch/pull/54371)).\r\n* Improved error message during Glow ONNXIFI ([#58069](https://github.com/pytorch/pytorch/pull/58069)).\r\n* Fixed if output shape mismatch error & graph input directly used as output ([#53219](https://github.com/pytorch/pytorch/pull/53219)) ([#54865](https://github.com/pytorch/pytorch/pull/54865)).\r\n* Fixed ComputeShapeFromReshape when `input_shape_size < reshape_size` ([#56171](https://github.com/pytorch/pytorch/pull/56171)).\r\n* Fixed -Wrange-loop-construct in onnx_exporter.cc ([#56759](https://github.com/pytorch/pytorch/pull/56759)).\r\n* Print `onnxifi` failed status code in readable format ([#53648](https://github.com/pytorch/pytorch/pull/53648)).\r\n\r\n### Vulkan\r\n\r\n* Fixed kernel registration errors in Vulkan test and benchmark binaries by adding `nonVarTypeModeGuard` ([#52535](https://github.com/pytorch/pytorch/pull/52535)).\r\n* Fixed the `glslc` path in CMake for desktop builds ([#56507](https://github.com/pytorch/pytorch/pull/56507)).\r\n* Fixed build failures caused by `warnings-treated-as-error` for Linux builds. ([#52781](https://github.com/pytorch/pytorch/pull/52781)).\r\n* Remove constant duplication for Vulkan optimize_for_mobile ([#59276](https://github.com/pytorch/pytorch/pull/59276)).\r\n\r\n### Benchmark\r\n\r\n* Fix timer overflow on small, fast snippets ([#55200](https://github.com/pytorch/pytorch/pull/55200))\r\n\r\n### Misc\r\n\r\n* [memory format] Fixed channels last bug in upsample kernels to now correctly pass `memory_format` information from the input to the output tensors ([#53535](https://github.com/pytorch/pytorch/pull/53535)).\r\n* [memory format] Fixed silent correctness bug for CUDA upsample kernels to correctly handle `torch.channels_last` contiguous tensors ([#54744](https://github.com/pytorch/pytorch/pull/54744)).\r\n* Workaround intermittent gcc-7.5 ICE in cpp tests ([#57016](https://github.com/pytorch/pytorch/pull/57016)).\r\n* Improve build quality on Windows ([#52729](https://github.com/pytorch/pytorch/pull/52729), [#53562](https://github.com/pytorch/pytorch/pull/53562), [#54132](https://github.com/pytorch/pytorch/pull/54132), [#55275](https://github.com/pytorch/pytorch/pull/55275)).\r\n* Search for static OpenBLAS compiled with OpenMP ([#59428](https://github.com/pytorch/pytorch/pull/59428)).\r\n\r\n\r\n# Performance\r\n\r\n### Python API\r\n\r\n* Optimized memory usage for `out=` version of `torch`.`logsumexp` ([#51239](https://github.com/pytorch/pytorch/pull/51239)).\r\n* Added vectorization for `torch.floor_divide` ([#55380](https://github.com/pytorch/pytorch/pull/55380)).\r\n* Reimplemented `torch.flip()` using advanced indexing ([#56713](https://github.com/pytorch/pytorch/pull/56713)).\r\n* Improved performance for `torch.take()` and `torch.Tensor.put_` on both CPU and CUDA ([#53356](https://github.com/pytorch/pytorch/pull/53356))\r\n* Generic performance improvement for operations performed on non-contiguous 2-dimensional tensors ([#53613](https://github.com/pytorch/pytorch/pull/53613)).\r\n* Added vectorization for `torch.copysign` on CPU ([#51792](https://github.com/pytorch/pytorch/pull/51792)).\r\n* Improved performance for bilinear interpolation on CPU ([#51653](https://github.com/pytorch/pytorch/pull/51653)).\r\n* Improved performance for backward computations on `torch.cumsum` and `torch.cumprod` on both CPU and CUDA ([#53711](https://github.com/pytorch/pytorch/pull/53711)).\r\n* Improved performance for `torch.Tensor.copy_`  when performing copies between small tensors of `torch.float` and `torch.half` data types ([#53800](https://github.com/pytorch/pytorch/pull/53800)).\r\n* Enabled vectorization for `torch.Tensor.copy_` and `torch.cat` for BFloat16 tensors ([#54671](https://github.com/pytorch/pytorch/pull/54671), [#54674](https://github.com/pytorch/pytorch/pull/54674)).\r\n* Added a fast path for a common case for `torch.addmm` on CUDA ([#55026](https://github.com/pytorch/pytorch/pull/55026)).\r\n* In collaboration with NVIDIA, the CUDA performance of many linear algebra operations has been improved by increasing use of the cuSOLVER and cuBLAS libraries\r\n    * Added cuBLAS support for `torch.triangular_solve` ([#53147](https://github.com/pytorch/pytorch/pull/53147)) and batched `torch.geqrf` ([#56253](https://github.com/pytorch/pytorch/pull/56253)).\r\n    * Added cuSOLVER support for `torch.linalg.eigh/eigvalsh` ([#53040](https://github.com/pytorch/pytorch/pull/53040)), `torch.cholesky_solve` ([#54315](https://github.com/pytorch/pytorch/pull/54315)), `torch.cholesky_inverse` ([#54676](https://github.com/pytorch/pytorch/pull/54676)), and `torch.linalg.q`r ([#56256](https://github.com/pytorch/pytorch/pull/56256)).\r\n    * Added cuBLAS and cuSOLVER support for `torch.linalg.lstsq` ([#57317](https://github.com/pytorch/pytorch/pull/57317)).\r\n* Improved performance for `torch.nonzero` ([#58468](https://github.com/pytorch/pytorch/pull/58468)).\r\n* Removed device check from a few indexing methods ([#58800](https://github.com/pytorch/pytorch/pull/58800)).\r\n\r\n### Complex Numbers\r\n\r\n* Added a faster path for `torch.is_complex()` by skipping unnecessary  dispatch ([#50054](https://github.com/pytorch/pytorch/pull/50054)).\r\n\r\n### Autograd\r\n\r\n* Sped up autograd\u2019s graph discovery algorithm by skipping some nodes using sequence number ([#52180](https://github.com/pytorch/pytorch/pull/52180), [#52057](https://github.com/pytorch/pytorch/pull/52057)).\r\n* Added a new fast gradcheck ([#54480](https://github.com/pytorch/pytorch/pull/54480)).\r\n\r\n### torch.nn\r\n\r\n* `Module.forward`: Add fast path for the case of no hooks ([#52576](https://github.com/pytorch/pytorch/pull/52576)).\r\n* Fix `mkldnn` heuristic for multithreaded convolution ([#52909](https://github.com/pytorch/pytorch/pull/52909)).\r\n* `linear`: Remove one refcount bump ([#54936](https://github.com/pytorch/pytorch/pull/54936)).\r\n* Improve `native_batch_norm_backward` performance on CUDA ([#58240](https://github.com/pytorch/pytorch/pull/58240)).\r\n* `nll_loss`: Use cascade summation on CPU ([#55841](https://github.com/pytorch/pytorch/pull/55841)).\r\n* `nn.BatchNorm1d`: Improve training performance on CPU ([#57033](https://github.com/pytorch/pytorch/pull/57033)).\r\n* Simplify convolution double backward gradInput formulas ([#54840](https://github.com/pytorch/pytorch/pull/54840)).\r\n* Move RNN cell size check to cpp ([#51964](https://github.com/pytorch/pytorch/pull/51964)).\r\n* Remove syncs in `one_hot` ([#57902](https://github.com/pytorch/pytorch/pull/57902)).\r\n* Enable and enhance bf16 threshold ([#54384](https://github.com/pytorch/pytorch/pull/54384)).\r\n* `nn.Conv3d`: Enable `channels_last_3d` for cuDNN ([#48430](https://github.com/pytorch/pytorch/pull/48430)).\r\n* Increase token count threshold for calling thrust sort in embedding backward ([#49913](https://github.com/pytorch/pytorch/pull/49913)).\r\n* CPU convolution benchmark harness for some popular models ([#56455](https://github.com/pytorch/pytorch/pull/56455)).\r\n* Improved performance for `torch.nn.BatchNorm1d` on both CPU and CUDA ([#57033](https://github.com/pytorch/pytorch/pull/57033), [#57786](https://github.com/pytorch/pytorch/pull/57786)).\r\n* Added optimized generic interpolation for `torch.nn.functional.{upsample_nearest`, `upsample_bicubic}` and speed up for channels first and last cases ([#54500](https://github.com/pytorch/pytorch/pull/54500)).\r\n* Added shape documentation for CosineEmbeddingLoss ([#58403](https://github.com/pytorch/pytorch/pull/58403)).\r\n\r\n### C++ API\r\n\r\n* Fixed nest openmp performance bug in `thnn_conv2d` ([#52577](https://github.com/pytorch/pytorch/pull/52577)).\r\n* Added c10::MaybeOwned and Tensor::expect_contiguous ([#53317](https://github.com/pytorch/pytorch/pull/53317))\r\n* Added DimVector variant of infer_size ([#54882](https://github.com/pytorch/pytorch/pull/54882))\r\n* Added logic to use `DimVector` for inputs to `as_strided `that don't grow dim ([#55016](https://github.com/pytorch/pytorch/pull/55016)).\r\n* Reduce ref-counting by borrowing in/out Tensors in TensorIterator ([#55690](https://github.com/pytorch/pytorch/pull/55690)).\r\n* Reduce ref-counting by migrating add operators to borrow Tensors in TensorIteratorBase ([#55691](https://github.com/pytorch/pytorch/pull/55691)).\r\n* Reduce ref-counting by migrating copy_ operators to borrow input/output Tensors ([#56031](https://github.com/pytorch/pytorch/pull/56031)).\r\n* Added logic to use `expect_contiguous` in `layer_norm` ([#58067](https://github.com/pytorch/pytorch/pull/58067)).\r\n\r\n### CUDA\r\n\r\n* Construct only necessary elements in OffsetCalculator ([#55107](https://github.com/pytorch/pytorch/pull/55107)).\r\n* Migrated `torch.index_put` to use cub instead of thrust ([#55693](https://github.com/pytorch/pytorch/pull/55693)).\r\n* Added cuSOLVER `potrf` and `potrfBatched` to the backend of `torch.cholesky_decomposition` ([#53104](https://github.com/pytorch/pytorch/pull/53104)).\r\n* Implemented `torch.sort` with cub::DeviceSegmentedRadixSort ([#56821](https://github.com/pytorch/pytorch/pull/56821)).\r\n* Added cuSOLVER path for `torch.geqrf` ([#56252](https://github.com/pytorch/pytorch/pull/56252)).\r\n* Enabled cuSOLVER `torch.potrf` batched for Cholesky decomposition when CUDA >= 11.3 ([#57788](https://github.com/pytorch/pytorch/pull/57788)).\r\n* Fewer CUDA sync in unique by using cub instead of thrust ([#57323](https://github.com/pytorch/pytorch/pull/57323)).\r\n* Removed sync for `randperm` on small tensors ([#54113](https://github.com/pytorch/pytorch/pull/54113)).\r\n* Simplify convolution double backward gradInput formulas ([#54840](https://github.com/pytorch/pytorch/pull/54840)).\r\n\r\n### Composability\r\n\r\n* We\u2019ve landed lots of performance optimizations for 1.9, both large and small. See individual PRs for details:\r\n    * Inline `tensor.device()` ([#50848](https://github.com/pytorch/pytorch/pull/50848))\r\n    * Skip a second call to `shouldUseRecordFunction` for BackendSelect ops ([#50891](https://github.com/pytorch/pytorch/pull/50891))\r\n    * Re-order `TensorImpl` fields to save a word ([#50920](https://github.com/pytorch/pytorch/pull/50920))\r\n    * Devirtualize `TensorImpl::storage()` ([#51050](https://github.com/pytorch/pytorch/pull/51050))\r\n    * Reduce template expansion in `call_functor_with_args_from_stack` (build time) ([#51313](https://github.com/pytorch/pytorch/pull/51313))\r\n    * Eliminate `WrapFunctionIntoRuntimeFunctor `use in CppFunction constructors ([#51315](https://github.com/pytorch/pytorch/pull/51315))\r\n    * Remove `reference_cast` in `make_boxed_from_unboxed_functor` (build time) ([#51319](https://github.com/pytorch/pytorch/pull/51319))\r\n    * Debug-gate `static_assert` in `KernelFunction::makeFromUnboxedFunctor` (build time) ([#51367](https://github.com/pytorch/pytorch/pull/51367))\r\n    * Use real `if constexpr` behind macro in hot template (build time) ([#51368](https://github.com/pytorch/pytorch/pull/51368), [#52420](https://github.com/pytorch/pytorch/pull/52420))\r\n    * Outline `DispatchStub::get_call_ptr()` ([#51908](https://github.com/pytorch/pytorch/pull/51908))\r\n    * Use `torchCheckFail` in `TORCH_INTERNAL_ASSERT` ([#52086](https://github.com/pytorch/pytorch/pull/52086))\r\n    * Add `Storage::set_data_ptr_noswap` and use where possible ([#52244](https://github.com/pytorch/pytorch/pull/52244))\r\n    * Make shared empty string static instead of thread_local ([#52220](https://github.com/pytorch/pytorch/pull/52220))\r\n    * Avoid `std::string` in `TORCH_CHECK` when possible ([#52221](https://github.com/pytorch/pytorch/pull/52221))\r\n    * Make `c10::str(const char*)` return `const char*` ([#52222](https://github.com/pytorch/pytorch/pull/52222))\r\n    * Sync `TORCH_INTERNAL_ASSERT` optimizations with `TORCH_CHECK` ([#52226](https://github.com/pytorch/pytorch/pull/52226))\r\n    * Save a single add instruction in the dispatcher ([#52543](https://github.com/pytorch/pytorch/pull/52543))\r\n    * Inline `TensorIteratorConfig` setters ([#52661](https://github.com/pytorch/pytorch/pull/52661))\r\n    * Use `DimVector` for sizes and strides in `view` ([#53001](https://github.com/pytorch/pytorch/pull/53001))\r\n    * Avoid TLS in `has_names` ([#53003](https://github.com/pytorch/pytorch/pull/53003))\r\n    * Don't inline `Dispatcher::call` on mobile (binary size) ([#53197](https://github.com/pytorch/pytorch/pull/53197))\r\n    * Skip dispatch for `is_floating_point` ([#53242](https://github.com/pytorch/pytorch/pull/53242))\r\n    * Move non-template part of `TensorImpl::Resize` to cpp (binary size, build time) ([#53388](https://github.com/pytorch/pytorch/pull/53388))\r\n    * Don't copy vector arguments to `Tensor::Resize` ([#53389](https://github.com/pytorch/pytorch/pull/53389))\r\n    * Skip dispatch trip for CPU in `resize_` ([#53575](https://github.com/pytorch/pytorch/pull/53575))\r\n    * Pass `Scalar` by reference ([#53583](https://github.com/pytorch/pytorch/pull/53583))\r\n    * Don't use static for template declarations in headers (binary size) ([#53602](https://github.com/pytorch/pytorch/pull/53602))\r\n    * Boxing logic forwards arguments to stack ([#53624](https://github.com/pytorch/pytorch/pull/53624))\r\n    * `Speed up Tensor::data_ptr by using static item size (`[`#53723`](https://github.com/pytorch/pytorch/pull/53723)`)`\r\n    * `Skip dispatch for is_signed (`[`#53847`](https://github.com/pytorch/pytorch/pull/53847)`)`\r\n    * Allow inlining of more Tensor methods ([#53905](https://github.com/pytorch/pytorch/pull/53905))\r\n    * `Tensor::register_hook`: Avoid wrapping hook in two levels of `std::function` ([#53917](https://github.com/pytorch/pytorch/pull/53917))\r\n    * Take advantage of string literals in `TORCH_WARN` ([#54032](https://github.com/pytorch/pytorch/pull/54032))\r\n    * Inline `Tensor` keyset-checking methods & similar getters ([#54806](https://github.com/pytorch/pytorch/pull/54806))\r\n    * `TensorIterator::output` returns const reference ([#54811](https://github.com/pytorch/pytorch/pull/54811))\r\n    * Avoid refcount bump in `TensorArg` ([#54934](https://github.com/pytorch/pytorch/pull/54934))\r\n    * Move `Tensor::has_names` inline ([#54965](https://github.com/pytorch/pytorch/pull/54965))\r\n    * `OperandInfo` ctor should take rvalue reference ([#54972](https://github.com/pytorch/pytorch/pull/54972))\r\n    * Don't bother with `SmallVector` in `TensorMaker` ([#55125](https://github.com/pytorch/pytorch/pull/55125))\r\n    * Eliminate device guard in generic dispatch key kernel wrappers ([#55131](https://github.com/pytorch/pytorch/pull/55131))\r\n    * Move logic to skip a redispatch directly inside of `resize_output` ([#55162](https://github.com/pytorch/pytorch/pull/55162))\r\n    * Use `infer_size_dimvector` in `ExpandUtils` ([#55180](https://github.com/pytorch/pytorch/pull/55180))\r\n    * Don't create intermediate Tensor for `at::result_type` w/Scalar ([#55232](https://github.com/pytorch/pytorch/pull/55232))\r\n    * Use `sizes()[x]` instead of `size(x)` in `addr` ([#55247](https://github.com/pytorch/pytorch/pull/55247))\r\n    * Add & use `inferExpandGeometry_dimvector` ([#55316](https://github.com/pytorch/pytorch/pull/55316))\r\n    * Mark borrowed case as `C10_LIKELY` in `MaybeOwned` ([#55553](https://github.com/pytorch/pytorch/pull/55553))\r\n    * Avoid double indirection in `MaybeOwned`'s borrowed state ([#55685](https://github.com/pytorch/pytorch/pull/55685))\r\n    * Make `VariableVersion::DISABLED` the default constructor for `VariableVersion`. ([#55572](https://github.com/pytorch/pytorch/pull/55572))\r\n    * Don't set `version_counter` on inference tensor for `unsafe_` ops. ([#55819](https://github.com/pytorch/pytorch/pull/55819))\r\n    * Add & document `borrow_from_optional_tensor` ([#56647](https://github.com/pytorch/pytorch/pull/56647))\r\n    * Migrate hacky wrapper removal to `borrow_from_optional_tensor` ([#56648](https://github.com/pytorch/pytorch/pull/56648))\r\n    * Optimize `at::repeat` ([#56994](https://github.com/pytorch/pytorch/pull/56994))\r\n    * Optimize `intrusive_ptr(TTarget*) ` ctor (`pybind`) ([#57053](https://github.com/pytorch/pytorch/pull/57053))\r\n\r\n### torch.fx\r\n\r\n* Use precompiled regex in graph name processing ([#52853](https://github.com/pytorch/pytorch/pull/52853)).\r\n* Optimize module path finding in `Tracer` ([#52990](https://github.com/pytorch/pytorch/pull/52990)).\r\n* Speed up `_Namespace.create_name` ([#55580](https://github.com/pytorch/pytorch/pull/55580)).\r\n\r\n### Profiler\r\n\r\n* Sped up post processing ([#58021](https://github.com/pytorch/pytorch/pull/58021)).\r\n\r\n### TorchScript\r\n\r\n* Generate arithmetic vs logical right shift as appropriate ([#51749](https://github.com/pytorch/pytorch/pull/51749))\r\n* Introduced likely/unlikely `CompareSelect` hint ([#51751](https://github.com/pytorch/pytorch/pull/51751)).\r\n* Implemented log approximation using the VML approach ([#51752](https://github.com/pytorch/pytorch/pull/51752)).\r\n* Updated `TensorExpr` to use `LLVM` as the default backend ([#52314](https://github.com/pytorch/pytorch/pull/52314)).\r\n* Added support for `aten::hardtanh` (a hot operation in mobilenet v2/v3) ([#52394](https://github.com/pytorch/pytorch/pull/52394))\r\n* Implemented `hardtanh` ([#57750](https://github.com/pytorch/pytorch/pull/57750)).\r\n* Add `aten::batch_norm` into fuser when in inference mode ([#54204](https://github.com/pytorch/pytorch/pull/54204)).\r\n* NNC\r\n    * Added a new API to perform loop fusion ([#54461](https://github.com/pytorch/pytorch/pull/54461)).\r\n    * Implemented depthwise `conv2d` ([#54920](https://github.com/pytorch/pytorch/pull/54920)).\r\n    * Integrated NNC `conv2d` with fuser ([#55213](https://github.com/pytorch/pytorch/pull/55213)).\r\n    * Added logic to use NNC to generate `logit`, `relu` and `tanh` ([#52322](https://github.com/pytorch/pytorch/pull/52322)).\r\n    * Use VML-inspired logarithm with NNC, tweak scheduling ([#52423](https://github.com/pytorch/pytorch/pull/52423)).\r\n    * Generate `sigmoid` with NNC ([#52424](https://github.com/pytorch/pytorch/pull/52424)).\r\n    * Enabled CPU fusion only when `num_threads == 1` ([#56120](https://github.com/pytorch/pytorch/pull/56120)).\r\n    * Use NNC's `call_raw` API to reduce call overheads. ([#57553](https://github.com/pytorch/pytorch/pull/57553)).\r\n    * Started codegen\u2019ing some external calls ([#58118](https://github.com/pytorch/pytorch/pull/58118)).\r\n* Reduce memory use for inference path in `OneDNN MaxPooling` ([#52728](https://github.com/pytorch/pytorch/pull/52728)).\r\n* Removed redundant `gather_ranges` when fusing ([#53323](https://github.com/pytorch/pytorch/pull/53323)).\r\n* Optimized `sigrid_hash` ([#53065](https://github.com/pytorch/pytorch/pull/53065)).\r\n* Updated `create_empty_from` to directly use the native version of `at::empty` ([#53216](https://github.com/pytorch/pytorch/pull/53216)).\r\n* Added a minimum fusion group size ([#50217](https://github.com/pytorch/pytorch/pull/50217)).\r\n* Added CUDNN `Conv-Add-Relu` fusion for Frozen Model Optimization ([#52102](https://github.com/pytorch/pytorch/pull/52102)).\r\n* Avoid dispatch overhead in call to MKLDNN convolution ([#52614](https://github.com/pytorch/pytorch/pull/52614)).\r\n* Added re-inplacing to MKLDNN subgraphs ([#53908](https://github.com/pytorch/pytorch/pull/53908)).\r\n* Set `requires_gradient` to help autodiff prune unneeded gradients ([#54374](https://github.com/pytorch/pytorch/pull/54374)).\r\n* Use type cache in erasing shape information ([#55828](https://github.com/pytorch/pytorch/pull/55828)).\r\n* Added heuristic to avoid perf incompatible MKLDNN formats for binary ops ([#56089](https://github.com/pytorch/pytorch/pull/56089))\r\n* Added `adaptive_avgpool2d` to the set of fusible ops ([#56180](https://github.com/pytorch/pytorch/pull/56180)).\r\n* Lazily initialize `AliasDb` in `remove_mutation` opt ([#55949](https://github.com/pytorch/pytorch/pull/55949))\r\n* Made DataPtr extraction in CUDAFuture faster for Python values ([#56918](https://github.com/pytorch/pytorch/pull/56918)).\r\n* Lazily initialize `AliasDb` in DCE ([#56649](https://github.com/pytorch/pytorch/pull/56649)).\r\n* Add explicit checks for in-place ops in `ReplaceWithCopy` ([#54657](https://github.com/pytorch/pytorch/pull/54657)).\r\n    \r\n\r\n### Quantization\r\n\r\n* Optimized quantized `torch.cat` ([#54813](https://github.com/pytorch/pytorch/pull/54813)).\r\n\r\n### Mobile\r\n\r\n* Enabled `QNNPACK` for Apple Silicon builds ([#52308](https://github.com/pytorch/pytorch/pull/52308)).\r\n* Sped up model loading for per-channel quantized models using `QNNPACK` ([#53726](https://github.com/pytorch/pytorch/pull/53726)).\r\n* Added `XNNPACK` implementations for various operationss (`hardswish, global average pool`) ([#56714](https://github.com/pytorch/pytorch/pull/56714), [#56715](https://github.com/pytorch/pytorch/pull/56715), [#55791](https://github.com/pytorch/pytorch/pull/55791)).\r\n* Made various performance improvements for iOS GPU (Metal) ([#57664](https://github.com/pytorch/pytorch/pull/57664), [#57665](https://github.com/pytorch/pytorch/pull/57665), [#57666](https://github.com/pytorch/pytorch/pull/57666), [#57667](https://github.com/pytorch/pytorch/pull/57667), [#57668](https://github.com/pytorch/pytorch/pull/57668)).\r\n\r\n### Distributed\r\n\r\n`torch.distributed`\r\n\r\n* Avoid 2 extra copies when reducing sparse tensors ([#57822](https://github.com/pytorch/pytorch/pull/57822))\r\n\r\n### Vulkan\r\n\r\n* Switched to a more performant implementation of matrix multiplication ([#49609](https://github.com/pytorch/pytorch/pull/49609)).\r\n* Updated the version of Vulkan Memory Allocator used ([#52938](https://github.com/pytorch/pytorch/pull/52938)).\r\n* Increased the command buffer submission rate ([#57196](https://github.com/pytorch/pytorch/pull/57196)).\r\n* Updated the Vulkan tensors to use 2D textures whenever possible, instead of always using 3D textures ([#57198](https://github.com/pytorch/pytorch/pull/57198)).\r\n* Updated convolution shaders to receive the bias tensor as a texture as opposed to a buffer ([#57201](https://github.com/pytorch/pytorch/pull/57201)).\r\n\r\n# Docs\r\n\r\n### Python API\r\n\r\n* Added `torch.testing` docs ([#57247](https://github.com/pytorch/pytorch/pull/57247)).\r\n* Updated docs to mention CUDA support for Future ([#50048](https://github.com/pytorch/pytorch/pull/50048)).\r\n* Included `memory_format` , an already accepted argument, in `torch.empty` doc ([#54664](https://github.com/pytorch/pytorch/pull/54664)).\r\n* Improved the documentation for torch.matrix_exp() ([#55626](https://github.com/pytorch/pytorch/pull/55626)).\r\n* Updated use_deterministic_algorithms docs ([#55413](https://github.com/pytorch/pytorch/pull/55413)).\r\n* Added the `generator`  argument to `torch.rand` and `torch.randn` docs ([#56242](https://github.com/pytorch/pytorch/pull/56242)).\r\n* Added an example to show how to use learning rate schedulers in Optimizers ([#56705](https://github.com/pytorch/pytorch/pull/56705)).\r\n* Corrected the torch.ceil formula in docs ([#55039](https://github.com/pytorch/pytorch/pull/55039))\r\n* Fixed docs to use autosummary on tensors.rst ([#55042](https://github.com/pytorch/pytorch/pull/55042))\r\n* Improved testing documentation in `CONTRIBUTING.md` ([#54904](https://github.com/pytorch/pytorch/pull/54904))\r\n* Updated `torch.fft` docs to include `out=` argument ([#56732](https://github.com/pytorch/pytorch/pull/56732)).\r\n* Updated rounding_mode documentation to remove `\"true\"` ([#52202](https://github.com/pytorch/pytorch/pull/52202)).\r\n* Added a note about error handling for non-chained futures ([#53212](https://github.com/pytorch/pytorch/pull/53212)).\r\n* Updated `torch.stft` documentation to clarify output shape ([#54877](https://github.com/pytorch/pytorch/pull/54877)).\r\n* Added an example for `torch.is_tensor` and `torch.is_storage` ([#55052](https://github.com/pytorch/pytorch/pull/55052)).\r\n\r\n### Autograd\r\n\r\n* Added a note describing gradcheck internals ([#55966](https://github.com/pytorch/pytorch/pull/55966)).\r\n* Split up autograd documentation into separate pages ([#55672](https://github.com/pytorch/pytorch/pull/55672)).\r\n* `torch.utils.checkpoint` : Updated docs to state that `input` flag in `.backward()` is disallowed when checkpointing ([#51746](https://github.com/pytorch/pytorch/pull/51746)).\r\n* Added section in autograd mechanics note describing how to use inference/no_grad ([#58513](https://github.com/pytorch/pytorch/pull/58513)).\r\n* Added doc string for `torch.is_inference_mode_enabled` and `torch.is_grad_enabled` ([#59047](https://github.com/pytorch/pytorch/pull/59047)).\r\n* Added no-grad inference mode note ([#58513](https://github.com/pytorch/pytorch/pull/58513)).\r\n* Add docstring for is_inference_mode_enabled ([#59047](https://github.com/pytorch/pytorch/pull/59047)).\r\n\r\n### torch.nn\r\n\r\n* `nn.TripletMarginLoss` / `torch.reciprocal`: Fix formatting in docs ([#51650](https://github.com/pytorch/pytorch/pull/51650))\r\n* `nn.FractionalMaxPool3d`: Add to pooling layer docs ([#52556](https://github.com/pytorch/pytorch/pull/52556))\r\n* `F.fractional_max_pool`: Add to `nn.functional` docs ([#52557](https://github.com/pytorch/pytorch/pull/52557))\r\n* `Module.share_memory`: Add link to `Tensor.share_memory_` in docs ([#52561](https://github.com/pytorch/pytorch/pull/52561))\r\n* `nn.SiLU`: Mention alternative name of Swish within docs ([#53239](https://github.com/pytorch/pytorch/pull/53239))\r\n* Remove redundant hardsigmoid() in docstring to show up `inplace` parameter ([#52559](https://github.com/pytorch/pytorch/pull/52559))\r\n* Clarify docs for lazy modules ([#53495](https://github.com/pytorch/pytorch/pull/53495))\r\n* `torch.nn`: Grammatically update docs ([#54370](https://github.com/pytorch/pytorch/pull/54370))\r\n* `nn.Sequential`: Expand docs, including comparison with `nn.ModuleList` ([#53380](https://github.com/pytorch/pytorch/pull/53380))\r\n* `F.embedding_bag`: Fix formatting in docs ([#54666](https://github.com/pytorch/pytorch/pull/54666))\r\n* `F.group_norm`: Add to docs ([#54673](https://github.com/pytorch/pytorch/pull/54673))\r\n* Add separate autosummary for flatten layer docs ([#54663](https://github.com/pytorch/pytorch/pull/54663))\r\n* `LazyModuleMixin`: Add missing attr in docs to improve formatting ([#53363](https://github.com/pytorch/pytorch/pull/53363))\r\n* `conv1d`: Fix example error in docs ([#57356](https://github.com/pytorch/pytorch/pull/57356))\r\n* `nn.functional`: Split docs into a table-of-contents page and a sub-page per function ([#55038](https://github.com/pytorch/pytorch/pull/55038))\r\n* `nn.LSTM` / `nn.RNN` / `nn.GRU`: Clarify `batch_first` behavior ([#58809](https://github.com/pytorch/pytorch/pull/58809))\r\n* `nn.CosineEmbeddingLoss`: Add shape info to docs ([#58403](https://github.com/pytorch/pytorch/pull/58403))\r\n* Add doc warnings for default SELU gain ([#54057](https://github.com/pytorch/pytorch/pull/54057)).\r\n* Clarify batch_first behavior for `nn.LSTM, nn.RNN, and nn.GRU` ([#58809](https://github.com/pytorch/pytorch/pull/58809)).\r\n* Add UninitializedBuffer to nn docs ( [#59021](https://github.com/pytorch/pytorch/pull/59021)).\r\n* Document factory_kwargs in nn.Quantize + remove Attributes section ([#59025](https://github.com/pytorch/pytorch/pull/59025)).\r\n\r\n### Dataloader\r\n\r\n* Added DataPipes Typing Doc ([#54773](https://github.com/pytorch/pytorch/pull/54773)).\r\n* Added docs to document the default NumPy seed for DataLoader workers ([#56528](https://github.com/pytorch/pytorch/pull/56528)).\r\n\r\n### AMD\r\n\r\n* Added HIP semantics doc ([#57871](https://github.com/pytorch/pytorch/pull/57871)).\r\n\r\n### CUDA\r\n\r\n* Added `scatter_add` to amp docs ([#54908](https://github.com/pytorch/pytorch/pull/54908)) \r\n* Added `reset_peak_memory_stats` in cuda.rst ([#54668](https://github.com/pytorch/pytorch/pull/54668)).\r\n\r\n### torch.fx\r\n\r\n* Make some modifications to limitation section ([#51928](https://github.com/pytorch/pytorch/pull/51928))\r\n* Added docstring for concrete_args on `Tracer.trace` ([#53151](https://github.com/pytorch/pytorch/pull/53151)).\r\n* Change Dynamic Control Flow example to a *more* dynamic version ([#53250](https://github.com/pytorch/pytorch/pull/53250)).\r\n* Render inherited methods in fx.Tracer API reference ([#53630](https://github.com/pytorch/pytorch/pull/53630)).\r\n* Add docs for `ShapeProp` ([#54554](https://github.com/pytorch/pytorch/pull/54554)).\r\n* Hide module paths leaking in the documentation. ([#54585](https://github.com/pytorch/pytorch/pull/54585)).\r\n\r\n### Profiler\r\n\r\n* Updated profiler recipe doc (https://github.com/pytorch/tutorials/pull/1528).\r\n\r\n### TorchScript\r\n\r\n* Added NNC IR specification ([#52912](https://github.com/pytorch/pytorch/pull/52912)).\r\n* Added starter content for new TorchScript language reference ([#53837](https://github.com/pytorch/pytorch/pull/53837)).\r\n* Added documentation for `torch.jit.Attribute` and `torch.jit.annotate` ([#54485](https://github.com/pytorch/pytorch/pull/54485)).\r\n* Updated TorchScript language reference section for types ([#53673](https://github.com/pytorch/pytorch/pull/53673)).\r\n* Documented the TorchScript type system ([#53244](https://github.com/pytorch/pytorch/pull/53244)).\r\n* Added language reference for Python builtin functions, statements,  and values in TorchScript ([#52847](https://github.com/pytorch/pytorch/pull/52847), [#52830](https://github.com/pytorch/pytorch/pull/52830)).\r\n* Added `torch.*` API section for TorchScript language reference ([#53236](https://github.com/pytorch/pytorch/pull/53236)).\r\n* Added \u201cConditionals in TE\u201d doc ([#56949](https://github.com/pytorch/pytorch/pull/56949)).\r\n    \r\n\r\n### torch.package\r\n\r\n* Added API reference ([#55812](https://github.com/pytorch/pytorch/pull/55812), [#56547](https://github.com/pytorch/pytorch/pull/56547)).\r\n* Add explanation, tutorial, and preamble sections for `torch.package` ([#59833](https://github.com/pytorch/pytorch/pull/59833), [#59503](https://github.com/pytorch/pytorch/pull/59503), [#59499](https://github.com/pytorch/pytorch/pull/59499), [#59491](https://github.com/pytorch/pytorch/pull/59491), [#59842](https://github.com/pytorch/pytorch/pull/59842), [#59843](https://github.com/pytorch/pytorch/pull/59843), [#59602](https://github.com/pytorch/pytorch/pull/59602)).\r\n* Add pickle security warning to package docs ([#59959](https://github.com/pytorch/pytorch/pull/59959)).\r\n\r\n### Quantization\r\n\r\n* Added docs for storage and tensors for quantized Tensor ([#51817](https://github.com/pytorch/pytorch/pull/51817)).\r\n* Fixed FX Graph Mode Quantization tutorial link ([#54715](https://github.com/pytorch/pytorch/pull/54715)).\r\n* Added fx graph mode quant api doc ([#55306](https://github.com/pytorch/pytorch/pull/55306)).\r\n* FX Graph Mode Quantization - fixed preamble ([#52192](https://github.com/pytorch/pytorch/pull/52192)).\r\n* Fixed broken link to fx graph quant guide in quantization.rst ([#56776](https://github.com/pytorch/pytorch/pull/56776)).\r\n\r\n### Mobile\r\n\r\n* Added doc string for lite interpreter related API in Android ([#53136](https://github.com/pytorch/pytorch/pull/53136)).\r\n* Improved `export_opnames` Documentation ([#52333](https://github.com/pytorch/pytorch/pull/52333)).\r\n\r\n### Distributed\r\n\r\n`torch.distributed.Store`\r\n\r\n* Documentation for TCPStore\u2019s `compare_set` API ([#57203](https://github.com/pytorch/pytorch/pull/57203))\r\n\r\n`torch.distributed.optim`\r\n\r\n* Update distributed optimizer documentation ([#58084](https://github.com/pytorch/pytorch/pull/58084))\r\n* Update and expose ZeroRedundancyOptimizer docs ([#53112](https://github.com/pytorch/pytorch/pull/53112), [#53113](https://github.com/pytorch/pytorch/pull/53113))\r\n\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Upstream `torchelastic` documentation to PyTorch. ([#56811](https://github.com/pytorch/pytorch/pull/56811))\r\n* Revise the note section of RendezvousHandler doc ([#57723](https://github.com/pytorch/pytorch/pull/57723))\r\n* Update the rendezvous documentation ([#57973](https://github.com/pytorch/pytorch/pull/57973))\r\n\r\n\r\n`DistributedDataParallel`\r\n\r\n* Add register_comm_hook API to DDP communication hooks documentation page ([#51846](https://github.com/pytorch/pytorch/pull/51846),[](https://github.com/pytorch/pytorch/pull/51986)[#51986](https://github.com/pytorch/pytorch/pull/51986))\r\n* Enhance documentation around `DistributedDataParallel` uneven input support ([#57448](https://github.com/pytorch/pytorch/pull/57448))\r\n* Enhance communication hook documentation ([#58170](https://github.com/pytorch/pytorch/pull/58170), [#58168](https://github.com/pytorch/pytorch/pull/58168), [#53253](https://github.com/pytorch/pytorch/pull/53253), [#53855](https://github.com/pytorch/pytorch/pull/53855), [#53596,](https://github.com/pytorch/pytorch/pull/53596)[#53955](https://github.com/pytorch/pytorch/pull/53955), [#54052](https://github.com/pytorch/pytorch/pull/54052). [#55031](https://github.com/pytorch/pytorch/pull/55031))\r\n\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Add a disclaimer about limited CUDA support in RPC ([#58023](https://github.com/pytorch/pytorch/pull/58023)) \r\n* `torch.distributed.rpc`:  Add a link to the tutorial in RemoteModule docstring ([#57875](https://github.com/pytorch/pytorch/pull/57875))\r\n* `torch.distributed.rpc`:  Mentioned `RemoteModule` in RPC documentation ([#57876](https://github.com/pytorch/pytorch/pull/57876))\r\n\r\n\r\n`torch.distributed.nn.RemoteModule`\r\n\r\n* Add RemoteModule to master RPC docs. ([#53084](https://github.com/pytorch/pytorch/pull/53084))\r\n* Add `remote_parameters` and `get_module_rref` to RemoteModule docs. ([#54645](https://github.com/pytorch/pytorch/pull/54645))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* Enhance Pipe docs to explicitly mention RPC initialization. ([#55187](https://github.com/pytorch/pytorch/pull/55187))\r\n* Add tutorials to pipeline docs. ([#55209](https://github.com/pytorch/pytorch/pull/55209))\r\n\r\n`torch.distributed`\r\n\r\n* Update documentation for `get_future` support ([#58107](https://github.com/pytorch/pytorch/pull/58107))\r\n* Mention distributed profiling in documentation ([#58286](https://github.com/pytorch/pytorch/pull/58286))\r\n* Update distributed doc table for `alltoall`  ([#54277](https://github.com/pytorch/pytorch/pull/54277))\r\n*  fix docstring signature in `all_reduce_multigpu` ([#54665](https://github.com/pytorch/pytorch/pull/54665))\r\n* `torch.distributed`: Improve dist.new_group doc ([#55660](https://github.com/pytorch/pytorch/pull/55660))\r\n\r\n### ONNX\r\n\r\n* Updated ONNX documentation ([#51362](https://github.com/pytorch/pytorch/pull/51362)) ([#53313](https://github.com/pytorch/pytorch/pull/53313)).\r\n* Updated scripting docs ([#54634](https://github.com/pytorch/pytorch/pull/54634)) ([#54868](https://github.com/pytorch/pytorch/pull/54868)).\r\n* Fixed docstring signature of torch.{onnx,utils} ([#54662](https://github.com/pytorch/pytorch/pull/54662)).\r\n* onnx.symbolic_helper.parse_args: document and clean up ([#56956](https://github.com/pytorch/pytorch/pull/56956)) ([#57598](https://github.com/pytorch/pytorch/pull/57598)).\r\n\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.9.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.9.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.9.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/43927405", "release_id": 43927405, "date_created": "2021-06-11T20:43:27Z", "date_published": "2021-06-15T16:06:52Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/40418343", "tag": "v1.8.1", "name": "Small bug fix release ", "author": {"name": "albanD", "type": "User"}, "description": "# PyTorch 1.8.1 Release Notes\r\n\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Documentation\r\n\r\n# New Features\r\n\r\n### Revamp of profiling tools in `torch.profiler`\r\n\r\nThe [`torch.profiler`](https://pytorch.org/docs/stable/profiler.html) submodule is now available. It leveraged the newly released kineto library for profiling.\r\nYou can find more details in this blogpost: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/\r\n\r\n### Enable use of autocast for pytorch xla ([#48570](https://github.com/pytorch/pytorch/pull/48570))\r\n\r\nThe `torch.cuda.autocast` package can now be used in conjunction with torch xla to provide easy mixed-precision training.\r\n\r\n# Improvements\r\n\r\n* Make `torch.` submodule import more autocomplete-friendly ([#52339](https://github.com/pytorch/pytorch/pull/52339))\r\n* Add support in ONNX for `torch.{isinf,any,all}` ([#53529](https://github.com/pytorch/pytorch/pull/53529))\r\n* Replace thrust with cub in GPU implementation of `torch.randperm` for performance ([#54537](https://github.com/pytorch/pytorch/pull/54537))\r\n\r\n# Bug fixes\r\n\r\n## Misc\r\n\r\n* Fixes for `torch.distributions` validation checks ([](https://github.com/pytorch/pytorch/commit/e991cdaf58bda3169a284e2dead254262b450787)[#53763](https://github.com/pytorch/pytorch/pull/53763)[](https://github.com/pytorch/pytorch/commit/e991cdaf58bda3169a284e2dead254262b450787))\r\n* Allow changing the padding vector for `nn.Embedding` ([#53447](https://github.com/pytorch/pytorch/pull/53447))\r\n* Fix TensorPipe for large copies and interoperability with CUDA ([#53804](https://github.com/pytorch/pytorch/pull/53804))\r\n* Properly de-sugar `Ellipsis` in TorchScript ([#53766](https://github.com/pytorch/pytorch/pull/53766))\r\n* Stop using OneDNN for group convolutions when groups size is a multiple of `24` ([#54015](https://github.com/pytorch/pytorch/pull/54015))\r\n* Use `int8_t` instead of `char` in `{load,store}_scalar` ([#52616](https://github.com/pytorch/pytorch/pull/52616))\r\n* Make ideep honor `torch.set_num_thread` ([#53871](https://github.com/pytorch/pytorch/pull/53871))\r\n* Fix dimension out of range in `pixel_{un}shuffle` ([#54178](https://github.com/pytorch/pytorch/pull/54178))\r\n* Update kineto to fix libtorch builds ([#54205](https://github.com/pytorch/pytorch/pull/54205))\r\n* Fix distributed autograd CUDA stream synchronization for send/recv operations ([#54358](https://github.com/pytorch/pytorch/pull/54358))\r\n\r\n## ONNX\r\n\r\n* Update error handling in ONNX to avoid `ValueError` ([#53548](https://github.com/pytorch/pytorch/pull/53548))\r\n* Update assign output shape for nested structure and dict output ([#53311](https://github.com/pytorch/pytorch/pull/53311))\r\n* Update embedding export wrt `padding_idx` ([#53931](https://github.com/pytorch/pytorch/pull/53931))\r\n\r\n# Documentation\r\n\r\n* Doc update for `torch.fx` ([#53674](https://github.com/pytorch/pytorch/pull/53674))\r\n* Fix `distributed.rpc.options.TensorPipeRpcBackendOptions.set_device_map` ([#53508](https://github.com/pytorch/pytorch/pull/53508))\r\n* Update example for `nn.LSTMCell` ([#51983](https://github.com/pytorch/pytorch/pull/51983))\r\n* Update doc for the `padding_idx` argument for `nn.Embedding` ([#53809](https://github.com/pytorch/pytorch/pull/53809))\r\n* Update general doc template ([#54141](https://github.com/pytorch/pytorch/pull/54141))\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.8.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.8.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.8.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/40418343", "release_id": 40418343, "date_created": "2021-03-24T02:28:21Z", "date_published": "2021-03-25T16:07:08Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/39280362", "tag": "v1.8.0", "name": "PyTorch 1.8 Release, including Compiler and Distributed Training updates, New Mobile Tutorials and more", "author": {"name": "albanD", "type": "User"}, "description": "# PyTorch 1.8.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:\r\n\r\n1. Support for doing python to python functional transformations via `torch.fx`;\r\n2. Added or stabilized APIs to support FFTs (`torch.fft`), Linear Algebra functions (`torch.linalg`), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and\r\n3. Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression. See the full release notes [here](https://github.com/pytorch/pytorch/releases).\r\n\r\nAlong with 1.8, we are also releasing major updates to PyTorch libraries including [TorchCSPRNG](https://github.com/pytorch/csprng), [TorchVision](https://github.com/pytorch/vision), [TorchText](https://github.com/pytorch/text) and [TorchAudio](https://github.com/pytorch/audio). For more on the library releases, see the post [here](http://pytorch.org/blog/pytorch-1.8-new-library-releases). As previously noted, features in PyTorch releases are classified as Stable, Beta and Prototype. You can learn more about the definitions in the post [here](https://pytorch.org/blog/pytorch-1.8-new-library-releases).\r\n\r\nYou can find more details on all the highlighted features in the [PyTorch 1.8 Release blogpost](https://pytorch.org/blog/pytorch-1.8-released/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n### Fix Tensor inplace modulo in python ([#49390](https://github.com/pytorch/pytorch/pull/49390))\r\n\r\nInplace modulo in python `%=` was wrongfully done out of place for Tensors. This change fixes the behavior.\r\nPrevious code that was relying on this operation being done out of place should be updated to use the out of place version `t = t % other` instead of `t %= other`.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.7.1</th><th>1.8.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.arange(0, 10)\r\n>>> b = a\r\n>>> b %= 3\r\n>>> print(a)\r\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n>>> print(b)\r\ntensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.arange(0, 10)\r\n>>> b = a\r\n>>> b %= 3\r\n>>> print(a)\r\ntensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\r\n>>> print(b)\r\ntensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Standardize `torch.clamp` edge cases ([#43288](https://github.com/pytorch/pytorch/pull/43288))\r\n\r\nFor ease of exposition let `a_min` be the value of the \"min\" argument to clamp, and `a_max` be the value of the \"max\" argument to clamp.\r\n\r\nThis PR changes the behavior of torch.clamp to always compute `min(max(a, a_min), a_max)`. `torch.clamp` currently computes this in its vectorized CPU implementation but uses different approaches for other backends.\r\nThese implementations are the same when `a_min < a_max`, but divergent when `a_min > a_max`. This divergence is easily triggered:\r\n\r\n```python\r\n>>> t = torch.arange(200).to(torch.float)\r\n>>> torch.clamp(t, 4, 2)[0]\r\ntensor(2.)\r\n\r\n>>> torch.clamp(t.cuda(), 4, 2)[0]\r\ntensor(4., device='cuda:0')\r\n\r\n>>> torch.clamp(torch.tensor(0), 4, 2)\r\ntensor(4)\r\n```\r\n\r\nThis PR makes the behavior consistent with NumPy's `clip`. C++'s `std::clamp`'s behavior is undefined when `a_min > a_max`. Python has no standard clamp implementation.\r\n\r\n### Tensor deepcopy now properly copies the `.grad` field ([#50663](https://github.com/pytorch/pytorch/pull/50663))\r\n\r\nThe deepcopy protocol will now properly copy the `.grad` field of Tensors when it exists.\r\nThe old behavior can be recovered by setting the `.grad` field to `None` after doing the deepcopy.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.7.1</th><th>1.8.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.grad\r\ntensor([0.8883, 0.5765])\r\n>>> deepcopy(t).grad\r\nNone\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.grad\r\ntensor([0.8883, 0.5765])\r\n>>> deepcopy(t).grad\r\ntensor([0.8883, 0.5765])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Fix `torch.fmod` type promotion ([#47323](https://github.com/pytorch/pytorch/pull/47323), [#48278](https://github.com/pytorch/pytorch/pull/48278))\r\n\r\n1.7.1\r\nRaises RuntimeError for integral tensor and floating-point tensor.\r\nThe dtype of output is determined by the first input.\r\n\r\n```python\r\n>>> x = torch.arange(start=1, end=6, dtype=torch.int32) # tensor([1, 2, 3, 4, 5])\r\n>>> y = torch.arange(start=1.1, end=2.1, step=0.2, dtype=torch.float32) # tensor([1.1, 1.3, 1.5, 1.7, 1.9])\r\n>>> torch.fmod(x, y)\r\nRuntimeError: result type Float can't be cast to the desired output type Int\r\n>>> z = torch.arange(start=0.2, end=1.1, step=0.2, dtype=torch.float64) # tensor([0.2, 0.4, 0.6, 0.8, 1.], dtype=torch.float64)\r\n>>> torch.fmod(y, z).dtype\r\ntorch.float32\r\n>>> torch.fmod(z, y).dtype\r\ntorch.float64\r\n>>> torch.fmod(x, 1.2)\r\ntensor([0, 0, 0, 0, 0], dtype=torch.int32)\r\n```\r\n\r\n\r\n1.8.0:\r\nSupport integral tensor and floating-point tensor as inputs.\r\nThe dtype of output is determined by both inputs.\r\n\r\n```python\r\n>>> x = torch.arange(start=1, end=6, dtype=torch.int32) # tensor([1, 2, 3, 4, 5])\r\n>>> y = torch.arange(start=1.1, end=2.1, step=0.2, dtype=torch.float32) # tensor([1.1, 1.3, 1.5, 1.7, 1.9])\r\n>>> torch.fmod(x, y)\r\ntensor([1.0000, 0.7000, 0.0000, 0.6000, 1.2000])\r\n>>> z = torch.arange(start=0.2, end=1.1, step=0.2, dtype=torch.float64) # tensor([0.2, 0.4, 0.6, 0.8, 1.], dtype=torch.float64)\r\n>>> torch.fmod(y, z).dtype\r\ntorch.float64\r\n>>> torch.fmod(z, y).dtype\r\ntorch.float64\r\n>>> torch.fmod(x, 1.2)\r\ntensor([1.0000, 0.8000, 0.6000, 0.4000, 0.2000])\r\n```\r\n\r\n### Preserve non-dense or overlapping tensor's layout in *_like functions ([#46046](https://github.com/pytorch/pytorch/pull/46046))\r\n\r\nAll the `*_like` factory functions will now generate the same striding as out of place operations would.\r\nThis means in particular that non-contiguous tensors will produce non-contiguous outputs.\r\nIf you require a contiguous output, you can pass the `memory_format=torch.contiguous` keyword argument to the factory function. Such factory functions include `clone`, `to`, `float`, `cuda,` `*_like`, `zeros`, `rand{n}`, etc.\r\n\r\n### Make output of `torch.norm` and `torch.linalg.norm` consistent for complex inputs ([#48284](https://github.com/pytorch/pytorch/pull/48284))\r\n\r\nPreviously, when given a complex input, `torch.linalg.norm` and `torch.norm` would return a complex output. `torch.linalg.cond` would sometimes return a complex output and sometimes return a real output when given a complex input, depending on its `p` argument. This PR changes this behavior to match `numpy.linalg.norm` and `numpy.linalg.cond`, so that a complex input will result in a real number type, consistent with NumPy.\r\n\r\n### Make `torch.svd` return `V`, not `V.conj()` for complex inputs ([#51012](https://github.com/pytorch/pytorch/pull/51012))\r\n\r\n`torch.svd` added support for complex inputs in PyTorch 1.7, but was not documented as doing so. The complex `V` tensor returned was actually the complex conjugate of what's expected. This PR fixes the discrepancy.\r\nUsers that were already using the previous version of `torch.svd` with complex inputs can recover the previous behavior by taking the complex conjugate of the returned `V`.\r\n\r\n### `torch.angle`: properly handle pure real numbers ([#49163](https://github.com/pytorch/pytorch/pull/49163))\r\n\r\nThis PR updates PyTorch's `torch.angle` operator to be consistent with NumPy's. Previously `torch.angle` would return zero for all real inputs (including NaN). Now angle returns `pi` for negative real inputs, zero for non-negative real inputs, and propagates NaNs.\r\n\r\n### Enable distribution validation by default for `torch.distributions` ([#48743](https://github.com/pytorch/pytorch/pull/48743))\r\n\r\nThis may slightly slow down some models. Concerned users may disable validation by using `torch.distributions.Distribution.set_default_validate_args(False)` or by disabling individual distribution validation via `MyDistribution(..., validate_args=False)`.\r\n\r\nThis may cause new `ValueErrors` in models that rely on unsupported behavior, e.g. `Categorical.log_prob()` applied to continuous-valued tensors (only {0,1}-valued tensors are supported).\r\nSuch models should be fixed but the previous behavior can be recovered by disabling argument validation using the methods mentioned above.\r\n\r\n### Prohibit assignment to a sparse tensor ([#50040](https://github.com/pytorch/pytorch/pull/50040))\r\n\r\nAssigning to a sparse Tensor did not work properly and resulted in a no-op. The following code now properly raises an error:\r\n```python\r\n>>> t = torch.rand(10).to_sparse()\r\n>>> t[0] = 42\r\nTypeError: Cannot assign to a sparse tensor\r\n```\r\n\r\n### C++ API: operators that take a list of optional `Tensor`s cannot be called with `ArrayRef<Tensor>` anymore ([#49138](https://github.com/pytorch/pytorch/pull/49138))\r\n\r\nThis PR changes the C++ API representation of lists of optional Tensors (e.g. in the `Tensor::``index` method) from `ArrayRef<Tensor>` to  `List<optional<Tensor>>`. This change breaks backwards compatibility, since there is no implicit conversion from `ArrayRef<Tensor>` to `List<optional<Tensor>>`. \r\n\r\nA common call pattern is `tensor.index({indices_tensor})`, where `indices_tensor` is a `Tensor`. This will continue to work because the `{}` initializer_list constructor for `List<optional<Tensor>>` can take `Tensor` elements that are implicitly converted to `optional<Tensor>`. \r\n\r\nHowever, another common call pattern is `tensor.index(indices_tensor)`, where previously the `Tensor` got implicitly converted to an `ArrayRef<Tensor>`. To implicitly convert `Tensor` -> `optional<Tensor>` -> `List<optional<Tensor>>` would chain two implicit conversions, which C++ doesn't allow. So those call sites should be rewritten to use the  `tensor.index({indices_tensor})` pattern.\r\n\r\n### Autograd view creation informations are now properly propagated when views are chained\r\n\r\nAfter this fix, an error will properly be thrown to avoid wrong gradients when an in-place operation is performed on a view of a view, when in-place operation were not allowed on the first view.\r\nThis means that code that used to return wrong gradients in 1.7.1 (such as `t.unbind()[0].select(0, 0).add_(1)`) will now properly raise an error.\r\n\r\n### End of deprecation cycle for spectral ops in the torch. namespace ([#48594](https://github.com/pytorch/pytorch/pull/48594))\r\n\r\nThis PR removes the deprecated `torch.{fft,rfft,ifft,irfft}` and their corresponding methods on `torch.Tensor`. PyTorch programs using these functions must now update to use the `torch.fft` namespace.\r\n\r\n### `torch.digamma` : properly handle all inputs ([#48302](https://github.com/pytorch/pytorch/pull/48302))\r\n\r\nThis PR updates PyTorch's `torch.digamma` function to be consistent with SciPy's `special.digamma` function. This changes the result of the `torch.digamma` function on the nonpositive integers, where the gamma function is not defined. Since the gamma function is undefined at these points, the (typical) derivative of the logarithm of the gamma function is also undefined at these points, and for negative integers this PR updates `torch.digamma` to return `NaN`. For zero, however, it returns `-inf` to be consistent with SciPy.\r\n\r\nInterestingly, SciPy made a similar change, which was noticed by at least one user: [scipy/scipy#9663](https://github.com/scipy/scipy/issues/9663#issue-396587679)\r\n\r\nSciPy's returning of negative infinity at zero is intentional:\r\nhttps://github.com/scipy/scipy/blob/59347ae8b86bcc92c339efe213128f64ab6df98c/scipy/special/cephes/psi.c#L163\r\n\r\nThis change is consistent with the C++ standard for the gamma function:\r\nhttps://en.cppreference.com/w/cpp/numeric/math/tgamma\r\n\r\n### Fix `torch.remainder` type promotion ([#48668](https://github.com/pytorch/pytorch/pull/48668))\r\n\r\n1.7.1:\r\nIn the case where the second argument is a python number, the result is casted to the dtype of the first argument.\r\n\r\n```python\r\n>>> torch.remainder(x, 1.2)\r\ntensor([0, 0, 0, 0, 0], dtype=torch.int32)\r\n```\r\n\r\n\r\n1.8.0\r\nIn the case where the second argument is a python number, the dtype of result is determined by type promotion of both inputs.\r\n\r\n```python\r\n>>> torch.remainder(x, 1.2)\r\ntensor([1.0000, 0.8000, 0.6000, 0.4000, 0.2000])\r\n```\r\n\r\n### Changes to onnx export API to better handle named arguments ([#47367](https://github.com/pytorch/pytorch/pull/47367))\r\n\r\nThe `args` input argument of the `torch.onnx.export` function is updated to better support optional arguments. An optional dictionary can be passed in addition as the last argument in the `args` tuple, specifying inputs with the corresponding named parameter. Note that this is backward breaking for cases where the last input is also of a dictionary type. In the new API, for such cases, it is mandatory to have an empty dictionary as the last argument in the `args` tuple.\r\nMore details can be found at: https://pytorch.org/docs/1.8.0/onnx.html?highlight=onnx#using-dictionaries-to-handle-named-arguments-as-model-inputs.\r\n\r\n### Update signature of `torch.quantization.quantize` function [#48537](https://github.com/pytorch/pytorch/pull/48537)\r\n\r\nThe `run_args` argument must now contain a list or tuple containing the positional arguments, even if there is only a single argument.\r\nIn particular, code like: `qmodel = quantize(float_model, default_eval_fn, img_data)` that was working in 1.7.1 will now raise the error: `TypeError: default_eval_fn() takes 2 positional arguments but 3 were given`.\r\nYou should update this code to provide the image in a list for example: `qmodel = quantize(float_model, default_eval_fn, [img_data])`\r\n\r\n### Change the way we quantize relu, leaky relu and sigmoid([#47415](https://github.com/pytorch/pytorch/pull/47415), [#48038](https://github.com/pytorch/pytorch/pull/48038), [#45702,](https://github.com/pytorch/pytorch/pull/45702)[#45711](https://github.com/pytorch/pytorch/pull/45711), [#45883](https://github.com/pytorch/pytorch/pull/45883) [#45883](https://github.com/pytorch/pytorch/pull/45883), [#45882](https://github.com/pytorch/pytorch/pull/45882), [#47660](https://github.com/pytorch/pytorch/pull/47660)**)**\r\n\r\nStarting with version 1.8.0, in the eager mode quantization flow, relu is not observed anymore as it is not needed.\r\nIn previous versions, quantized `leaky_relu` and `sigmoid` did not require observation and just inherited the quantization parameters from their input, but that does not work very well in eager mode quantization. Starting with version 1.8.0, they are observed operator so that they work better in eager mode quantization.\r\n\r\n### Update direction numbers to 21201 dims in the SobolEngine ([#49710](https://github.com/pytorch/pytorch/pull/49710))\r\n\r\nThis update is BC-breaking because the values drawn by the engine will be different from the ones drawn in 1.7.1 even with the same seed.\r\n\r\n\r\n<p align=\"center\">\r\n  <table  align=\"center\">\r\n    <tr><th>1.7.1</th><th>1.8.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> from torch.quasirandom import SobolEngine\r\n>>> eng = SobolEngine(1)\r\n>>> eng.draw(3)\r\ntensor([[0.5000],\r\n            [0.7500],\r\n            [0.2500]])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> from torch.quasirandom import SobolEngine\r\n>>> eng = SobolEngine(1)\r\n>>> eng.draw(3)\r\ntensor([[0.0000],\r\n            [0.5000],\r\n            [0.7500]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### Deprecate old style `nn.Module` backward hooks ([#46163](https://github.com/pytorch/pytorch/pull/46163))\r\n\r\nOld style `nn.Module` backward hooks have been broken for a long time (they do not behave as advertised in the documentation). We now have new `nn.Module.register_full_backward_hook` that provide a fully working implementation of these hooks.\r\nThe old function should not be used and migrated to the new full version.\r\n\r\nAn example of this discrepancy is shown in the example below where a Linear layer takes as input a single Tensor of size 5 and returns a single Tensor of size 5 but old style hook would return two gradients with respect to the input for only one input.\r\n\r\n1.7.1:\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nmod = nn.Linear(5, 5)\r\ndef hook(mod, grad_inp, grad_out):\r\n    print(f\"grad input size: \" + \" \".join(str(g.size()) for g in grad_inp))\r\n    print(f\"grad output size: \" + \" \".join(str(g.size()) for g in grad_out))\r\nmod.register_backward_hook(hook)\r\n\r\nmod(torch.rand(5, requires_grad=True)).sum().backward()\r\n>>> `grad input size: torch.Size([5]) torch.Size([5]) # One too many\r\n>>> grad output size: torch.Size([5])`\r\n```\r\n\r\n1.8.0:\r\nOld style hooks are deprecated and will warn when providing wrong result.\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nmod = nn.Linear(5, 5)\r\ndef hook(mod, grad_inp, grad_out):\r\n    print(f\"grad input size: \" + \" \".join(str(g.size()) for g in grad_inp))\r\n    print(f\"grad output size: \" + \" \".join(str(g.size()) for g in grad_out))\r\nmod.register_backward_hook(hook)\r\n\r\nmod(torch.rand(5, requires_grad=True)).sum().backward()\r\n>>> grad input size: torch.Size([5]) torch.Size([5]) # One too many\r\n>>> grad output size: torch.Size([5])\r\n>>> `UserWarning: Using a non-full backward hook when the forward contains multiple\r\nautograd Nodes is deprecated and will be removed in future versions. This hook\r\nwill be missing some grad_input.`\r\n```\r\n\r\nFull hooks should be used to get the proper result all the time and avoid warnings\r\n\r\n```python\r\nmod.register_full_backward_hook(hook)\r\n\r\nmod(torch.rand(5, requires_grad=True)).sum().backward()\r\n>>> grad input size: torch.Size([5])\r\n>>> grad output size: torch.Size([5])\r\n```\r\n\r\n### `torch.stft`: Deprecate default value of the `require_complex` argument ([#49022](https://github.com/pytorch/pytorch/pull/49022), [#50102](https://github.com/pytorch/pytorch/pull/50102))\r\n\r\nPreviously `torch.stft` took an optional `return_complex` parameter that indicated whether the output would be a real tensor or a complex tensor. `return_complex` has the default value of `False`. This default value is deprecated (meaning that this optional argument is becoming mandatory) and will be removed in future versions. You can pass this argument explicitly to avoid this deprecation.\r\n\r\n### Deprecate `torch.set_deterministic` in favor of `torch.use_deterministic_algorithms` ([#49904](https://github.com/pytorch/pytorch/pull/49904))\r\n\r\nThis beta feature is being renamed for improved clarity. Users should migrate to use the new name.\r\n\r\n### Deprecate `torch.*` linear algebra functions in favor of the `torch.linalg.*` variant for `cholesky` ([#51460](https://github.com/pytorch/pytorch/pull/51460)), `slogdet` ([#51354](https://github.com/pytorch/pytorch/pull/51354)), `inverse` ([#51672](https://github.com/pytorch/pytorch/pull/51672)), `pinverse` ([#51671](https://github.com/pytorch/pytorch/pull/51671))\r\n\r\nAll the linear algebra functions are being moved to the `torch.linalg` submodule that provided a compatible API with NumPy. These new functions have the same set of features as the `torch.` ones and should be used instead.\r\n\r\n# New features\r\n\r\n### Python API\r\n\r\n* New functions (most of them to improve numpy compatibility): `torch.nan_to_num` ([#44592](https://github.com/pytorch/pytorch/pull/44592)), `torch.tensor_split` ([#45168](https://github.com/pytorch/pytorch/pull/45168)), `torch.nanmedian` ([#45847](https://github.com/pytorch/pytorch/pull/45847)), `torch.ravel` ([#46098](https://github.com/pytorch/pytorch/pull/46098)), `torch.igamma` ([#46183](https://github.com/pytorch/pytorch/pull/46183)), `torch.igammac` ([#48171](https://github.com/pytorch/pytorch/pull/48171)), `torch.{column_stack,row_stack}` ([#46313](https://github.com/pytorch/pytorch/pull/46313)), `torch.kron` ([#45358](https://github.com/pytorch/pytorch/pull/45358)), `torch.copysign` ([#46396](https://github.com/pytorch/pytorch/pull/46396)), `Tensor.new_empty_strided` ([#47225](https://github.com/pytorch/pytorch/pull/47225)), `torch.{swapdims,swapaxes}` ([#46041](https://github.com/pytorch/pytorch/pull/46041)), `torch.tile` ([#47974](https://github.com/pytorch/pytorch/pull/47974)), `torch.float_power` ([#44937](https://github.com/pytorch/pytorch/pull/44937)), `torch.moveaxis` ([#48581](https://github.com/pytorch/pytorch/pull/48581)), `torch.inner` ([#46716](https://github.com/pytorch/pytorch/pull/46716)), `torch.msort` ([#48440](https://github.com/pytorch/pytorch/pull/48440)), `torch.sinc` ([#48740](https://github.com/pytorch/pytorch/pull/48740)), `torch.broadcast_to` ([#48997](https://github.com/pytorch/pytorch/pull/48997)), `torch.xlogy` ([#48777](https://github.com/pytorch/pytorch/pull/48777)), `torch.f{max,min}` ([#49312](https://github.com/pytorch/pytorch/pull/49312)), `torch.diff` ([#50569](https://github.com/pytorch/pytorch/pull/50569)), `torch.ldexp` ([#45370](https://github.com/pytorch/pytorch/pull/45370)), `torch.broadcast_shapes` ([#43935](https://github.com/pytorch/pytorch/pull/43935)), \r\n* `torch.fft` new features: 2D FFT functions ([#45164](https://github.com/pytorch/pytorch/pull/45164)), use new FFT operators in stft ([#47601](https://github.com/pytorch/pytorch/pull/47601)), helper functions ([#44877](https://github.com/pytorch/pytorch/pull/44877)), fuzzing benchmark ([#47872](https://github.com/pytorch/pytorch/pull/47872))\r\n* `torch.linalg` new features: `linalg.tensorsolve` ([#46142](https://github.com/pytorch/pytorch/pull/46142)), `linalg.cholesky` ([#46083](https://github.com/pytorch/pytorch/pull/46083)), `linalg.tensorinv` ([#45969](https://github.com/pytorch/pytorch/pull/45969)), `linalg.{eigh,eigvalsh}` ([#45526](https://github.com/pytorch/pytorch/pull/45526)), `linalg.matrix_rank` ([#48206](https://github.com/pytorch/pytorch/pull/48206)), `linalg.solve` ([#48456](https://github.com/pytorch/pytorch/pull/48456)), `linalg.qr` ([#47764](https://github.com/pytorch/pytorch/pull/47764),  [#50046](https://github.com/pytorch/pytorch/pull/50046)), `linalg.svd` ([#45562](https://github.com/pytorch/pytorch/pull/45562)), `linalg.inv` ([#48261](https://github.com/pytorch/pytorch/pull/48261)), `linalg.pinv` ([#48399](https://github.com/pytorch/pytorch/pull/48399)), `linalg.slogdet` ([#49194](https://github.com/pytorch/pytorch/pull/49194)), `linalg.cond` ([#45832](https://github.com/pytorch/pytorch/pull/45832))\r\n* New `torch.nn` Modules: `nn.PixelUnshuffle` ([#49334](https://github.com/pytorch/pytorch/pull/49334)), `nn.GaussianNLLLoss` ([#50886](https://github.com/pytorch/pytorch/pull/50886))\r\n* Automatic shape inference in `torch.nn`: new `nn.LazyLinear` ([#44538](https://github.com/pytorch/pytorch/pull/44538)), `nn.LazyConv{1,2,3}d` and `nn.LazyConvTranspose{1,2,3}d` ([#47350](https://github.com/pytorch/pytorch/pull/47350))\r\n* Add channels last support for `torch.nn.AdaptiveAvgPool2d` ([#48916](https://github.com/pytorch/pytorch/pull/48916))\r\n* Add option to produce standalone executable with `cpp_extensions` ([#47862](https://github.com/pytorch/pytorch/pull/47862))\r\n* Add sparse-sparse matrix multiplication support ([#39526](https://github.com/pytorch/pytorch/pull/39526))\r\n* Add `torch.futures.Future.add_done_callback` ([#45675](https://github.com/pytorch/pytorch/pull/45675))\r\n* Add `three_phase` optional argument to `torch.optim.lr_scheduler.OneCycleLR` ([#42715](https://github.com/pytorch/pytorch/pull/42715))\r\n* Add `bicubic` option for the `mode` argument of `torch.nn.functional.grid_sampler` ([#44780](https://github.com/pytorch/pytorch/pull/44780))\r\n* Add new distributions to `torch.distributions`:  `Kumaraswamy` ([#48285](https://github.com/pytorch/pytorch/pull/48285)), `LKJCholesky` ([#48798](https://github.com/pytorch/pytorch/pull/48798))\r\n* Add reparameterization support to `torch.distributions.OneHotCategorical` ([#46610](https://github.com/pytorch/pytorch/pull/46610))\r\n* Add new transforms to `torch.distributions`: `CorrCholeskyTransform` ([#48041](https://github.com/pytorch/pytorch/pull/48041))\r\n* Add new constraint to `torch.distributions`: `independent` ([#50547](https://github.com/pytorch/pytorch/pull/50547), [#50302](https://github.com/pytorch/pytorch/pull/50302))\r\n* Add zero annealing epochs to SWA optimizer ([#47579](https://github.com/pytorch/pytorch/pull/47579))\r\n* Add `close` method to `torch.hub.tqdm` mock ([#46040](https://github.com/pytorch/pytorch/pull/46040))\r\n* Add support for pruning based on custom importance scores via the `importance_scores` keyword argument ([#48378](https://github.com/pytorch/pytorch/pull/48378))\r\n* Add torch vitals ([#51047](https://github.com/pytorch/pytorch/pull/51047))\r\n\r\n### Complex Numbers\r\n\r\n* Complex Number support on CPU and CUDA for `torch.symeig` ([#45121](https://github.com/pytorch/pytorch/pull/45121)), `torch.pinverse` ([#45819](https://github.com/pytorch/pytorch/pull/45819)), `torch.det` ([#45980](https://github.com/pytorch/pytorch/pull/45980)), `torch.diagflat`  ([#47564](https://github.com/pytorch/pytorch/pull/47564)), `torch.{addcmul, addcdiv} `([#46639](https://github.com/pytorch/pytorch/pull/46639)), `torch.lu_solve` ([#48028](https://github.com/pytorch/pytorch/pull/48028)), `torch.matrix_exp` ([#48363](https://github.com/pytorch/pytorch/pull/48363)), `torch.eig` ([#49168](https://github.com/pytorch/pytorch/pull/49168)), `torch.{acosh, asinh, atanh}`  ([#50387](https://github.com/pytorch/pytorch/pull/50387)), `torch.masked_scatter` ([#51281](https://github.com/pytorch/pytorch/pull/51281)),  `torch.bmm` and `torch.baddbmm` ([#42553](https://github.com/pytorch/pytorch/pull/42553)), `torch.orgqr` ([#50502](https://github.com/pytorch/pytorch/pull/50502)), `torch.index_fill_` ([#50578](https://github.com/pytorch/pytorch/pull/50578)), `torch.cholesky_inverse` ([#50269](https://github.com/pytorch/pytorch/pull/50269))\r\n* Complex Number support on CUDA for  `torch.qr` ([#45032](https://github.com/pytorch/pytorch/pull/45032)), `torch.lu (`[`#45898`](https://github.com/pytorch/pytorch/pull/45898)`), torch.prod`([#45980](https://github.com/pytorch/pytorch/pull/45980)), `torch.triangular_solve `([#46916](https://github.com/pytorch/pytorch/pull/46916)), `torch.solve `([#47045](https://github.com/pytorch/pytorch/pull/47045)),  `torch.cholesky_solve` ([#47047](https://github.com/pytorch/pytorch/pull/47047)), `torch.mean` ([#47048](https://github.com/pytorch/pytorch/pull/47048)), `torch.svd` ([#45795](https://github.com/pytorch/pytorch/pull/45795)), `torch.inverse` ([#47595](https://github.com/pytorch/pytorch/pull/47595)),  `torch.Tensor.index_put_` ([#51148](https://github.com/pytorch/pytorch/pull/51148))\r\n* Complex Number support on CPU for  `torch.trace` ([#50380](https://github.com/pytorch/pytorch/pull/50380))\r\n* Complex Number support for `torch.nn.DataParallel` ([#48686](https://github.com/pytorch/pytorch/pull/48686)),  `torch.nn.L1Loss` ([#49912](https://github.com/pytorch/pytorch/pull/49912)), Padding functions ([#50594](https://github.com/pytorch/pytorch/pull/50594))\r\n* Complex Number support  for `torch.distributed.{all_reduce, all_gather}`  ([#45879](https://github.com/pytorch/pytorch/pull/45879), [#46270](https://github.com/pytorch/pytorch/pull/46270))\r\n* Complex Autograd support for `torch.{atan, log, log10, log1p, log2, reciprocal, tan, pow, rsqrt, tanh, asinh, acosh}` ([#46275](https://github.com/pytorch/pytorch/pull/46275)),  `torch.{cholesky, triangular_solve, mm, mv, ger} `([#45737](https://github.com/pytorch/pytorch/pull/45737)),  `torch.take(), torch.Tensor.fill_()` ([#46860](https://github.com/pytorch/pytorch/pull/46860)), `torch.matrix_exp` ([#48363](https://github.com/pytorch/pytorch/pull/48363)), `torch.{baddbmm, addbmm, addmm, addmv}` ([#50632](https://github.com/pytorch/pytorch/pull/50632)), `torch.qr` ([#48489](https://github.com/pytorch/pytorch/pull/48489)), `torch.svd` and `torch.pinverse` ([#47761](https://github.com/pytorch/pytorch/pull/47761)), `torch.sqrt` ([#49461](https://github.com/pytorch/pytorch/pull/49461)), `torch.diag` ([#51268](https://github.com/pytorch/pytorch/pull/51268)), `torch.trace` ([#51537](https://github.com/pytorch/pytorch/pull/51537)), `torch.exp` ([#47194](https://github.com/pytorch/pytorch/pull/47194)), `torch.mean` ([#47566](https://github.com/pytorch/pytorch/pull/47566)),  `torch.addr` ([#50667](https://github.com/pytorch/pytorch/pull/50667)), torch.{`stack, gather, index_select}, torch.Tensor.index_add_`([#49552](https://github.com/pytorch/pytorch/pull/49552)), `torch.{masked_scatter, masked_select}` ([#51281](https://github.com/pytorch/pytorch/pull/51281)),  `torch.{addcmul, addcdiv} `([#46639](https://github.com/pytorch/pytorch/pull/46639)),  `torch.{acosh, asinh, atanh}`  ([#50387](https://github.com/pytorch/pytorch/pull/50387)), `torch.solve `([#47045](https://github.com/pytorch/pytorch/pull/47045)), `torch.cholesky_solve` ([#47047](https://github.com/pytorch/pytorch/pull/47047)), `torch.inverse` ([#47595](https://github.com/pytorch/pytorch/pull/47595))\r\n* Add complex autograd support for named tensors ([#47289](https://github.com/pytorch/pytorch/pull/47289))\r\n* Allow converting parameters and buffers of `torch.nn.Module` to complex dtypes ([#44788](https://github.com/pytorch/pytorch/pull/44788))\r\n* Add complex support to IValues ([#50883](https://github.com/pytorch/pytorch/pull/50883), [#51476](https://github.com/pytorch/pytorch/pull/51476))\r\n* Add TorchScript type annotation logic for complex numbers ([#50884](https://github.com/pytorch/pytorch/pull/50884))\r\n* Add serialization logic for complex numbers ([#51287](https://github.com/pytorch/pytorch/pull/51287))\r\n* Add support for complex number lists in JIT ([#51145](https://github.com/pytorch/pytorch/pull/51145))\r\n* Add support for complex valued keys for dict in TorchScript ([#51472](https://github.com/pytorch/pytorch/pull/51472))\r\n* Add `scalar.conj()` ([#46596](https://github.com/pytorch/pytorch/pull/46596))\r\n* Add `Tensor.copy_()` for `ComplexHalf` tensors ([#45339](https://github.com/pytorch/pytorch/pull/45339))\r\n\r\n### Profiler\r\n\r\n* New profiler API ([#48280](https://github.com/pytorch/pytorch/pull/48280))\r\n* Use libkineto in profiler ([#46470](https://github.com/pytorch/pytorch/pull/46470))\r\n* Add FLOPS computation support to the new profiler API ([#51734](https://github.com/pytorch/pytorch/pull/51734))\r\n* Add high level profiling trace for dataloading and optimizer ([#47655](https://github.com/pytorch/pytorch/pull/47655))\r\n* Add support for SVG visualization ([#48438](https://github.com/pytorch/pytorch/pull/48438))\r\n\r\n### Autograd\r\n\r\n* Add `inputs` argument to `autograd.backward()` both in python and c++ ([#46855](https://github.com/pytorch/pytorch/pull/46855), [#47214](https://github.com/pytorch/pytorch/pull/47214))\r\n* Add support for Tensor-like objects in `torch.autograd.gradcheck` ([#45732](https://github.com/pytorch/pytorch/pull/45732))\r\n* Add experimental `vectorize` flag to `torch.autograd.functional.{jacobian, hessian}` ([#50915](https://github.com/pytorch/pytorch/pull/50915), [#51638](https://github.com/pytorch/pytorch/pull/51638))\r\n* Add anomaly mode in C++ API ([#46981](https://github.com/pytorch/pytorch/pull/46981), [#47164](https://github.com/pytorch/pytorch/pull/47164))\r\n* Make `torch.lu` differentiable. ([#46284](https://github.com/pytorch/pytorch/pull/46284))\r\n* Add support for generators in autograd decorators like `torch.no_grad` ([#49017](https://github.com/pytorch/pytorch/pull/49017))\r\n\r\n### Dataloader\r\n\r\n* Add `BufferedShuffleDataset` ([#45290](https://github.com/pytorch/pytorch/pull/45290))\r\n* Add warning if DataLoader is going to create excessive number of thread ([#46867](https://github.com/pytorch/pytorch/pull/46867))\r\n* Add prototype of `BatchIterDataPipe` ([#49186, #51880](https://github.com/pytorch/pytorch/pull/49186))\r\n* Add prototype of `SamplerIterDataPipe` ([#49363, #52104](https://github.com/pytorch/pytorch/pull/49363))\r\n* Implement `BucketBatchIterDataPipe` ([#51126, #51880](https://github.com/pytorch/pytorch/pull/51126))\r\n* Add Tar DataPipe-s ([#51398](https://github.com/pytorch/pytorch/pull/51398))\r\n* Add `MapIterDataPipe` ([#51488](https://github.com/pytorch/pytorch/pull/51488)[](https://github.com/pytorch/pytorch/commit/9eb70c3c78ca971bf0277b9991f0932b4896bfed)[#51879](https://github.com/pytorch/pytorch/pull/51879))\r\n\r\n### CUDA\r\n\r\n* Allow user to specify a fraction of the GPU memory with `set_per_process_memory_fraction`. ([#48172](https://github.com/pytorch/pytorch/pull/48172))\r\n* CUDA BFloat16 TopK ([#44755](https://github.com/pytorch/pytorch/pull/44755))\r\n* Add LazyNVRTC ([#45674](https://github.com/pytorch/pytorch/pull/45674))\r\n* Enable CUDA Fuser for ROCm ([#45965](https://github.com/pytorch/pytorch/pull/45965))\r\n* Define the record_stream method in native_functions.yaml ([#44301](https://github.com/pytorch/pytorch/pull/44301))\r\n* Add CUDA 11.1 docker build ([#46283](https://github.com/pytorch/pytorch/pull/46283))\r\n* Add nvtx.range() context manager ([#42925](https://github.com/pytorch/pytorch/pull/42925))\r\n* CUDA BFloat16 gelu, hardswish, hardsigmoid ([#44997](https://github.com/pytorch/pytorch/pull/44997))\r\n* [ROCm] enable stream priorities ([#47136](https://github.com/pytorch/pytorch/pull/47136))\r\n* Add bfloat support for torch.randn and torch.norm ([#47143](https://github.com/pytorch/pytorch/pull/47143))\r\n* CUDA BFloat16 Dropout ([#45005](https://github.com/pytorch/pytorch/pull/45005)), batchnorm (non-cuDNN) ([#44994](https://github.com/pytorch/pytorch/pull/44994)), backwards ([#48809](https://github.com/pytorch/pytorch/pull/48809)), sparse ([#48807](https://github.com/pytorch/pytorch/pull/48807)),  indexing ([#48801](https://github.com/pytorch/pytorch/pull/48801)), embedding ([#44848](https://github.com/pytorch/pytorch/pull/44848)), signal windows ([#45155](https://github.com/pytorch/pytorch/pull/45155)), norm ([#48806](https://github.com/pytorch/pytorch/pull/48806)), isinf and isfinite ([#49356](https://github.com/pytorch/pytorch/pull/49356)), gemms on arch other than ampere ([#50442](https://github.com/pytorch/pytorch/pull/50442)), clamp, remainder, lshift, rshift ([#45247](https://github.com/pytorch/pytorch/pull/45247))\r\n* Make CUDAGeneratorImpl capturable ([#48694](https://github.com/pytorch/pytorch/pull/48694))\r\n* Adding support for CuDNN-based LSTM with projections ([#47725](https://github.com/pytorch/pytorch/pull/47725))\r\n* Add `torch.cuda.can_device_access_peer` ([#50446](https://github.com/pytorch/pytorch/pull/50446))\r\n* Add torch::cuda::ncll::all2all ([#45900](https://github.com/pytorch/pytorch/pull/45900)) \r\n\r\n### C++ API\r\n\r\n* Add distance-agnostic triplet margin loss ([#45377](https://github.com/pytorch/pytorch/pull/45377))\r\n* Add `torch::nn::ModuleDict` ([#47707](https://github.com/pytorch/pytorch/pull/47707))\r\n* Add `torch::cuda::synchronize` ([#50072](https://github.com/pytorch/pytorch/pull/50072))\r\n* Add new XPU backend type for Intel heterogeneous computation platform. ([#49786](https://github.com/pytorch/pytorch/pull/49786))\r\n\r\n### TorchScript\r\n\r\n* `torch::jit::freeze` C++ api introduced ([#52337](https://github.com/pytorch/pytorch/pull/52337), [#52392](https://github.com/pytorch/pytorch/pull/52392))\r\n* Add API for ignoring arbitrary module attributes during compilation ([#45262](https://github.com/pytorch/pytorch/pull/45262))\r\n* Support tracing tensor `__setitem__` with dynamic shape ([#45828](https://github.com/pytorch/pytorch/pull/45828))\r\n* Expose script_if_tracing as public API ([#46494](https://github.com/pytorch/pytorch/pull/46494))\r\n* Support %-based string formatting ([#45976](https://github.com/pytorch/pytorch/pull/45976))\r\n* Add `torch.jit.isinstance` support for typed containers ([#46062](https://github.com/pytorch/pytorch/pull/46062))\r\n* Allow for source code comments at any level of indentation ([#46548](https://github.com/pytorch/pytorch/pull/46548))\r\n* Support hashing of various data types by implementing generic hashing for IValues ([#46441](https://github.com/pytorch/pytorch/pull/46441))\r\n* Support doc string for TorchBind custom classes ([#46576](https://github.com/pytorch/pytorch/pull/46576))\r\n* Add API for selective lowering of modules to custom JIT backend ([#43613](https://github.com/pytorch/pytorch/pull/43613))\r\n* add list() support ([#42382](https://github.com/pytorch/pytorch/pull/42382))\r\n* Support using lambda function as TorchBind constructor ([#47819](https://github.com/pytorch/pytorch/pull/47819))\r\n* Support user defined classes as constants ([#45556](https://github.com/pytorch/pytorch/pull/45556))\r\n* Allow del statements with multiple targets ([#48876](https://github.com/pytorch/pytorch/pull/48876))\r\n* Tuple Slice with both negative and positive stepped size ([#48660](https://github.com/pytorch/pytorch/pull/48660))\r\n* Expose run_async function on torch::jit::Method ([#48607](https://github.com/pytorch/pytorch/pull/48607))\r\n* Add flag torch_jit_disable_warning_prints to allow disabling all warnings.warn ([#49313](https://github.com/pytorch/pytorch/pull/49313))\r\n* Add dict comprehension ([#47774](https://github.com/pytorch/pytorch/pull/47774))\r\n* Adding support for bitwise augassignment operators (`+=` style statements) ([#44621](https://github.com/pytorch/pytorch/pull/44621))\r\n* Support the `in` operator with str ([#47057](https://github.com/pytorch/pytorch/pull/47057))\r\n* Adding JIT support for cuda streams and events ([#48020](https://github.com/pytorch/pytorch/pull/48020))\r\n* Add `Type::{castRaw,expectRef}` ([#50061](https://github.com/pytorch/pytorch/pull/50061))\r\n* Allow arbitrary docstrings to be inside torchscript interface methods ([#50271](https://github.com/pytorch/pytorch/pull/50271))\r\n* Change list striding parameters to take optional integer ([#48719](https://github.com/pytorch/pytorch/pull/48719))\r\n* Add support for scripting and running module level hooks in JIT ([#49544](https://github.com/pytorch/pytorch/pull/49544), [#49975](https://github.com/pytorch/pytorch/pull/49975), [#49545](https://github.com/pytorch/pytorch/pull/49545), [#49546](https://github.com/pytorch/pytorch/pull/49546), [#49547](https://github.com/pytorch/pytorch/pull/49547))\r\n* Support default argument values of a method ([#48863](https://github.com/pytorch/pytorch/pull/48863))\r\n* Graceful invalidation of Python Node/Value/Block when C++ object is deleted ([#50326](https://github.com/pytorch/pytorch/pull/50326))\r\n* Support `Union[NoneType, T]` as input type ([#51605](https://github.com/pytorch/pytorch/pull/51605))\r\n* Allow implicit boolean conversion of lists, strings, and dictionaries ([#51683](https://github.com/pytorch/pytorch/pull/51683))\r\n\r\n### Mobile\r\n\r\n* Add instance_key into mobile stats logging. ([#45517](https://github.com/pytorch/pytorch/pull/45517))\r\n* Profiling allocator for mobile. ([#43951](https://github.com/pytorch/pytorch/pull/43951))\r\n* [Metal] Add Metal/MPSCNN support on iOS ([#46112](https://github.com/pytorch/pytorch/pull/46112))\r\n* [Metal] Introduce USE_PYTORCH_METAL ([#46383](https://github.com/pytorch/pytorch/pull/46383))\r\n* [Metal] Support Resnet models (b63ddd6f57)\r\n* PyTorch NNAPI integration prototype ([#46780](https://github.com/pytorch/pytorch/pull/46780))\r\n* [Metal] Enable Metal on macosx ([#47635](https://github.com/pytorch/pytorch/pull/47635))\r\n* [Metal] Enable optimize_for_mobile on Linux ([#46384](https://github.com/pytorch/pytorch/pull/46384))\r\n* [Android] Fix YUV camera image to tensor ([#50871](https://github.com/pytorch/pytorch/pull/50871))\r\n* [Android] turn on USE_VULKAN for android builds by default ([#51291](https://github.com/pytorch/pytorch/pull/51291))\r\n* Add windows JNI support ([#44257](https://github.com/pytorch/pytorch/pull/44257))\r\n* \r\n* Enable partial loading of GPU models on linux CPU machines ([#51236](https://github.com/pytorch/pytorch/pull/51236))\r\n\r\n### Distributed\r\n\r\n* Support `send` and `recv` in c10d NCCL backend ([#44921](https://github.com/pytorch/pytorch/pull/44921), [#44922](https://github.com/pytorch/pytorch/pull/44922))\r\n* Add support for NCCL alltoall ([#44374](https://github.com/pytorch/pytorch/pull/44374))\r\n* Upstream `fairscale.nn.Pipe` into PyTorch as `torch.distributed.pipeline` ([#44090](https://github.com/pytorch/pytorch/pull/44090))\r\n* Add a `--logdir` option to log subprocess output to files in DDP launcher. ([#33193](https://github.com/pytorch/pytorch/pull/33193))\r\n* Support `RRef.backward()` for local RRefs. ([#46568](https://github.com/pytorch/pytorch/pull/46568)) and Owner RRefs. ([#46641](https://github.com/pytorch/pytorch/pull/46641))\r\n* Support C++ implementation for DDP communication hook. ([#46566](https://github.com/pytorch/pytorch/pull/46566))\r\n* Provide 2 default C++ comm hooks for DDP ([#46701](https://github.com/pytorch/pytorch/pull/46701))\r\n* Support remote device format `\"worker_name/device\"` ([#46773](https://github.com/pytorch/pytorch/pull/46773))\r\n* Enable creation and transfer of `ScriptModule` over RPC ([#48293](https://github.com/pytorch/pytorch/pull/48293))\r\n* Enable TCPStore on Windows ([#47749](https://github.com/pytorch/pytorch/pull/47749))\r\n* Support `torch.distributed.irecv(src=None, ...)` as `recv_anysource` ([#49383](https://github.com/pytorch/pytorch/pull/49383))\r\n* Implement layer-wise PowerSGD as a DDP comm hook ([#49639](https://github.com/pytorch/pytorch/pull/49639))\r\n* Support `alltoall_single` in TorchScript ([#48345](https://github.com/pytorch/pytorch/pull/48345))\r\n* Enable GPU-to-GPU comm in `TensorPipeAgent` ([#44418](https://github.com/pytorch/pytorch/pull/44418))\r\n* Support timeout in `rref._get_type()` ([#50498](https://github.com/pytorch/pytorch/pull/50498))\r\n* Support timeout for RRef proxy functions ([#50499](https://github.com/pytorch/pytorch/pull/50499))\r\n* Add optimizer state sharding as `ZeroRedundancyOptimizer` ([#46750](https://github.com/pytorch/pytorch/pull/46750))\r\n* Add distributed functional `Adam` optimizer ([#50624](https://github.com/pytorch/pytorch/pull/50624)),  `sgd` optimizer ([#50618](https://github.com/pytorch/pytorch/pull/50618)),  `Adadelta` optimizer ([#50623](https://github.com/pytorch/pytorch/pull/50623)),  `RMSprop` optimizer ([#50619](https://github.com/pytorch/pytorch/pull/50619)), l `AdamW` optimizer ([#50620](https://github.com/pytorch/pytorch/pull/50620))\r\n* Create a DDPLoggingData struct and expose it to python interface ([#50622](https://github.com/pytorch/pytorch/pull/50622))\r\n* Implement autograd functions for c10d communication operations ([#40762](https://github.com/pytorch/pytorch/pull/40762))\r\n* Enable TensorPipe's SHM transport ([#50760](https://github.com/pytorch/pytorch/pull/50760))\r\n* Support device map for distributed autograd while using TensorPipe. ([#44859](https://github.com/pytorch/pytorch/pull/44859))\r\n* Create PyTorch DDP logging APIs for applications to use ([#50637](https://github.com/pytorch/pytorch/pull/50637))\r\n* Add `set_exception` API in `torch.futures.Future` ([#50983](https://github.com/pytorch/pytorch/pull/50983))\r\n* Add `scatter_object_list` API for c10d ([#43930](https://github.com/pytorch/pytorch/pull/43930))\r\n* Provide parameter to pass GPU ID in barrier function ([#49069](https://github.com/pytorch/pytorch/pull/49069))\r\n* Enable TensorPipe CUDA fallback channel ([#50675](https://github.com/pytorch/pytorch/pull/50675))\r\n* Enable TensorPipe's InfiniBand transport ([#50761](https://github.com/pytorch/pytorch/pull/50761))\r\n\r\n### torch.fx\r\n\r\n* allow custom behavior for args, kwargs, and bool ([#45193](https://github.com/pytorch/pytorch/pull/45193))\r\n* Mutable Graph APIs ([#45227](https://github.com/pytorch/pytorch/pull/45227))\r\n* Make output a non-special Node ([#45599](https://github.com/pytorch/pytorch/pull/45599))\r\n* Make `Tracer.trace()` just return a Graph ([#45704](https://github.com/pytorch/pytorch/pull/45704))\r\n* Preserve type annotations on generated code in Graph ([#45880](https://github.com/pytorch/pytorch/pull/45880))\r\n* Make `graph_copy` examine existing values in val_map ([#46104](https://github.com/pytorch/pytorch/pull/46104))\r\n* Allow tracing free functions ([#46268](https://github.com/pytorch/pytorch/pull/46268))\r\n* Make sure args/kwargs are immutable ([#46325](https://github.com/pytorch/pytorch/pull/46325))\r\n* Make wrapped functions traceable ([#46692](https://github.com/pytorch/pytorch/pull/46692))\r\n* Added `GraphModule.to_folder` ([#47544](https://github.com/pytorch/pytorch/pull/47544))\r\n* Support default args in symbolic tracing ([#47615](https://github.com/pytorch/pytorch/pull/47615))\r\n* Add `Node.all_input_nodes` ([#48270](https://github.com/pytorch/pytorch/pull/48270))\r\n* Support torchbind as attribute in torch.fx symbolic tracing ([#48732](https://github.com/pytorch/pytorch/pull/48732))\r\n* Create subgraph rewriter API ([#49540](https://github.com/pytorch/pytorch/pull/49540))\r\n* Make len traceable and scriptable with wrap ([#50184](https://github.com/pytorch/pytorch/pull/50184))\r\n* Add Interpreter and Transformer APIs ([#50420](https://github.com/pytorch/pytorch/pull/50420))\r\n* Add alternative prettyprinting method to `Graph` ([#50878](https://github.com/pytorch/pytorch/pull/50878))\r\n* Move some heavily used passes out of experimental ([#51392](https://github.com/pytorch/pytorch/pull/51392))\r\n* Added partial concrete values for symbolic tracing ([#51609](https://github.com/pytorch/pytorch/pull/51609))\r\n\r\n### Quantization\r\n\r\n* Quantized Operators and Modules\r\n    *  Embedding and EmbeddingBag operator support\r\n        * creating quint4x2 dtype for quantized tensors ([#44678](https://github.com/pytorch/pytorch/pull/44678))\r\n        * PerChannelFloatQParams support for quint4x2 dtype ([#45594](https://github.com/pytorch/pytorch/pull/45594))\r\n        * Add 4-bit embedding_bag prepack/unpack support using quint4x2 ([#45751](https://github.com/pytorch/pytorch/pull/45751))\r\n        * Support 4-bit embedding_bag operators using the dtype quint4x2 ([#45752](https://github.com/pytorch/pytorch/pull/45752))\r\n        * Support for 4-bit quantized EmbeddingBag module ([#45865](https://github.com/pytorch/pytorch/pull/45865))\r\n        * Refactor qembeddingbag to remove duplicate code ([#45881](https://github.com/pytorch/pytorch/pull/45881))\r\n        * Rename the sparse argument for embedding_bag ops ([#46003](https://github.com/pytorch/pytorch/pull/46003))\r\n        * Add support for pruned weights in embedding_bag_byte lookup ([#47329](https://github.com/pytorch/pytorch/pull/47329))\r\n        * fp16 -> fp32 EmbeddingBag moved into CPU impl ([#47076](https://github.com/pytorch/pytorch/pull/47076))\r\n        * Add non-fbgemm fallback implementation for embedding lookup ops ([#50706](https://github.com/pytorch/pytorch/pull/50706))\r\n        * Out variant for embedding_bag_4bit_rowwise_offsets ([#51324](https://github.com/pytorch/pytorch/pull/51324))\r\n        * Using int32 as indices for embedding_bag operators ([#45878](https://github.com/pytorch/pytorch/pull/45878))\r\n    * Add transposed conv support for fbgemm backend for 1d, 2d, 3d ([#46607](https://github.com/pytorch/pytorch/pull/46607), [#46608](https://github.com/pytorch/pytorch/pull/46608))\r\n    * Add quantized flip dispatch ([#46235](https://github.com/pytorch/pytorch/pull/46235))\r\n    * Add support for ReflectionPad2d ([#48036](https://github.com/pytorch/pytorch/pull/48036))\r\n    * Dynamic GRU quantization support ([#49448](https://github.com/pytorch/pytorch/pull/49448))\r\n    * Quantizable LSTM ([#49671](https://github.com/pytorch/pytorch/pull/49671))\r\n* Quantization Flow/API\r\n    * quantization: Linear + BatchNorm1d fusion ([#50748](https://github.com/pytorch/pytorch/pull/50748))\r\n    * compare_model_stub_fx API implementation ([#48951](https://github.com/pytorch/pytorch/pull/48951))\r\n    * Add additional_fuser_method_mapping to config ([#46355](https://github.com/pytorch/pytorch/pull/46355))\r\n    * Compare Weights FX Implementation ([#48056](https://github.com/pytorch/pytorch/pull/48056))\r\n    * Numeric Suite: Swap with shadow modules only for quantized part of model ([#51052](https://github.com/pytorch/pytorch/pull/51052))\r\n* FX Graph Mode Quantization\r\n    * Add prepare_custom_config_dict and convert_custom_config_dict ([#46223](https://github.com/pytorch/pytorch/pull/46223), [#46364](https://github.com/pytorch/pytorch/pull/46364))\r\n    * Add FixedQParamsFakeQuantize module ([#46657](https://github.com/pytorch/pytorch/pull/46657))\r\n    * Add support for additional_fuse_method_mapping ([#46345](https://github.com/pytorch/pytorch/pull/46345)), additional_{fusion/quant}_pattern ([#46346](https://github.com/pytorch/pytorch/pull/46346))\r\n    * Support in qat sigmoid/hardsigmoid/tanh ([#46871](https://github.com/pytorch/pytorch/pull/46871)), convbn{relu}1d ([#47248](https://github.com/pytorch/pytorch/pull/47248)),  FloatFunctional ([#46634](https://github.com/pytorch/pytorch/pull/46634))\r\n    * custom_module support static/dynamic/weight_only quant ([#46786](https://github.com/pytorch/pytorch/pull/46786))\r\n    * Support standalone_module_class ([#47705](https://github.com/pytorch/pytorch/pull/47705))\r\n    * Embedding/EmbeddingBag works in static quant qconfig ([#48062](https://github.com/pytorch/pytorch/pull/48062))\r\n    * Add MatchAllNode in pattern matching ([#48979](https://github.com/pytorch/pytorch/pull/48979))\r\n    * Add support for dynamic quant for RNN and RNNCell ([#49126](https://github.com/pytorch/pytorch/pull/49126)), ConvTranspose{n}d ([#49717](https://github.com/pytorch/pytorch/pull/49717)), quantizing functional linear + {functional relu/module relu} ([#50975](https://github.com/pytorch/pytorch/pull/50975)), functional conv2d + relu ([#51079](https://github.com/pytorch/pytorch/pull/51079)), functional conv1d and conv3d (#51155) ([#51254](https://github.com/pytorch/pytorch/pull/51254)), Scalar as first input for add/mul ([#46751](https://github.com/pytorch/pytorch/pull/46751)), leaky relu ([#45712](https://github.com/pytorch/pytorch/pull/45712)), Embedding ([#46677](https://github.com/pytorch/pytorch/pull/46677)), EmbeddingBag ([#46678](https://github.com/pytorch/pytorch/pull/46678))\r\n    * Remove inplace option for convert_fx ([#46955](https://github.com/pytorch/pytorch/pull/46955))\r\n    * Support non_traceable_module/module_class ([#46298](https://github.com/pytorch/pytorch/pull/46298))\r\n    * Add additional_object_mapping argument to convert ([#46338](https://github.com/pytorch/pytorch/pull/46338))\r\n    * Keep linear op unchanged when qconfig is not supported ([#48067](https://github.com/pytorch/pytorch/pull/48067))\r\n    * Move {input|output}_quantized_idxs cfg from convert to prepare ([#49238](https://github.com/pytorch/pytorch/pull/49238))\r\n    * Allow user to specify qconfig for call_method ([#49621](https://github.com/pytorch/pytorch/pull/49621))\r\n    * Do not observe bias on F.conv and F.linear ([#49623](https://github.com/pytorch/pytorch/pull/49623), [#49628](https://github.com/pytorch/pytorch/pull/49628))\r\n    * Linear work with float_qparam_dynamic_qconfig ([#47068](https://github.com/pytorch/pytorch/pull/47068))\r\n    * Fix error that DefaultQuantizer is not inserted after a module configured with None qconfig ([#47316](https://github.com/pytorch/pytorch/pull/47316))\r\n    * Scope support for call_method in QuantizationTracer ([#50173](https://github.com/pytorch/pytorch/pull/50173))\r\n    * Support preserved_attributes in prepare_fx ([#50306](https://github.com/pytorch/pytorch/pull/50306))\r\n    * Add option to leave graph inputs and/or outputs quantized ([#48624](https://github.com/pytorch/pytorch/pull/48624))\r\n    * Support quantization for custom module ([#44074](https://github.com/pytorch/pytorch/pull/44074))\r\n    * Remove `inplace` option for fuse_fx ([#46953](https://github.com/pytorch/pytorch/pull/46953)) and prepare_fx ([#46954](https://github.com/pytorch/pytorch/pull/46954))\r\n    * Scope support for call_function in QuantizationTracer ([#51086](https://github.com/pytorch/pytorch/pull/51086))\r\n\r\n### ONNX\r\n\r\n* Preprocess index_put with bool inputs to `torch.masked_{scatter,fill}` ([#45584](https://github.com/pytorch/pytorch/pull/45584))\r\n* Export `torch.{var,var_mean,std_mean}` ops ([#45678](https://github.com/pytorch/pytorch/pull/45678))\r\n* Enable NoneType inputs to export API ([#45792](https://github.com/pytorch/pytorch/pull/45792))\r\n* Add export of prim::dtype, prim::tolist ([#46019](https://github.com/pytorch/pytorch/pull/46019))\r\n* Enable onnx shape inference in export by default ([#46629](https://github.com/pytorch/pytorch/pull/46629))\r\n* Add `torch.silu` operator support for onnx ([#51519](https://github.com/pytorch/pytorch/pull/51519))\r\n* Support list remove for onnx export ([#51526](https://github.com/pytorch/pytorch/pull/51526))\r\n* Added `torch.hardswish` symbolic in opset 9 ([#48423](https://github.com/pytorch/pytorch/pull/48423))\r\n* Add export of `aten::is_floating` point ([#46442](https://github.com/pytorch/pytorch/pull/46442))\r\n* Add `torch.logical_{and,or,xor}` torch op support in pytorch exporter ([#50909](https://github.com/pytorch/pytorch/pull/50909))\r\n* Add `torch.binary_cross_entropy_with_logits` op to ONNX opset version 12 ([#50908](https://github.com/pytorch/pytorch/pull/50908))\r\n* Support opset13 `nn.Squeeze` and `nn.Unsqueeze` ([#50906](https://github.com/pytorch/pytorch/pull/50906))\r\n* Add export of `prim::data` ([#45747](https://github.com/pytorch/pytorch/pull/45747))\r\n* Support `torch.nonzero(*, as_tuple=True)` export ([#47421](https://github.com/pytorch/pytorch/pull/47421))\r\n* Update Reducesum operator for opset 13 ([#50907](https://github.com/pytorch/pytorch/pull/50907))\r\n\r\n### Misc\r\n\r\n* Enable python code coverage on windows ([#44548](https://github.com/pytorch/pytorch/pull/44548)) and onnx ([#47387](https://github.com/pytorch/pytorch/pull/47387))\r\n* Fix PyTorch compilation on Apple M1 chips ([#48275](https://github.com/pytorch/pytorch/pull/48275), [#49701](https://github.com/pytorch/pytorch/pull/49701))\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Add integer support (by promoting integer to float) to `torch.{cos,sin,tan}` ([#45733](https://github.com/pytorch/pytorch/pull/45733), [#46706](https://github.com/pytorch/pytorch/pull/46706)), `torch.log{2,10}` ([#46810](https://github.com/pytorch/pytorch/pull/46810)), `torch.{a}tanh` ([#47064](https://github.com/pytorch/pytorch/pull/47064)), `torch.a{cos, tan}` ([#47005](https://github.com/pytorch/pytorch/pull/47005)), `torch.a{cosh, sinh}` ([#47152](https://github.com/pytorch/pytorch/pull/47152)), `torch.sqrt` ([#47293](https://github.com/pytorch/pytorch/pull/47293)), `torch.log1p`  ([#48002](https://github.com/pytorch/pytorch/pull/48002)). `torch.erf{c}` ([#48472](https://github.com/pytorch/pytorch/pull/48472)), `torch.asin` ([#48461](https://github.com/pytorch/pytorch/pull/48461)), `torch.sigmoid` ([#47551](https://github.com/pytorch/pytorch/pull/47551)), `torch.sinh` ([#48644](https://github.com/pytorch/pytorch/pull/48644)), `torch.cosh` ([#48923](https://github.com/pytorch/pytorch/pull/48923)), `torch.exp{2, m1}`([#48926](https://github.com/pytorch/pytorch/pull/48926)), `torch.reciprocal` ([#49102](https://github.com/pytorch/pytorch/pull/49102)), `torch.erfinv` ([#49155](https://github.com/pytorch/pytorch/pull/49155)), `torch.rsqrt` ([#47909](https://github.com/pytorch/pytorch/pull/47909)), `torch.exp`  ([#50093](https://github.com/pytorch/pytorch/pull/50093)), `torch.lgamma` ([#50140](https://github.com/pytorch/pytorch/pull/50140))\r\n* Add optional `dtype` argument to `Tensor.view` ([#47951](https://github.com/pytorch/pytorch/pull/47951))\r\n* Add `out` optional arguments to `torch.{reshape,flatten}`  ([#51249](https://github.com/pytorch/pytorch/pull/51249)), `torch.tensordot` ([#47278](https://github.com/pytorch/pytorch/pull/47278)), `torch.fft.*` ([#49335](https://github.com/pytorch/pytorch/pull/49335)), `torch.narrow_copy` ([#49502](https://github.com/pytorch/pytorch/pull/49502))\r\n* Add support for int32 indices and offset in `nn.Embedding` and `nn.EmbeddingBag` ([#46758](https://github.com/pytorch/pytorch/pull/46758))\r\n* Add boolean type support to `torch.where` ([#47454](https://github.com/pytorch/pytorch/pull/47454)), `torch.mul` and `Tensor.__mul__` ([#48637](https://github.com/pytorch/pytorch/pull/48637)), `torch.diag` ([#47455](https://github.com/pytorch/pytorch/pull/47455)), `torch.{all,any}` ([#44790](https://github.com/pytorch/pytorch/pull/44790)), ` Tensor.to_dense` ([#50019](https://github.com/pytorch/pytorch/pull/50019))\r\n* Add inplace version of `torch.cum{sum,prod}_` ([#47651](https://github.com/pytorch/pytorch/pull/47651))\r\n* Add sparse support to `torch.sqrt` ([#50088](https://github.com/pytorch/pytorch/pull/50088))\r\n* Add support for both `dtype` and `ord` arguments in `torch.linalg.norm` ([#46637](https://github.com/pytorch/pytorch/pull/46637))\r\n* Make `torch.nn` Module accept batch size of 0: `nn.ReplicationPad` ([#39137](https://github.com/pytorch/pytorch/pull/39137)), `nn.Unfold` ([#40689](https://github.com/pytorch/pytorch/pull/40689)), `nn.PixelShuffle`  ([#49187](https://github.com/pytorch/pytorch/pull/49187)), `nn.AvgPool{1,2,3}d`  ([#50008](https://github.com/pytorch/pytorch/pull/50008)), `nn.MultiLabelMarginLoss` and `nn.MultiMarginLoss` ([#50007](https://github.com/pytorch/pytorch/pull/50007))\r\n* `utils.cpp_extensions` Ensure default extra_compile_args are properly handled ([#45956](https://github.com/pytorch/pytorch/pull/45956))\r\n* `torch.LongTensor` legacy construction improved error message ([#46147](https://github.com/pytorch/pytorch/pull/46147))\r\n* `torch.utils.checkpoint` allow having Tensors that don\u2019t require gradients ([#45934](https://github.com/pytorch/pytorch/pull/45934))\r\n* `torch.nan_to_num`: fix deprecated warnings ([#46309](https://github.com/pytorch/pytorch/pull/46309))\r\n* Remove more use of \u201cblacklist\u201d  ([#45512](https://github.com/pytorch/pytorch/pull/45512), [#45781](https://github.com/pytorch/pytorch/pull/45781))\r\n* Add type annotation to submodules: `torch.nn.cpp` ([#46490](https://github.com/pytorch/pytorch/pull/46490)), `torch.nn.parallel.comm` ([#46736](https://github.com/pytorch/pytorch/pull/46736)), `torch.nn.modules.*` ([#46828](https://github.com/pytorch/pytorch/pull/46828), [#45772](https://github.com/pytorch/pytorch/pull/45772), [#46013](https://github.com/pytorch/pytorch/pull/46013), [#49957](https://github.com/pytorch/pytorch/pull/49957), [#49479](https://github.com/pytorch/pytorch/pull/49479), [#49045](https://github.com/pytorch/pytorch/pull/49045), [#49035](https://github.com/pytorch/pytorch/pull/49035), [#49494](https://github.com/pytorch/pytorch/pull/49494), [#48969](https://github.com/pytorch/pytorch/pull/48969)), autograd functions from c++ ([#46622](https://github.com/pytorch/pytorch/pull/46622)), `torch.distributed` functions from c++  ([#46623](https://github.com/pytorch/pytorch/pull/46623)), `torch.storage`  ([#46876](https://github.com/pytorch/pytorch/pull/46876)), `torch._tensor_str` ([#48463](https://github.com/pytorch/pytorch/pull/48463), [#48584](https://github.com/pytorch/pytorch/pull/48584)), `torch.nn.modules.pooling` ([#48412](https://github.com/pytorch/pytorch/pull/48412)), `common_nn` ([#48190](https://github.com/pytorch/pytorch/pull/48190)), `torch.lobpcg` ([#47680](https://github.com/pytorch/pytorch/pull/47680)), `torch.nn.functional`  ([#50106](https://github.com/pytorch/pytorch/pull/50106)), `torch.overrides` ([#50824](https://github.com/pytorch/pytorch/pull/50824)), `torch.generate_torch_version`  ([#51637](https://github.com/pytorch/pytorch/pull/51637)), `torch.distributions` ([#45689](https://github.com/pytorch/pytorch/pull/45689)), `torch.quantization.quantize_jit` ([#45548](https://github.com/pytorch/pytorch/pull/45548)), `torch.utils.tensorboard` ([#49834](https://github.com/pytorch/pytorch/pull/49834)), `torch.multiprocessing` ([#47756](https://github.com/pytorch/pytorch/pull/47756)), `torch.cuda` ([#47134](https://github.com/pytorch/pytorch/pull/47134)), `torch._C._distributed_rpc` ([#46624](https://github.com/pytorch/pytorch/pull/46624)), `torch.distributed.*` ([#47531](https://github.com/pytorch/pytorch/pull/47531), [#47532](https://github.com/pytorch/pytorch/pull/47532), [#47533](https://github.com/pytorch/pytorch/pull/47533), [#47534](https://github.com/pytorch/pytorch/pull/47534)), `torch.nn.parallel._functions` ([#49687](https://github.com/pytorch/pytorch/pull/49687))\r\n* Make comparison fail when dtypes don\u2019t match ([#47288](https://github.com/pytorch/pytorch/pull/47288))\r\n* Allow large inputs for `torch.svd` ([#47440](https://github.com/pytorch/pytorch/pull/47440))\r\n* Add nondeterministic alerts to `torch.index_copy`, `torch.median` on CUDA and `torch.kthvalue` on CUDA  ([#46942](https://github.com/pytorch/pytorch/pull/46942))\r\n* Add float16 and  bfloat16 support to `torch.where` ([#49004](https://github.com/pytorch/pytorch/pull/49004)), `torch.matmul` ([#47873](https://github.com/pytorch/pytorch/pull/47873))\r\n* Add float16 support for CPU and bfloat16 support for CPU & CUDA to `torch.flip` and `torch.flip{lr, ud}` ([#49895](https://github.com/pytorch/pytorch/pull/49895))\r\n* Add support for providing `indices` as a Tensor for `torch.tensor_split` ([#49169](https://github.com/pytorch/pytorch/pull/49169))\r\n* Add support for SELU activation in `torc.nn.init.calculate_gain` ([#50664](https://github.com/pytorch/pytorch/pull/50664))\r\n* Add function version of `torch.optim` optimizers and refactor existing classes to use the functional version: SGD ([#45597](https://github.com/pytorch/pytorch/pull/45597)), Adadelta ([#50409](https://github.com/pytorch/pytorch/pull/50409)), RMSProp ([#50410](https://github.com/pytorch/pytorch/pull/50410)), AdamW ([#50411](https://github.com/pytorch/pytorch/pull/50411))\r\n* Improve error message when window is on wrong device for `torch.fft.stft` ([#51128](https://github.com/pytorch/pytorch/pull/51128))\r\n* Add rounding_mode selection to `torch.div` ([#51706](https://github.com/pytorch/pytorch/pull/51706), [#52242](https://github.com/pytorch/pytorch/pull/52242))\r\n* Remove spurious numpy writable warning ([#47271](https://github.com/pytorch/pytorch/pull/47271))\r\n* Enable deterministic mode for rocBLAS ([#48654](https://github.com/pytorch/pytorch/pull/48654))\r\n* Hipify submodule revamp and improved integration with cpp_extensions ([#48715](https://github.com/pytorch/pytorch/pull/48715))\r\n* Remove warning about saving state in `torch.optim.lr_scheduler.LambdaLR` ([#46813](https://github.com/pytorch/pytorch/pull/46813))\r\n* Improve typing of `torch.nn.Unflatten` ([#49838](https://github.com/pytorch/pytorch/pull/49838))\r\n* Add exception classification to `torch.multiprocessing.spawn`\r\n\r\n### Autograd\r\n\r\n* Add double backward checks for the `torch.fft` submodule ([#46004](https://github.com/pytorch/pytorch/pull/46004))\r\n* Detect inplace modifications of views of leaf Tensors earlier to improve error ([#46204](https://github.com/pytorch/pytorch/pull/46204))\r\n\r\n### torch.utils\r\n\r\n* `data.TensorDataset`: Add more specific error message ([#46905](https://github.com/pytorch/pytorch/pull/46905))\r\n* `data.DistributedSampler`: Additional validation ([#48865](https://github.com/pytorch/pytorch/pull/48865))\r\n\r\n### Complex Numbers\r\n\r\n* Improve error message thrown by `torch.sign` for complex tensors ([#43280](https://github.com/pytorch/pytorch/pull/43280))\r\n* Remove unnecessary dtype checks for complex types and disable complex dispatch for CPU `torch.{min,max}` pointwise ops ([#50465](https://github.com/pytorch/pytorch/pull/50465))\r\n\r\n### CUDA\r\n\r\n* Allow consumer ops to sync on autograd engine base gradient ([#45787](https://github.com/pytorch/pytorch/pull/45787))\r\n* Add `torch::cuda::nccl::{send,recv}` ([#45926](https://github.com/pytorch/pytorch/pull/45926))\r\n* Cusolver inverse check info ([#46625](https://github.com/pytorch/pytorch/pull/46625))\r\n* Make numpy optional dependency for `torch.cuda.amp` ([#48154](https://github.com/pytorch/pytorch/pull/48154))\r\n* Support all visible cards when building a cuda extension ([#48891](https://github.com/pytorch/pytorch/pull/48891))\r\n* Enable using `torch.utils.checkpoint.checkpoint` and `torch.cuda.amp` at the same time ([#49757](https://github.com/pytorch/pytorch/pull/49757))\r\n* Make `DeviceCachingAllocator`'s error handling more defensive and a bit easier to read ([#51158](https://github.com/pytorch/pytorch/pull/51158))\r\n\r\n### Distributed\r\n\r\n* Create NCCL communicator for send/recv on demand ([#44922](https://github.com/pytorch/pytorch/pull/44922))\r\n* Reduce the peak memory of fp16 compression DDP comm hook by avoiding converting to fp32 ([#46078](https://github.com/pytorch/pytorch/pull/46078))\r\n* Allow RPC framework to use rank in addition to `WorkerInfo` and name. ([#46221](https://github.com/pytorch/pytorch/pull/46221))\r\n* Add to the `HashStore` `getNumKeys()`  ([#46048](https://github.com/pytorch/pytorch/pull/46048)) and `deleteKey()` ([#46049](https://github.com/pytorch/pytorch/pull/46049))\r\n* Print exception message on both RPC caller and callee ([#46372](https://github.com/pytorch/pytorch/pull/46372))\r\n* Add RRef proxy support for `ScriptModule` methods ([#48339](https://github.com/pytorch/pytorch/pull/48339))\r\n* Support retrieving the RRef to the remote module ([#48983](https://github.com/pytorch/pytorch/pull/48983))\r\n* Add a c++ interface in processGroup to get its backend name ([#51066](https://github.com/pytorch/pytorch/pull/51066))\r\n* Enable `NamedTuple` data type to work with DDP ([#44220](https://github.com/pytorch/pytorch/pull/44220))\r\n* Support send/recv to/from self when communicator is created on demand ([#45873](https://github.com/pytorch/pytorch/pull/45873))\r\n* Add Error log when ProcessGroupNCCL takes down a process ([#44988](https://github.com/pytorch/pytorch/pull/44988))\r\n* Provide additional information about NCCL error codes. ([#45950](https://github.com/pytorch/pytorch/pull/45950))\r\n* Avoid scatter for single-device case in DDP ([#46304](https://github.com/pytorch/pytorch/pull/46304))\r\n* Use Blocking Wait if both Blocking Wait and Async Error Handling Are Set ([#47926](https://github.com/pytorch/pytorch/pull/47926))\r\n* Providing more information while crashing a process in async error handling ([#47246](https://github.com/pytorch/pytorch/pull/47246))\r\n* Add PowerSGD comm hook ([#48060](https://github.com/pytorch/pytorch/pull/48060))\r\n* Define a customized state for PowerSGD comm hook ([#48348](https://github.com/pytorch/pytorch/pull/48348))\r\n* Add a random generator to PowerSGD state for initializing low-rank matrix Q ([#48507](https://github.com/pytorch/pytorch/pull/48507))\r\n* Replace the key of `error_dict` in PowerSGD state with bucket index ([#48867](https://github.com/pytorch/pytorch/pull/48867))\r\n* Make `CUDAFuture` remember and restore current device in callback ([#48789](https://github.com/pytorch/pytorch/pull/48789))\r\n* Update pipeline API to accept arbitrary sequence of Tensors and not just Tuple ([#48467](https://github.com/pytorch/pytorch/pull/48467))\r\n* Use `group.WORLD` appropriately in process group initialization. ([#48767](https://github.com/pytorch/pytorch/pull/48767))\r\n* Add error feedback to layerwise PowerSGD ([#49418](https://github.com/pytorch/pytorch/pull/49418))\r\n* Warm-start of PowerSGD by reusing states from previous iteration is possible ([#49451](https://github.com/pytorch/pytorch/pull/49451))\r\n* Change `wait()` to `value()` in some callbacks of PowerSGD communication hook ([#49709](https://github.com/pytorch/pytorch/pull/49709))\r\n* Ensure DDP + Pipe works with `find_unused_parameters`. ([#49908](https://github.com/pytorch/pytorch/pull/49908))\r\n* Enable TensorPipe CUDA sending to self ([#50674](https://github.com/pytorch/pytorch/pull/50674)) and  GDR channel ([#50763](https://github.com/pytorch/pytorch/pull/50763))\r\n* Add warning to distributed optimizer ([#50630](https://github.com/pytorch/pytorch/pull/50630))\r\n* Make python object collective API args consistent ([#50625](https://github.com/pytorch/pytorch/pull/50625))\r\n* Add option to make `rref.get_type` non-blocking. ([#50977](https://github.com/pytorch/pytorch/pull/50977))\r\n* Unescape string in RPC error message ([#49373](https://github.com/pytorch/pytorch/pull/49373))\r\n* Event Logging for NCCL Async Error Handling Process Crash ([#47244](https://github.com/pytorch/pytorch/pull/47244))\r\n* Remove `balance` and `devices` parameter from Pipe. ([#48432](https://github.com/pytorch/pytorch/pull/48432))\r\n* Error feedback for PowerSGD DDP comm hook ([#48670](https://github.com/pytorch/pytorch/pull/48670))\r\n* Add an index field to `GradBucket` for PowerSGD ([#48757](https://github.com/pytorch/pytorch/pull/48757))\r\n* Have `FutureNCCL` record streams w/ allocator in addCallback ([#48496](https://github.com/pytorch/pytorch/pull/48496)) and events in current stream ([#48497](https://github.com/pytorch/pytorch/pull/48497))\r\n* Use fresh stream from pool for each `FutureNCCL` callback ([#48498](https://github.com/pytorch/pytorch/pull/48498))\r\n* Record CUDA events for \"follow-up\" `FutureNCCL` inside `markCompleted()` ([#48499](https://github.com/pytorch/pytorch/pull/48499))\r\n* Fix `FutureNCCL`'s `completed()` disagreeing with `wait()` ([#48503](https://github.com/pytorch/pytorch/pull/48503))\r\n* Fix `FutureNCCL` not recording `DataPtr`s with caching alloc in `wait()` ([#48563](https://github.com/pytorch/pytorch/pull/48563))\r\n* Add multi-GPU support to `FutureNCCL` ([#48500](https://github.com/pytorch/pytorch/pull/48500))\r\n* Don't store device indices separately on `FutureNCCL` ([#48501](https://github.com/pytorch/pytorch/pull/48501))\r\n* Support wider range of types in `FutureNCCL` ([#48502](https://github.com/pytorch/pytorch/pull/48502))\r\n* Split `FutureNCCL`'s CUDA-specific parts from generic future logic ([#48504](https://github.com/pytorch/pytorch/pull/48504))\r\n* Merge common parts of FutureNCCL into `at::ivalue::Future` ([#48505](https://github.com/pytorch/pytorch/pull/48505))\r\n* Split out reusable `CUDAFuture` from `FutureNCCL` ([#48506](https://github.com/pytorch/pytorch/pull/48506))\r\n* Cache the `DataPtr`s in `CUDAFuture` ([#48788](https://github.com/pytorch/pytorch/pull/48788))\r\n* Modify `Pipe` to return an RRef. ([#47829](https://github.com/pytorch/pytorch/pull/47829))\r\n* Cleanup APIs for pipeline parallelism. ([#48630](https://github.com/pytorch/pytorch/pull/48630))\r\n* Fix TCPStore type coercion ([#49685](https://github.com/pytorch/pytorch/pull/49685))\r\n* Simplify the implementation of error feedback and warm-start ([#50981](https://github.com/pytorch/pytorch/pull/50981))\r\n* Explicitly specify the `dtype` of the error tensor ([#50985](https://github.com/pytorch/pytorch/pull/50985))\r\n* Check `start_PowerSGD_iter > 1` and add guidance on tuning PowerSGD configs. ([#51427](https://github.com/pytorch/pytorch/pull/51427))\r\n* Check if the backend is NCCL when a DDP communication hook is registered ([#51759](https://github.com/pytorch/pytorch/pull/51759))\r\n\r\n### TorchScript\r\n\r\n* Add multiline string dedent support ([#45580](https://github.com/pytorch/pytorch/pull/45580))\r\n* Add string versions of argument funcs in jit Node ([#45464](https://github.com/pytorch/pytorch/pull/45464))\r\n* Make sure each `warnings.warn` only executes once inside TorchScript. ([#45382](https://github.com/pytorch/pytorch/pull/45382))\r\n* Allow slicing multiple dimensions with indexes if not Tuple ([#45239](https://github.com/pytorch/pytorch/pull/45239))\r\n* Change type inferred from empty annotation ([#45360](https://github.com/pytorch/pytorch/pull/45360))\r\n* Fix stride printing/parsing formatting ([#45156](https://github.com/pytorch/pytorch/pull/45156))\r\n* Make objects throw Python AttributeError on nonexistant attr access ([#45911](https://github.com/pytorch/pytorch/pull/45911))\r\n* Make InsertInstruction overflow check a warning instead of fatal ([#46369](https://github.com/pytorch/pytorch/pull/46369))\r\n* Add an option to getWriteableTensorData to avoid copy CUDA tensor to CPU ([#46524](https://github.com/pytorch/pytorch/pull/46524))\r\n* Add error messages and workaround for RET failure of containers with a torch class type ([#46543](https://github.com/pytorch/pytorch/pull/46543))\r\n* Correctly mark unannotated NamedTuple field to be inferred TensorType ([#46969](https://github.com/pytorch/pytorch/pull/46969))\r\n* Enable ModuleDict non-literal indexing ([#45716](https://github.com/pytorch/pytorch/pull/45716))\r\n* Add an attribute to the torchscript model exported by metal ([#47174](https://github.com/pytorch/pytorch/pull/47174))\r\n* Print out interface mismatch for prim::ModuleDictIndex ([#47300](https://github.com/pytorch/pytorch/pull/47300))\r\n* better message for bad type annotation ([#47464](https://github.com/pytorch/pytorch/pull/47464))\r\n* Resolve string literal type annotations using `Resolver::resolveType` ([#47731](https://github.com/pytorch/pytorch/pull/47731))\r\n* Resolve `torch.device` in recursive compilation of classes ([#47734](https://github.com/pytorch/pytorch/pull/47734))\r\n* Metacompile boolean constants ([#46721](https://github.com/pytorch/pytorch/pull/46721))\r\n* Allow JIT unpickler to accept CUDA DataPtr from read_record_ ([#46827](https://github.com/pytorch/pytorch/pull/46827))\r\n* Skip None submodule during JIT-tracing ([#49765](https://github.com/pytorch/pytorch/pull/49765))\r\n* Add `__prepare_scriptable__` duck typing to allow replacing `nn.Module`s with scriptable preparations (#45645) ([#49242](https://github.com/pytorch/pytorch/pull/49242))\r\n* Fix deprecation warning in scalar_type_analysis ([#50218](https://github.com/pytorch/pytorch/pull/50218))\r\n* Support scripting classmethod called with object instances ([#49967](https://github.com/pytorch/pytorch/pull/49967))\r\n* Use FileStore in TorchScript for store registry ([#50248](https://github.com/pytorch/pytorch/pull/50248))\r\n* Treat has_torch_function and object_has_torch_function as static False when scripting ([#48966](https://github.com/pytorch/pytorch/pull/48966))\r\n* Print better error when class attribute IValue conversion fails ([#50255](https://github.com/pytorch/pytorch/pull/50255))\r\n* Clean up some type annotations in test/jit/...../test_class_type.py ([#50156](https://github.com/pytorch/pytorch/pull/50156))\r\n* Type annotations in test/jit ([#50293](https://github.com/pytorch/pytorch/pull/50293))\r\n* Eliminate static default_extra_files_mobile from header import.h ([#50832](https://github.com/pytorch/pytorch/pull/50832))\r\n* Dump torch::jit::AliasDb objects as Graphviz files ([#50452](https://github.com/pytorch/pytorch/pull/50452))\r\n* Fix test_jit_cuda_archflags on machine with more than one arch ([#50405](https://github.com/pytorch/pytorch/pull/50405))\r\n* Provide more info when attribute fails to convert ([#50870](https://github.com/pytorch/pytorch/pull/50870))\r\n* Adding correct error message for for..else ([#51258](https://github.com/pytorch/pytorch/pull/51258))\r\n* Handle error during dict expansion ([#51374](https://github.com/pytorch/pytorch/pull/51374))\r\n\r\n### Mobile\r\n\r\n* Update default output extension in optimize_for_mobile.cc ([#45598](https://github.com/pytorch/pytorch/pull/45598))\r\n* Add named tuple's error message and workaround for RET failure ([#46347](https://github.com/pytorch/pytorch/pull/46347))\r\n* [Metal] Add metal backend type ([#46455](https://github.com/pytorch/pytorch/pull/46455))\r\n* [Metal] Add the Python binding for optimize_for_mobile ([#46456](https://github.com/pytorch/pytorch/pull/46456))\r\n* [Metal] Add pin_memory check in empty_strided ([#47228](https://github.com/pytorch/pytorch/pull/47228))\r\n* [Metal] Calculate strides for metal tensors ([#50309](https://github.com/pytorch/pytorch/pull/50309))\r\n* [Metal] Clean up the operator tests ([#50311](https://github.com/pytorch/pytorch/pull/50311))\r\n* Add an overload for deserialize() that doesn't accept the extra_files map. ([#50932](https://github.com/pytorch/pytorch/pull/50932))\r\n* bundled_inputs: Preserve bundled input related methods when calling optimize_for_mobile ([#49170](https://github.com/pytorch/pytorch/pull/49170))\r\n* bundled_inputs: Preserved all functions generated by bundled inputs ([#51496](https://github.com/pytorch/pytorch/pull/51496))\r\n* bundled_inputs: Expanded Bundled Inputs To Any Public Function ([#51153](https://github.com/pytorch/pytorch/pull/51153))\r\n* Expose _export_operator_list to python ([#51312](https://github.com/pytorch/pytorch/pull/51312))\r\n\r\n### Quantization\r\n\r\n* Quantized Operators and Modules\r\n    * Add reflection padding to conv ([#49011](https://github.com/pytorch/pytorch/pull/49011))\r\n    * Add support for 2D indices for quantized embedding operators ([#47766](https://github.com/pytorch/pytorch/pull/47766))\r\n    * quantize_tensor_per_channel ARM implementation ([#46018](https://github.com/pytorch/pytorch/pull/46018))\r\n    * Support either min or max in qclamp ([#45937](https://github.com/pytorch/pytorch/pull/45937))\r\n    * Add preliminary support for advanced indexing ([#49346](https://github.com/pytorch/pytorch/pull/49346))\r\n    * Add backend_independent option for quantized linear module ([#48192](https://github.com/pytorch/pytorch/pull/48192))\r\n    * Add out-variant for the reflection pad ([#48037](https://github.com/pytorch/pytorch/pull/48037))\r\n    * Support 2 dim input in quantized batchnorm 1d ([#51597](https://github.com/pytorch/pytorch/pull/51597))\r\n* Typing, Formatting, Error Messages, Logging and Tests\r\n    * numeric suite: add types to eager ([#51168](https://github.com/pytorch/pytorch/pull/51168))\r\n    * Enable type check for torch.quantization.fake_quantize ([#45701](https://github.com/pytorch/pytorch/pull/45701))\r\n    * Type check for `torch.quantization.observer` ([#45630](https://github.com/pytorch/pytorch/pull/45630)), `torch.quantization._numeric_suite` ([#46330](https://github.com/pytorch/pytorch/pull/46330)), `torch.quantization.stubs` ([#46475](https://github.com/pytorch/pytorch/pull/46475)), `quantization.fx.Quantizer` ([#48343](https://github.com/pytorch/pytorch/pull/48343)), `quantization.fx.Quantizer` ([#48350](https://github.com/pytorch/pytorch/pull/48350)), `quantization_mappings.py` ([#49179](https://github.com/pytorch/pytorch/pull/49179)), `fusion_patterns.py` ([#49606](https://github.com/pytorch/pytorch/pull/49606)), `torch/nn/quantized/modules` ([#49941](https://github.com/pytorch/pytorch/pull/49941)), quantization-related files in `torch/jit` ([#49939](https://github.com/pytorch/pytorch/pull/49939)), fuser ([#48844](https://github.com/pytorch/pytorch/pull/48844)), quantization_patterns ([#48851](https://github.com/pytorch/pytorch/pull/48851)), observed_module.py ([#49607](https://github.com/pytorch/pytorch/pull/49607)), quantization ([#49942](https://github.com/pytorch/pytorch/pull/49942))\r\n    * Enable mypy on `torch/quantization/fx/*` ([#48331](https://github.com/pytorch/pytorch/pull/48331))\r\n    * Make each line of fx/quantize.py <=80 chars ([#48357](https://github.com/pytorch/pytorch/pull/48357))\r\n    * Add more typehints ([#48774](https://github.com/pytorch/pytorch/pull/48774), [#48794](https://github.com/pytorch/pytorch/pull/48794), [#48792](https://github.com/pytorch/pytorch/pull/48792))\r\n    * Nice error message on convtranspose with per-channel weight ([#49899](https://github.com/pytorch/pytorch/pull/49899))\r\n    * Throw a nice error message for allclose with quantized inputs ([#49802](https://github.com/pytorch/pytorch/pull/49802))\r\n    * Add type annotations to torch.nn.quantized.modules.conv ([#49702](https://github.com/pytorch/pytorch/pull/49702))\r\n    * Add type annotations to conv_fused/blas_compare/blas_compare_setup ([#51235](https://github.com/pytorch/pytorch/pull/51235))\r\n    * Add API usage logging to numeric suite ([#46504](https://github.com/pytorch/pytorch/pull/46504)) and quantization ([#46095](https://github.com/pytorch/pytorch/pull/46095))\r\n* Sparsity\r\n    * Block Sparse kernel ([#50585](https://github.com/pytorch/pytorch/pull/50585))\r\n    * Add A matrix pretransformed based sparse kernels for linear ([#50587](https://github.com/pytorch/pytorch/pull/50587))\r\n    * Add dyanmic linear sparse kernel for arm64 ([#50591](https://github.com/pytorch/pytorch/pull/50591))\r\n* Others\r\n    * Use tensor's quantized properties directly in pickler ([#46267](https://github.com/pytorch/pytorch/pull/46267))\r\n    * Remove register api and rename get_*mapping to get_default*_mapping ([#46337](https://github.com/pytorch/pytorch/pull/46337))\r\n    * Update HistogramObserver to be scriptable ([#51081](https://github.com/pytorch/pytorch/pull/51081))\r\n    * Support varying size input in numeric suite ([#47391](https://github.com/pytorch/pytorch/pull/47391))\r\n    * Backend string for the quantized types ([#49965](https://github.com/pytorch/pytorch/pull/49965))\r\n    * Disable pruning on embedding look up operators when compressed_indices_mapping = {0} ([#48672](https://github.com/pytorch/pytorch/pull/48672))\r\n    * Support out variant of embedding_bag_byte_rowwise_offsets_out ([#49561](https://github.com/pytorch/pytorch/pull/49561))\r\n\r\n### ONNX\r\n\r\n* Update embedding_bag export ([#44693](https://github.com/pytorch/pytorch/pull/44693))\r\n* Improve error handling for adaptive_pool ([#45874](https://github.com/pytorch/pytorch/pull/45874))\r\n* Support nd mask index in opset >= 11 ([#45252](https://github.com/pytorch/pytorch/pull/45252))\r\n* Update peephole pass for prim::ListUnpack ([#46264](https://github.com/pytorch/pytorch/pull/46264))\r\n* Slightly improve indexing with ellipsis under scripting ([#46571](https://github.com/pytorch/pytorch/pull/46571))\r\n* Update batch_norm symbolic to handle track_running_stats=False ([#47135](https://github.com/pytorch/pytorch/pull/47135))\r\n* Cast Gather index to Long if needed ([#47653](https://github.com/pytorch/pytorch/pull/47653))\r\n* Handle dynamic input axes for prim_ConstantChunk ([#48176](https://github.com/pytorch/pytorch/pull/48176))\r\n* Remove usage of isCompleteTensor() in symbolic functions ([#48162](https://github.com/pytorch/pytorch/pull/48162))\r\n* Changes to export API to better handle named arguments ([#47367](https://github.com/pytorch/pytorch/pull/47367))\r\n* Modified var_mean symbolic to support more combinations of dims ([#48949](https://github.com/pytorch/pytorch/pull/48949))\r\n* Support gelu for fp16 export ([#50911](https://github.com/pytorch/pytorch/pull/50911))\r\n* Enable Constant Folding for ONNX Opset 13 ([#51523](https://github.com/pytorch/pytorch/pull/51523))\r\n* Export and shape inference for prim uninitialized in If subblock ([#46094](https://github.com/pytorch/pytorch/pull/46094))\r\n* Scripting support for inputs to index_put ([#46866](https://github.com/pytorch/pytorch/pull/46866))\r\n* Track and list model params for scripting ([#47348](https://github.com/pytorch/pytorch/pull/47348))\r\n* Modifications in remove inplace ops passes to better handle binary inplace ops ([#51572](https://github.com/pytorch/pytorch/pull/51572))\r\n* Improve error message for parse_arg in symbolic functions ([#51516](https://github.com/pytorch/pytorch/pull/51516))\r\n* Update error message that displays when encountering an op unsupported for ONNX export ([#51522](https://github.com/pytorch/pytorch/pull/51522))\r\n* Preserve param names during in-place op removal ([#50955](https://github.com/pytorch/pytorch/pull/50955))\r\n* Handle sequence output shape and type inference ([#50599](https://github.com/pytorch/pytorch/pull/50599))\r\n* Update constant-folding of Gather op to include cases where rank of indices input is 0 ([#51514](https://github.com/pytorch/pytorch/pull/51514))\r\n* Update unsafe_chunk() method to support new version 13 of Split operator ([#51524](https://github.com/pytorch/pytorch/pull/51524))\r\n* Replace optional parameters of Resize with placeholder for ops13 ([#50954](https://github.com/pytorch/pytorch/pull/50954))\r\n\r\n### Vulkan\r\n\r\nThis release brings about a complete rewrite of PyTorch\u2019s Vulkan backend with primary focus on improved performance, robustness, and better code structure and organization.  These changes are transparent to the end user.  Considering that this is a rewrite, many of these changes also qualify as performance improvements.\r\n\r\n* Add Vulkan Tensor factory. ([#44016](https://github.com/pytorch/pytorch/pull/44016))\r\n* Redo Vulkan command and descriptor pools. ([#44496](https://github.com/pytorch/pytorch/pull/44496))\r\n* Add low level utilities image sampler ([#45037](https://github.com/pytorch/pytorch/pull/45037)), fence ([#45148](https://github.com/pytorch/pytorch/pull/45148)), tensor copy ([#46481](https://github.com/pytorch/pytorch/pull/46481)), job dispatch and flush ([#46008](https://github.com/pytorch/pytorch/pull/46008)), \r\n* Add more ops Add ([#44017](https://github.com/pytorch/pytorch/pull/44017)), Mul ([#47021](https://github.com/pytorch/pytorch/pull/47021)), Mm, Pool, Upsample ([#47063](https://github.com/pytorch/pytorch/pull/47063)), Conv2D ([#46900](https://github.com/pytorch/pytorch/pull/46900), [#48266](https://github.com/pytorch/pytorch/pull/48266), [#48816](https://github.com/pytorch/pytorch/pull/48816)), clamp ([#47196](https://github.com/pytorch/pytorch/pull/47196)), reshape ([#47252](https://github.com/pytorch/pytorch/pull/47252)), mean ([#47312](https://github.com/pytorch/pytorch/pull/47312)), \r\n* Add CMake option to enable Vulkan [v2] API. ([#46503](https://github.com/pytorch/pytorch/pull/46503))\r\n* Add `Tensor.is_vulkan` ([#46655](https://github.com/pytorch/pytorch/pull/46655))\r\n\r\n### Misc\r\n\r\n* Factory operators (at::empty, at::zeroes,...) now have a new overload in the C++ API that takes ScalarType, Layout, Device and pin_memory parameters separately, in addition to the previously existing overload that takes one TensorOptions argument. ([#44087](https://github.com/pytorch/pytorch/pull/44087))\r\n\r\n# Bug fixes\r\n\r\n### Python API\r\n\r\n* Fix `torch.nn.BatchNorm{1,2,3}d` channels_last contiguity check ([#50659](https://github.com/pytorch/pytorch/pull/50659))\r\n* Fix `torch.nn.ConstantPadNd` not preserving memory format ([#50898](https://github.com/pytorch/pytorch/pull/50898))\r\n* Fix dtype of first sample in `torch.quasirandom.SobolEngine` ([#51578](https://github.com/pytorch/pytorch/pull/51578))\r\n* Fixes bug in `torch.sspaddmm` ([#45963](https://github.com/pytorch/pytorch/pull/45963))\r\n* Check `support_as_strided` before using `torch.empty_strided` ([#46746](https://github.com/pytorch/pytorch/pull/46746))\r\n* Fix internal assert for `torch.heaviside` with cuda tensor and cpu scalar tensor ([#46831](https://github.com/pytorch/pytorch/pull/46831))\r\n* Fix negative column numbers for `torch.eye` ([#46841](https://github.com/pytorch/pytorch/pull/46841))\r\n* Fix segfault with `torch.orgqr` ([#46700](https://github.com/pytorch/pytorch/pull/46700))\r\n* Fix `torch.nn.functional.embedding` padding_idx behavior ([#46714](https://github.com/pytorch/pytorch/pull/46714))\r\n* Fix `torch.nn.Embedding.from_pretrained` to properly handle the `padding_idx` argument ([#47184](https://github.com/pytorch/pytorch/pull/47184))\r\n* Fix functions not handling discontiguous Tensors properly: `torch.dropout` ([#47552](https://github.com/pytorch/pytorch/pull/47552)), `torch.median` ([#46917](https://github.com/pytorch/pytorch/pull/46917))\r\n* Fix max_pool2d with ceil_mode ([#46558](https://github.com/pytorch/pytorch/pull/46558))\r\n* Fix type promotion for `torch.trace` on CPU ([#47305](https://github.com/pytorch/pytorch/pull/47305))\r\n* Fix `torch.kthvalue` error for scalar input ([#47600](https://github.com/pytorch/pytorch/pull/47600))\r\n* Fix multinomial when input has 0 probability ([#47386](https://github.com/pytorch/pytorch/pull/47386))\r\n* Fix incorrect warnings in `torch.nn.Parameter{List,Dict}` ([#48315](https://github.com/pytorch/pytorch/pull/48315))\r\n* Fix printing of `torch.device` ([#48655](https://github.com/pytorch/pytorch/pull/48655))\r\n* Fix parameter generator exhaustion in `torch.optim.SparseAdam` ([#47724](https://github.com/pytorch/pytorch/pull/47724))\r\n* Fix `torch.pow` bug for complex exponents ([#49809](https://github.com/pytorch/pytorch/pull/49809))\r\n* Fix gradient for `torch.norm` when `p=+inf` ([#48611](https://github.com/pytorch/pytorch/pull/48611))\r\n* Fix `SyncBatchNorm` when stats tracking is disabled ([#50126](https://github.com/pytorch/pytorch/pull/50126))\r\n* Fix `torch.elu` backward when alpha is negative ([#49272](https://github.com/pytorch/pytorch/pull/49272))\r\n* Fix pickling for Tensor-like objects ([#47732](https://github.com/pytorch/pytorch/pull/47732))\r\n* Fix `torch.distributions.Half{Cauchy,Normal}` support for `validate_args=True` ([#50403](https://github.com/pytorch/pytorch/pull/50403), [#50492](https://github.com/pytorch/pytorch/pull/50492))\r\n* Fix `torch.distributions.CatTransform` for `event_dim` > 0 ([#49111](https://github.com/pytorch/pytorch/pull/49111))\r\n* Fix `torch.distributions.Binomial` to retain lazy logit initialization ([#46055](https://github.com/pytorch/pytorch/pull/46055))\r\n* Fix `torch.pow` when exponent is provided as a scalar Tensor and on different device ([#46185](https://github.com/pytorch/pytorch/pull/46185), [#46320](https://github.com/pytorch/pytorch/pull/46320))\r\n* Fix classmethod override argument passing for Tensor-like objects ([#47114](https://github.com/pytorch/pytorch/pull/47114))\r\n* Fix internal assert when inputs are on the wrong device for `torch.`{`maximum, minimum}` ([#48446](https://github.com/pytorch/pytorch/pull/48446))\r\n* Fix `torch.distributions.utils.broadcast_all` crashing on Tensor-like objects ([#48169](https://github.com/pytorch/pytorch/pull/48169))\r\n* Fix vectorized conversion of `-nan` from float16 to float32 ([#41280](https://github.com/pytorch/pytorch/pull/41280))\r\n* Fix `torch.silu` backward for all backends other than CPU and CUDA ([#49439](https://github.com/pytorch/pytorch/pull/49439))\r\n* Fix wrong output when `torch.kthvalue` `out=` argument overlaps with input ([#48254](https://github.com/pytorch/pytorch/pull/48254))\r\n* Fix advanced indexing for Tensor-like objects ([#49324](https://github.com/pytorch/pytorch/pull/49324))\r\n* Fix `torch.distributions.TransformedDistribution` shape logic([#50581](https://github.com/pytorch/pytorch/pull/50581))\r\n* Fix `torch.nn.functional.interpolate` backward on GPU for nearest interpolation ([#51240](https://github.com/pytorch/pytorch/pull/51240))\r\n* Fix `torch.svd` ignoring `some` keyword argument for empty inputs ([#51109](https://github.com/pytorch/pytorch/pull/51109))\r\n* Fix `torch.distributions.Dirichlet` `arg_constraints` ([#51369](https://github.com/pytorch/pytorch/pull/51369))\r\n* Use deterministic implementation of `torch.index_put` and `torch.index` backward CPU in deterministic mode ([#51388](https://github.com/pytorch/pytorch/pull/51388))\r\n* Removes spurious warning in `torch.nonzero` ([#51618](https://github.com/pytorch/pytorch/pull/51618))\r\n* Fix calculation of number of elements to not overflow in many c++ implementations ([#46997](https://github.com/pytorch/pytorch/pull/46997))\r\n* Fix Parameter detection as Tensor in c++ backend ([#48963](https://github.com/pytorch/pytorch/pull/48963))\r\n* Fix bug in miopen findAlgorithm ([#46852](https://github.com/pytorch/pytorch/pull/46852))\r\n\r\n### Autograd\r\n\r\n* Fix deadlock on Windows due to bad thread termination in autograd engine ([#43532](https://github.com/pytorch/pytorch/pull/43532))\r\n* Fix deadlock in tsan builds due to bad locking in the engine ([#45867](https://github.com/pytorch/pytorch/pull/45867))\r\n* Avoid NaN values in `torch.cdist` backward for p<1 ([#45720](https://github.com/pytorch/pytorch/pull/45720))\r\n* Fix handling of `requires_grad` arg for `torch.new_`{`full,empty,zeros}` ([#46486](https://github.com/pytorch/pytorch/pull/46486))\r\n* Fix inplace check logic to be triggered when written-to Tensor does not require gradients ([#46296](https://github.com/pytorch/pytorch/pull/46296))\r\n* Set proper output differentiability for `torch.unique` ([#47930](https://github.com/pytorch/pytorch/pull/47930)), `torch.count_nonzero` ([#50866](https://github.com/pytorch/pytorch/pull/50866))\r\n* Fix race in autograd engine that lead can lead to `std::out_of_range` error ([#50164](https://github.com/pytorch/pytorch/pull/50164), [#50372](https://github.com/pytorch/pytorch/pull/50372))\r\n* Fix autograd thread crash on destruction with python-3.9 ([#50998](https://github.com/pytorch/pytorch/pull/50998))\r\n* Fix autograd side effects when printing ([#51364](https://github.com/pytorch/pytorch/pull/51364))\r\n* Fix memory leak in anomaly mode ([#51610](https://github.com/pytorch/pytorch/pull/51610))\r\n* fix `torch.hardsigmoid` backward at boundary values ([#51454](https://github.com/pytorch/pytorch/pull/51454))\r\n\r\n### CUDA\r\n\r\n* Fix incorrect CUDA `torch.nn.Embedding` result when `max_norm` is not `None` and indices are not sorted ([#45248](https://github.com/pytorch/pytorch/pull/45248))\r\n* Ensure kernel launches are checked ([#46474](https://github.com/pytorch/pytorch/pull/46474), [#46727](https://github.com/pytorch/pytorch/pull/46727))\r\n* Fix bit math ([#46837](https://github.com/pytorch/pytorch/pull/46837))\r\n* Fix test_inverse_singular for cublas path; fix cusolver inverse multi-stream issue ([#47026](https://github.com/pytorch/pytorch/pull/47026))\r\n* Fix indices computation for trilinear interpolate backwards ([#50084](https://github.com/pytorch/pytorch/pull/50084))\r\n* Fix for possible RNG offset calculation bug in cuda vectorized dropout with VEC=2 ([#50110](https://github.com/pytorch/pytorch/pull/50110))\r\n* Disable cuDNN persistent RNN on `sm_86` devices ([#49534](https://github.com/pytorch/pytorch/pull/49534))\r\n* Fix Error with `torch.flip` for cuda tensors when `dims=()` ([#50325](https://github.com/pytorch/pytorch/pull/50325))\r\n* Fix replication_pad CUDA launch configuration ([#50565](https://github.com/pytorch/pytorch/pull/50565))\r\n* Workaround for MAGMA accessing illegal memory in batched cholesky ([#50957](https://github.com/pytorch/pytorch/pull/50957))\r\n* Fix `torch.cdist` backward CUDA error due to illegal gridDim setting ([#51569](https://github.com/pytorch/pytorch/pull/51569))\r\n* Prevent CUDAFuture from using uninitialized device index ([#51505](https://github.com/pytorch/pytorch/pull/51505))\r\n* Fix incorrect usage of CUDACachingAllocator ([#48817](https://github.com/pytorch/pytorch/pull/48817))\r\n* Fix `torch.cuda.memory_allocated` to return `{}` if not initialized ([#51179](https://github.com/pytorch/pytorch/pull/51179))\r\n* Fix crash when trying to reset memory stats when no cuda device is available ([#48406](https://github.com/pytorch/pytorch/pull/48406))\r\n\r\n### torch.utils\r\n\r\n* `data.DistributedSampler`: Fix possible padding length overflow ([#45329](https://github.com/pytorch/pytorch/pull/45329))\r\n* `data.DataLoader`: Fix hang with large sampler ([#48669](https://github.com/pytorch/pytorch/pull/48669))\r\n* `data.DataLoader`: Fix unintended error when worker force kill happens #43455 ([#43462](https://github.com/pytorch/pytorch/pull/43462))\r\n* `data.DataLoader`: Fix persistent_workers + pin_memory ([#48543](https://github.com/pytorch/pytorch/pull/48543))\r\n\r\n### Complex Number\r\n\r\n* Make `torch.view_as_real` raise a proper error for backends where it is not supported ([#47018](https://github.com/pytorch/pytorch/pull/47018))\r\n* Fix bug in `toComplexWithDefault` ([#43841](https://github.com/pytorch/pytorch/pull/43841))\r\n* Fix `torch.cat` backward formula to return correct gradient values for R -> C case ([#51681](https://github.com/pytorch/pytorch/pull/51681))\r\n* Update backward formulas for `torch.{add, sub}` to correctly handle R -> C case. ([#46596](https://github.com/pytorch/pytorch/pull/46596))\r\n* Add custom implementation for `torch.csqrt` if libc++ is used ([#52018](https://github.com/pytorch/pytorch/pull/52018))\r\n\r\n### C++ API\r\n\r\n* Refine `ConvParams::use_nnpack()` to allow NNPACK convolution algorithm only be used for kernels up to 16x16.([#49464](https://github.com/pytorch/pytorch/pull/49464))\r\n\r\n### Distributed\r\n\r\n* Record FutureNCCL callback stream on CUDA caching allocator ([#45318](https://github.com/pytorch/pytorch/pull/45318))\r\n* Fix object-based collectives API to use `torch.cuda.current_device` instead of rank ([#46897](https://github.com/pytorch/pytorch/pull/46897))\r\n* Explicitly restrict the scope of `torch.cuda.synchronize` to the current device in PowerSGD ([#49711](https://github.com/pytorch/pytorch/pull/49711))\r\n* Fix Hang in Async Error Handling due to Work logging ([#46265](https://github.com/pytorch/pytorch/pull/46265))\r\n* Add missing `recordStream` in `ProcessGroupNCCL::alltoall_base` ([#46603](https://github.com/pytorch/pytorch/pull/46603))\r\n* Allow DataParallel to run zero input Module ([#46565](https://github.com/pytorch/pytorch/pull/46565))\r\n* Fix DDP issue where parameters share same `grad_accumulator` ([#46755](https://github.com/pytorch/pytorch/pull/46755))\r\n* Fix ProcessGroupNCCL profiling when profiler is not run with `use_cuda` ([#48946](https://github.com/pytorch/pytorch/pull/48946))\r\n* Refactor RPC `matchBuiltInOp` to get rid of exception swallowing ([#49009](https://github.com/pytorch/pytorch/pull/49009))\r\n* Solve zombie process problem in DDP launcher ([#49305](https://github.com/pytorch/pytorch/pull/49305))\r\n* Fix memory leak in TensorPipeAgent. ([#50564](https://github.com/pytorch/pytorch/pull/50564))\r\n* Fix warm-start for PowerSGD layer-wise compression ([#50283](https://github.com/pytorch/pytorch/pull/50283))\r\n* Fix CUDA RPC Stream Synchronization ([#50949](https://github.com/pytorch/pytorch/pull/50949))\r\n* Fix `benchmarks/distributed/ddp/benchmark.py` ([#51095](https://github.com/pytorch/pytorch/pull/51095))\r\n* Fix store based barrier to only use `add` ([#49930](https://github.com/pytorch/pytorch/pull/49930))\r\n\r\n### Mobile\r\n\r\n* Fix out-of-bounds access for caching allocator calls ([#46439](https://github.com/pytorch/pytorch/pull/46439))\r\n* Fix CPUCaching allocator guard bug ([#46922](https://github.com/pytorch/pytorch/pull/46922))\r\n* [Metal] Make the dst tensor contiguous when copying from metal (25833e5d1c)\r\n* [Metal] Fix the broken strides value for 2d transpose ([#50310](https://github.com/pytorch/pytorch/pull/50310))\r\n* [Android] Fix yuv conversion ([#50951](https://github.com/pytorch/pytorch/pull/50951))\r\n\r\n### TorchScript\r\n\r\n* Fix bugs in a number of ops in CUDA fuser ([#47795](https://github.com/pytorch/pytorch/pull/47795), [#49143,](https://github.com/pytorch/pytorch/pull/49143) [#49396](https://github.com/pytorch/pytorch/pull/49396) ,[#48329](https://github.com/pytorch/pytorch/pull/48329) and others)\r\n* Fix dict update ([#45857](https://github.com/pytorch/pytorch/pull/45857))\r\n* Fix Dict bug in constant hashing ([#45929](https://github.com/pytorch/pytorch/pull/45929))\r\n* Fix TypeError when `torch.jit.load` is passed a pathlib.Path ([#45825](https://github.com/pytorch/pytorch/pull/45825))\r\n* Fix missing call to `__setstate__` when cloning modules ([#45858](https://github.com/pytorch/pytorch/pull/45858))\r\n* Prevent caching of `graph` attribute. ([#46960](https://github.com/pytorch/pytorch/pull/46960))\r\n* Fix traced training attribute ([#47211](https://github.com/pytorch/pytorch/pull/47211))\r\n* Correctly compare Stream IValues ([#47303](https://github.com/pytorch/pytorch/pull/47303))\r\n* Correctly print out sign of near-zero double values ([#47081](https://github.com/pytorch/pytorch/pull/47081))\r\n* Properly serialize types that only appear at function input ([#47775](https://github.com/pytorch/pytorch/pull/47775))\r\n* Fix bug in get_annotation_str for ast.Subscript ([#48741](https://github.com/pytorch/pytorch/pull/48741))\r\n* Fix include files for out-of-tree compilation ([#48827](https://github.com/pytorch/pytorch/pull/48827))\r\n* Fix constant propagation schemas ([#49605](https://github.com/pytorch/pytorch/pull/49605))\r\n* Fix return type Any for Ternary ops ([#49165](https://github.com/pytorch/pytorch/pull/49165))\r\n* Fix for module_has_exports ([#50680](https://github.com/pytorch/pytorch/pull/50680))\r\n* Properly convert Python strings implictly to device ([#51340](https://github.com/pytorch/pytorch/pull/51340))\r\n* Add missing support for `torch.jit.Final` in python 3.6 ([#47393](https://github.com/pytorch/pytorch/pull/47393))\r\n\r\n### torch.fx\r\n\r\n* Fix recursion depth issue on Graph deepcopy ([#46669](https://github.com/pytorch/pytorch/pull/46669))\r\n* Fix handling of `inf` and `nan` literals ([#46894](https://github.com/pytorch/pytorch/pull/46894))\r\n* Fix corner case in name sanitization ([#46958](https://github.com/pytorch/pytorch/pull/46958))\r\n* Fix submodule naming for subgraph split ([#47869](https://github.com/pytorch/pytorch/pull/47869))\r\n* Fix create_arg for NamedTuple ([#48986](https://github.com/pytorch/pytorch/pull/48986))\r\n* Fix python code having spurious newlines from placeholders ([#49720](https://github.com/pytorch/pytorch/pull/49720))\r\n* Make `split_module` results deterministic ([#50470](https://github.com/pytorch/pytorch/pull/50470))\r\n* Fix tracing a free function with embedded constant ([#50639](https://github.com/pytorch/pytorch/pull/50639))\r\n* Fix using `fx.wrap` as a decorator ([#50677](https://github.com/pytorch/pytorch/pull/50677))\r\n* Fix annotation in generated code ([#50777](https://github.com/pytorch/pytorch/pull/50777), [#52021](https://github.com/pytorch/pytorch/pull/52021))\r\n\r\n### Quantization\r\n\r\n* Remove fake_quant after add/mul nodes during eager mode QAT ([#49213](https://github.com/pytorch/pytorch/pull/49213))\r\n* `torch.mean` add path for unsupported QNNPACK modes ([#45533](https://github.com/pytorch/pytorch/pull/45533))\r\n* Set type for GetAttr nodes in remapTypes ([#46250](https://github.com/pytorch/pytorch/pull/46250))\r\n* Avoid inserting fakequant for sigmoid/hardsigmoid/tanh in eval mode ([#47297](https://github.com/pytorch/pytorch/pull/47297))\r\n* Ensure observer respects device affinity ([#47514](https://github.com/pytorch/pytorch/pull/47514))\r\n* Fix quant type classification for float_qparam qconfig ([#48069](https://github.com/pytorch/pytorch/pull/48069))\r\n* Fix quant_type classification for fp16, fp16 ([#48073](https://github.com/pytorch/pytorch/pull/48073))\r\n* Fix a bug in leakyReLU ([#48265](https://github.com/pytorch/pytorch/pull/48265))\r\n* Fix quantization for qat.ConvBnReLU1d ([#48059](https://github.com/pytorch/pytorch/pull/48059))\r\n* Add bias once in conv_fused ([#48593](https://github.com/pytorch/pytorch/pull/48593))\r\n* Do not return unitialized qschame from getQSchemeAndQParamVector ([#49391](https://github.com/pytorch/pytorch/pull/49391))\r\n* Fix quantization for DeQuantStub ([#49428](https://github.com/pytorch/pytorch/pull/49428))\r\n* Ensure observers do not crash for empty Tensors ([#49800](https://github.com/pytorch/pytorch/pull/49800))\r\n* fake_quant: fix device affinity and buffer resizing for state_dict ([#50868](https://github.com/pytorch/pytorch/pull/50868))\r\n* Fix memory leak in qnnpack ops ([#51612](https://github.com/pytorch/pytorch/pull/51612))\r\n* Remove set_quantizer_ from native_functions.yaml ([#49463](https://github.com/pytorch/pytorch/pull/49463))\r\n* Make choose_qparams_optimized return Tensors to preserve dtype ([#45530](https://github.com/pytorch/pytorch/pull/45530))\r\n* Use PlaceholderObserver as default dynamic quant observer ([#45343](https://github.com/pytorch/pytorch/pull/45343))\r\n* FixedQParamsFakeQuantize: adjust default quant_min and quant_max ([#47423](https://github.com/pytorch/pytorch/pull/47423))\r\n* Add bias once in conv_fused (#48593) ([#48661](https://github.com/pytorch/pytorch/pull/48661))\r\n* Fix unused var warning when building for different archs. ([#48730](https://github.com/pytorch/pytorch/pull/48730))\r\n* Make the CUDA fake quantize logic consistent with CPU fake quantize logic ([#49808](https://github.com/pytorch/pytorch/pull/49808))\r\n* eager quant: fix error with removing forward hooks ([#49813](https://github.com/pytorch/pytorch/pull/49813))\r\n\r\n### ONNX\r\n\r\n* Fix `torch.flatten` operator ([#45632](https://github.com/pytorch/pytorch/pull/45632))\r\n* Reimplement _var_mean to ensure non-negative ([#47240](https://github.com/pytorch/pytorch/pull/47240))\r\n* Fix scripting of `torch.{rand,randn,where}` ([#45793](https://github.com/pytorch/pytorch/pull/45793))\r\n* Fix `torch.eye` export ([#47016](https://github.com/pytorch/pytorch/pull/47016))\r\n* Fix dtype for log_softmax export ([#46627](https://github.com/pytorch/pytorch/pull/46627))\r\n* Fix graph position to insert clone node for inplace op removal ([#51520](https://github.com/pytorch/pytorch/pull/51520))\r\n* Fix graph sequence output from loop node ([#51521](https://github.com/pytorch/pytorch/pull/51521))\r\n* Do not dereference nullptr in scalar type analysis ([#50237](https://github.com/pytorch/pytorch/pull/50237))\r\n* Fix bug in `torch.unfold` symbolic ([#51515](https://github.com/pytorch/pytorch/pull/51515))\r\n* Fix opset 11 ConstantChunk with negative dim ([#51525](https://github.com/pytorch/pytorch/pull/51525))\r\n* Fix bug in scatter_add ([#51527](https://github.com/pytorch/pytorch/pull/51527))\r\n\r\n### Vulkan\r\n\r\n* Fix interval midpoint calculation ([#46839](https://github.com/pytorch/pytorch/pull/46839))\r\n* Fix Vulkan `torch.empty` (and family) breakage as a result of API update. ([#47937](https://github.com/pytorch/pytorch/pull/47937))\r\n* Fix Addmm prepacking to persist after GPU flush ([#48313](https://github.com/pytorch/pytorch/pull/48313))\r\n* Properly forbid dilation > 1 for conv2d ([#48800](https://github.com/pytorch/pytorch/pull/48800))\r\n\r\n### Misc\r\n\r\n* Fix c++ extension ninja CUDA build ([#49344](https://github.com/pytorch/pytorch/pull/49344))\r\n* Only include dataclasses for py < 3.8 to make `setup.py` compatible with older python versions ([#45611](https://github.com/pytorch/pytorch/pull/45611))\r\n\r\n# Performance\r\n\r\n### Python API\r\n\r\n* Rewrite `torch.kron` to improve performance and support more dtypes ([#50927](https://github.com/pytorch/pytorch/pull/50927))\r\n* Enable the faster combined weight branch in MHA when query/key/value is same object with NaN ([#48126](https://github.com/pytorch/pytorch/pull/48126))\r\n\r\n### Autograd\r\n\r\n* `autograd.gradcheck` update to reduce computations ([#45757](https://github.com/pytorch/pytorch/pull/45757))\r\n* Reduce memory usage for `torch.mm` when only one input requires gradient ([#45777](https://github.com/pytorch/pytorch/pull/45777))\r\n* Reduce autograd engine startup cost ([#47592](https://github.com/pytorch/pytorch/pull/47592))\r\n* Make `torch.svd` backward formula more memory and computationally efficient. ([#50109](https://github.com/pytorch/pytorch/pull/50109))\r\n\r\n### CUDA\r\n\r\n* Fix perfornance issue of GroupNorm on CUDA when feature map is small. ([#46170](https://github.com/pytorch/pytorch/pull/46170))\r\n* Concat fast path with empty tensor ([#46805](https://github.com/pytorch/pytorch/pull/46805))\r\n* Support the strided tensor on input for `torch.cat` ([#46859](https://github.com/pytorch/pytorch/pull/46859))\r\n* Pin destination memory for `cuda_tensor.to(\"cpu\", non_blocking=True)` ([#46878](https://github.com/pytorch/pytorch/pull/46878))\r\n* Add proper maximum number of threads per block for sm_86 as 1536 ([#45889](https://github.com/pytorch/pytorch/pull/45889)) \r\n* Use MTA for amp grad unscaling, enforce op math type in MTA functors, and allow op lambdas ([#44778](https://github.com/pytorch/pytorch/pull/44778))\r\n* Improve performance of CUDA trilinear interpolate backward  ([#52649](https://github.com/pytorch/pytorch/pull/52649))\r\n\r\n### C++ API\r\n\r\n* Avoid computing AutogradKey if not needed to speed up low level C++ calls ([#46252](https://github.com/pytorch/pytorch/pull/46252))\r\n* VariableKernel calls into scattered C++ api ([#44158](https://github.com/pytorch/pytorch/pull/44158))\r\n* Make validate debug-only in Device constructor ([#49123](https://github.com/pytorch/pytorch/pull/49123))\r\n* Add macro to optionally devirtualize `TensorImpl::numel()` ([#49766](https://github.com/pytorch/pytorch/pull/49766)) and `TensorImpl::sizes()` ([#50176](https://github.com/pytorch/pytorch/pull/50176))\r\n* Inline access to low level Dispatcher ([#50644](https://github.com/pytorch/pytorch/pull/50644))\r\n\r\n### Distributed\r\n\r\n* Only track variables with grad accumulator for find_unused_parameters=True in DDP to save memory ([#45942](https://github.com/pytorch/pytorch/pull/45942))\r\n* Benchmark combining Distributed Data Parallel and Distributed RPC ([#46993](https://github.com/pytorch/pytorch/pull/46993))\r\n* Drop FutureNCCL in favor of vanilla CUDAFuture ([#49014](https://github.com/pytorch/pytorch/pull/49014))\r\n* Pytorch Distributed RPC Reinforcement Learning Benchmark (Throughput and Latency) ([#46901](https://github.com/pytorch/pytorch/pull/46901))\r\n\r\n### TorchScript\r\n\r\n* Optimized hot path in JIT graph executor ([#47465](https://github.com/pytorch/pytorch/pull/47465), [#48061](https://github.com/pytorch/pytorch/pull/48061),[#48034](https://github.com/pytorch/pytorch/pull/48034))\r\n* Added support for `is_nan`, `to`, and `lgamma` in CUDA fuser([#45791](https://github.com/pytorch/pytorch/pull/45791), [#48973](https://github.com/pytorch/pytorch/pull/48973), [#48976](https://github.com/pytorch/pytorch/pull/48976))\r\n* Added additional optimizations as part of `torch.jit.freeze` (Conv-Batchnorm, Conv-Add, and Conv-Mul folding, Dropout Removal) ([#50222](https://github.com/pytorch/pytorch/pull/50222)).\r\n* Fast TypeMeta/ScalarType conversion ([#45544](https://github.com/pytorch/pytorch/pull/45544))\r\n* Fix getCustomClassType() perf ([#48981](https://github.com/pytorch/pytorch/pull/48981))\r\n* Avoid move-constructing a List in listConstruct ([#49355](https://github.com/pytorch/pytorch/pull/49355))\r\n* Specialize `list_element_from` for `IValue` to avoid extra move/copy ([#50124](https://github.com/pytorch/pytorch/pull/50124))\r\n\r\n### Mobile\r\n\r\n* Avoid inlining kernel lambdas on mobile ([#46249](https://github.com/pytorch/pytorch/pull/46249))\r\n* Free original weight after prepacking in XNNPACK based op ([#46541](https://github.com/pytorch/pytorch/pull/46541))\r\n* [Metal] Make permuteWeights inline ([#47634](https://github.com/pytorch/pytorch/pull/47634))\r\n* [Metal] Use MPSCNN kernels for binary elementwise ops (c18403a693)\r\n\r\n### Vulkan\r\n\r\n* Enable prepacked addmm/mm for linear layers ([#47815](https://github.com/pytorch/pytorch/pull/47815))\r\n* Tweak memory use. ([#47728](https://github.com/pytorch/pytorch/pull/47728))\r\n* Add linear memory allocator. ([#48569](https://github.com/pytorch/pytorch/pull/48569))\r\n* Optimize Vulkan command buffer submission rate. ([#49112](https://github.com/pytorch/pytorch/pull/49112))\r\n\r\n### torch.fx\r\n\r\n* Speed up non-parameter tensor lookup ([#47325](https://github.com/pytorch/pytorch/pull/47325))\r\n\r\n### Quantization\r\n\r\n* Parallelize the quantization conversion operators ([#45536](https://github.com/pytorch/pytorch/pull/45536))\r\n* Add a more memory efficient version of fake quant ([#50561](https://github.com/pytorch/pytorch/pull/50561))\r\n* mem-efficient learnable fake quantization ([#49315](https://github.com/pytorch/pytorch/pull/49315), [#51255](https://github.com/pytorch/pytorch/pull/51255), [#51159](https://github.com/pytorch/pytorch/pull/51159))\r\n* Remove contiguous calls in qembeddingbag ([#48993](https://github.com/pytorch/pytorch/pull/48993))\r\n* Update embedding module to not store qweight ([#50418](https://github.com/pytorch/pytorch/pull/50418))\r\n\r\n### Misc\r\n\r\n* Extra sampling of record function events for the profiler ([#49114](https://github.com/pytorch/pytorch/pull/49114))\r\n\r\n# Documentation\r\n\r\n### Python API\r\n\r\n* Add information how to control randomness in `DataLoader` ([#45749](https://github.com/pytorch/pytorch/pull/45749))\r\n* Revamp reproducibility notes ([#45748](https://github.com/pytorch/pytorch/pull/45748))\r\n* Revamp `torch.optim` doc for better understanding ([#45944](https://github.com/pytorch/pytorch/pull/45944))\r\n* Revamp `torch.sparse` tensor documentation. ([#45400](https://github.com/pytorch/pytorch/pull/45400))\r\n* Add doc for `torch.overrides` submodule. ([#48170](https://github.com/pytorch/pytorch/pull/48170))\r\n* Add note on `nn.Module` overview and design principles ([#51536](https://github.com/pytorch/pytorch/pull/51536))\r\n* Add helper functions section to `torch.fft` doc ([#46032](https://github.com/pytorch/pytorch/pull/46032))\r\n* Add object-based collective APIs to public docs ([#48909](https://github.com/pytorch/pytorch/pull/48909))\r\n* Fix diverse typos and rendering issues in `torch.` doc ([#46328](https://github.com/pytorch/pytorch/pull/46328), [#46589](https://github.com/pytorch/pytorch/pull/46589), [#47545](https://github.com/pytorch/pytorch/pull/47545), [#48316](https://github.com/pytorch/pytorch/pull/48316), [#48328](https://github.com/pytorch/pytorch/pull/48328), [#48673](https://github.com/pytorch/pytorch/pull/48673), [#48787](https://github.com/pytorch/pytorch/pull/48787), [#47762](https://github.com/pytorch/pytorch/pull/47762), [#48970](https://github.com/pytorch/pytorch/pull/48970), [#49136](https://github.com/pytorch/pytorch/pull/49136), [#49388](https://github.com/pytorch/pytorch/pull/49388), [#49413](https://github.com/pytorch/pytorch/pull/49413), [#49584](https://github.com/pytorch/pytorch/pull/49584), [#49667](https://github.com/pytorch/pytorch/pull/49667), [#41887](https://github.com/pytorch/pytorch/pull/41887), [#50254](https://github.com/pytorch/pytorch/pull/50254), [#51053](https://github.com/pytorch/pytorch/pull/51053), [#51212](https://github.com/pytorch/pytorch/pull/51212), [#51439](https://github.com/pytorch/pytorch/pull/51439), [#51286](https://github.com/pytorch/pytorch/pull/51286), [#49648](https://github.com/pytorch/pytorch/pull/49648))\r\n* Fix diverse typo and rendering issues in `torch.nn` doc ([#45662](https://github.com/pytorch/pytorch/pull/45662), [#45660](https://github.com/pytorch/pytorch/pull/45660), [#45587](https://github.com/pytorch/pytorch/pull/45587), [#45763](https://github.com/pytorch/pytorch/pull/45763), [#46853](https://github.com/pytorch/pytorch/pull/46853), [#48577](https://github.com/pytorch/pytorch/pull/48577), [#48775](https://github.com/pytorch/pytorch/pull/48775), [#49950](https://github.com/pytorch/pytorch/pull/49950), [#50430](https://github.com/pytorch/pytorch/pull/50430), [#48596](https://github.com/pytorch/pytorch/pull/48596))\r\n* Fix diverse typo and rendering issues in `torch.linalg` doc ([#51459](https://github.com/pytorch/pytorch/pull/51459), [#51353](https://github.com/pytorch/pytorch/pull/51353), [#51620](https://github.com/pytorch/pytorch/pull/51620), [#51641](https://github.com/pytorch/pytorch/pull/51641), [#51651](https://github.com/pytorch/pytorch/pull/51651), [#51658](https://github.com/pytorch/pytorch/pull/51658), [#51659](https://github.com/pytorch/pytorch/pull/51659), [#51660](https://github.com/pytorch/pytorch/pull/51660))\r\n* Update docs for `torch.nn`:  in-place modification of weight in `nn.Embedding` ([#45595](https://github.com/pytorch/pytorch/pull/45595))\r\n* Update docs for `torch.distributions`: `NegativeBinomial` ([#45693](https://github.com/pytorch/pytorch/pull/45693)), `Categorical` ([#45804](https://github.com/pytorch/pytorch/pull/45804)), `LKJCholesky` ([#52904](https://github.com/pytorch/pytorch/pull/52904))\r\n* Improve `torch.matmul` doc regarding broadcasting ([#45699](https://github.com/pytorch/pytorch/pull/45699))\r\n* Add function signature for `torch.pixel_shuffle` ([#45661](https://github.com/pytorch/pytorch/pull/45661))\r\n* Fix signature for `torch.poisson` ([#45656](https://github.com/pytorch/pytorch/pull/45656))\r\n* Add 3D reduction example to `torch.tensordot` ([#45697](https://github.com/pytorch/pytorch/pull/45697))\r\n* Fix `torch.matrix_exp` ([#45909](https://github.com/pytorch/pytorch/pull/45909))\r\n* Fix typo in `torch.load` docstring for the `f` parameter ([#49350](https://github.com/pytorch/pytorch/pull/49350))\r\n* Document fix for `torch.logspace` and `torch.linspace` ([#46056](https://github.com/pytorch/pytorch/pull/46056))\r\n* Improve clarity of `torch.norm` ([#42696](https://github.com/pytorch/pytorch/pull/42696))\r\n* Fix info on the shape of pivots in `torch.lu` ([#46844](https://github.com/pytorch/pytorch/pull/46844))\r\n* Add `generator` param in `torch.randperm` doc ([#47231](https://github.com/pytorch/pytorch/pull/47231))\r\n* Updated doc for `torch.{v}dot` ([#47242](https://github.com/pytorch/pytorch/pull/47242))\r\n* Update doc of `torch.eig` about backward([#47598](https://github.com/pytorch/pytorch/pull/47598))\r\n* Fix `torch.swap{dim/axes}` to properly appear in doc ([#48376](https://github.com/pytorch/pytorch/pull/48376))\r\n* Add global `nn.Module` hooks to nn doc ([#48374](https://github.com/pytorch/pytorch/pull/48374))\r\n* Added `torch.linalg.cond` to doc([#48941](https://github.com/pytorch/pytorch/pull/48941))\r\n* Improve new_group example in the context of `torch.nn.SyncBatchNorm` ([#48897](https://github.com/pytorch/pytorch/pull/48897))\r\n* Update `is_floating_point()` docs to mention bfloat16 ([#49611](https://github.com/pytorch/pytorch/pull/49611))\r\n* Improve docs for `torch.{scatter,gather}` ([#49679](https://github.com/pytorch/pytorch/pull/49679))\r\n* Rename \"Arguments:\" to \"Args:\" in all doc ([#49736](https://github.com/pytorch/pytorch/pull/49736))\r\n* Fix a KaTeX crash and many docstring issues ([#49684](https://github.com/pytorch/pytorch/pull/49684))\r\n* Improve `torch.flatten` doc ([#49501](https://github.com/pytorch/pytorch/pull/49501))\r\n* Add note about `torch.flip` returning new tensor and not view. ([#50041](https://github.com/pytorch/pytorch/pull/50041))\r\n* Add instructional error message for cudnn RNN double backward workaround ([#33884](https://github.com/pytorch/pytorch/pull/33884))\r\n* Add centered FFT example to `torch.fft.fftshift` doc ([#51223](https://github.com/pytorch/pytorch/pull/51223))\r\n* Add `torch.sgn` to doc ([#51479](https://github.com/pytorch/pytorch/pull/51479))\r\n\r\n### Autograd\r\n\r\n* Fix many typos and rendering issues in `torch.autograd` doc ([#48765](https://github.com/pytorch/pytorch/pull/48765), [#45849](https://github.com/pytorch/pytorch/pull/45849), [#50166](https://github.com/pytorch/pytorch/pull/50166), [#51035](https://github.com/pytorch/pytorch/pull/51035), [#51335](https://github.com/pytorch/pytorch/pull/51335))\r\n* Update the error message explaining when to use the `retain_grad` flag ([#47084](https://github.com/pytorch/pytorch/pull/47084))\r\n\r\n### Complex Number\r\n\r\n* Fix typo in complex autograd docs ([#49755](https://github.com/pytorch/pytorch/pull/49755))\r\n* Doc update for complex numbers ([#51129](https://github.com/pytorch/pytorch/pull/51129), [#51661](https://github.com/pytorch/pytorch/pull/51661))\r\n* Document that `torch.remainder` does not support complex inputs ([#48024](https://github.com/pytorch/pytorch/pull/48024))\r\n\r\n### CUDA\r\n\r\n* Add a Note on CUDA Stream ([#45754](https://github.com/pytorch/pytorch/pull/45754%20(http:/#45754))), [#45754](https://github.com/pytorch/pytorch/pull/45754))\r\n* Add docs on how to toggle TF32 flags on C++ ([#47331](https://github.com/pytorch/pytorch/pull/47331))\r\n* Fix syntax issue in C++ cuda api note ([#48434](https://github.com/pytorch/pytorch/pull/48434))\r\n* Change \u201ctruncating\u201d to \u201crounding\u201c in TF32 docs ([#49625](https://github.com/pytorch/pytorch/pull/49625))\r\n* Add docstring to `torch.cuda.get_device_properties` ([#49792](https://github.com/pytorch/pytorch/pull/49792))\r\n* Add doc for `cuda.memory_fraction` and `cuda.gpu_process` ([#51372](https://github.com/pytorch/pytorch/pull/51372))\r\n\r\n### C++ API\r\n\r\n* Add guide for choosing dispatch keys in `native_functions.yaml` ([#46126](https://github.com/pytorch/pytorch/pull/46126))\r\n* Add a few more comments on dispatch key computation methods ([#46128](https://github.com/pytorch/pytorch/pull/46128))\r\n* Improve error messages for operator registration API ([#47636](https://github.com/pytorch/pytorch/pull/47636))\r\n* Add Math/DefaultBackend to dispatch key guide, introduce `PythonDispatcher` ([#50854](https://github.com/pytorch/pytorch/pull/50854))\r\n\r\n### Distributed\r\n\r\n* Clarify callback behavior when future is completed ([#50978](https://github.com/pytorch/pytorch/pull/50978))\r\n* Enhance `new_group` doc to mention using NCCL concurrently. ([#48872](https://github.com/pytorch/pytorch/pull/48872))\r\n* Adding c10d Store API Docs ([#45543](https://github.com/pytorch/pytorch/pull/45543))\r\n* Fix distributed documentation for asynchronous collective Work objects ([#45709](https://github.com/pytorch/pytorch/pull/45709))\r\n* Fix DDP documentation ([#46861](https://github.com/pytorch/pytorch/pull/46861))\r\n* Fix inaccurate note in `DistributedDataParallel` ([#47156](https://github.com/pytorch/pytorch/pull/47156))\r\n* Minor doc fixes for `init_process_group` ([#47644](https://github.com/pytorch/pytorch/pull/47644))\r\n* Docs fixes for `HashStore` API ([#47643](https://github.com/pytorch/pytorch/pull/47643))\r\n* Update links in DDP note ([#47663](https://github.com/pytorch/pytorch/pull/47663))\r\n* Small documentation changes for `RRef` and Dist Autograd ([#48123](https://github.com/pytorch/pytorch/pull/48123))\r\n* Add examples for new object-based c10d APIs ([#43932](https://github.com/pytorch/pytorch/pull/43932))\r\n* Minor update of the comments on PowerSGD. ([#49246](https://github.com/pytorch/pytorch/pull/49246))\r\n* Updating `init_process_group` docs to indicate correct rank range ([#49131](https://github.com/pytorch/pytorch/pull/49131))\r\n* Store Python API Docs Fixes ([#49130](https://github.com/pytorch/pytorch/pull/49130))\r\n* Fix link in distributed contributing doc and add link ([#49141](https://github.com/pytorch/pytorch/pull/49141))\r\n* Updating Docs to Reflect `FileStore` changes ([#49557](https://github.com/pytorch/pytorch/pull/49557))\r\n* Improve documentation for pipeline parallelism. ([#48638](https://github.com/pytorch/pytorch/pull/48638))\r\n* Reorder `torch.distributed.rpc.init_rpc` docstring arguments ([#50419](https://github.com/pytorch/pytorch/pull/50419))\r\n* Add documentation page for pipeline parallelism. ([#50791](https://github.com/pytorch/pytorch/pull/50791))\r\n* Update the doc of `DistributedOptimizer` ([#51314](https://github.com/pytorch/pytorch/pull/51314))\r\n* Fix doc inconsistency about callback args in `torch.futures.Future` ([#50979](https://github.com/pytorch/pytorch/pull/50979))\r\n\r\n### TorchScript\r\n\r\n* Added a developer tutorial for tensor expressions - the core technology used in CUDA fuser ([#45527](https://github.com/pytorch/pytorch/pull/45527))\r\n* Fix jit model loading example ([#48104](https://github.com/pytorch/pytorch/pull/48104))\r\n* Fix archive file extension in examples and docs ([#50649](https://github.com/pytorch/pytorch/pull/50649))\r\n* Fix `ScriptModule` docstring ([#48608](https://github.com/pytorch/pytorch/pull/48608))\r\n* Clarify logic in `ir_emitter` ([#51299](https://github.com/pytorch/pytorch/pull/51299))\r\n\r\n### torch.fx\r\n\r\n* Add `torch.fx` section to doc ([#48814](https://github.com/pytorch/pytorch/pull/48814), [#50291](https://github.com/pytorch/pytorch/pull/50291), [#50562](https://github.com/pytorch/pytorch/pull/50562), [#50896](https://github.com/pytorch/pytorch/pull/50896), [#50966](https://github.com/pytorch/pytorch/pull/50966), [#51728](https://github.com/pytorch/pytorch/pull/51728))\r\n* Add example on how to split up an FX graph into smaller subgraphs with own submodules ([#45404](https://github.com/pytorch/pytorch/pull/45404))\r\n* Shape propagation example ([#45637](https://github.com/pytorch/pytorch/pull/45637))\r\n* Add many docstrings and improve their rendering ([#47719](https://github.com/pytorch/pytorch/pull/47719), [#48100](https://github.com/pytorch/pytorch/pull/48100), [#48738](https://github.com/pytorch/pytorch/pull/48738), [#48871](https://github.com/pytorch/pytorch/pull/48871), [#50145](https://github.com/pytorch/pytorch/pull/50145), [#50396](https://github.com/pytorch/pytorch/pull/50396), [#50555](https://github.com/pytorch/pytorch/pull/50555))\r\n* Document single op replacement ([#50116](https://github.com/pytorch/pytorch/pull/50116), [#50377](https://github.com/pytorch/pytorch/pull/50377))\r\n* Document example of Proxy use ([#50583](https://github.com/pytorch/pytorch/pull/50583))\r\n* Add limitations of symbolic tracing ([#50638](https://github.com/pytorch/pytorch/pull/50638))\r\n* Added how to write transformations section ([#51278](https://github.com/pytorch/pytorch/pull/51278))\r\n* Added invert example ([#51478](https://github.com/pytorch/pytorch/pull/51478))\r\n* Document FX debugging ([#51530](https://github.com/pytorch/pytorch/pull/51530))\r\n* Write FX Subgraph Rewriter tutorial ([#51531](https://github.com/pytorch/pytorch/pull/51531))\r\n* Add note about more use cases of FX ([#51576](https://github.com/pytorch/pytorch/pull/51576))\r\n\r\n### Quantization\r\n\r\n* Add API summary section in quantization docs ([#45848](https://github.com/pytorch/pytorch/pull/45848), [#50681](https://github.com/pytorch/pytorch/pull/50681), [#50187](https://github.com/pytorch/pytorch/pull/50187))\r\n* Fix misleading doc string in quint8.h ([#48418](https://github.com/pytorch/pytorch/pull/48418))\r\n* Add fx graph mode quantization to quantization docs ([#49515](https://github.com/pytorch/pytorch/pull/49515))\r\n* Add common errors section ([#49902](https://github.com/pytorch/pytorch/pull/49902))\r\n* Adding a table comparing eager and fx graph mode ([#50413](https://github.com/pytorch/pytorch/pull/50413))\r\n* Add docs for embedding/embedding_bag ([#51770](https://github.com/pytorch/pytorch/pull/51770))\r\n* Add fake_quantize functions documentation ([#51748](https://github.com/pytorch/pytorch/pull/51748))\r\n\r\n### ONNX\r\n\r\n* Update ONNX doc for indexing export ([#46349](https://github.com/pytorch/pytorch/pull/46349))\r\n* Update ONNX doc for writing pytorch model ([#46961](https://github.com/pytorch/pytorch/pull/46961))\r\n\r\n### Misc\r\n\r\n* Add `docs/README.md` to make existing doc build info more discoverable ([#49286](https://github.com/pytorch/pytorch/pull/49286))\r\n* Update CONTRIBUTING for doc build ([#47539](https://github.com/pytorch/pytorch/pull/47539))\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.8.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.8.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.8.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/39280362", "release_id": 39280362, "date_created": "2021-02-26T22:13:54Z", "date_published": "2021-03-04T20:44:39Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/35104340", "tag": "v1.7.1", "name": "Bug fix release with updated binaries for Python 3.9 and cuDNN 8.0.5", "author": {"name": "albanD", "type": "User"}, "description": "# PyTorch 1.7.1 Release Notes\r\n\r\n* New Features\r\n* Critical Fixes\r\n* Other Fixes\r\n\r\n# New Features\r\n\r\n### Add Python 3.9 binaries for linux and macOS ([#48133](https://github.com/pytorch/pytorch/pull/48133)) and Windows ([#48218](https://github.com/pytorch/pytorch/pull/48218))\r\n\r\n*NOTE*: Conda installs for Python 3.9 will require the `conda-forge` channel, example:\r\n`conda install -y -c pytorch -c conda-forge pytorch`.\r\n\r\n### Upgrade CUDA binaries to use cuDNN 8.0.5 (builder repo [#571](https://github.com/pytorch/builder/pull/571))\r\n\r\nThis upgrade fix regressions on Ampere cards introduced in cuDNN 8.0.4.\r\nIt will improve performance for 3090 RTX cards, and may improve performance in other RTX-30 series card.\r\n\r\n# Critical Fixes\r\n\r\n### Python 3.9\r\n\r\n- Use custom version of pybind11 to work around Python 3.9 issues ([#48312](https://github.com/pytorch/pytorch/pull/48312))\r\n- Fix jit Python 3.9 parsing ([#48744](https://github.com/pytorch/pytorch/pull/48744))\r\n- Fix cpp_extension to work with Python 3.9 ([#48768](https://github.com/pytorch/pytorch/pull/48768))\r\n\r\n### Build\r\n\r\n- Fix cpp_extension to properly handle env variable on Windows ([#48937](https://github.com/pytorch/pytorch/pull/48937))\r\n- Properly package libomp.dylib for macOS binaries ([#48337](https://github.com/pytorch/pytorch/pull/48337))\r\n- Fix build for statically linked OpenBLAS on aarch64 ([#48819](https://github.com/pytorch/pytorch/pull/48819))\r\n\r\n### Misc\r\n\r\n- `torch.sqrt`: fix wrong output values for very large complex input ([#48216](https://github.com/pytorch/pytorch/pull/48216))\r\n- `max_pool1d`: fix for discontiguous inputs ([#48219](https://github.com/pytorch/pytorch/pull/48219))\r\n- `collect_env`: fix detection of DEBUG flag ([#48319](https://github.com/pytorch/pytorch/pull/48319))\r\n- `collect_env`: Fix to work when PyTorch is not installed ([#48311](https://github.com/pytorch/pytorch/pull/48311))\r\n- Fix `amp` memory usage when running in `no_grad()` mode ([#48936](https://github.com/pytorch/pytorch/pull/48936))\r\n- `nn.ParameterList` and `nn.ParameterDict`: Remove spurious warnings ([#48215](https://github.com/pytorch/pytorch/pull/48215))\r\n- Tensor Expression fuser bugfixes ([#48137](https://github.com/pytorch/pytorch/pull/48137))\r\n\r\n# Other Fixes\r\n\r\n- Tensor Expression fix for CUDA 11.0 ([#48309](https://github.com/pytorch/pytorch/pull/48309))\r\n- `torch.overrides`: doc fix ([#47843](https://github.com/pytorch/pytorch/pull/47843))\r\n- `torch.max`: Fix output type for Tensor subclasses ([#47735](https://github.com/pytorch/pytorch/pull/47735))\r\n- `torch.mul`: Add support for boolean Tensors ([#48310](https://github.com/pytorch/pytorch/pull/48310))\r\n- Add user friendly error when trying to compile from source with Python 2 ([#48317](https://github.com/pytorch/pytorch/pull/48317))\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.7.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.7.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.7.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/35104340", "release_id": 35104340, "date_created": "2020-12-07T19:28:38Z", "date_published": "2020-12-10T17:19:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/33111309", "tag": "v1.7.0", "name": "PyTorch 1.7 released w/ CUDA 11, New APIs for FFTs, Windows support for Distributed training and more", "author": {"name": "albanD", "type": "User"}, "description": "# PyTorch 1.7.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nThe PyTorch 1.7 release includes a number of new APIs including support for NumPy-Compatible FFT operations, profiling tools and major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. In addition, several features moved to [stable](https://pytorch.org/docs/stable/index.html#pytorch-documentation) including custom C++ Classes, the memory profiler, the creation of custom tensor-like objects, user async functions in RPC and a number of other features in torch.distributed such as Per-RPC timeout, DDP dynamic bucketing and RRef helper. \r\n\r\nA few of the highlights include: \r\n\r\n* CUDA 11 is now officially supported with binaries available at [PyTorch.org](http://pytorch.org/)\r\n* Updates and additions to profiling and performance for RPC, TorchScript and Stack traces in the autograd profiler\r\n* (Beta) Support for NumPy compatible Fast Fourier transforms (FFT) via torch.fft\r\n* (Prototype) Support for Nvidia A100 generation GPUs and native TF32 format \r\n* (Prototype) Distributed training on Windows now supported\r\n\r\nTo reiterate, starting [PyTorch 1.6](https://pytorch.org/blog/pytorch-feature-classification-changes/), features are now classified as stable, beta and prototype. You can see the detailed announcement [here](https://pytorch.org/blog/pytorch-feature-classification-changes/). Note that the prototype features listed in this blog are available as part of this release. \r\n\r\n## Front End APIs\r\n\r\n### [Beta] NumPy Compatible torch.fft module\r\n\r\nFFT-related functionality is commonly used in a variety of scientific fields like signal processing. While PyTorch has historically supported a few FFT-related functions, the 1.7 release adds a new torch.fft module that implements FFT-related functions with the same API as NumPy.  \r\n\r\nThis new module must be imported to be used in the 1.7 release, since its name conflicts with the historic (and now deprecated) torch.fft function.\r\n\r\n**Example usage:**\r\n\r\n```python\r\n>>> import torch.fft\r\n>>> t = torch.arange(4)\r\n>>> t\r\ntensor([0, 1, 2, 3])\r\n\r\n>>> torch.fft.fft(t)\r\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\r\n\r\n>>> t = tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\r\n>>> torch.fft.fft(t)\r\ntensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\r\n```\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/fft.html#torch-fft)\r\n\r\n### [Beta] C++ Support for Transformer NN Modules\r\n\r\nSince [PyTorch 1.5](https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/), we\u2019ve continued to maintain parity between the python and C++ frontend APIs. This update allows developers to use the nn.transformer module abstraction from the C++ Frontend. And moreover, developers no longer need to save a module from python/JIT and load into C++ as it can now be used it in C++ directly. \r\n\r\n* Documentation | [Link](https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_transformer_impl.html#_CPPv4N5torch2nn15TransformerImplE)\r\n\r\n### [Beta] torch.set_deterministic \r\n\r\nReproducibility (bit-for-bit determinism) may help identify errors when debugging or testing a program. To facilitate reproducibility, PyTorch 1.7 adds the  `torch.set_deterministic(bool)` function that can direct PyTorch operators to select deterministic algorithms when available, and to throw a runtime error if an operation may result in nondeterministic behavior. By default, the flag this function controls is false and there is no change in behavior, meaning PyTorch may implement its operations nondeterministically by default. \r\n\r\nMore precisely, when this flag is true:\r\n\r\n* Operations known to not have a deterministic implementation throw a runtime error;\r\n* Operations with deterministic variants use those variants (usually with a performance penalty versus the non-deterministic version); and\r\n* `torch.backends.cudnn.deterministic = True` is set.\r\n\r\nNote that this is necessary, **but not sufficient**, for determinism **within a single run of a PyTorch program**. Other sources of randomness like random number generators, unknown operations, or asynchronous or distributed computation may still cause nondeterministic behavior.\r\n\r\nSee the documentation for `torch.set_deterministic(bool)` for the list of affected operations.\r\n\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/15359)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/generated/torch.set_deterministic.html)\r\n\r\n## Performance & Profiling\r\n\r\n### [Beta] Stack traces added to profiler\r\n\r\nUsers can now see not only operator name/inputs in the profiler output table but also where the operator is in the code. The workflow requires very little change to take advantage of this capability. The user uses the [autograd profiler](https://pytorch.org/docs/stable/autograd.html#profiler) as before but with optional new parameters: `with_stack` and `group_by_stack_n`. Caution: regular profiling runs should not use this feature as it adds significant overhead. \r\n\r\n* Details | [Link](https://github.com/pytorch/pytorch/pull/43898/)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/autograd.html)\r\n\r\n## Distributed Training & RPC \r\n\r\n### [Stable] TorchElastic now bundled into PyTorch docker image\r\n\r\nTorchelastic offers a strict superset of the current `torch.distributed.launch` CLI with the added features for fault-tolerance and elasticity. If the user is not be interested in fault-tolerance, they can get the exact functionality/behavior parity by setting `max_restarts=0` with the added convenience of auto-assigned `RANK` and `MASTER_ADDR|PORT` (versus manually specified in `torch.distributed.launch)`.\r\n\r\nBy bundling `torchelastic` in the same docker image as PyTorch, users can start experimenting with torchelastic right-away without having to separately install `torchelastic`. In addition to convenience, this work is a nice-to-have when adding support for elastic parameters in the existing Kubeflow\u2019s distributed PyTorch operators.\r\n\r\n* Usage examples and how to get started | [Link](https://pytorch.org/elastic/0.2.0/examples.html)\r\n\r\n### [Beta] Support for uneven dataset inputs in DDP\r\n\r\nPyTorch 1.7 introduces a new context manager to be used in conjunction with models trained using `torch.nn.parallel.DistributedDataParallel` to enable training with uneven dataset size across different processes. This feature enables greater flexibility when using DDP and prevents the user from having to manually ensure dataset sizes are the same across different process. With this context manager, DDP will handle uneven dataset sizes automatically, which can prevent errors or hangs at the end of training.\r\n\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/38174)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join)\r\n\r\n### [Beta] NCCL Reliability - Async Error/Timeout Handling\r\n\r\nIn the past, NCCL training runs would hang indefinitely due to stuck collectives, leading to a very unpleasant experience for users. This feature will abort stuck collectives and throw an exception/crash the process if a potential hang is detected. When used with something like torchelastic (which can recover the training process from the last checkpoint), users can have much greater reliability for distributed training. This feature is completely opt-in and sits behind an environment variable that needs to be explicitly set in order to enable this functionality (otherwise users will see the same behavior as before).\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/46874)\r\n\r\n### [Beta] TorchScript `remote` and `rpc_sync`\r\n\r\n`torch.distributed.rpc.rpc_async` has been available in TorchScript in prior releases. For PyTorch 1.7, this functionality will be extended the remaining two core RPC APIs, `torch.distributed.rpc.rpc_sync` and `torch.distributed.rpc.remote`. This will complete the major RPC APIs targeted for support in TorchScript, it allows users to use the existing python RPC APIs within TorchScript (in a script function or script method, which releases the python Global Interpreter Lock) and could possibly improve application performance in multithreaded environment.\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc.html#rpc)\r\n* Usage examples | [Link](https://github.com/pytorch/pytorch/blob/58ed60c259834e324e86f3e3118e4fcbbfea8dd1/torch/testing/_internal/distributed/rpc/jit/rpc_test.py#L505-L525)\r\n\r\n### [Beta] Distributed optimizer with TorchScript support\r\n\r\nPyTorch provides a broad set of optimizers for training algorithms, and these have been used repeatedly as part of the python API. However, users often want to use multithreaded training instead of multiprocess training as it provides better resource utilization and efficiency in the context of large scale distributed training (e.g. Distributed Model Parallel) or any RPC-based training application). Users couldn\u2019t do this with with distributed optimizer before because we need to get rid of the python Global Interpreter Lock (GIL) limitation to achieve this.\r\n\r\nIn PyTorch 1.7, we are enabling the TorchScript support in distributed optimizer to remove the GIL, and make it possible to run optimizer in multithreaded applications. The new distributed optimizer has the exact same interface as before but it automatically converts optimizers within each worker into TorchScript to make each GIL free. This is done by leveraging a functional optimizer concept and allowing the distributed optimizer to convert the computational portion of the optimizer into TorchScript. This will help use cases like distributed model parallel training and improve performance using multithreading. \r\n\r\nCurrently, the only optimizer that supports automatic conversion with TorchScript is `Adagrad` and all other optimizers will still work as before without TorchScript support. We are working on expanding the coverage to all PyTorch optimizers and expect more to come in future releases. The usage to enable TorchScript support is automatic and exactly the same with existing python APIs, here is an example of how to use this:\r\n\r\n```python\r\nimport torch.distributed.autograd as dist_autograd\r\nimport torch.distributed.rpc as rpc\r\nfrom torch import optim\r\nfrom torch.distributed.optim import DistributedOptimizer\r\n\r\nwith dist_autograd.context() as context_id:\r\n  # Forward pass.\r\n  rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n  rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\r\n  loss = rref1.to_here() + rref2.to_here()\r\n\r\n  # Backward pass.\r\n  dist_autograd.backward(context_id, [loss.sum()])\r\n\r\n  # Optimizer, pass in optim.Adagrad, DistributedOptimizer will\r\n  # automatically convert/compile it to TorchScript (GIL-free)\r\n  dist_optim = DistributedOptimizer(\r\n     optim.Adagrad,\r\n     [rref1, rref2],\r\n     lr=0.05,\r\n  )\r\n  dist_optim.step(context_id)\r\n```\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim)\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/46883)\r\n\r\n### [Beta] Enhancements to RPC-based Profiling\r\n\r\nSupport for using the PyTorch profiler in conjunction with the RPC framework was first introduced in PyTorch 1.6. In PyTorch 1.7, the following enhancements have been made:\r\n\r\n* Implemented better support for profiling TorchScript functions over RPC\r\n* Achieved parity in terms of profiler features that work with RPC\r\n* Added support for asynchronous RPC functions on the server-side (functions decorated with `rpc.functions.async_execution)`.\r\n\r\nUser are now able to use familiar profiling tools such as `with torch.autograd.profiler.profile()` and `with torch.autograd.profiler.record_function,` and this works transparently with the RPC framework with full feature support, profiles asynchronous functions, and TorchScript functions.\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/39675)\r\n* Usage examples | [Link](https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html)\r\n\r\n### [Prototype] Windows support for Distributed Training\r\n\r\nPyTorch 1.7 brings prototype support for `DistributedDataParallel` and collective communications on the Windows platform. In this release, the support only covers Gloo-based `ProcessGroup` and `FileStore`.\r\nTo use this feature across multiple machines, please provide a file from a shared file system in `init_process_group`. \r\n\r\n```python\r\n# initialize the process group\r\ndist.init_process_group(\r\n    \"gloo\",\r\n    # multi-machine example:\r\n    # Shared files need six \"/\"\r\n    # init_method = `\"file://////{machine}/{share_folder}/file\"`\r\n    # Local file need three \"/\"\r\n    init_method=\"file:///{your local file path}\",\r\n    rank=rank,\r\n    world_size=world_size\r\n)\r\n\r\nmodel = DistributedDataParallel(local_model, device_ids=[rank])\r\n```\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/42095)\r\n* Documentation | [Link](https://pytorch.org/docs/master/distributed.html#backends-that-come-with-pytorch)\r\n* Acknowledgement | [gunandrose4u](https://github.com/gunandrose4u)\r\n\r\n## Mobile\r\n\r\nPyTorch Mobile supports both [iOS](https://pytorch.org/mobile/ios) and [Android](https://pytorch.org/mobile/android/) with binary packages available in [Cocoapods](https://cocoapods.org/) and J[Center](https://mvnrepository.com/repos/jcenter) respectively. You can learn more about PyTorch-Mobile [here](https://pytorch.org/mobile/home/). \r\n\r\n### [Beta] PyTorch Mobile Caching allocator for performance improvements\r\n\r\nOn some mobile platforms, such as Pixel, we observed that memory is returned to the system more aggressively. This results in frequent page faults as PyTorch being a functional framework does not maintain state for the operators. Thus outputs are allocated dynamically on each execution of the op, for the most ops. To ameliorate performance penalties due to this, PyTorch 1.7 provides a simple caching allocator for CPU. The allocator caches allocations by tensor sizes and, is currently, available only via the PyTorch C++ API. The caching allocator itself is owned by client and thus the lifetime of the allocator is also maintained by client code. Such a client owned caching allocator can then be used with scoped guard, `c10::WithCPUCachingAllocatorGuard`, to enable the use of cached allocation within that scope.\r\n\r\n**Example usage:**\r\n\r\n```cpp\r\n#include <c10/mobile/CPUCachingAllocator.h>\r\n.....\r\nc10::CPUCachingAllocator caching_allocator;\r\n  // Owned by client code. Can be a member of some client class so as to tie the\r\n  // the lifetime of caching allocator to that of the class.\r\n.....\r\n{\r\n  c10::optional<c10::WithCPUCachingAllocatorGuard> caching_allocator_guard;\r\n  if (FLAGS_use_caching_allocator) {\r\n    caching_allocator_guard.emplace(&caching_allocator);\r\n  }\r\n  ....\r\n  model.forward(..);\r\n}\r\n.....\r\n```\r\n\r\n**NOTE**: Caching allocator is only available on mobile builds, thus the use of caching allocator outside of mobile builds won\u2019t be effective.\r\n\r\n* Documentation | [Link](https://github.com/pytorch/pytorch/blob/master/c10/mobile/CPUCachingAllocator.h#L13-L43)\r\n* Usage examples | [Link](https://github.com/pytorch/pytorch/blob/master/binaries/speed_benchmark_torch.cc#L207)\r\n\r\n\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### `torch.conj` now returns the input as-is for real Tensors ([#43270](https://github.com/pytorch/pytorch/pull/43270))\r\n\r\nPreviously, `torch.conj` and `Tensor.conj` were making a clone for Tensors of real dtype. It now returns the Tensor as-is to improve performance.\r\nYou can recover the original behavior by adding a `.clone()` for real Tensors.\r\nNote that this behavior is different from `numpy` for which `np.conj` returns a new ndarray and `ndarray.conj` returns the ndarray as-is.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.is_complex()\r\nFalse\r\n>>> t.conj() is t\r\nFalse   \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.is_complex()\r\nFalse\r\n>>> t.conj() is t\r\nTrue\r\n>>>t.conj().clone() is t\r\nFalse   \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.tensor`, `torch.as_tensor`, and `torch.sparse_coo_tensor` now use the input Tensor\u2019s device when it is not specified ([#41984](https://github.com/pytorch/pytorch/pull/41984))\r\n\r\nThis will change the device on which the Tensor is created and so the user can start seeing device mismatch errors.\r\nIt also means for sparse Tensors that both of the provided Tensors must be on the same device if the device is not specified.\r\nYou can recover the original behavior by passing the `device` argument.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.device\r\ndevice(type=\u2018cuda:0\u2019)\r\n>>> # tensor constructor\r\n>>> torch.tensor(t, dtype=torch.float32).device\r\ndevice(type=\u2018cpu\u2019)\r\n>>> # sparse constructor\r\n>>> torch.sparse_coo_tensor(\r\n            torch.tensor(([0], [2]), device=\"cpu\"),\r\n            torch.tensor(([1.],), device=\"cuda\"),\r\n            size=(3, 3, 1)).device\r\ndevice(type='cuda', index=0)    \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.device\r\ndevice(type=\u2018cuda:0\u2019)\r\n>>> # tensor constructor\r\n>>> torch.tensor(t, dtype=torch.float32).device\r\ndevice(type=\u2018cuda:0\u2019)\r\n>>> # Specify the device to get the same behavior as 1.6\r\n>>> torch.tensor(t, dtype=torch.float32, device='cpu').device\r\ndevice(type=\u2018cpu\u2019)\r\n>>> # sparse constructor\r\n>>> torch.sparse_coo_tensor(\r\n            torch.tensor(([0], [2]), device=\"cpu\"),\r\n            torch.tensor(([1.],), device=\"cuda\"),\r\n            size=(3, 3, 1)).device\r\nRuntimeError: backend of indices (CPU) must match backend\r\nof values (CUDA)\r\n>>> # Specify the device to get the same behavior as 1.6\r\n>>> torch.sparse_coo_tensor(\r\n            torch.tensor(([0], [2]), device=\"cpu\"),\r\n            torch.tensor(([1.],), device=\"cuda\"),\r\n            size=(3, 3, 1),\r\n            device=\"cuda:0\").device\r\ndevice(type='cuda', index=0)    \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.nn.utils.pack_padded_sequence`: remove hidden cross-device copy for `lengths` ([#41984](https://github.com/pytorch/pytorch/pull/41984))\r\n\r\nIn previous versions, when the lengths argument was a CUDA tensor, it would incorrectly be moved to the CPU silently.\r\nThis can lead to surprising performances and CPU/GPU sync when using CUDA so this has been removed.\r\nYou need to make sure that the provided `lenghts` is a CPU Tensor when it is provided as a Tensor.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> inp = torch.rand(10, 2, 3, device=\"cuda\")\r\n>>> lengths = torch.tensor([10, 7], device=\"cuda\")\r\n>>> torch.nn.utils.rnn.pack_padded_sequence(inp, lengths)\r\n>>> # Implicitly move lengths to the CPU and runs fine\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> inp = torch.rand(10, 2, 3, device=\"cuda\")\r\n>>> lengths = torch.tensor([10, 7], device=\"cuda\")\r\n>>> torch.nn.utils.rnn.pack_padded_sequence(inp, lengths)\r\nRuntimeError: 'lengths' argument should be a 1D CPU int64 tensor,\r\nbut got 1D cuda:0 Long tensor\r\n>>> # Ensure the lenghts is already on the right device\r\n>>> lengths = lengths.cpu()\r\n>>> torch.nn.utils.rnn.pack_padded_sequence(inp, lengths)\r\n>>> # Runs fine with no implicit move across device\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Improve `torch.norm` handling of `keepdim=True` ([#41956](https://github.com/pytorch/pytorch/pull/41956))\r\n\r\nBefore this change, when calling `torch.norm` with `keepdim=True` and `p='fro'` or `p=number`, leaving all other optional arguments as their default values, the keepdim argument would be ignored. It is now properly respected.\r\nAlso, any time `torch.norm` was called with `p='nuc'` and `keepdim=True`, the result would have one fewer dimension than the input, and the dimensions could be out of order depending on which dimensions were being reduced. It is now properly keeping all the dimensions.\r\nYou can recover the original behavior by setting `keepdim=False`.\r\n**NOTE: this function is now deprecated (see below) and we recommend you use `torch.linalg.norm`, which follows NumPy\u2019s conventions.**\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.size()\r\ntorch.Size([4, 4])\r\n>>> t.norm(p=\u2018fro\u2019, keepdim=True).size()\r\ntorch.size([])\r\n>>> t.norm(p=3, keepdim=True).size()\r\ntorch.size([])\r\n>>> t.norm(p=\u2018nuc\u2019, keepdim=True).size()\r\ntorch.size([1]) \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.size()\r\ntorch.Size([4, 4])\r\n>>> t.norm(p=\u2018fro\u2019, keepdim=True).size()\r\ntorch.size([1, 1])\r\n>>> t.norm(p=3, keepdim=True).size()\r\ntorch.size([1, 1])\r\n>>> t.norm(p=\u2018nuc\u2019, keepdim=True).size()\r\ntorch.size([1, 1])  \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.split` and `torch.chunk`: Fix view tracking for the autograd ([#41567](https://github.com/pytorch/pytorch/pull/41567))\r\n\r\nThe autograd system is able to correctly handle modifications through views of Tensors by explicitly tracking known view operations. In prior releases, `torch.split` and `torch.chunk` were not marked as known view operations, which could lead to silently wrong gradients.\r\n\r\nNote that since v1.5, inplace modification of views created by functions that return multiple views is deprecated. Such case is not properly handled by the autograd and can lead to internal errors or wrong gradients. So, as a side effect of this view fix, inplace modifications of the outputs of `torch.split` and `torch.chunk` will now raise a warning and can lead to internal errors or wrong gradients while they were previously silently computing wrong gradients.\r\nIf you see such a warning, you should replace the inplace operation with an out of place one.\r\nYou can recover the original behavior by using the new `torch.unsafe_split` and `torch.unsafe_chunk`. Note that these functions are only here to ease the transition and will also be removed in a future version.\r\n\r\n### `torch.{argmin,argmax}` now always return the first min/max index ([#42004](https://github.com/pytorch/pytorch/pull/42004))\r\n\r\n`torch.argmin` (`torch.argmax`) now always returns the index of the first minimum (maximum) element. This choice is consistent with NumPy. Previously if there were multiple minima (maxima) the index returned could be the index of any of them.\r\nYou cannot recover the original behavior as it was platform dependent and not guaranteed. If your code was relying on a specific index for your specific platform, you should update it to work with the first index and this new code will work on all platforms.\r\n\r\n### `torch.{min,max,median}`: Update backward formula when doing full reduction (`dim` argument not provided) ([#43519](https://github.com/pytorch/pytorch/pull/43519))\r\n\r\nWhen no dimension is specified, full reduction is performed and the gradient will now flow back evenly towards all the input that realized the output value. The old behavior was to propagate the gradient only for one of such input selected arbitrarily.\r\nThis should improve stability of training by gradient descent.\r\nTo recover the previous behavior, you can perform the reduction with the `dim=` argument. It will ensure that the gradient only flows back for the input whose index was returned.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a\r\ntensor([3, 2, 3])\r\n>>> a.max().backward()\r\n>>> a.grad\r\ntensor([0, 0, 1])   \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a\r\ntensor([3, 2, 3])\r\n>>> a.max().backward()\r\n>>> a.grad\r\ntensor([0.5, 0, 0.5])\r\n>>> a.max(dim=0).max(dim=0).max(dim=0).backward()\r\n>>> a.grad\r\ntensor([0, 0, 1])   \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `nn.BCELoss` size mismatch warning is now an error ([#41426](https://github.com/pytorch/pytorch/pull/41426))\r\n\r\nThis is the end of the deprecation cycle for this op to make sure it does not have different broadcasting semantic compared to numpy\u2019s broadcasting semantic used everywhere else in PyTorch\u2019s codebase.\r\nYou need to make sure all inputs are the same size to avoid the error.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> bceloss = nn.BCELoss()\r\n>>> a = torch.rand(25)\r\n>>> b = torch.rand(25, 1)\r\n>>> bceloss(a, b)\r\nUserWarning: Using a target size (torch.Size([25, 1]))\r\nthat is different to the input size (torch.Size([25]))\r\nis deprecated. Please ensure they have the same size.\r\ntensor(1.0604)  \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> bceloss = nn.BCELoss()\r\n>>> a = torch.rand(25)\r\n>>> b = torch.rand(25, 1)\r\n>>> bceloss(a, b)\r\nValueError: Using a target size (torch.Size([25, 1]))\r\nthat is different to the input size (torch.Size([25]))\r\nis deprecated. Please ensure they have the same size.\r\n>>> b = b.reshape(25)\r\n>>> bceloss(a, b)\r\ntensor(1.0604)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Custom `autograd.Function` stop materializing `None` output Tensors ([#41490](https://github.com/pytorch/pytorch/pull/41490))\r\n\r\nTo improve performance, the custom `autograd.Function` will not create a Tensor full of zeros when an input is differentiable but the user\u2019s `backward` function returns `None` for it. This means that code for which the `.backward()` or `autograd.grad()` final result will now be `None` while it used to be a Tensor full of zeros.\r\nYou can recover the previous behavior by having your custom `autograd.Function` materialize the zero Tensor with `torch.zeros_like(input)` to replace the `None` output for the `backward` method.\r\n\r\n```python\r\nimport torch\r\n\r\n# Custom Function that returns None for the gradient\r\nclass GetTwos(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, inp):\r\n        return inp.clone().fill_(2)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        # To recover the 1.6 behavior, replace the line below with `return torch.zeros_like(grad_out)`\r\n        return None\r\n\r\na = torch.rand(10, requires_grad=True)\r\nb = GetTwos.apply(a)\r\nb.sum().backward()\r\n\r\nprint(a.grad)\r\n# In PyTorch 1.6 this will print\r\n# tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\r\n# In PyTorch 1.7 this will print\r\n# None\r\n```\r\n\r\n### Fix inplace detection for non-differentiable outputs ([#41269](https://github.com/pytorch/pytorch/pull/41269))\r\n\r\nWe fixed a bug in the inplace detection code that was preventing the detection of some inplace operations for output that are not differentiable (like integer type Tensors).\r\nThis can lead to code that used to run fine to throw the error \u201ca Tensor that was needed for backward was modified in an inplace operation\u201d.\r\nSuch failure is true and the user code must be fixed to compute proper gradients. In general, this involves cloning the Tensor before modifying it inplace to make sure the backward pass can happen safely.\r\n\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.rand(10, requires_grad=True)\r\nwith torch.no_grad():\r\n    a[2] = 10\r\n\r\nb, ind = a.max(dim=0)\r\n# ind is 2 here\r\n\r\nwith torch.no_grad():\r\n    t = torch.rand(10)\r\n    t[4] = 10\r\n    res = torch.max(t, dim=0, out=(torch.Tensor(), ind))\r\n    # ind becomes 4 here\r\n\r\n# This backward runs in 1.6 but will fail in 1.7\r\nb.sum().backward()\r\nprint(a.grad)\r\n# tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\r\n# The value is wrong is at index 4 while it should be at index 2\r\n\r\n# The issue is avoided by not modifying ind inplace by replacing the line\r\n# above with:\r\n# res = torch.max(t, dim=0, out=(torch.Tensor(), ind.clone()))\r\n```\r\n\r\n### Add `__torch_functions__` for methods ([#37091](https://github.com/pytorch/pytorch/pull/37091))\r\n\r\nFunctions, slicing and Tensor methods will now properly preserve the subclass type when possible.\r\n\r\n```python\r\n>>> class SubTensor(torch.Tensor):\r\n...     pass\r\n>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\r\n'SubTensor'\r\n>>> type(torch.add(SubTensor([0]), torch.Tensor([1]))).__name__\r\n'SubTensor'\r\n```\r\n\r\nThe old behavior of \u201cany operations on your subclass produces a torch.Tensor instead of the subclass\u201d can be recovered by doing:\r\n\r\n```python\r\nfrom torch._C import _disabled_torch_function_impl\r\n    \r\nclass SubTensor(torch.Tensor):\r\n    __torch_function__ = _disabled_torch_function_impl\r\n```\r\n\r\nFor all details on how to use this feature, please refer to the [doc](https://pytorch.org/docs/stable/notes/extending.html#extending-torch) page for it.\r\n\r\n### `tensor.__iter__`: Use `torch.unbind` instead of a for loop ([#40884](https://github.com/pytorch/pytorch/pull/40884))\r\n\r\nThis improves performances significantly but it changes the behavior of in-place operations on the value returned by the iterator. This happens only if either the input Tensor or any argument of the in-place operation is a Tensor that requires gradients. And it will fail with \"Output X of UnbindBackward is a view and is being modified inplace\".\r\nYou can recover the previous behavior by manually slicing the Tensor: `[t[i] for i in range(t.size(0))]` as shown in the example below.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(5, 10, requires_grad=True)\r\n>>> for i, v in enumerate(x):\r\n>>>     v.fill_(i)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(5, 10, requires_grad=True)\r\n>>> for i, v in enumerate([x[j] for j in range(x.size(0))]):\r\n>>>   v.fill_(i)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Updated most function that take zero, one or two Tensor arguments and indexing op to check for memory overlap in the Tensor being worked on ([#43418](https://github.com/pytorch/pytorch/pull/43418), [#43419](https://github.com/pytorch/pytorch/pull/43419), [#43](https://github.com/pytorch/pytorch/pull/43420)[420](https://github.com/pytorch/pytorch/pull/43420), [#43421](https://github.com/pytorch/pytorch/pull/43421), [#43423](https://github.com/pytorch/pytorch/pull/43423), [#43422](https://github.com/pytorch/pytorch/pull/43422))\r\n\r\nIt fixes silent correctness errors: something that used to be silently incorrect now errors out. Code that raises this error must be updated to avoid doing such op that was returning wrong results as shown in the example below:\r\n\r\n```python\r\n>>> x = torch.randn(1, 3)\r\n>>> # Create a tensor that has internal memory overlap\r\n>>> y = x.expand(2, 3)\r\n\r\n# In 1.6, this would not error out, but in 1.7, this errors out\r\n>>> torch.nn.functional.elu(y, inplace=True)\r\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single m\r\nemory location. Please clone() the tensor before performing the operation.\r\n\r\n# Here is the fix in 1.7\r\n>>> torch.nn.functional.elu(y, inplace=False)\r\n```\r\n\r\nc++ API: Any external users of `TensorIterator` now always get the memory overlap check. The previous behavior can be recovered by setting `set_check_mem_overlap(false)` when creating the iterator.\r\n\r\n## TorchScript\r\n\r\n### TorchScript now correctly supports various exception type and custom exception message ([#41907](https://github.com/pytorch/pytorch/pull/41907))\r\n\r\n* Exceptions raised in TorchScript was traditionally replaced with a generic runtime error that doesn\u2019t carry exception type or message, leading to crashes that are difficult to pin-point and debug. We improved TorchScript to correctly parse exception types and messages and surface them to users. \r\n* This change is backward incompatible because TorchScript now attempts to compile user code that creates custom exception messages instead of ignoring them. Any TorchScript-incompatible Python features used in those code snippets would lead to failures.\r\n* There is no fixed formula to fix this backward incompatibility failure other than updating code that generates exceptions to be TorchScript-able.\r\n\r\n### TorchScript now supports properties of TorchScript classes and ScriptModules ([#42389](https://github.com/pytorch/pytorch/pull/42389), [#42390](https://github.com/pytorch/pytorch/pull/42390))\r\n\r\n* TorchScript added support for `@property` of TorchScript classes and ScriptModules. Custom setters and getters are also supported. Custom deleters are not supported.\r\n* This improvement is backward incompatible because TorchScript now attempts to script properties of existing classes and `Modules`. If these properties use Python or Pytorch features that are not supported in Torchscript, scripting will fail.\r\n* There are two ways of fixing backward incompatibility failures introduced by this change. One is using `@torch.jit.unused` to annotate problematic properties, the other is to update the implementation of the property so that the getter and setter are scriptable.\r\n\r\n## Quantization\r\n\r\n### The convolution parameters now support versioning.\r\n\r\n* This change means that any quantized convolution module **saved** using PyTorch 1.7+ cannot be loaded in v1.6 and lower.\r\n* But this change is backward compatible: if the model (with conv layers) is saved in version 1.6, it can be safely loaded in version 1.7.\r\n\r\n## Some undocumented functions that were mistakenly made public have been removed\r\n\r\n* `torch.absolute_` has been removed, the Tensor method (`Tensor.absolute_`) should be used instead just like all other inplace ops.\r\n* `torch.ExtraFilesMap` is an internal jit construct and should not be used.\r\n\r\n## TorchScript Compiler Update\r\n\r\nIn 1.7, we are enabling a Profiling Executor and a new Tensor-Expressions-based (TE) Fuser. All compilations will now go through one (an adjustable setting) profiling run and one optimization run. For the profiling run, complete tensor shapes are recorded and used by the new Fuser. For the optimization run, the focus is on finding (in `torch.jit.ScriptModule`s) and fusing element-wise operations over CUDA tensors into a single CUDA kernel.\r\n\r\nThe TE fuser is expected to deliver performance similar to the old fuser used in 1.6. It however unlocks more opportunities for performance improvements in future releases. In rare cases, performance of some models may degrade 5-10%. If you experience any regressions please report it on Github, so we can address them as soon as possible! For 1.7, we are providing an option for our users to revert back to the old fuser by calling `torch._C._jit_set_profiling_executor(False)` in Python and `torch::jit::getExecutorMode()`` = false;` in C++. For more information, please see [\u201cGraph Executor\u201d section](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#graph-executor) in our documentation.\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### `torch.norm` and `torch.functional.norm` are deprecated in favor of `torch.linalg.norm` ([#44321](https://github.com/pytorch/pytorch/pull/44321))\r\n\r\nThe new `torch.linalg.norm` has the same behavior as `numpy.linalg.norm`\r\nBoth deprecated functions had odd behaviors for matrix and vector norms. You should refer to the doc [here](https://pytorch.org/docs/stable/generated/torch.norm.html?highlight=norm#torch.norm) to find the exact behavior they had and how to replicate it with the new API.\r\n\r\n### Deprecate fft functions in `torch.` namespace in favor of `torch.fft.` namespace ([#44876](https://github.com/pytorch/pytorch/pull/44876))\r\n\r\nPlease use `torch.fft.foo` as a drop-in replacement for `torch.foo` for the following functions: `fft`, `ifft`, `rfft` and `irfft`.\r\n\r\n### Warns when some `out=` functions need to resize an output which is not 0-size ([#42079](https://github.com/pytorch/pytorch/pull/42079))\r\n\r\nThis behavior is dangerous and leads to an API that is hard to use. It is being deprecated to be able to fix that API in future versions.\r\nYou should resize the output before-hand to avoid any issue in the future:\r\n\r\n```python\r\na = torch.rand(5)\r\nb = torch.rand(25)\r\n\r\n# This is deprecated\r\ntorch.add(a, a, out=b)\r\n\r\n# This has the same behavior but will work in future versions\r\ntorch.add(a, a, out=b.resize_(0))\r\n```\r\n\r\n### `torch.optim`: Warn for duplicate params in param group ([#41597](https://github.com/pytorch/pytorch/pull/41597))\r\n\r\nProviding multiple times the same Parameter in a single param group is most likely due to user error and is being deprecated.\r\nPlease open an issue if you have a valid use case that require this feature.\r\n\r\n### `torch.linspace` and `torch.logspace`: Not giving the step argument is deprecated ([#43860](https://github.com/pytorch/pytorch/pull/43860))\r\n\r\nThe default `steps` argument that has been used historically in PyTorch is not consistent with other libraries and so is being removed to avoid confusion.\r\nFor both functions, passing `steps=100` keyword argument can be used to recover the original behavior.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.linspace(0, 10).size()\r\ntorch.Size([100])   \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.linspace(0, 10).size()\r\nUserWarning: Not providing a value for linspace's\r\nsteps is deprecated and will throw a runtime error\r\nin a future release.\r\ntorch.Size([100])\r\n>>> torch.linspace(0, 10, steps=100).size()\r\ntorch.Size([100])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Distributed\r\n\r\n* Make TensorPipe the default backend for RPC ([#43246](https://github.com/pytorch/pytorch/pull/43246))\r\n* Infer RPC backend type to preserve backward compatibility as we make TensorPipe the default ([#45065](https://github.com/pytorch/pytorch/pull/45065))\r\n* Add deprecation warning to ProcessGroup backend and make TensorPipe backend stable. ([#45356](https://github.com/pytorch/pytorch/pull/45356))\r\n* Add warnings on `ProcessGroup` and `ProcessGroup::Work` APIs which will be retired soon. ([#46366](https://github.com/pytorch/pytorch/pull/46366))\r\n\r\n# New features\r\n\r\n### Python API\r\n\r\nNew namespaces:\r\n\r\n* `torch.fft` added ([#41911](https://github.com/pytorch/pytorch/pull/41911))\r\n* `torch.linalg` added ([#42664](https://github.com/pytorch/pytorch/pull/42664))\r\n* `torch.optim.functional` added ([#44715](https://github.com/pytorch/pytorch/pull/44715))\r\n\r\nNew operators:\r\n\r\n* `torch.count_nonzero` added ([#39992](https://github.com/pytorch/pytorch/pull/39992))\r\n* `nn.SiLU` activation added ([#41034](https://github.com/pytorch/pytorch/pull/41034))\r\n* `torch.logit` added ([#41062](https://github.com/pytorch/pytorch/pull/41062))\r\n* `torch.gcd`, `torch.lcm` added ([#40651](https://github.com/pytorch/pytorch/pull/40651), [#41552](https://github.com/pytorch/pytorch/pull/41552), [#42254](https://github.com/pytorch/pytorch/pull/42254))\r\n* `torch.functional.atleast_{1d/2d/3d}` added ([#41317](https://github.com/pytorch/pytorch/pull/41317))\r\n* `torch.isreal` added ([#41298](https://github.com/pytorch/pytorch/pull/41298))\r\n* `nn.Unflatten` added ([#41564](https://github.com/pytorch/pytorch/pull/41564))\r\n* `torch.movedim` added ([#41480](https://github.com/pytorch/pytorch/pull/41480))\r\n* `torch.isposinf`, `torch.isneginf` added ([#41588](https://github.com/pytorch/pytorch/pull/41588))\r\n* `torch.signbit` added ([#41589](https://github.com/pytorch/pytorch/pull/41589))\r\n* `torch.absolute` added ([#42586](https://github.com/pytorch/pytorch/pull/42586))\r\n* `torch.clip` alias added ([#42770](https://github.com/pytorch/pytorch/pull/42770))\r\n* `torch.quantile` added ([#42755](https://github.com/pytorch/pytorch/pull/42755))\r\n* `torch.linalg.det` and `torch.outer` alias added ([#42802](https://github.com/pytorch/pytorch/pull/42802))\r\n* `torch.nansum` added ([#38628](https://github.com/pytorch/pytorch/pull/38628))\r\n* `torch.hypot` added ([#42291](https://github.com/pytorch/pytorch/pull/42291))\r\n* `torch.nextafter` added ([#42580](https://github.com/pytorch/pytorch/pull/42580))\r\n* `torch.hstack`, `torch.vstack`, `torch.dstack` added ([#42799](https://github.com/pytorch/pytorch/pull/42799))\r\n* `torch.arccosh` alias added ([#43107](https://github.com/pytorch/pytorch/pull/43107))\r\n* `Tensor.movedim` as a method added ([#43122](https://github.com/pytorch/pytorch/pull/43122))\r\n* `torch.matrix_exp` added ([#40161](https://github.com/pytorch/pytorch/pull/40161))\r\n* `torch.fix` alias added ([#43326](https://github.com/pytorch/pytorch/pull/43326))\r\n* `torch.arccos`, `torch.arcsin`, `torch.arctan` aliases added ([#43319](https://github.com/pytorch/pytorch/pull/43319))\r\n* `torch.negative` alias added ([#43400](https://github.com/pytorch/pytorch/pull/43400))\r\n* `torch.maximum`, `torch.minimum` added ([#42579](https://github.com/pytorch/pytorch/pull/42579))\r\n* `torch.arctanh`, `torch.arcsinh` aliases added ([#43762](https://github.com/pytorch/pytorch/pull/43762))\r\n* `torch.linalg.norm` added ([#42749](https://github.com/pytorch/pytorch/pull/42749), [#43907](https://github.com/pytorch/pytorch/pull/43907))\r\n* `torch.amax`, `torch.amin` added ([#43819](https://github.com/pytorch/pytorch/pull/43819))\r\n* `torch.heaviside` added ([#42523](https://github.com/pytorch/pytorch/pull/42523))\r\n* `torch.i0` added ([#43132](https://github.com/pytorch/pytorch/pull/43132))\r\n* `torch.not_equal`, `torch.greater`, `torch.greater_equal`, `torch.less`, `torch.less_equal` aliases added ([#43870](https://github.com/pytorch/pytorch/pull/43870))\r\n* `torch.exp2` added ([#44184](https://github.com/pytorch/pytorch/pull/44184))\r\n* `torch.kaiser_window` added ([#44271](https://github.com/pytorch/pytorch/pull/44271))\r\n* `torch.nanquantile` added ([#44393](https://github.com/pytorch/pytorch/pull/44393))\r\n* `torch.multiply`, `torch.divide` aliases added ([#44463](https://github.com/pytorch/pytorch/pull/44463))\r\n* `nn.TripletMarginWithDistanceLoss` added ([#43680](https://github.com/pytorch/pytorch/pull/43680))\r\n* `torch.fft.fft`, `torch.fft.ifft`, `torch.fft.rfft`, `torch.fft.irfft`, `torch.fft.hfft`, `torch.fft.ihfft` added ([#43011](https://github.com/pytorch/pytorch/pull/43011))\r\n* `torch.fft.fftn`, `torch.fft.ifftn`, `torch.fft.rfftn`, `torch.fft.irfftn` added ([#44550](https://github.com/pytorch/pytorch/pull/44550))\r\n* `optim.functional.adagrad` added ([#44715](https://github.com/pytorch/pytorch/pull/44715))\r\n* `optim.functional.adam` added ([#44791](https://github.com/pytorch/pytorch/pull/44791))\r\n* `torch.complex`,  `torch.polar` added ([#39617](https://github.com/pytorch/pytorch/pull/39617))\r\n* `Tensor.__complex__` added ([#43844](https://github.com/pytorch/pytorch/pull/43844))\r\n* `torch.vdot` added ([#43004](https://github.com/pytorch/pytorch/pull/43004))\r\n\r\nAPI extension:\r\n\r\n* `torch.full` added support for bool and integer dtypes ([#41912](https://github.com/pytorch/pytorch/pull/41912))\r\n* `torch.lt` and `torch.masked_select` added support for half dtype ([#43704](https://github.com/pytorch/pytorch/pull/43704))\r\n* `torch.div`, `torch.true_divide`, `torch.atan2` added support for integer to float type promotion in ([#42359](https://github.com/pytorch/pytorch/pull/42359))\r\n* `unflatten`  added support for non-named dimensions ([#42563](https://github.com/pytorch/pytorch/pull/42563))\r\n* `torch.polygamma`  added support for n >= 2 ([#42499](https://github.com/pytorch/pytorch/pull/42499))\r\n* `torch.qr` added backward support for wide input matrices ([#42216](https://github.com/pytorch/pytorch/pull/42216))\r\n* `nn.Linear`  for MKLDNN added support for no-bias ([#43703](https://github.com/pytorch/pytorch/pull/43703))\r\n* `torch.lerp` added support for half dtype ([#43541](https://github.com/pytorch/pytorch/pull/43541))\r\n* Updates `torch.div` to perform true division (end of deprecation cycle) ([#42907](https://github.com/pytorch/pytorch/pull/42907))\r\n* `torch.scatter` added support for reductions on CUDA ([#41977](https://github.com/pytorch/pytorch/pull/41977))\r\n* BFloat16 support type promotion ([#41698](https://github.com/pytorch/pytorch/pull/41698), [#43324](https://github.com/pytorch/pytorch/pull/43324))\r\n* BFloat16 support on CUDA for `torch.pow` ([#44760](https://github.com/pytorch/pytorch/pull/44760)), unary ops and activations ([#44813](https://github.com/pytorch/pytorch/pull/44813), [#44824](https://github.com/pytorch/pytorch/pull/44824), [#44834](https://github.com/pytorch/pytorch/pull/44834)), `torch.i0` ([#44750](https://github.com/pytorch/pytorch/pull/44750)), `softmax` ([#44837](https://github.com/pytorch/pytorch/pull/44837)), `div`, `addcdiv`, `addcmul`, `mean`, `var` ([#44758](https://github.com/pytorch/pytorch/pull/44758)), `layernorm` ([#45002](https://github.com/pytorch/pytorch/pull/45002)),all pooling layers ([#44836](https://github.com/pytorch/pytorch/pull/44836), [#45151](https://github.com/pytorch/pytorch/pull/45151))), `torch.logspace` (CPU and CUDA) ([#44675](https://github.com/pytorch/pytorch/pull/44675)), random kernels on Windows ([#44918](https://github.com/pytorch/pytorch/pull/44918)), `torch.addmm`, `torch.addmv` ([#44986](https://github.com/pytorch/pytorch/pull/44986)), loss functions ([#45011](https://github.com/pytorch/pytorch/pull/45011)), batched gemm ([#45167](https://github.com/pytorch/pytorch/pull/45167)), nccl path ([#38515](https://github.com/pytorch/pytorch/pull/38515)), binary logical operators ([#42485](https://github.com/pytorch/pytorch/pull/42485)), `torch.neg` ([#45240](https://github.com/pytorch/pytorch/pull/45240)), Conv (non-cuDNN) ([#45007](https://github.com/pytorch/pytorch/pull/45007)), `torch.abs` ([#44804](https://github.com/pytorch/pytorch/pull/44804)), `torch.erfinv` ([#43399](https://github.com/pytorch/pytorch/pull/43399)), comparison ops ([#44748](https://github.com/pytorch/pytorch/pull/44748))\r\n* `torch.asin`, `torch.neg` added support for sparse Tensors ([#44028](https://github.com/pytorch/pytorch/pull/44028))\r\n* `torch.softmax` added support for CUDA ([#42307](https://github.com/pytorch/pytorch/pull/42307))\r\n* `Tensor.{real,imag}` added setter for these attributes ([#39860](https://github.com/pytorch/pytorch/pull/39860))\r\n* `torch.{addmm,addmv}` added support for complex on CUDA ([#40431](https://github.com/pytorch/pytorch/pull/40431), [#43827](https://github.com/pytorch/pytorch/pull/43827))\r\n* `torch.bmm` added support for complex on CPU [#42383](https://github.com/pytorch/pytorch/pull/42383),\r\n* `torch.{dot, vdot}` added support for complex ([#42745](https://github.com/pytorch/pytorch/pull/42745))\r\n* `torch.stft`, `torch.istft` added support for complex ([#43886](https://github.com/pytorch/pytorch/pull/43886))\r\n* `torch.cholesky` added support for complex ([#44895](https://github.com/pytorch/pytorch/pull/44895), [#45267](https://github.com/pytorch/pytorch/pull/45267))\r\n* `torch.sgn` added (to support complex) ([#39955](https://github.com/pytorch/pytorch/pull/39955))\r\n* Binary ops added support for complex ([#43174](https://github.com/pytorch/pytorch/pull/43174))\r\n* Add allowlist for complex backward ([#45461](https://github.com/pytorch/pytorch/pull/45461))\r\n\r\n### Autograd\r\n\r\n* Don't automatically materialize output grads with zeros for `autograd.Function` ([#41821](https://github.com/pytorch/pytorch/pull/41821))\r\n* Benchmark tool for `autograd.functional` API ([#43428](https://github.com/pytorch/pytorch/pull/43428))\r\n* Added `reset_grad` API to remove gradient instead of setting them to zero ([#44423](https://github.com/pytorch/pytorch/pull/44423))\r\n* Allow Tensor-like objects in `torch.autograd.gradcheck` ([#43877](https://github.com/pytorch/pytorch/pull/43877))\r\n* Added support for nested call for `@torch.no_grad()` decorator ([#44633](https://github.com/pytorch/pytorch/pull/44633))\r\n* Added support for `torch.lobpcg` backward ([#43002](https://github.com/pytorch/pytorch/pull/43002))\r\n\r\n### CUDA\r\n\r\n* Added TF32 support ([#41498](https://github.com/pytorch/pytorch/pull/41498))\r\n* CUDA RTX30 series support ([#45489](https://github.com/pytorch/pytorch/pull/45489), [#45130](https://github.com/pytorch/pytorch/pull/45130))\r\n    * **Note: **At the time of the 1.7 release, the currently available and stable Nvidia CUDA libraries are not fully tuned for the RTX 3080 and 3090 so users might see performance regressions.\r\n* `torch.cuda.amp.GradScaler` now supports sparse gradients ([#36786](https://github.com/pytorch/pytorch/pull/36786))\r\n* Autocast support for cudnn RNNs ([#42385](https://github.com/pytorch/pytorch/pull/42385))\r\n* Support AMP in nn.parallel ([#43102](https://github.com/pytorch/pytorch/pull/43102))\r\n* Support for tf32 in cudnn and `backends.cudnn.allow_tf32` flag to control it ([#40737](https://github.com/pytorch/pytorch/pull/40737))\r\n* Added `torch.cuda.memory.list_gpu_processes` to list running processes on a give GPU ([#44616](https://github.com/pytorch/pytorch/pull/44616))\r\n* Add env variable to bypass CUDACachingAllocator for debugging ([#45294](https://github.com/pytorch/pytorch/pull/45294))\r\n* Add non-deterministic alert to CUDA operations that use `atomicAdd()` ([#41538](https://github.com/pytorch/pytorch/pull/41538))\r\n\r\n### C++ API\r\n\r\n* `nn::TransformerEncoderLayer` added ([#42633](https://github.com/pytorch/pytorch/pull/42633))\r\n* `nn::TransformerDecoderLayer` added ([#42717](https://github.com/pytorch/pytorch/pull/42717))\r\n* `nn::TransformerEncoder` added ([#43187](https://github.com/pytorch/pytorch/pull/43187))\r\n* `nn::TransformerDecoder` added ([#42886](https://github.com/pytorch/pytorch/pull/42886))\r\n* `nn::Transformer` added ([#44333](https://github.com/pytorch/pytorch/pull/44333))\r\n* `nn::Unflatten` added ([#42613](https://github.com/pytorch/pytorch/pull/42613))\r\n* `nn.ParameterList` added ([#41259](https://github.com/pytorch/pytorch/pull/41259))\r\n* `torch::cuda::manual_seed` and `torch::cuda::manual_seed_all` added ([#42638](https://github.com/pytorch/pytorch/pull/42638))\r\n\r\n### Mobile\r\n\r\n* Support Tensor MemoryFormat in java wrappers ([#40785](https://github.com/pytorch/pytorch/pull/40785))\r\n* Add `mobile_optimized` boolean flag to optimized model. ([#45479](https://github.com/pytorch/pytorch/pull/45479))\r\n\r\n### Vulkan\r\n\r\n* Backend added ([#36491](https://github.com/pytorch/pytorch/pull/36491), [#43076](https://github.com/pytorch/pytorch/pull/43076))\r\n* Add many operators `adaptive_avg_pool2d` ([#41220](https://github.com/pytorch/pytorch/pull/41220)), `mm` ([#41221](https://github.com/pytorch/pytorch/pull/41221)), `reshape` ([#41223](https://github.com/pytorch/pytorch/pull/41223)), `max_pool2d` ([#41379](https://github.com/pytorch/pytorch/pull/41379)), `add_` and `relu_` ([#41380](https://github.com/pytorch/pytorch/pull/41380)), `cat` ([#41434](https://github.com/pytorch/pytorch/pull/41434)), `add` and `mul` ([#42674](https://github.com/pytorch/pytorch/pull/42674)) and `avg_pool2d` ([#42675](https://github.com/pytorch/pytorch/pull/42675)).\r\n* Model preparation via `torch.utils.optimize_for_vulkan` ([#44903](https://github.com/pytorch/pytorch/pull/44903))\r\n* Add to Java API option to load on Vulkan and test app ([#44896](https://github.com/pytorch/pytorch/pull/44896), [#44897](https://github.com/pytorch/pytorch/pull/44897))\r\n\r\n### Distributed\r\n\r\n* Support alltoall collective in ProcessGroupGloo ([#41424](https://github.com/pytorch/pytorch/pull/41424), [#41690](https://github.com/pytorch/pytorch/pull/41690))\r\n* Add a DDP Communication Hook providing the flexibility to completely override DDP gradient communication ([#40848](https://github.com/pytorch/pytorch/pull/40848))\r\n* Examples on how to use the DDP communication hook ([#43310](https://github.com/pytorch/pytorch/pull/43310))\r\n* Add NCCL Alltoall to NCCL process group ([#42514](https://github.com/pytorch/pytorch/pull/42514))\r\n* Support allgather and gather APIs for Python Objects ([#42189](https://github.com/pytorch/pytorch/pull/42189))\r\n* Join-based API to support uneven inputs in DDP ([#42577](https://github.com/pytorch/pytorch/pull/42577))\r\n* broadcast_object API for c10d ([#43887](https://github.com/pytorch/pytorch/pull/43887))\r\n* Async Error Handling support for ProcessGroupNCCL ([#41050](https://github.com/pytorch/pytorch/pull/41050), [#41051](https://github.com/pytorch/pytorch/pull/41051), [#41052](https://github.com/pytorch/pytorch/pull/41052), [#41053](https://github.com/pytorch/pytorch/pull/41053), [#41054](https://github.com/pytorch/pytorch/pull/41054), [#44163](https://github.com/pytorch/pytorch/pull/44163))\r\n* Add a \u201cgradient_as_bucket_view\" parameter to DDP to reduce memory overhead ([#44344](https://github.com/pytorch/pytorch/pull/44344))\r\n* Add getNumKeys API to c10d TCPStore ([#43962](https://github.com/pytorch/pytorch/pull/43962))\r\n* Add DeleteKey API for c10d TCP Store ([#45401](https://github.com/pytorch/pytorch/pull/45401))\r\n\r\n### Quantization\r\n\r\n* New quantized ops\r\n    * Adaptive average pooling ([#40271](https://github.com/pytorch/pytorch/pull/40271))\r\n    * Max pooling ([#45152](https://github.com/pytorch/pytorch/pull/45152))\r\n    * Embedding and EmbeddingBag quantization (8-bit + partial support for 4-bit): ([#40076](https://github.com/pytorch/pytorch/pull/40076), [#41293](https://github.com/pytorch/pytorch/pull/41293), [#41612](https://github.com/pytorch/pytorch/pull/41612), [#42924](https://github.com/pytorch/pytorch/pull/42924), [#42762](https://github.com/pytorch/pytorch/pull/42762), [#42881](https://github.com/pytorch/pytorch/pull/42881), [#43077](https://github.com/pytorch/pytorch/pull/43077), [#43088](https://github.com/pytorch/pytorch/pull/43088), [#43090](https://github.com/pytorch/pytorch/pull/43090), [#43176](https://github.com/pytorch/pytorch/pull/43176), [#43296](https://github.com/pytorch/pytorch/pull/43296), [#43433](https://github.com/pytorch/pytorch/pull/43433), [#43989](https://github.com/pytorch/pytorch/pull/43989), [#44008](https://github.com/pytorch/pytorch/pull/44008), [#44207](https://github.com/pytorch/pytorch/pull/44207), [#44208](https://github.com/pytorch/pytorch/pull/44208), [#44217](https://github.com/pytorch/pytorch/pull/44217), [#45149](https://github.com/pytorch/pytorch/pull/45149), [#44845](https://github.com/pytorch/pytorch/pull/44845), [#44048](https://github.com/pytorch/pytorch/pull/44048), [#42690](https://github.com/pytorch/pytorch/pull/42690), [#42612](https://github.com/pytorch/pytorch/pull/42612))\r\n    * QNNPACK Transposed convolution2D and 3D ([#39714](https://github.com/pytorch/pytorch/pull/39714), [#40351](https://github.com/pytorch/pytorch/pull/40351), [#40360](https://github.com/pytorch/pytorch/pull/40360), [#40370](https://github.com/pytorch/pytorch/pull/40370), [#40371](https://github.com/pytorch/pytorch/pull/40371), [#44844](https://github.com/pytorch/pytorch/pull/44844), [#45078](https://github.com/pytorch/pytorch/pull/45078), [#45081](https://github.com/pytorch/pytorch/pull/45081))\r\n    * Operations on quantized tensors\r\n        * `aten::repeat` ([#40644](https://github.com/pytorch/pytorch/pull/40644))\r\n        * `aten::apend` ([#40743](https://github.com/pytorch/pytorch/pull/40743))\r\n        * `stack` ([#42187](https://github.com/pytorch/pytorch/pull/42187))\r\n        * `fill_` ([#43303](https://github.com/pytorch/pytorch/pull/43303))\r\n        * `clone` for per channel affine quantized tensor ([#44573](https://github.com/pytorch/pytorch/pull/44573))\r\n        * `append` (graphmode) ([#44641](https://github.com/pytorch/pytorch/pull/44641))\r\n    * 1D batch normalization support ([#42491](https://github.com/pytorch/pytorch/pull/42491))\r\n    * N-Dimensional constant padding ([#43304](https://github.com/pytorch/pytorch/pull/43304))\r\n    * CELU operator ([#39199](https://github.com/pytorch/pytorch/pull/39199))\r\n* Support for FP16 quantization ([#40708](https://github.com/pytorch/pytorch/pull/40708), [#40709](https://github.com/pytorch/pytorch/pull/40709), [#40710](https://github.com/pytorch/pytorch/pull/40710), [#42147](https://github.com/pytorch/pytorch/pull/42147), [#42221](https://github.com/pytorch/pytorch/pull/42221), [#42222](https://github.com/pytorch/pytorch/pull/42222), [#42348](https://github.com/pytorch/pytorch/pull/42348), [#41049](https://github.com/pytorch/pytorch/pull/41049))\r\n* Add Quantizer support to IValue ([#42438](https://github.com/pytorch/pytorch/pull/42438))\r\n* Custom module support ([#44835](https://github.com/pytorch/pytorch/pull/44835))\r\n* Preserving pre and post forward hooks ([#37233](https://github.com/pytorch/pytorch/pull/37233))\r\n\r\n### Misc\r\n\r\n* `torch.set_deterministic` and `torch.is_deterministic`: Raise error when the flag is set and a non-deterministic operation is used ([#15359](https://github.com/pytorch/pytorch/issues/15359), [#41377](https://github.com/pytorch/pytorch/issues/41377))\r\n* Add CUDA 11 to nightly binaries ([#44086](https://github.com/pytorch/pytorch/pull/44086), [#43366](https://github.com/pytorch/pytorch/pull/43366))\r\n* Dev Tool: Nightly checkout tool and doc in `CONTRIBUTING.md` ([#42635](https://github.com/pytorch/pytorch/pull/42635),  [#43294](https://github.com/pytorch/pytorch/pull/43294))\r\n* Website: Add docs for tagged version (include rc) on the general website ([#45204](https://github.com/pytorch/pytorch/pull/45204))\r\n* Build: Added BUILD_CAFFE2 flag to be able to disable caffe2 compilation ([#43673](https://github.com/pytorch/pytorch/pull/43673))\r\n* Dataloader: Add `prefetch_factor` argument to control the number of batch loaded ahead of time([#41130](https://github.com/pytorch/pytorch/pull/41130))\r\n* Dataloader: Allow handling of `np.memmap` objects ([#39847](https://github.com/pytorch/pytorch/pull/39847))\r\n* ROCm: Add support torch `utils.cpp_extension` ([#41257](https://github.com/pytorch/pytorch/pull/41257), [#43528](https://github.com/pytorch/pytorch/pull/43528))\r\n* ROCm: Enable complex BLAS ([#43744](https://github.com/pytorch/pytorch/pull/43744))\r\n* docker: Add torchelastic to docker image ([#45438](https://github.com/pytorch/pytorch/pull/45438))\r\n* docker: Add CUDA 11 support ([#45071](https://github.com/pytorch/pytorch/pull/45071))\r\n* docker: Use python 3.8 in pytorch docker image ([#45466](https://github.com/pytorch/pytorch/pull/45466))\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Use tree-based sum for floats to avoid numerical instability ([#39516](https://github.com/pytorch/pytorch/pull/39516))\r\n* `nn.ReflectionPad`: Add support for 0-dim batch sizes. ([#39231](https://github.com/pytorch/pytorch/pull/39231))\r\n* `torch.scatter`: Add reductions for CPU ([#36447](https://github.com/pytorch/pytorch/pull/36447))\r\n* Allow any valid ASCII python identifiers as dimnames ([#40871](https://github.com/pytorch/pytorch/pull/40871))\r\n* Improve Python warning prints when there is also an error ([#41116](https://github.com/pytorch/pytorch/pull/41116))\r\n* `torch.iinfo`, `torch.finfo`: Improve printing ([#40488](https://github.com/pytorch/pytorch/pull/40488))\r\n* `torch.where`: Add support for scalar input ([#40336](https://github.com/pytorch/pytorch/pull/40336))\r\n* `torch.nonzero`: Remove deprecation warning for `as_tuple` argument ([#45413](https://github.com/pytorch/pytorch/pull/45413))\r\n* `torch.distributions.Categorical`: Clamp logit to avoid `-inf` when calculating entropy ([#41002](https://github.com/pytorch/pytorch/pull/41002))\r\n* `torch.futures.Future`: Add `done` function to query the status of the future ([#42013](https://github.com/pytorch/pytorch/pull/42013))\r\n\r\n### torch.nn\r\n\r\n* `nn.EmbeddingBag`: Add support for `incude_last_offset=True` when reduction is mean or max ([#42215](https://github.com/pytorch/pytorch/pull/42215))\r\n* `nn.AvgPooling{1,2,3}d`: Ensure all cells are valid in ceil mode to avoid division by 0 ([#41368](https://github.com/pytorch/pytorch/pull/41368))\r\n* `nn,[Adaptive]MaxPool{1,2,3}d`: Handle edge case when input is filled with -inf ([#40665](https://github.com/pytorch/pytorch/pull/40665))\r\n* `nn.Hardsigmoid`, `nn.Hardswish`: Add inplace option ([#42346](https://github.com/pytorch/pytorch/pull/42346))\r\n* `nn.MSELoss`, `nn.L1Loss`, `nn.SmoothL1Loss`: Add support for target that requires gradients. ([#44437](https://github.com/pytorch/pytorch/pull/44437), [#44471](https://github.com/pytorch/pytorch/pull/44471), [#44486](https://github.com/pytorch/pytorch/pull/44486))\r\n* `nn.Parameter{List,Dict}`: Add warning when improperly used (with DataParallel or weight_norm) ([#44405](https://github.com/pytorch/pytorch/pull/44405))\r\n* `nn.functional.smooth_l1`: Add beta parameter ([#44433](https://github.com/pytorch/pytorch/pull/44433))\r\n\r\n### Build\r\n\r\n* Report error when ATEN_THEADING is OMP and USE_OPENMP is turned off. ([#40146](https://github.com/pytorch/pytorch/pull/40146))\r\n* Raise nice error when trying to build PyTorch on 32-bit Windows system ([#40321](https://github.com/pytorch/pytorch/pull/40321))\r\n* Make setup.py Python-2 syntactically correct and work for version >= 3.9 ([#41960](https://github.com/pytorch/pytorch/pull/41960), [#46388](https://github.com/pytorch/pytorch/pull/46388))\r\n* Don't proceed into setup.py too far if Python version is unsupported ([#42870](https://github.com/pytorch/pytorch/pull/42870))\r\n\r\n### Distributed\r\n\r\n* Support profiling rpc_async in TorchScript ([#40652](https://github.com/pytorch/pytorch/pull/40652))\r\n* Allow RPC to be initialized again after shutdown. ([#42723](https://github.com/pytorch/pytorch/pull/42723))\r\n* Support rpc_sync, rpc.remote in TorchScript ([#43043](https://github.com/pytorch/pytorch/pull/43043), [#43046](https://github.com/pytorch/pytorch/pull/43046))\r\n* Make async_execution compatible with RRef helpers ([#44666](https://github.com/pytorch/pytorch/pull/44666))\r\n* Extend RPC profiling to support async function execution over RPC. ([#44664](https://github.com/pytorch/pytorch/pull/44664))\r\n* Support record_shapes in RPC profiling ([#44419](https://github.com/pytorch/pytorch/pull/44419))\r\n* Add variants for cuda.comm.broadcast/gather/scatter which store the result in a provided \u201cout\u201d parameter ([#39681](https://github.com/pytorch/pytorch/pull/39681))\r\n* Explicitly abort NCCL Communicators on ProcessGroupNCCL Destruction ([#40585](https://github.com/pytorch/pytorch/pull/40585))\r\n* Helper function to print out all DDP-relevant env vars ([#41297](https://github.com/pytorch/pytorch/pull/41297))\r\n* Add timeout to ProcessGroup Work Wait ([#40944](https://github.com/pytorch/pytorch/pull/40944))\r\n* Support Wait Timeout in ProcessGroupNCCL ([#40946](https://github.com/pytorch/pytorch/pull/40946))\r\n* Support work-level timeouts in ProcessGroupGloo ([#40948](https://github.com/pytorch/pytorch/pull/40948))\r\n* Support for torch.bool in ProcessGroupNCCL ([#41959](https://github.com/pytorch/pytorch/pull/41959))\r\n* DDP.train() returns self to stay consistent with nn.Module ([#42131](https://github.com/pytorch/pytorch/pull/42131))\r\n* Add a drop_last option in DistributedSampler to drop tail of the data to ensure data is even across ranks ([#41171](https://github.com/pytorch/pytorch/pull/41171))\r\n* Additional error checking for `torch.cuda.nccl` APIs. ([#43247](https://github.com/pytorch/pytorch/pull/43247))\r\n* Support work.result() to get result tensors for allreduce for Gloo, NCCL backends ([#43970](https://github.com/pytorch/pytorch/pull/43970))\r\n* Add a device parameter to RemoteModule ([#44254](https://github.com/pytorch/pytorch/pull/44254))\r\n* Add remote_parameters() API for RemoteModule. ([#43906](https://github.com/pytorch/pytorch/pull/43906))\r\n* Add a warning log when there is high skew of uneven inputs in DDP training ([#45238](https://github.com/pytorch/pytorch/pull/45238))\r\n\r\n### TorchScript\r\n\r\n* Support string concatenation (cc29c192a6) \r\n* Support using Python Enum in TorchScript ([#41390](https://github.com/pytorch/pytorch/pull/41390),[#41965,](https://github.com/pytorch/pytorch/pull/41965)[#42085,](https://github.com/pytorch/pytorch/pull/42085)[#42623,](https://github.com/pytorch/pytorch/pull/42623)[#42661,](https://github.com/pytorch/pytorch/pull/42661)[#42661,](https://github.com/pytorch/pytorch/pull/42661)[#42874,](https://github.com/pytorch/pytorch/pull/42874)[#43460,](https://github.com/pytorch/pytorch/pull/43460)[#43188,](https://github.com/pytorch/pytorch/pull/43188)[#44243,](https://github.com/pytorch/pytorch/pull/44243)[#44891](https://github.com/pytorch/pytorch/pull/44891))\r\n* Support sorting list of strings ([#42398](https://github.com/pytorch/pytorch/pull/42398))\r\n* Support boolean key in dictionary ([#42833](https://github.com/pytorch/pytorch/pull/42833))\r\n* Support `@torch.no_grad` ([#41371](https://github.com/pytorch/pytorch/pull/41371))\r\n* Support `del` to TorchScript classes ([#44352](https://github.com/pytorch/pytorch/pull/44352))\r\n* Speed up saving modules in case of having many classes ([#44589](https://github.com/pytorch/pytorch/pull/44589))\r\n* Support Python Slice class in TorchScript ([#44335](https://github.com/pytorch/pytorch/pull/44335))\r\n* Support sorting a list of tuples ([#43448](https://github.com/pytorch/pytorch/pull/43448))\r\n* Enable `@torch.jit.unused` syntax for ignoring properties ([#45261](https://github.com/pytorch/pytorch/pull/45261))\r\n* Enable ProfilingExecutor + TensorExpression (#45546) ([#45546](https://github.com/pytorch/pytorch/pull/45546))\r\n* Support `@torch.jit.unused` on a `@torch.no_grad` decorated function ([#41496](https://github.com/pytorch/pytorch/pull/41496))\r\n* Improve ModuleList indexing error msg ([#43361](https://github.com/pytorch/pytorch/pull/43361))\r\n* Better match behavior of loaded `ScriptModule``s vs. freshly created ones ([#43298](https://github.com/pytorch/pytorch/pull/43298))\r\n* Support backend-lowered submodules ([#41146](https://github.com/pytorch/pytorch/pull/41146))\r\n* Allow freezing of modules containing interface attribute ([#41860](https://github.com/pytorch/pytorch/pull/41860))\r\n* `to_backend` API now accepts wrapped modules ([#43612](https://github.com/pytorch/pytorch/pull/43612))\r\n* Allow submodule methods inference rules to be different ([#43872](https://github.com/pytorch/pytorch/pull/43872))\r\n* Support default values for arguments of class type methods ([#45098](https://github.com/pytorch/pytorch/pull/45098))\r\n* Improve sugared value's error message when closing over global variables ([#42889](https://github.com/pytorch/pytorch/pull/42889))\r\n* Support backend-lowered submodules ([#40841](https://github.com/pytorch/pytorch/pull/40841))\r\n* Turn on non-ASCII string literals serialization ([#40719](https://github.com/pytorch/pytorch/pull/40719))\r\n* Better printing of Tensor stride information (#[45156](https://github.com/pytorch/pytorch/pull/45156))\r\n\r\n### Mobile\r\n\r\n* Allow specifying PYTHON executable to build_android ([#41927](https://github.com/pytorch/pytorch/pull/41927))\r\n* Include all overloads for OSS custom build (a01e91e6b2)\r\n\r\n### Quantization\r\n\r\n* Change the `whitelist` to `allowlist` ([#41771](https://github.com/pytorch/pytorch/pull/41771), [#41802](https://github.com/pytorch/pytorch/pull/41802))\r\n* `dequantize` now supports list and tuple of tensors ([#41079](https://github.com/pytorch/pytorch/pull/41079))\r\n* User now has a way to add a activation post process hook using `register_activation_post_process_hook` function ([#42342](https://github.com/pytorch/pytorch/pull/42342))\r\n* `add`/`mul` now support different variants ([#42769](https://github.com/pytorch/pytorch/pull/42769))\r\n* Fake quantizer now has more info when printed ([#43031](https://github.com/pytorch/pytorch/pull/43031))\r\n* `OP_LIST_TO_FUSER_METHOD` is exposed to the user ([#43286](https://github.com/pytorch/pytorch/pull/43286))\r\n* `quantize_jit`  can handle new upsample overloads ([#43407](https://github.com/pytorch/pytorch/pull/43407))\r\n* Setter/getter method for quantization and fusion mappings ([#43990](https://github.com/pytorch/pytorch/pull/43990))\r\n* fake_quant and observer can be disabled in scriptmodule ([#44773](https://github.com/pytorch/pytorch/pull/44773))\r\n* `convert_jit` can now take `preserved_attrs` argument ([#44490](https://github.com/pytorch/pytorch/pull/44490))\r\n* `SyncBN`: preserve qconfig if it exists ([#45317](https://github.com/pytorch/pytorch/pull/45317))\r\n* Add quant APIs to save/load observer `state_dict` ([#44846](https://github.com/pytorch/pytorch/pull/44846))\r\n* Add version support for the `conv` parameters ([#43524](https://github.com/pytorch/pytorch/pull/43524), [#43086](https://github.com/pytorch/pytorch/pull/43086), [#43651](https://github.com/pytorch/pytorch/pull/43651), [#44671](https://github.com/pytorch/pytorch/pull/44671))\r\n\r\n### ONNX\r\n\r\nIn PyTorch 1.7, we have continued to add and improve PyTorch operator export to ONNX. We have enabled export of 10 new operators, and further enhanced and optimized export of 10+ torch operators to ONNX. We have also focused on improving export of TorchScript modules, in particular laying some groundwork required for better support in near future. We have also created an API  (torch.onnx.utils._find_missing_ops_onnx_export) as a diagnostic tool (preview only) to get a list of operators in a model that are not supported or implemented by ONNX exporter. Support for export of torch.quantization.FakeQuantize has also been added to help enable some QAT workflows. \r\n\r\n* Add support to export more torch ops `torch.view_as` ([#40496](https://github.com/pytorch/pytorch/pull/40496)), fake quantize functions ([#39738](https://github.com/pytorch/pytorch/pull/39738)), embedding_bag ([#41234](https://github.com/pytorch/pytorch/pull/41234), [#44693](https://github.com/pytorch/pytorch/pull/44693)), `torch.eye` ([#41357](https://github.com/pytorch/pytorch/pull/41357)), `Tensor.as_strided` ([#41569](https://github.com/pytorch/pytorch/pull/41569)), `torch.tensor` ([#41872](https://github.com/pytorch/pytorch/pull/41872)), addition between list of tensors ([#41888](https://github.com/pytorch/pytorch/pull/41888)), `Tensor.__floordiv__` ([#43022](https://github.com/pytorch/pytorch/pull/43022)), `torch.nn.KLDivLoss` ([#41858](https://github.com/pytorch/pytorch/pull/41858)), `Tensor.new_empty` and `Tensor.new_zeros` ([#43506](https://github.com/pytorch/pytorch/pull/43506))\r\n* Improves existing export logic and optimizing exported ONNX graph\r\n    * Add warning in ONNX export when constant folding is on in training-amenable mode ([#40546](https://github.com/pytorch/pytorch/pull/40546))\r\n    * Fix export of `torch.full_like` ([#40063](https://github.com/pytorch/pytorch/pull/40063))\r\n    * Add pass that fuses Conv and BatchNormalization ([#40547](https://github.com/pytorch/pytorch/pull/40547))\r\n    *  `torch.where` export, add support for ByteTensor ([#42264](https://github.com/pytorch/pytorch/pull/42264))\r\n    * Fix scalar type cast for comparison ops ([#37787](https://github.com/pytorch/pytorch/pull/37787))\r\n    * `torch.scatter` export, add support for src being scalar or different dtype ([#42765](https://github.com/pytorch/pytorch/pull/42765), [#43440](https://github.com/pytorch/pytorch/pull/43440))\r\n    * Fix Squeeze operator when applied to a dimension with shape > 1 ([#38476](https://github.com/pytorch/pytorch/pull/38476))\r\n    *  Extend support for `torch.where` ([#41544](https://github.com/pytorch/pytorch/pull/41544))\r\n    * Update ops `torch.slice` ([#42935](https://github.com/pytorch/pytorch/pull/42935)), `torch.split` ([#43670](https://github.com/pytorch/pytorch/pull/43670)), `torch.repeat` ([#43430](https://github.com/pytorch/pytorch/pull/43430)), `torch.arange` ([#43777](https://github.com/pytorch/pytorch/pull/43777)), `len` ([#43824](https://github.com/pytorch/pytorch/pull/43824)), `torch.narrow` ([#44039](https://github.com/pytorch/pytorch/pull/44039)), flatten ([#40418](https://github.com/pytorch/pytorch/pull/40418)), adaptive_pool ([#46100](https://github.com/pytorch/pytorch/pull/46100))\r\n* Update export to follow pytorch changes\r\n\r\n    * Update div export to perform true divide ([#44831](https://github.com/pytorch/pytorch/pull/))\r\n    * Enable true_divide scripting export with ONNX  shape inference ([#43991](https://github.com/pytorch/pytorch/pull/43911))\r\n\r\n### Misc\r\n\r\n* `torch.utils.collect_env`: Collect more informations (python 32/64bit, clang version, CPU architecture, ROCm version) ([#42887](https://github.com/pytorch/pytorch/pull/42887), [#42961](https://github.com/pytorch/pytorch/pull/42961), [#44106](https://github.com/pytorch/pytorch/pull/44106))\r\n* `torch.hub.load_local`: Allow to load models from any local directory ([#44204](https://github.com/pytorch/pytorch/pull/44204))\r\n* Add warning if `import torch` is called from the source root ([#39995](https://github.com/pytorch/pytorch/pull/39995))\r\n* Improve Dynamic Library loading for Windows ([#40365](https://github.com/pytorch/pytorch/pull/40365))\r\n* serialization: validate sparse tensors after loading ([#34059](https://github.com/pytorch/pytorch/pull/34059))\r\n* Add `--continue-through-error` option to run_test.sh script ([#41136](https://github.com/pytorch/pytorch/pull/41136))\r\n* Tensorboard: Support custom `run_name` and ``hparam_domain_discrete` in `add_hparams` ([#40660](https://github.com/pytorch/pytorch/pull/40660), [#40720](https://github.com/pytorch/pytorch/pull/40720))\r\n* MKLDNN: Enable conv3d, batchnorm3d, max_pool3d and avg_pool3d ([#40691](https://github.com/pytorch/pytorch/pull/40691), [#40995](https://github.com/pytorch/pytorch/pull/40995), [#40996](https://github.com/pytorch/pytorch/pull/40996))\r\n* Profiler: Do not record zero duration kernel events ([#41540](https://github.com/pytorch/pytorch/pull/41540))\r\n* Profiler: Improve cuda time counting ([#45209](https://github.com/pytorch/pytorch/pull/45209))\r\n* Profiler: Adding `with_source` parameter to enable tracking source code ([#43898](https://github.com/pytorch/pytorch/pull/43898))\r\n* Optim: Add verbose param for all schedulers ([#41580](https://github.com/pytorch/pytorch/pull/41580))\r\n* Pruning: check attributes before deleting ([#41913](https://github.com/pytorch/pytorch/pull/41913))\r\n* Autograd: In `zero_grad`, avoid using inpalce `detach` when it is not required ([#41283](https://github.com/pytorch/pytorch/pull/41283))\r\n* Autograd: Update the `torch.div` backward formula to improve numerical stability ([#43627](https://github.com/pytorch/pytorch/pull/43627))\r\n* Autograd: Print all traceback for higher order backwards in detect_anomaly ([#43626](https://github.com/pytorch/pytorch/pull/43626))\r\n* Autograd: Stop saving input of `torch.repeat` as only `input.dim()` is needed in backward ([#40766](https://github.com/pytorch/pytorch/pull/40766))\r\n* CUDA: Improve cuDNN error messages to include call parameters ([#45023](https://github.com/pytorch/pytorch/pull/45023))\r\n* CUDA: Improve `device_count` and cuda init error detection and messages ([#42249](https://github.com/pytorch/pytorch/pull/42249))\r\n* Improve Tensor layout propagation for pointwise ops to follow input layout more closely ([#42922](https://github.com/pytorch/pytorch/pull/42922))\r\n* Remove blacklist/whitelist references ([#41447](https://github.com/pytorch/pytorch/pull/41447), [#41644](https://github.com/pytorch/pytorch/pull/41644), [#41636](https://github.com/pytorch/pytorch/pull/41636), [#41777](https://github.com/pytorch/pytorch/pull/41777), [#41822](https://github.com/pytorch/pytorch/pull/41822), [#41691](https://github.com/pytorch/pytorch/pull/41691), [#41789](https://github.com/pytorch/pytorch/pull/41789), [#41979](https://github.com/pytorch/pytorch/pull/41979), [#41627](https://github.com/pytorch/pytorch/pull/41627), [#42011](https://github.com/pytorch/pytorch/pull/42011), [#41796](https://github.com/pytorch/pytorch/pull/41796), [#42067](https://github.com/pytorch/pytorch/pull/42067), [#42091](https://github.com/pytorch/pytorch/pull/42091), [#42097](https://github.com/pytorch/pytorch/pull/42097), [#42071](https://github.com/pytorch/pytorch/pull/42071), [#42089](https://github.com/pytorch/pytorch/pull/42089), [#42279](https://github.com/pytorch/pytorch/pull/42279), [#42047](https://github.com/pytorch/pytorch/pull/42047), [#42088](https://github.com/pytorch/pytorch/pull/42088), [#45260](https://github.com/pytorch/pytorch/pull/45260))\r\n\r\n### Python Type Annotations\r\n\r\n* Update some types in top level `torch/*.py` ([#40235](https://github.com/pytorch/pytorch/pull/40235), [#40873](https://github.com/pytorch/pytorch/pull/40873))\r\n* Added typing for `Tensor` attributes and methods: `T` and `grad_fn` ([#40879](https://github.com/pytorch/pytorch/pull/40879)),  `Tensor._version` ([#41125](https://github.com/pytorch/pytorch/pull/41125)), `ndim` ([#42909](https://github.com/pytorch/pytorch/pull/42909)), `nonzero`  ([#43053](https://github.com/pytorch/pytorch/pull/43053)), [#40499](https://github.com/pytorch/pytorch/pull/40499))\r\n* Added typing for `torch.serialization` ([#40862](https://github.com/pytorch/pytorch/pull/40862))\r\n* Added typing for `torch.tensor` ([#45077](https://github.com/pytorch/pytorch/pull/45077))\r\n* Added typing for  `torch.Size` ([#40879](https://github.com/pytorch/pytorch/pull/40879))\r\n* Added typing for `torch.futures` ([#41675](https://github.com/pytorch/pytorch/pull/41675))\r\n* Added typing for `torch.random` ([#42234](https://github.com/pytorch/pytorch/pull/42234))\r\n* Added typing for `torch.hub` ([#42252](https://github.com/pytorch/pytorch/pull/42252))\r\n* Added typing for `collect_env.py` ([#43062](https://github.com/pytorch/pytorch/pull/43062))\r\n* Added typing for `torch.utils` ([#39392](https://github.com/pytorch/pytorch/pull/39392), [#42647](https://github.com/pytorch/pytorch/pull/42647), [#42711](https://github.com/pytorch/pytorch/pull/42711), [#42960](https://github.com/pytorch/pytorch/pull/42960), [#43806](https://github.com/pytorch/pytorch/pull/43806), [#44136](https://github.com/pytorch/pytorch/pull/44136), [#44216](https://github.com/pytorch/pytorch/pull/44216))\r\n* Added typing for `torch.nn` ([#43044](https://github.com/pytorch/pytorch/pull/43044), [#44093](https://github.com/pytorch/pytorch/pull/44093), [#43080](https://github.com/pytorch/pytorch/pull/43080), [#42231](https://github.com/pytorch/pytorch/pull/42231), [#40669](https://github.com/pytorch/pytorch/pull/40669))\r\n* Added typing for `torch.sparse` ([#43108](https://github.com/pytorch/pytorch/pull/43108))\r\n* Added typing for `torch.cuda.nvtx` ([#43443](https://github.com/pytorch/pytorch/pull/43443))\r\n* Added typing for `torch.cuda.memory` ([#43444](https://github.com/pytorch/pytorch/pull/43444))\r\n* Added typing for `torch.functional` ([#43446](https://github.com/pytorch/pytorch/pull/43446))\r\n* Added typing for `torch.autograd` ([#44451](https://github.com/pytorch/pytorch/pull/44451), [#46206](https://github.com/pytorch/pytorch/pull/46206))\r\n* Added typing for `torch.quantization.fuse_modules` ([#43786](https://github.com/pytorch/pytorch/pull/43786))\r\n* Added typing for `torch.nn.quantized` ([#43186](https://github.com/pytorch/pytorch/pull/43186), [#44154](https://github.com/pytorch/pytorch/pull/44154), [#43110](https://github.com/pytorch/pytorch/pull/43110))\r\n* Added typing for `torch.testing._internal` submodules ([#44575](https://github.com/pytorch/pytorch/pull/44575), [#44805](https://github.com/pytorch/pytorch/pull/44805), [#44832](https://github.com/pytorch/pytorch/pull/44832), [#44911](https://github.com/pytorch/pytorch/pull/44911), [#44927](https://github.com/pytorch/pytorch/pull/44927), [#44985](https://github.com/pytorch/pytorch/pull/44985), [#44971](https://github.com/pytorch/pytorch/pull/44971), [#45107](https://github.com/pytorch/pytorch/pull/45107), [#45368](https://github.com/pytorch/pytorch/pull/45368), [#45375](https://github.com/pytorch/pytorch/pull/45375))\r\n* Added typing for `torch.backends.quantized` ([#44794](https://github.com/pytorch/pytorch/pull/44794))\r\n* Added typing for `torch.backends.cuda` ([#44916](https://github.com/pytorch/pytorch/pull/44916))\r\n* Added typing for `torch.cuda.{comm,nccl,amp}` ([#45350](https://github.com/pytorch/pytorch/pull/45350), [#45344](https://github.com/pytorch/pytorch/pull/45344), [#45480](https://github.com/pytorch/pytorch/pull/45480))\r\n* Added typing for `torch.quasirandom` ([#45434](https://github.com/pytorch/pytorch/pull/45434))\r\n* Fix typing for `jit.trace` and `onnx.export` ([#41093](https://github.com/pytorch/pytorch/pull/41093))\r\n* Fix typing for `torch/optim/lr_scheduler.pyi` ([#41775](https://github.com/pytorch/pytorch/pull/41775), [#41866](https://github.com/pytorch/pytorch/pull/41866))\r\n\r\n# Bug fixes\r\n\r\n### Python API\r\n\r\n* `torch.linspace`: Fix step computation for large integral types ([#40132](https://github.com/pytorch/pytorch/pull/40132))\r\n* `torch.pca_lowrank`: Fix un-expected memory consumption ([#40853](https://github.com/pytorch/pytorch/pull/40853))\r\n* `torch.linspace`: Fix behavior for non-contiguous inputs on CPU ([#41286](https://github.com/pytorch/pytorch/pull/41286))\r\n* `torch.div`: Fix division by low precision scalar ([#41446](https://github.com/pytorch/pytorch/pull/41446))\r\n* `torch.expm1`: disable mkl as it produces wrong values in some cases ([#41654](https://github.com/pytorch/pytorch/pull/41654))\r\n* `torch.utils.data.RandomSampler`: Stop generating samples one at a time when replacement=True ([#41682](https://github.com/pytorch/pytorch/pull/41682))\r\n* `torch.nn.functional.grid_sample`: Fix 64-bit indexing ([#41923](https://github.com/pytorch/pytorch/pull/41923))\r\n* `torch.nn.functional.grid_sample`: Fix crash when `grid` has NaNs ([#42703](https://github.com/pytorch/pytorch/pull/42703))\r\n* `torch.det`: Fix on CPU ([#35136](https://github.com/pytorch/pytorch/pull/35136))\r\n* `torch.interpolate`: Avoid zero division in cubic mode ([#42093](https://github.com/pytorch/pytorch/pull/42093))\r\n* `torch.fmod`: Fix to work with zero divisors consistently ([#41948](https://github.com/pytorch/pytorch/pull/41948))\r\n* `torch.masked_select`: Fix for discontiguous outputs ([#41841](https://github.com/pytorch/pytorch/pull/41841))\r\n* `torch.cummin`, `torch.cummax`: Fix for discontiguous inputs/outputs ([#42507](https://github.com/pytorch/pytorch/pull/42507))\r\n* `torch.einsum`: Fix for discontiguous inputs ([#42425](https://github.com/pytorch/pytorch/pull/42425))\r\n* `torch.orgqr`: Fix input size conditions ([#42825](https://github.com/pytorch/pytorch/pull/42825))\r\n* `torch.manual_seed`: Fix argument unpacking ([#42206](https://github.com/pytorch/pytorch/pull/42206))\r\n* `torch.searchsorted`: Properly mark output as non differentiable ([#42933](https://github.com/pytorch/pytorch/pull/42933))\r\n* `torch.bucketize`: Properly mark output as non differentiable ([#44102](https://github.com/pytorch/pytorch/pull/44102))\r\n* `torch.addmm`: Properly raise error on device mismatch  ([#43505](https://github.com/pytorch/pytorch/pull/43505))\r\n* `torch.chain_matmul`: Properly handle empty args ([#43553](https://github.com/pytorch/pytorch/pull/43553))\r\n* `torch.multinomial`: Properly handle 0 size dim ([#43775](https://github.com/pytorch/pytorch/pull/43775))\r\n* `torch.cholesky_solve`: Fix broadcast and error checking ([#43137](https://github.com/pytorch/pytorch/pull/43137))\r\n* `torch.movedim`: Fix uniqueness check  ([#44307](https://github.com/pytorch/pytorch/pull/44307))\r\n* `torch.min`, ` torch.max`, `torch.mean`: Properly throw error if dim is repeated ([#44281](https://github.com/pytorch/pytorch/pull/44281))\r\n* `torch.lerp`: Fix for discontiguous outputs on CUDA ([#44559](https://github.com/pytorch/pytorch/pull/44559))\r\n* `torch.addmv`, `torch.mv`: Fix beta=0 case in slow path ([#44681](https://github.com/pytorch/pytorch/pull/44681))\r\n* `torch.triangular_solve`: Fix error check on CPU ([#44720](https://github.com/pytorch/pytorch/pull/44720))\r\n* `torch.empty_like`, `torch.zeros_like`: Properly raise error if any memory format is provided with sparse input ([#44058](https://github.com/pytorch/pytorch/pull/44058))\r\n* `torch.atan2`: Fix type promotion ([#43466](https://github.com/pytorch/pytorch/pull/43466))\r\n* `torch.repeat`: Fix backward for 0 size repeats ([#45212](https://github.com/pytorch/pytorch/pull/45212))\r\n* `torch.min`, ` torch.max`, `torch.median`: Fix handling of nan in backward ([#45280](https://github.com/pytorch/pytorch/pull/45280))\r\n* `torch.rdiv`: Properly make it consistent with div ([#45407](https://github.com/pytorch/pytorch/pull/45407))\r\n* `torch.std`: Fix hanling of nan in backward ([#45468](https://github.com/pytorch/pytorch/pull/45468))\r\n* `torch.distributions.Binomial`: Fix CUDA sampling at extreme points ([#42702](https://github.com/pytorch/pytorch/pull/42702))\r\n* `torch.dot`, `torch.vdot`: Add complex support ([#45074](https://github.com/pytorch/pytorch/pull/45074))\r\n* `torch.pow`: Fix when scalar base is complex ([#45259](https://github.com/pytorch/pytorch/pull/45259))\r\n* `torch.round`, `torch.abs_`: Disable complex inputs ([#45330](https://github.com/pytorch/pytorch/pull/45330))\r\n* `torch.svd`: Fix memory corruption for complex inputs ([#45486](https://github.com/pytorch/pytorch/pull/45486))\r\n* `torch.view_as_complex`: Fix zero dimensional input ([#44175](https://github.com/pytorch/pytorch/pull/44175))\r\n* `torch.kthvalue`: Fix for non-contiguous input ([#46177](https://github.com/pytorch/pytorch/pull/46177))\r\n* `torch.save`: Fix python binding that could lead to out of bound read ([#46207](https://github.com/pytorch/pytorch/pull/46207))\r\n\r\n### Torch.nn\r\n\r\n* `nn.ModuleDict`: Fix input dict key ordering ([#40905](https://github.com/pytorch/pytorch/pull/40905))\r\n* `nn.LayerNorm`: Fix handling of `gamma` in the backward when `create_graph=True`  ([#41595](https://github.com/pytorch/pytorch/pull/41595))\r\n* `nn.functional.{max,avg}_pool{1,2,3}d`: Raise RuntimeError for zero stride ([#41819](https://github.com/pytorch/pytorch/pull/41819))\r\n* `nn.Module`: Fix missing attribute when loading model from older version ([#42290](https://github.com/pytorch/pytorch/pull/42290))\r\n* `nn.Embedding`: Raise proper error for 0-D weight ([#42550](https://github.com/pytorch/pytorch/pull/42550))\r\n* `nn.SyncBatchNorm`: Fix forward pass for non-default process group ([#43861](https://github.com/pytorch/pytorch/pull/43861))\r\n* `nn.functional.embedding_bag`: Fix for non-contiguous weight ([#44032](https://github.com/pytorch/pytorch/pull/44032))\r\n* `nn.functional.upsample`: Add nondeterministic checks (df6ea62526)\r\n* `nn.GroupNorm`: Fix bug when input does not require_grad on CUDA ([#44863](https://github.com/pytorch/pytorch/pull/44863))\r\n* `functional.{l1_loss,smoothl1_loss,mse_loss}`: Properly check that reduction strings are valid ([#43527](https://github.com/pytorch/pytorch/pull/43527))\r\n* `functional.smoothl1_loss`: Properly raise error for negative `beta` values ([#45759](https://github.com/pytorch/pytorch/pull/45759))\r\n* `functional.pad`: Fix extra memory allocation and invalid result for negative or zero pad when using circular padding ([#39273](https://github.com/pytorch/pytorch/pull/39273))\r\n\r\n### C++ API\r\n\r\n* `nn::MultiheadAttention`: Ensure all parameters are properly registered ([#42037](https://github.com/pytorch/pytorch/pull/42037))\r\n* `Tensor::grad`: Fix the thread safety issues ([#40887](https://github.com/pytorch/pytorch/pull/40887))\r\n* `Tensor::var`: Ensure that `var(0)` does not call the `var(bool keepdim)` overload but `var(int dim)` ([#40451](https://github.com/pytorch/pytorch/pull/40451))\r\n\r\n### Distributed\r\n\r\n* Fix RPC and ProcessGroup GIL deadlock ([#45088](https://github.com/pytorch/pytorch/pull/45088))\r\n* Relax size check in flatten_for_scatter_gather ([#40573](https://github.com/pytorch/pytorch/pull/40573))\r\n* BAND, BOR and BXOR for NCCL all_reduce should throw runtime errors ([#42669](https://github.com/pytorch/pytorch/pull/42669))\r\n* Disallow creation of ProcessGroupNCCL without GPUs ([#45642](https://github.com/pytorch/pytorch/pull/45642))\r\n* Fix read/write of bulk data ([#42504](https://github.com/pytorch/pytorch/pull/42504))\r\n* Fix thread safety issue with distributed optimizers and TorchScript ([#46071](https://github.com/pytorch/pytorch/pull/46071))\r\n\r\n### TorchScript\r\n\r\n* Fix type annotations in select assignments ([#40528](https://github.com/pytorch/pytorch/pull/40528))\r\n* Fix compilation issues with GCC-5.4 ([#41055](https://github.com/pytorch/pytorch/pull/41055), [#41063](https://github.com/pytorch/pytorch/pull/41063), [#43223](https://github.com/pytorch/pytorch/pull/43223))\r\n* Fix JIT not round to even if constant is folded ([#40897](https://github.com/pytorch/pytorch/pull/40897))\r\n* Fix `torch.jit.freeze` import ([#42319](https://github.com/pytorch/pytorch/pull/42319))\r\n* Fix `List[str].index` ([#40348](https://github.com/pytorch/pytorch/pull/40348))\r\n* Fix `torch.jit.is_tracing()` so that it is correctly called rather than returning the method itself ([#42486](https://github.com/pytorch/pytorch/pull/42486))\r\n* Fix Str -> Device implicit conversions ([#43213](https://github.com/pytorch/pytorch/pull/43213))\r\n* Fix `NaN` propagation in fuser's min/max implementation ([#43590](https://github.com/pytorch/pytorch/pull/43590))\r\n*  Cast return values of functions returning Any ([#42259](https://github.com/pytorch/pytorch/pull/42259))\r\n* Fix `NaN` propagation in TensorExpression fuser's min/max implementation ([#43609](https://github.com/pytorch/pytorch/pull/43609))\r\n* Fix segfault in attribute lookup on loaded `ScriptModules` ([#43284](https://github.com/pytorch/pytorch/pull/43284))\r\n* Fix casting of `unsigned char`, and `abs(int)` ([#44157](https://github.com/pytorch/pytorch/pull/44157))\r\n* Fix frac in CUDA fuser ([#44152](https://github.com/pytorch/pytorch/pull/44152))\r\n* Fix model_name not logged properly issue. ([#45488](https://github.com/pytorch/pytorch/pull/45488))\r\n* Fix `len`, `contains`, `getitem` inherited from interface class derived from nn container ([#40789](https://github.com/pytorch/pytorch/pull/40789))\r\n* Fix support for FP16 in CudaCodgen ([#44209](https://github.com/pytorch/pytorch/pull/44209))\r\n* Fix `torch.tensor` for empty multidimensional-typed lists ([#44652](https://github.com/pytorch/pytorch/pull/44652))\r\n* Fix freeze_module pass for sharedtype ([#42457](https://github.com/pytorch/pytorch/pull/42457))\r\n* Correctly clone schema in `insert_observers` ([#40624](https://github.com/pytorch/pytorch/pull/40624))\r\n*  Fix value association with dictionaries in the tracer ([#40885](https://github.com/pytorch/pytorch/pull/40885))\r\n* Fix preserve submodule attribute in freezing ([#45143](https://github.com/pytorch/pytorch/pull/45143))\r\n* Fix Half conversion of immediates in NNC Cuda backend ([#45213](https://github.com/pytorch/pytorch/pull/45213))\r\n* Fix a bug in `SplitWithMask` when splitting multiple times ([#45141](https://github.com/pytorch/pytorch/pull/45141))\r\n* Fix inlining interface call in fork subgraph ([#43790](https://github.com/pytorch/pytorch/pull/43790))\r\n* Fix operator order in combineMultilane in TensorExpr fuser([#45157](https://github.com/pytorch/pytorch/pull/45157))\r\n* Correctly mark Tensor types inferred from empty annotation as `inferred=True` (#[45360](https://github.com/pytorch/pytorch/pull/45360))\r\n* Fix some bugs in Round+Mod simplification in NNC ([#42934](https://github.com/pytorch/pytorch/pull/42934))\r\n* Fix `set_grad_enabled` scripted version ([#46060](https://github.com/pytorch/pytorch/pull/46060))\r\n* Fix for `dict.update()` scripted version ([#46105](https://github.com/pytorch/pytorch/pull/46105))\r\n* Fix segfault when scripting nested classes ([#46422](https://github.com/pytorch/pytorch/pull/46422))\r\n* Fix memory leak in Profiling Mode ([#46621](https://github.com/pytorch/pytorch/pull/46621))\r\n\r\n### Quantization\r\n\r\n* Resolved namespace conflict in qnnpack for init_win symbol (a7e09b8727)\r\n* Fix linking of qnnpack params on windows. ([#40920](https://github.com/pytorch/pytorch/pull/40920))\r\n* Adding zero point type check for per channel quantization ([#40811](https://github.com/pytorch/pytorch/pull/40811))\r\n* Remove activation_post_process in qat modules (#42343) ([#43015](https://github.com/pytorch/pytorch/pull/43015))\r\n* `qlinear_dynamic`: Fix ASAN error in QNNPACK's integration. ([#41967](https://github.com/pytorch/pytorch/pull/41967))\r\n* Change quantizer to account for input tensor's memory format. ([#42178](https://github.com/pytorch/pytorch/pull/42178))\r\n* Fixing the output shape for the linear ([#44513](https://github.com/pytorch/pytorch/pull/44513))\r\n* Ensure observers and fq modules are scriptable ([#44749](https://github.com/pytorch/pytorch/pull/44749))\r\n* histogram observer: ensure buffer shape consistency ([#44956](https://github.com/pytorch/pytorch/pull/44956))\r\n* Attach qconfig to all modules ([#42576](https://github.com/pytorch/pytorch/pull/42576))\r\n* Fix qnnpack quantized activations for NHWC memory format ([#46217](https://github.com/pytorch/pytorch/pull/46217))\r\n\r\n### ONNX\r\n\r\n* Fix crash  when exporting a model with `nn.Sequential` ([#19227](https://github.com/pytorch/pytorch/pull/19227))\r\n* Fix default `ignore_index` for nll loss ([#44816](https://github.com/pytorch/pytorch/pull/44816))\r\n* Rename Black to Block  for various files ([#42913](https://github.com/pytorch/pytorch/pull/42913))\r\n* Fix bug in `onnx::SsaRewrite` ([#42148](https://github.com/pytorch/pytorch/pull/42148))\r\n\r\n### Misc\r\n\r\n* Fix `torch.hub` for new zipfile format. ([#42333](https://github.com/pytorch/pytorch/pull/42333))\r\n* Preserve python backtrace in autograd engine errors. ([#43684](https://github.com/pytorch/pytorch/pull/43684))\r\n* `optim.SparseAdam`: Fix check that params are dense on init ([#43668](https://github.com/pytorch/pytorch/pull/43668))\r\n* Fix clang build ([#44934](https://github.com/pytorch/pytorch/pull/44934))\r\n* `nn::MultiheadAttention:` Fix parameter registration ([#42037](https://github.com/pytorch/pytorch/pull/42037))\r\n* MaxPool2D: Fix memory leak for XNNPACK ([#41874](https://github.com/pytorch/pytorch/pull/41874))\r\n* Numpy scalar detection for bool and complex types fixed ([#43644](https://github.com/pytorch/pytorch/pull/43644))\r\n* Add missing file to `BUILD.bazel` ([#40536](https://github.com/pytorch/pytorch/pull/40536))\r\n* `autograd.gradcheck`: Add support for complex ([#43208](https://github.com/pytorch/pytorch/pull/43208))\r\n* Fix bug in mobile-specific CPU caching allocator ([#43719](https://github.com/pytorch/pytorch/pull/43719))\r\n\r\n# Performance\r\n\r\n### Python API\r\n\r\n* `torch.{view_as_complex,view_as_real}`: Remove unnecessary temporary Tensor ([#44908](https://github.com/pytorch/pytorch/pull/44908))\r\n* `tensorboard.SummaryWriter.add_audio`: Remove unnecessary for loops ([#44201](https://github.com/pytorch/pytorch/pull/44201))\r\n* `Conv2d` and `Conv3d`:  bypass the im2col for 1x1 conv ([#40324](https://github.com/pytorch/pytorch/pull/40324))\r\n* Fix `max_pool2d` perf regression ([#41174](https://github.com/pytorch/pytorch/pull/41174))\r\n* Disable the mkldnn for `conv2d` in some special cases ([#40610](https://github.com/pytorch/pytorch/pull/40610))\r\n* `addmm`: Reduce constant time overhead ([#41374](https://github.com/pytorch/pytorch/pull/41374))\r\n* `cumsum, cumprod`: Enable non-synchronizing cub scan for cum* operations ([#42036](https://github.com/pytorch/pytorch/pull/42036))\r\n* `max_pool2d`: CUDA NCHW performance improvement ([#42182](https://github.com/pytorch/pytorch/pull/42182))\r\n* `arenge`: Vectorize CPU implementation ([#38697](https://github.com/pytorch/pytorch/pull/38697))\r\n* `istft`: optimize by using col2im ([#42826](https://github.com/pytorch/pytorch/pull/42826))\r\n* `LayerNorm`:  improved performance on CPU both forward and backward ([#35750](https://github.com/pytorch/pytorch/pull/35750))\r\n* `silu`: improved performance ([#42976](https://github.com/pytorch/pytorch/pull/42976))\r\n* `addmv`: improved performance for zero sized input cases ([#41824](https://github.com/pytorch/pytorch/pull/41824))\r\n* Mobile: Simple caching allocator for CPU ([#42006](https://github.com/pytorch/pytorch/pull/42006))\r\n* `MaxPool1d`: improved performance for cases without indices ([#43745](https://github.com/pytorch/pytorch/pull/43745))\r\n* `adaptive_avg_pool2d:` optimized code path for cases when output size is (1, 1) ([#44211](https://github.com/pytorch/pytorch/pull/44211))\r\n* Vectorized complex copy ([#44722](https://github.com/pytorch/pytorch/pull/44722))\r\n* `cat`: optimized cuda kernel ([#44833](https://github.com/pytorch/pytorch/pull/44833))\r\n* Vectorized int8_t on CPU ([#44759](https://github.com/pytorch/pytorch/pull/44759))\r\n* Vectorized `bitwise_not` ([#45103](https://github.com/pytorch/pytorch/pull/45103))\r\n* Added stateful XNNPack deconvolution2d operator to torch ([#43233](https://github.com/pytorch/pytorch/pull/43233))\r\n* Enabled mkldnn dilation convolution ([#40483](https://github.com/pytorch/pytorch/pull/40483))\r\n\r\n### Distributed\r\n\r\n* Skip allreducing `local_used_maps_dev_` when `find_unused_param=False` in DDP to improve performance ([#40407](https://github.com/pytorch/pytorch/pull/40407))\r\n* Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce ([#43543](https://github.com/pytorch/pytorch/pull/43543))\r\n* Add option to run NCCL operations on high priority cuda stream ([#43796](https://github.com/pytorch/pytorch/pull/43796))\r\n* Enhance DistributedOptimizer to be functional and torchscriptable to avoid GIL and global lock ([#45221](https://github.com/pytorch/pytorch/pull/45221))\r\n\r\n### TorchScript\r\n\r\n* JIT pass for add relu fusion. ([#39343](https://github.com/pytorch/pytorch/pull/39343))\r\n* Optimize autodiff subgraph slicing ([#41437](https://github.com/pytorch/pytorch/pull/41437))\r\n* Don't re-run CSE on every block ([#41479](https://github.com/pytorch/pytorch/pull/41479))\r\n* Add loop unroll optimization in NNC ([#42465](https://github.com/pytorch/pytorch/pull/42465))\r\n* Speed up CUDA kernel launch when block/thread extents are statically known ([#42899](https://github.com/pytorch/pytorch/pull/42899))\r\n* Support merging adjacent fusion groups in TensorExpression Fuser. ([#43671](https://github.com/pytorch/pytorch/pull/43671))\r\n* Add passes to profiling executor pipeline ([#43636](https://github.com/pytorch/pytorch/pull/43636))\r\n* Improve performance of `KernelSumMultipleAxes` ([#43905](https://github.com/pytorch/pytorch/pull/43905))\r\n* Latency improvements for pointwise + reduction fusion ([#45218](https://github.com/pytorch/pytorch/pull/45218))\r\n* Add simplification of Loop + Condition patterns in NNC ([#44764](https://github.com/pytorch/pytorch/pull/44764))\r\n* Fix fallback graph in specialize autogradzero ([#44654](https://github.com/pytorch/pytorch/pull/44654))\r\n* Fix masking for all block and thread dimensions in CudaCodeGen ([#44733](https://github.com/pytorch/pytorch/pull/44733))\r\n* Improve performance of simple reduction and softmax in nvFuser ([#40864](https://github.com/pytorch/pytorch/pull/40864))\r\n* Add a new optimization pass, the Registerizer, which looks for common Stores and Loads to a single item in a buffer and replaces them with a local temporary scalar which is cheaper to write. ([#42606](https://github.com/pytorch/pytorch/pull/42606))\r\n* Fuse identical conditions in NNC simplifier ([#44886](https://github.com/pytorch/pytorch/pull/44886))\r\n* Add _out variants and reuse memory in static runtime([#44128](https://github.com/pytorch/pytorch/pull/44128))\r\n\r\n### Mobile\r\n\r\n* Add add_relu fusion pass to optimize_for_mobile. ([#40252](https://github.com/pytorch/pytorch/pull/40252))\r\n* optimize_for_mobile: bring packed params to root module ([#42740](https://github.com/pytorch/pytorch/pull/42740))\r\n* Apply selective build on RNN operators ([#44132](https://github.com/pytorch/pytorch/pull/44132))\r\n* Add neon backend for vectorization ([#39341](https://github.com/pytorch/pytorch/pull/39341))\r\n\r\n### Quantization\r\n\r\n* Use the _min_max function instead of two separate calls for min and max([#41570](https://github.com/pytorch/pytorch/pull/41570), [#42957](https://github.com/pytorch/pytorch/pull/42957), [#44537](https://github.com/pytorch/pytorch/pull/44537))\r\n* Improve performance of the QNNPACK kernels ([#41342](https://github.com/pytorch/pytorch/pull/41342), [#42007](https://github.com/pytorch/pytorch/pull/42007), [#42008](https://github.com/pytorch/pytorch/pull/42008))\r\n* Speed up HistogramObserver by vectorizing critical path ([#41041](https://github.com/pytorch/pytorch/pull/41041))\r\n* Speed up AdaptivePool3d by checking if input is ChannelsLast or ChannelsLast3d ([#42780](https://github.com/pytorch/pytorch/pull/42780))\r\n* observers: use clamp instead of min/max in calculate_qparams ([#43150](https://github.com/pytorch/pytorch/pull/43150))\r\n* observers: use torch.all to check for valid min and max values ([#43151](https://github.com/pytorch/pytorch/pull/43151))\r\n* Avoid resizing in MinMaxObserver ([#43789](https://github.com/pytorch/pytorch/pull/43789))\r\n* observers: make eps a buffer ([#43149](https://github.com/pytorch/pytorch/pull/43149))\r\n\r\n### Misc\r\n\r\n* ROCm: Fix performance issues with `torch.cat` ([#46323](https://github.com/pytorch/pytorch/pull/46323))\r\n\r\n# Documentation\r\n\r\n### Python API\r\n\r\n* Numerous typo and grammatical improvements ([#39854](https://github.com/pytorch/pytorch/pull/39854), [#40217](https://github.com/pytorch/pytorch/pull/40217), [#40285](https://github.com/pytorch/pytorch/pull/40285), [#40544](https://github.com/pytorch/pytorch/pull/40544), [#40692](https://github.com/pytorch/pytorch/pull/40692), [#40617](https://github.com/pytorch/pytorch/pull/40617), [#41025](https://github.com/pytorch/pytorch/pull/41025), [#41031](https://github.com/pytorch/pytorch/pull/41031), [#40984](https://github.com/pytorch/pytorch/pull/40984), [#41066](https://github.com/pytorch/pytorch/pull/41066), [#41203](https://github.com/pytorch/pytorch/pull/41203), [#41263](https://github.com/pytorch/pytorch/pull/41263), [#41384](https://github.com/pytorch/pytorch/pull/41384), [#41526](https://github.com/pytorch/pytorch/pull/41526), [#41563](https://github.com/pytorch/pytorch/pull/41563), [#41632](https://github.com/pytorch/pytorch/pull/41632), [#41643](https://github.com/pytorch/pytorch/pull/41643), [#41599](https://github.com/pytorch/pytorch/pull/41599), [#41799](https://github.com/pytorch/pytorch/pull/41799), [#41679](https://github.com/pytorch/pytorch/pull/41679), [#41835](https://github.com/pytorch/pytorch/pull/41835), [#41851](https://github.com/pytorch/pytorch/pull/41851), [#41963](https://github.com/pytorch/pytorch/pull/41963), [#42016](https://github.com/pytorch/pytorch/pull/42016), [#42076](https://github.com/pytorch/pytorch/pull/42076), [#41946](https://github.com/pytorch/pytorch/pull/41946), [#42046](https://github.com/pytorch/pytorch/pull/42046), [#42065](https://github.com/pytorch/pytorch/pull/42065), [#42236](https://github.com/pytorch/pytorch/pull/42236), [#42184](https://github.com/pytorch/pytorch/pull/42184), [#42734](https://github.com/pytorch/pytorch/pull/42734), [#42923](https://github.com/pytorch/pytorch/pull/42923), [#42891](https://github.com/pytorch/pytorch/pull/42891), [#43063](https://github.com/pytorch/pytorch/pull/43063), [#43131](https://github.com/pytorch/pytorch/pull/43131), [#43395](https://github.com/pytorch/pytorch/pull/43395), [#43588](https://github.com/pytorch/pytorch/pull/43588), [#43583](https://github.com/pytorch/pytorch/pull/43583), [#43697](https://github.com/pytorch/pytorch/pull/43697), [#43779](https://github.com/pytorch/pytorch/pull/43779), [#43569](https://github.com/pytorch/pytorch/pull/43569), [#43893](https://github.com/pytorch/pytorch/pull/43893), [#43695](https://github.com/pytorch/pytorch/pull/43695), [#43973](https://github.com/pytorch/pytorch/pull/43973), [#44667](https://github.com/pytorch/pytorch/pull/44667), [#44753](https://github.com/pytorch/pytorch/pull/44753), [#44740](https://github.com/pytorch/pytorch/pull/44740), [#45045](https://github.com/pytorch/pytorch/pull/45045), [#45192](https://github.com/pytorch/pytorch/pull/45192), [#43308](https://github.com/pytorch/pytorch/pull/43308), [#40334](https://github.com/pytorch/pytorch/pull/40334))\r\n* Remove use of term \u201cblacklist\u201d ([#41450](https://github.com/pytorch/pytorch/pull/41450))\r\n* Add overflow notice for cuFFT on half precision ([#40551](https://github.com/pytorch/pytorch/pull/40551))\r\n* Add complex Note ([#41012](https://github.com/pytorch/pytorch/pull/41012), [#41252](https://github.com/pytorch/pytorch/pull/41252), [#40450](https://github.com/pytorch/pytorch/pull/40450))\r\n* Add documentation about data sharing for Tensors during serialization ([#40412](https://github.com/pytorch/pytorch/pull/40412))\r\n* Add `nn.Module.training` to docs ([#40923](https://github.com/pytorch/pytorch/pull/40923))\r\n*  `nn.CrossEntropyLoss`: Clarify that the mean argument is weighted ([#40991](https://github.com/pytorch/pytorch/pull/40991))\r\n* `torch.scatter_`: Update doc with support for reduction methods. ([#40962](https://github.com/pytorch/pytorch/pull/40962))\r\n* Fix HTTP links in documentation to HTTPS ([#40878](https://github.com/pytorch/pytorch/pull/40878))\r\n* Fix warnings when building docs ([#41068](https://github.com/pytorch/pytorch/pull/41068), [#41334](https://github.com/pytorch/pytorch/pull/41334), [#41335](https://github.com/pytorch/pytorch/pull/41335), [#44686](https://github.com/pytorch/pytorch/pull/44686))\r\n* Add PyTorch Glossary ([#40639](https://github.com/pytorch/pytorch/pull/40639))\r\n* Fix documentation references following page split ([#39086](https://github.com/pytorch/pytorch/pull/39086))\r\n* Update serialization note to explain versioned symbols and dynamic versioning ([#41395](https://github.com/pytorch/pytorch/pull/41395))\r\n* Make elementwise comparison docs more consistent ([#41626](https://github.com/pytorch/pytorch/pull/41626))\r\n* Update CONTRIBUTING.md to explain how to use ccache ([#41619](https://github.com/pytorch/pytorch/pull/41619))\r\n* Add doc warning for LSTM non-deterministic behavior ([#40893](https://github.com/pytorch/pytorch/pull/40893))\r\n* Document default dim for cross being None ([#41850](https://github.com/pytorch/pytorch/pull/41850))\r\n* Clarify Python 3.6 is the minimum supported version in the installation section. ([#41937](https://github.com/pytorch/pytorch/pull/41937))\r\n* Split quantization subsection into smaller pages ([#41321](https://github.com/pytorch/pytorch/pull/41321))\r\n* Documentation for `torch.optim.swa_utils` ([#41228](https://github.com/pytorch/pytorch/pull/41228))\r\n* Improve the documentation of DistributedDataParallel ([#42471](https://github.com/pytorch/pytorch/pull/42471))\r\n* Update docs about CUDA stream priority ([#41364](https://github.com/pytorch/pytorch/pull/41364))\r\n* Update the documentation for `torch.scatter` to include streams parameter. ([#42814](https://github.com/pytorch/pytorch/pull/42814))\r\n* Update `Tensor.clone` doc ([#42931](https://github.com/pytorch/pytorch/pull/42931), [#43098](https://github.com/pytorch/pytorch/pull/43098))\r\n* Update external links in the README.md ([#43100](https://github.com/pytorch/pytorch/pull/43100))\r\n* Update `torch.Tensor.is_set_to` documentation ([#43052](https://github.com/pytorch/pytorch/pull/43052))\r\n* Polish the nightly pull docs in CONTRIBUTING ([#43494](https://github.com/pytorch/pytorch/pull/43494))\r\n* Update the `torch.qr` documentation to include a warning about when the QR.backward is well-defined. ([#43547](https://github.com/pytorch/pytorch/pull/43547))\r\n* Update the instructions to build from source on windows ([#43479](https://github.com/pytorch/pytorch/pull/43479), [#45553](https://github.com/pytorch/pytorch/pull/45553))\r\n* Document the beta=0 behavior of BLAS functions ([#43823](https://github.com/pytorch/pytorch/pull/43823))\r\n* Fix docs for kwargs-only functions ([#43586](https://github.com/pytorch/pytorch/pull/43586), [#43589](https://github.com/pytorch/pytorch/pull/43589))\r\n* Documents `torch.sub` properly, adds `torch.subtract` alias ([#43850](https://github.com/pytorch/pytorch/pull/43850))\r\n* Update determinism documentation ([#41692](https://github.com/pytorch/pytorch/pull/41692))\r\n* Update instructions to build ([#42850](https://github.com/pytorch/pytorch/pull/42850))\r\n* Clarify `nn.Batchnorm` `track_running_stats` docs ([#44445](https://github.com/pytorch/pytorch/pull/44445))\r\n* Fix latex error in `torch.heaviside` docs ([#44481](https://github.com/pytorch/pytorch/pull/44481))\r\n* Update `torch.median` doc to explain returned value for even-sized input ([#44562](https://github.com/pytorch/pytorch/pull/44562))\r\n* Fix the `nn.ELU` formula in the docs ([#43764](https://github.com/pytorch/pytorch/pull/43764))\r\n* `torch.min`, `torch.max`: remove incorrect warning from docs ([#44615](https://github.com/pytorch/pytorch/pull/44615))\r\n* Reference `torch.cuda.amp` tutorial from core amp docs ([#44725](https://github.com/pytorch/pytorch/pull/44725))\r\n* Mention TF32 on related docs ([#44690](https://github.com/pytorch/pytorch/pull/44690))\r\n* Clarify that 5-D 'bilinear' grid_sample is actually trilinear ([#45090](https://github.com/pytorch/pytorch/pull/45090))\r\n* Update linalg warning + docs ([#45415](https://github.com/pytorch/pytorch/pull/45415))\r\n* Update `torch.floor_divide` documentation to clarify it's actually `torch.trunc_divide` ([#45411](https://github.com/pytorch/pytorch/pull/45411))\r\n* Update `torch.fft` doc and make warning clearer ([#45409](https://github.com/pytorch/pytorch/pull/45409))\r\n* Update for complex autograd ([#45270](https://github.com/pytorch/pytorch/pull/45270), [#46281](https://github.com/pytorch/pytorch/pull/46281))\r\n* Update `nn.Flatten` docs ([#42084](https://github.com/pytorch/pytorch/pull/42084))\r\n\r\n### Distributed\r\n\r\n* Add a CONTRIBUTING.md for the distributed package. ([#44224](https://github.com/pytorch/pytorch/pull/44224))\r\n* Added docs for Store API ([#45543](https://github.com/pytorch/pytorch/pull/45543))\r\n* Add `all_gather_object` and `gather_object` documentation ([#43772](https://github.com/pytorch/pytorch/pull/43772))\r\n\r\n### TorchScript\r\n\r\n* Fix `torch.jit.trace_module` documentation ([#40248](https://github.com/pytorch/pytorch/pull/40248))\r\n* Fix the docs for the inputs arg of `torch.jit.trace_module` ([#41586](https://github.com/pytorch/pytorch/pull/41586))\r\n* Add documentation for `PYTORCH_JIT_TYPE_VERBOSITY` ([#42241](https://github.com/pytorch/pytorch/pull/42241))\r\n* Grammatical corrections in JIT overview ([#43473](https://github.com/pytorch/pytorch/pull/43473))\r\n* Update docs for recently added JIT features, including Enum Support, `torch.no_grad` etc. ([#45232](https://github.com/pytorch/pytorch/pull/45232))\r\n* Add function signature for `pixel_shuffle` (#[45661](https://github.com/pytorch/pytorch/pull/45661))\r\n* Fix signature for `torch.poisson` in documentation (#[45656](https://github.com/pytorch/pytorch/pull/45656))\r\n\r\n### Mobile\r\n\r\n* Aar native linking add fbjni ([#40578](https://github.com/pytorch/pytorch/pull/40578))\r\n* fix scripts ([#44464](https://github.com/pytorch/pytorch/pull/44464))\r\n* [PyTorch Mobile] Move some string ops to register_prim_ops.cpp and make them selective ([#44500](https://github.com/pytorch/pytorch/pull/44500))\r\n\r\n### Quantization\r\n\r\n* Fix several quantization documentation typos ([#40567](https://github.com/pytorch/pytorch/pull/40567), [#43693](https://github.com/pytorch/pytorch/pull/43693))\r\n* API summary section ([#45848](https://github.com/pytorch/pytorch/pull/45848))\r\n* Documentation for dynamically quantized RNN cells ([#40896](https://github.com/pytorch/pytorch/pull/40896))\r\n\r\n### Misc\r\n\r\n* Update  ONNX docs for release ([#45086](https://github.com/pytorch/pytorch/pull/45086))", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.7.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.7.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.7.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/33111309", "release_id": 33111309, "date_created": "2020-10-23T19:31:23Z", "date_published": "2020-10-27T16:35:58Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/29030094", "tag": "v1.6.0", "name": "Stable release of automatic mixed precision (AMP). New Beta features include a TensorPipe backend for RPC, memory profiler, and several improvements to distributed training for both RPC and DDP.", "author": {"name": "zou3519", "type": "User"}, "description": "# PyTorch 1.6.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Changes\r\n* Deprecations\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Performance\r\n* Documentation\r\n\r\n\r\n# Highlights\r\n\r\nThe PyTorch 1.6 release includes a number of new APIs, tools for performance improvement and profiling, as well as major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. \r\n \r\nA few of the highlights include: \r\n\r\n1. Automatic mixed precision (AMP) training is now natively supported and a stable feature - thanks to NVIDIA\u2019s contributions; \r\n2. Native TensorPipe support now added for tensor-aware, point-to-point communication primitives built specifically for machine learning; \r\n3. New profiling tools providing tensor-level memory consumption information; and \r\n4. Numerous improvements and new features for both distributed data parallel (DDP) training and the remote procedural call (RPC) packages.\r\n\r\n Additionally, from this release onward, features will be classified as Stable, Beta and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can learn more about what this change means in the post [here](https://pytorch.org/blog/pytorch-feature-classification-changes/).\r\n\r\n## [Stable] Automatic Mixed Precision (AMP) Training \r\n\r\nAMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs. Using the natively supported `torch.cuda.amp` API, AMP provides convenience methods for mixed precision, where some operations use the `torch.float32` (`float`) datatype and other operations use `torch.float16` (`half`). Some ops, like linear layers and convolutions, are much faster in `float16`. Other ops, like reductions, often require the dynamic range of `float32`. Mixed precision tries to match each op to its appropriate datatype.\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/25081)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/amp.html)\r\n* Usage examples | [Link](https://pytorch.org/docs/stable/notes/amp_examples.html)\r\n\r\n## [Beta] TensorPipe backend for RPC\r\n\r\nPyTorch 1.6 introduces a new backend for the RPC module which leverages the TensorPipe library, a tensor-aware point-to-point communication primitive targeted at machine learning, intended to complement the current primitives for distributed training in PyTorch (Gloo, MPI, ...) which are collective and blocking. The pairwise and asynchronous nature of TensorPipe lends itself to new networking paradigms that go beyond data parallel: client-server approaches (e.g., parameter server for embeddings, actor-learner separation in Impala-style RL, ...) and model and pipeline parallel training (think GPipe), gossip SGD, etc.\r\n\r\n```python\r\n# One-line change needed to opt in\r\ntorch.distributed.rpc.init_rpc(\r\n    ...\r\n    backend=torch.distributed.rpc.BackendType.TENSORPIPE,\r\n)\r\n\r\n# No changes to the rest of the RPC API\r\ntorch.distributed.rpc.rpc_sync(...)\r\n```\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/35251)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc/index.html)\r\n\r\n## [Beta] Memory Profiler \r\n\r\nThe `torch.autograd.profiler` API now includes a memory profiler that lets you inspect the tensor memory cost of different operators inside your CPU and GPU models.\r\n\r\nHere is an example usage of the API:\r\n\r\n```python\r\nimport torch\r\nimport torchvision.models as models\r\nimport torch.autograd.profiler as profiler\r\n\r\nmodel = models.resnet18()\r\ninputs = torch.randn(5, 3, 224, 224)\r\nwith profiler.profile(profile_memory=True, record_shapes=True) as prof:\r\n    model(inputs)\r\n\r\n# NOTE: some columns were removed for brevity\r\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\r\n# ---------------------------  ---------------  ---------------  ---------------\r\n# Name                         CPU Mem          Self CPU Mem     Number of Calls\r\n# ---------------------------  ---------------  ---------------  ---------------\r\n# empty                        94.79 Mb         94.79 Mb         123\r\n# resize_                      11.48 Mb         11.48 Mb         2\r\n# addmm                        19.53 Kb         19.53 Kb         1\r\n# empty_strided                4 b              4 b              1\r\n# conv2d                       47.37 Mb         0 b              20\r\n# ---------------------------  ---------------  ---------------  ---------------\r\n```\r\n\r\n* PR | [Link](https://github.com/pytorch/pytorch/pull/37775) \r\n* Documentation | [Link](https://pytorch.org/docs/stable/autograd.html#profiler)\r\n\r\n## Distributed and RPC Features and Improvements\r\n\r\n### [Beta] DDP+RPC \r\n\r\nPyTorch Distributed supports two powerful paradigms: DDP for full sync data parallel training of models and the RPC framework which allows for distributed model parallelism. Currently, these two features work independently and users can\u2019t mix and match these to try out hybrid parallelism paradigms.\r\n\r\nStarting PyTorch 1.6, we\u2019ve enabled DDP and RPC to work together seamlessly so that users can combine these two techniques to achieve both data parallelism and model parallelism. An example is where users would like to place large embedding tables on parameter servers and use the RPC framework for embedding lookups, but store smaller dense parameters on trainers and use DDP to synchronize the dense parameters. Below is a simple code snippet. \r\n\r\n```python\r\n// On each trainer\r\n\r\nremote_emb = create_emb(on=\"ps\", ...)\r\nddp_model = DDP(dense_model)\r\n\r\nfor data in batch:\r\n   with torch.distributed.autograd.context():\r\n      res = remote_emb(data)\r\n      loss = ddp_model(res)\r\n      torch.distributed.autograd.backward([loss])\r\n```\r\n\r\n* DDP+RPC Tutorial | [Link](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc/index.html)\r\n* Usage Examples | [Link](https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc)\r\n\r\n### **[Beta] RPC - Asynchronous User Functions**\r\n\r\nRPC Asynchronous User Functions supports the ability to yield and resume on the server side when executing a user-defined function. Prior to this feature, when an callee processes a request, one RPC thread waits until the user function returns. If the user function contains IO (e.g., nested RPC) or signaling (e.g., waiting for another request to unblock), the corresponding RPC thread would sit idle waiting for these events. As a result, some applications have to use a very large number of threads and send additional RPC requests, which can potentially lead to performance degradation. To make a user function yield on such events, applications need to: 1) Decorate the function with the `@rpc.functions.async_execution` decorator; and 2) Let the function return a `torch.futures.Future` and install the resume logic as callbacks on the `Future` object. See below for an example:\r\n\r\n```python\r\n@rpc.functions.async_execution\r\ndef async_add_chained(to, x, y, z):\r\n    return rpc.rpc_async(to, torch.add, args=(x, y)).then(\r\n        lambda fut: fut.wait() + z\r\n    )\r\n\r\nret = rpc.rpc_sync(\r\n    \"worker1\", \r\n    async_add_chained, \r\n    args=(\"worker2\", torch.ones(2), 1, 1)\r\n)\r\n        \r\nprint(ret)  # prints tensor([3., 3.])\r\n```\r\n\r\n* Tutorial for performant batch RPC using Asynchronous User Functions| [Link](https://pytorch.org/tutorials/intermediate/rpc_async_execution.html)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution)\r\n* Usage examples | [Link](https://github.com/pytorch/examples/tree/master/distributed/rpc/batch)\r\n\r\n## [Beta] Fork/Join Parallelism \r\n\r\nThis release adds support for a language-level construct as well as runtime support for coarse-grained parallelism in TorchScript code. This support is useful for situations such as running models in an ensemble in parallel, or running bidirectional components of recurrent nets in parallel, and allows the ability to unlock the computational power of parallel architectures (e.g. many-core CPUs) for task level parallelism.\r\n\r\n Parallel execution of TorchScript programs is enabled through two primitives: `torch.jit.fork` and `torch.jit.wait`. In the below example, we parallelize execution of `foo:`\r\n\r\n```python\r\nimport torch\r\nfrom typing import List\r\n\r\ndef foo(x):\r\n    return torch.neg(x)\r\n\r\n@torch.jit.script\r\ndef example(x):\r\n    futures = [torch.jit.fork(foo, x) for _ in range(100)]\r\n    results = [torch.jit.wait(future) for future in futures]\r\n    return torch.sum(torch.stack(results))\r\n\r\nprint(example(torch.ones([])))\r\n```\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/jit.html)\r\n\r\n# Backwards Incompatible Changes\r\n\r\n### Dropped support for Python <= 3.5 ([#39879](https://github.com/pytorch/pytorch/pull/39879))\r\n\r\nThe minimum version of Python we support now is 3.6. Please upgrade your Python to match. If you use conda, instructions for setting up a new environment with Python >= 3.6 can be [found here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html#installing-a-different-version-of-python).\r\n\r\n\r\n### Throw a RuntimeError for deprecated `torch.div` and `torch.addcdiv` integer floor division behavior ([#38762](https://github.com/pytorch/pytorch/pull/38762), [#38620](https://github.com/pytorch/pytorch/pull/38620))\r\n\r\nIn 1.5.1 and older PyTorch releases `torch.div` , `torch.addcdiv`, and the `/` operator perform integer floor division. In 1.6 attempting to perform integer division throw a RuntimeError, and in 1.7 the behavior will change so that these operations always perform true division (consistent with Python and NumPy division).\r\n\r\nTo floor divide integer tensors, please use `torch.floor_divide` instead.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.5.1</th><th>1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(3) / torch.tensor(2)\r\n../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer\r\ndivision of tensors using div or / is deprecated, and in a future\r\nrelease div will perform true division as in Python 3. Use true_divide\r\nor floor_divide (// in Python) instead.\r\ntensor(1)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> # NB: the following is equivalent to \r\n>>> # torch.floor_divide(torch.tensor(3), torch.tensor(2))\r\n>>> torch.tensor(3) // torch.tensor(2)\r\ntensor(1)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThe fix for `torch.addcdiv` is similar.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.5.1</th><th>1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> torch.addcdiv(input, tensor, other, value=value)\r\n../aten/src/ATen/native/PointwiseOps.cpp:81: UserWarning:\r\nInteger division with addcdiv is deprecated, and in a future \r\nrelease addcdiv will perform a true division of tensor1 and\r\ntensor2. The current addcdiv behavior can be replicated using\r\nfloor_divide for integral inputs (self + value * tensor1 // tensor2)\r\nand division for float inputs (self + value * tensor1 / tensor2).\r\nThe new addcdiv behavior can be implemented with\r\ntrue_divide (self + value * torch.true_divide(tensor1, tensor2).\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> (input + torch.floor_divide(value * tensor, other))\r\ntensor(0)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Prevent cross-device data movement for zero-dimension CUDA tensors in binary pointwise PyTorch operators ([#38998](https://github.com/pytorch/pytorch/pull/38998))\r\n\r\nIn previous versions of PyTorch, zero dimensional CUDA tensors could be moved across devices implicitly while performing binary pointwise operations (e.g. addition, subtraction, multiplication, division, and others). For example,\r\n\r\n```python\r\ntorch.tensor(5, device='cuda:0') + torch.tensor((1, 1), device='cuda:1')\r\n```\r\n\r\nwould work, even though the tensors are on different CUDA devices. This is a frequent source of user confusion, however, and PyTorch generally does not move data across devices without it being explicit. This functionality is removed in PyTorch 1.6.\r\n\r\nTo perform binary pointwise operations on data of different devices, please cast the tensors to the correct device by using `Tensor.to`: \r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5, device='cuda:0') + torch.tensor((1, 1), device='cuda:1')\r\ntorch.tensor([6, 6], device='cuda:1')\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5, device='cuda:0').to('cuda:1') + torch.tensor((1, 1), device='cuda:1')\r\ntorch.tensor([6, 6], device='cuda:1')\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### Dropped support for CUDA 9.2 on Windows\r\n\r\nIn previous versions of PyTorch, we provided an installation option for Windows environments running CUDA 9.2. Starting from PyTorch 1.6.0, we are no longer providing those binaries. Please upgrade your CUDA version to 10.1 or 10.2 and install a PyTorch binary for one of those CUDA versions instead.\r\n\r\n\r\n### PyTorch release binaries dropped dedicated bytecode for CUDA compute capability 6.1; removed PTX for CUDA compute capability 3.7\r\n\r\nTo check whether you are affected, please find your GPU in a table in[this link](https://developer.nvidia.com/cuda-gpus).\r\n\r\nIf you are using a Nvidia GPU with compute capability 6.1, you may notice a performance hit when using the release binaries (installed via pip or conda). We stopped building for CUDA compute capability 6.1 but PyTorch programs should still continue to work with those devices. If you do notice a performance hit, a workaround is to [compile PyTorch from source](https://github.com/pytorch/pytorch#from-source).\r\n\r\nIf you are using a Nvidia GPU with compute capability 3.7 and relied on PTX, we have dropped support for that in our release binaries (installed via pip or conda). Potential workarounds are: install a previous version of PyTorch or to [compile PyTorch from source](https://github.com/pytorch/pytorch#from-source).\r\n\r\n\r\n### Changed how bool tensors are constructed from non-bool values to match Python, C++, and NumPy ([#38392](https://github.com/pytorch/pytorch/pull/38392))\r\n\r\nIn previous versions of PyTorch, when a bool tensor is constructed from a floating-point tensor, we would first convert the tensor to a long tensor, then to float tensor. This is not consistent with how bools are interpreted in Python, C++, and NumPy (just to name a few), which interpret 0 floating-point values as False and everything else as True.\r\n\r\nIf you were relying on the previous behavior, the following code will achieve the same effect.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor([-2, -1, -0.9, 0, 0.9, 1, 2], dtype=torch.bool)\r\ntensor([ True,  True, False, False, False,  True,  True])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor([-2, -1, -0.9, 0, 0.9, 1, 2]).long().bool()\r\ntensor([ True,  True, False, False, False,  True,  True])\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Throw RuntimeError when torch.full would infer a float dtype from a bool or integral fill value (#40364)\r\n\r\nIn PyTorch 1.6 bool and integral fill values given to torch.full must set the dtype our out keyword arguments. In prior versions of PyTorch these fill values would return float tensors by default, but in PyTorch 1.7 they will return a bool or long tensor, respectively. The documentation for torch.full has been updated to reflect this.\r\n\r\n\r\n### Enabled thread parallelism for autograd on CPU ([#33157](https://github.com/pytorch/pytorch/pull/33157))\r\n\r\nIn previous versions of PyTorch, running `.backward()` in multiple threads causes them to be serialized in a specific order, resulting in no parallelism on CPU. In PyTorch 1.6.0, running `.backward()` in multiple threads no longer serializes the execution and instead autograd will run those in parallel.\r\n\r\nThis is BC-breaking for the following two use cases:\r\n\r\n* If any weights are shared among threads, gradient accumulation that was previously deterministic may become non-deterministic in 1.6 as two different threads will write to the .grad attribute in a non-deterministic order.\r\n* If you use any C++ hooks, those are not guaranteed to be thread-safe. Please change them to be thread-safe.\r\n\r\n\r\nIn more detail, in 1.6.0, when you run `backward()` or `grad()` via python, TorchScript or the C++ API in multiple threads on CPU, you should expect to see extra concurrency. For example, you can manually write multithreaded Hogwild training code like:\r\n\r\n```python\r\n# Define a train function to be used in different threads\r\ndef train_fn(model, input):\r\n    # forward\r\n    y = model(input)\r\n    # backward\r\n    y.sum().backward()\r\n    # potential optimizer update\r\n\r\n# define your model in python or in TorchScript\r\nmodel = Model()\r\n# User write their own threading code to drive the train_fn\r\nthreads = []\r\nfor _ in range(10):\r\n    # define or load the data\r\n    input = torch.ones(5, 5, requires_grad=True)\r\n    p = threading.Thread(target=train_fn, args=(model, input))\r\n    p.start()\r\n    threads.append(p)\r\n\r\nfor p in threads:\r\n    p.join()\r\n```\r\n\r\n\r\nNote when you use the same model and call `backward()` concurrently in multiple threads, model parameters are automatically shared across threads. The gradient accumulation might become non-deterministic as two backward calls might access and try to accumulate the same .grad attribute. Although we do proper locking to avoid data corruption, we don't guarantee the order in which the ops are executed, so non-determinism might arise, but this is an expected pattern in multithread training. You could use the functional API `torch.autograd.grad()` to calculate the gradients instead of backward() to avoid the non-determinism.\r\n\r\nFor thread safety:\r\n\r\n* The custom Python/C++ Autograd Functions (both forward and backward) are properly protected and are guaranteed to be thread safe in 1.6.0.\r\n* For hooks, both Python/C++ hooks will run concurrently. Note that in C++, just like in regular C++ threading, you will need to do proper locking when writing shared objects, so previous custom C++ hooks might not work nicely under a  multithreaded environment in 1.6.0. In Python, just like in regular python threading, you can read/write objects safely but the order (and thus determinism) is not guaranteed.\r\n\r\n### Change autograd gradient accumulation logic to yield `.grad`s that match the weights' memory layout ([#40358](https://github.com/pytorch/pytorch/pull/40358))\r\n\r\nIn previous versions of PyTorch, autograd would yield contiguous gradients. Now, gradients have the same memory layout as their respective weights. This should result in silent performance improvements. Since PyTorch operators generally support non-contiguous tensors, this should have no functional effect on most PyTorch programs.  A known exception is when accessing `param.grad` and performing an operation that requires a contiguous tensor, such as `param.grad.view(-1)`. In this case, you will receive an error as follows:\r\n`RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead`.\r\n\r\nIf a user wants to force accumulation into a grad with a particular layout, they can preset `param.grad` to a zeroed tensor with the desired strides or manually set grad to have the desired strides ( `param.grad = param.grad.contiguous(desired format)`.)\r\n\r\nSee the below section on \u201cNote: BC-breaking memory format changes\u201d for more details.\r\n\r\n\r\n### Change memory format promotion rules of pointwise operators ([#37968](https://github.com/pytorch/pytorch/pull/37968))\r\n\r\nIn previous versions of PyTorch, performing a binary pointwise operation between a Contiguous and a Channels Last tensor produced a Channels Last. In PyTorch 1.6, this now returns a tensor with the layout of the first operand.\r\n\r\nSee the below section on\u201cNote: BC-breaking memory format changes\u201d for more details.\r\n\r\n\r\n### Note: BC-breaking memory format changes\r\n\r\nOperations that now return tensors in a different memory format generally should have no functional effect on most PyTorch programs because PyTorch operators generally support non-contiguous tensors.\r\n\r\nThe most common incompatibility with Python programs is with the `view` operator, which has specific stride requirements. If these requirements are no longer met as a result of this change, you will get an error message indicating that you should use `reshape` instead, i.e. \"RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\"\r\n\r\nAnother possible exception incompatibility is if you have a (usually) C++ operator implementation that works directly on memory (i.e. calls data_ptr and relies on the strides being contiguous).\r\n\r\n\r\n### `nn.functional.interpolate`: `recompute_scale_factor` default behavior changed from `True` to `False` (#39453)\r\n\r\nIn PyTorch 1.5.1 and older versions, `nn.functional.interpolate(input, size, scale_factor, ..., recompute_scale_factor)` has a default of `recompute_scale_factor = True`. In PyTorch 1.6, we\u2019ve changed the default to `recompute_scale_factor = False`. \r\n\r\nDepending on the precision of the `scale_factor`, this may result in an output tensor with different values than before. To retain the old behavior, simply change your code to use `recompute_scale_factor = True`.\r\n\r\nMore concretely, what `recompute_scale_factor = True` means is, if the user passes in a `scale_factor:`\r\n\r\n1. We will first compute the new output size; and\r\n2. Then, we will compute a new `scale_factor` by dividing the output size by the input size and sending it to an internal helper function.\r\n3. The new `scale_factor` is used in the interpolate computation but in some cases is different from the `scale_factor` the user passed in.\r\n\r\nThis behavior resulted in loss of precision so we deprecated it in PyTorch 1.5.0. In PyTorch 1.6 and onward, `recompute_scale_factor` has a default of `False`, which means that we pass it directly to an internal helper function.\r\n\r\n\r\n### `out=` arguments of pointwise and reduction functions no longer participate in type promotion (#39655)\r\n\r\nIn PyTorch 1.5 passing the out= kwarg to some functions, like torch.add, could affect the computation. That is,\r\n\r\n```python\r\nout = torch.add(a, b)\r\n```\r\n\r\ncould produce a different result than\r\n\r\n```python\r\ntorch.add(a, b, out=out)\r\n```\r\n\r\nThis is because previously the out argument participated in the type promotion rules. For greater consistency with NumPy, Python, and C++, in PyTorch 1.6 the out argument no longer participates in type promotion, and has no effect on the computation performed.\r\n\r\n\r\n### Changed `torch.quasirandom.SobolEngine(..., scramble=True, seed=None)` to respect `torch.manual_seed` when a seed has not been provided ([#36427](https://github.com/pytorch/pytorch/pull/36427))\r\n\r\nIn previous versions of PyTorch, `SobolEngine(..., scramble=True, seed=None)` did not respect any calls to `torch.manual_seed`. The expected behavior for random number generation functions is to respect the seed set by `torch.manual_seed`, so we\u2019ve changed `SobolEngine` to match.\r\n\r\nIf you were relying on the old behavior where `SobolEngine` ignores `torch.manual_seed`, please explicitly pass a different seed to `SobolEngine`:\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.manual_seed(1337)\r\n# SobolEngine ignores the manual_seed and instead uses its own.\r\n>>> `x1 = SobolEngine(dimension=1, scramble=True, seed=None).draw(3)`\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> import time\r\n>>> torch.manual_seed(1337)\r\n# To replicate the old behavior of, pass a seed to SobolEngine.\r\n>>> ms_since_epoch = int(round(time.now() * 1000))\r\n>>> x1 = SobolEngine(dimension=1, scramble=True, seed=ms_since_epoch).draw(3)\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### `Tensor.random_(to, from)`: Enforce check that `from` and `to` are within the bounds of the Tensor\u2019s `dtype` ([#37507](https://github.com/pytorch/pytorch/pull/37507))\r\n\r\nIn previous versions of PyTorch, `to` and `from` did not have to be within the bounds of the tensor\u2019s `dtype` (this raised a warning). The behavior of `random_` in that case can be unexpected. We are making this a hard error starting from PyTorch 1.6.0; please modify your code if you run into the error.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.zeros(10, dtype=torch.uint8)\r\n# 256 is the maximum value for `to` for `torch.uint8`\r\n>>> tensor.random_(0, 257)\r\nUserWarning: to - 1 is out of bounds for unsigned char.\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.zeros(10, dtype=torch.uint8)\r\n# 256 is the maximum value for `to` for `torch.uint8`\r\n>>> tensor.random_(0, 256)\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### Dropped support for CUDA < 9.2 from for source builds (#38977, #36846)\r\n\r\nIf you build PyTorch from source, we\u2019ve dropped support for using CUDA < 9.2 (run `nvcc --version` to check your CUDA version). Users who install PyTorch packages via conda and/or pip are unaffected.\r\n\r\n\r\n### `DataLoader`\u2019s `__len__` changed to return number of batches when holding an `IterableDataset` ([#38925](https://github.com/pytorch/pytorch/pull/38925)) \r\n\r\nIn previous versions of PyTorch, `len(<instance of dataloader holding an IterableDataset>)` would return the number of examples in the dataset. We\u2019ve changed it to be the number of batches (e.g., the number of examples divided by the DataLoader\u2019s `batch_size`) to be consistent with the computation of length when the DataLoader has a BatchedSampler.\r\n\r\n\r\n### `torch.backends.cudnn.flags`: deleted unused `verbose` flag ([#39228](https://github.com/pytorch/pytorch/pull/39228))\r\n\r\nThe verbose flag did nothing, so we deleted it. If you were passing a value to `flags` for `verbose`, please remove it.\r\n\r\n\r\n## RPC\r\n\r\n`RpcBackendOptions` takes `float` instead of `timedelta` for `timeout` argument to stay consistent with `timeout` types in other TorchScriptable RPC APIs.\r\n\r\n\r\n```python\r\n# v1.5\r\nrpc.init_rpc(\r\n    \"worker1\",\r\n    rank=0,\r\n    world_size=2,\r\n    rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(\r\n        num_send_recv_threads=16,\r\n        datetime.timedelta(seconds=20)\r\n    )\r\n)\r\n```\r\n\r\n```python\r\n# v1.6\r\nrpc.init_rpc(\r\n    \"worker1\",\r\n    rank=0,\r\n    world_size=2,\r\n    rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(\r\n        num_send_recv_threads=16,\r\n        20 # seconds\r\n    )\r\n)\r\n```\r\n\r\n## TorchScript\r\n\r\n### The Default Executor Is Rolled Back To Legacy ([#41017](https://github.com/pytorch/pytorch/pull/41017))\r\n\r\nWe rolled back to the old fuser and the legacy executor in this release in order to recover some reported performance regressions. In future releases we plan to reach the same or better performance with a new redesigned executor and fuser.\r\n\r\nIn order to switch back to the executor used in the 1.5 release one could use the following API:\r\n\r\n* in Python: call `torch._C._jit_set_profiling_executor(True)` before you call your model for the first time,\r\n* in C++: include `#include <torch/csrc/jit/runtime/graph_executor.h>` and set `getExecutorMode() = true` before you invoke your model for the first time.\r\n\r\n### Added dynamic versioning ([#40279](https://github.com/pytorch/pytorch/pull/40279))  \r\n\r\nNote: this isn\u2019t actually BC-breaking but we are listing it here because it is BC-Improving.\r\n\r\nThe PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch\u2019s release notes, and modules relying on functionality that has changed may need to be updated to continue working properly.\r\n\r\nIn this release, the historic behavior of `torch.div` and `torch.full` is preserved for models saved via `torch.jit.save` in previous versions of PyTorch.  Modules saved with the current version of PyTorch will use the latest `torch.div` and `torch.full` behavior.  See the notes above for the BC changes to those operators.\r\n\r\n## Internals\r\n\r\nThe following are a list of BC-breaking changes to some of PyTorch\u2019s internal components.\r\n\r\n### Dispatcher C++ API has had some spring cleaning. This is still considered an \u201cinternal\u201d API, but it is becoming more public facing as it stabilizes.\r\n\r\n* Renamed callUnboxed() to call() in Dispatcher, OperatorHandle, KernelFunction ([#37999](https://github.com/pytorch/pytorch/pull/37999))\r\n* The TensorId suffix has been removed from most DispatchKey enum entries ([#36240](https://github.com/pytorch/pytorch/pull/36240))\r\n* Removed ::callOp(); use Dispatcher::call instead (renamed in [#37797](https://github.com/pytorch/pytorch/pull/37797), removed in [#38351](https://github.com/pytorch/pytorch/pull/38351), [#38742](https://github.com/pytorch/pytorch/pull/38742))\r\n* Removed `KernelFunction::makeFromUnboxedFunctorFactory`; use makeFromUnboxedFunctor directly instead ([#35488](https://github.com/pytorch/pytorch/pull/35488))\r\n* Renamed boxing/unboxing files and utilities in ATen/core/boxing ([#35411](https://github.com/pytorch/pytorch/pull/35411))\r\n\r\n\r\n\r\n### `autograd.gradcheck` and `autograd.gradgradcheck`: Added a new default-true argument `check_undefined_grad` ([#39400](https://github.com/pytorch/pytorch/pull/39400))\r\n\r\nInternally, in the autograd engine, we use a special undefined Tensor value to represent zero-filled gradients and expect backward functions and user-defined `torch.autograd.Function`s to gracefully handle those values. When `check_undefined_grad` is True (the default for PyTorch 1.6+), `gradcheck/gradgradcheck` test that the operation in question supports undefined output gradients. This may cause a previously succeeding `gradcheck` to fail. \r\n\r\nYou can turn the check off by setting `check_undefined_grad` to False. As long as autograd does not error out due to an undefined gradient in your model, then everything should be fine.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.autograd.gradcheck(my_custom_function, inputs)\r\nTrue\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> # To keep the previous behavior\r\n>>> torch.autograd.gradcheck(my_custom_function, inputs, check_undefined_grad=False)\r\nTrue\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### [C++ API] Changed the TensorIterator API ([#39803](https://github.com/pytorch/pytorch/pull/39803))\r\n\r\n TensorIterator is an implementation detail for writing kernels that is exposed in our C++ API. We\u2019ve modified how developers interact with TensorIterator, please see the Pull Request for more details.\r\n\r\n\r\n### Removed `torch._min` and `torch._max`([#38440](https://github.com/pytorch/pytorch/pull/38440))\r\n\r\n`torch._min` and `torch._max` are undocumented and were intended to be an implementation detail; we expect very few users, if any at all, to be using it. We\u2019ve deleted it in PyTorch 1.6.0. Please use `torch.min/torch.max` instead if you are using `torch._min/torch._max`.\r\n\r\n\r\n# Deprecations\r\n\r\n### Deprecated old `torch.save` serialization format ([#39460](https://github.com/pytorch/pytorch/pull/39460), [#39893](https://github.com/pytorch/pytorch/pull/39893), [#40288](https://github.com/pytorch/pytorch/pull/40288), #40793)\r\n\r\nWe have switched `torch.save` to use a zip file-based format by default rather than the old Pickle-based format. `torch.load` has retained the ability to load the old format, but use of the new format is recommended. The new format is:\r\n\r\n* more friendly for inspection and building tooling for manipulating the save files\r\n* fixes a long-standing issue wherein serialization (`__getstate__`, `__setstate__`) functions on `Modules` that depended on serialized `Tensor` values were getting the wrong data\r\n* the same as the TorchScript serialization format, making serialization more consistent across PyTorch\r\n\r\nUsage is as follows:\r\n\r\n```python\r\nm = MyMod()\r\ntorch.save(m.state_dict(), 'mymod.pt') # Saves a zipfile to mymod.pt\r\n```\r\n\r\nTo use the old format, pass the flag `_use_new_zipfile_serialization=False`\r\n\r\n```python\r\nm = MyMod()\r\ntorch.save(m.state_dict(), 'mymod.pt', _use_new_zipfile_serialization=False) # Saves pickle\r\n```\r\n\r\n### Fixed missing deprecation warning for Tensor.nonzero() ([#40187](https://github.com/pytorch/pytorch/pull/40187))\r\n\r\nCalling `torch.nonzero(tensor, as_tuple=False)` with one argument or `Tensor.nonzero(as_tuple=False)` with no arguments is deprecated and will be removed in a future version of PyTorch. Please specify the `as_tuple` argument.\r\n\r\n# New Features\r\n\r\n### Python API\r\n\r\nNew Utilities\r\n\r\n* Added global hooks to `torch.nn.Module` ([#38972](https://github.com/pytorch/pytorch/pull/38972))\r\n* Added option to enable cpp stack traces with `TORCH_SHOW_CPP_STACKTRACES=1` ([#38127](https://github.com/pytorch/pytorch/pull/38127))\r\n* Added `torch.utils.show_pickle` for showing pickle contents in saved models ([#35168](https://github.com/pytorch/pytorch/pull/35168))\r\n\r\n\r\nNew Operators\r\n\r\n* `torch.logcumsumexp` added ([#36308](https://github.com/pytorch/pytorch/pull/36308))\r\n* `torch.logaddexp` added ([#38384](https://github.com/pytorch/pytorch/pull/38384))\r\n* `torch.rad2deg`, `torch.deg2rad` added ([#38852](https://github.com/pytorch/pytorch/pull/38852))\r\n* `torch.arccosh`, `torch.arcsinh`, `torch.arctanh` added ([#38388](https://github.com/pytorch/pytorch/pull/38388))\r\n* `torch.flip{lr, ud}` added ([#38599](https://github.com/pytorch/pytorch/pull/38599))\r\n* `torch.bucketize`, `torch.searchsorted` added ([#34577](https://github.com/pytorch/pytorch/pull/34577))\r\n* `torch.istft` (Inverse Short Time Fourier Transform) added ([#35569](https://github.com/pytorch/pytorch/pull/35569))\r\n* `torch.vander`: added support for generating Vandermonde matrices ([#36725](https://github.com/pytorch/pytorch/pull/36725))\r\n* `torch.block_diag` added ([#33449](https://github.com/pytorch/pytorch/pull/33449))\r\n* `nn.Hardswish`, `nn.functional.hardswish` added ([#34747](https://github.com/pytorch/pytorch/pull/34747))\r\n* `torch.nn.init.trunc_normal_` (truncated normal initializer) added ([#32397](https://github.com/pytorch/pytorch/pull/32397))\r\n* Added Stochastic Weight Averaging. See `torch.optim.AveragedModel` and `torch.optim.SWALR` for more details.([#35032](https://github.com/pytorch/pytorch/pull/35032))\r\n\r\n### C++ API\r\n\r\n* Added Optimizer `AdamW` to C++ frontend ([#40009](https://github.com/pytorch/pytorch/pull/40009))\r\n* Custom C++ autograd function now supports c10::optional<Tensor> as parameters ([#37700](https://github.com/pytorch/pytorch/pull/37700))\r\n* torch::Tensor now supports bitwise NOT(!), AND(&), OR(|), XOR(^) operators ([#38691](https://github.com/pytorch/pytorch/pull/38691))\r\n* Cpp extension now supports load and `load_inline` under ROCm ([#35897](https://github.com/pytorch/pytorch/pull/35897))\r\n\r\n\r\n\r\n### [Beta] Complex Tensor support\r\n\r\nThe PyTorch 1.6 release brings beta-level support for complex tensors.  The UX is similar to existing PyTorch tensors and the new complex-specific functionality is compatible with NumPy\u2019s complex arrays.  In particular, you\u2019ll be able to create and manipulate complex tensors, interop with previously existing code that represented complex tensors as tensors of size `(..., 2)`, and more.\r\n\r\nWhile this is an early version of this feature, and we expect it to improve over time, the overall goal is provide a NumPy compatible user experience that leverages PyTorch\u2019s ability to run on accelerators and work with autograd to better support the scientific computing and ML communities.\r\n\r\nPlease find the full documentation [here](https://pytorch.org/docs/master/complex_numbers.html?highlight=complex).\r\n\r\n**Python API:**\r\n\r\n* Added `torch.is_signed() `for complex tensors. ([#33773](https://github.com/pytorch/pytorch/pull/33773))\r\n* Added dtype inference for complex tensors. ([#33713](https://github.com/pytorch/pytorch/pull/33713))\r\n* Added `torch.randn` and `torch.normal_` for complex tensors. ([#34037](https://github.com/pytorch/pytorch/pull/34037), [#35056](https://github.com/pytorch/pytorch/pull/35056))\r\n* Added complex type inference for  `torch.full`. ([#34709](https://github.com/pytorch/pytorch/pull/34709))\r\n* Added type promotion logic for complex numbers. ([#34093](https://github.com/pytorch/pytorch/pull/34093))\r\n* Added `is_complex` tensor attribute for complex numbers. ([#34093](https://github.com/pytorch/pytorch/pull/34093))\r\n* Added torch.fill for complex tensors. ([#34973](https://github.com/pytorch/pytorch/pull/34973))\r\n* Added `torch.rand` for complex dtypes. ([#34924](https://github.com/pytorch/pytorch/pull/34924), [#35585](https://github.com/pytorch/pytorch/pull/35585))\r\n* Fixed complex conversions, used in `torch.copy_` , on cuda. ([#35344](https://github.com/pytorch/pytorch/pull/35344))\r\n* Added `torch.from_numpy` for complex dtypes. ([#35531](https://github.com/pytorch/pytorch/pull/35531))\r\n* Added a check to throw error for in place modification of non-complex tensors with complex number values. ([#35883](https://github.com/pytorch/pytorch/pull/35883))\r\n* Fixed `torch.exp` CPU implementation for complex tensors. ([#35715](https://github.com/pytorch/pytorch/pull/35715))\r\n* Added `torch.masked_fill` for complex tensors. ([#36335](https://github.com/pytorch/pytorch/pull/36335))\r\n* Updated `torch.abs` to return float tensors for complex tensors. ([#35871](https://github.com/pytorch/pytorch/pull/35871))\r\n* Added `torch.isfinite` and `torch.isinf` for complex tensors. ([#36648](https://github.com/pytorch/pytorch/pull/36648))\r\n* Added `torch.isclose` for complex tensors. ([#36456](https://github.com/pytorch/pytorch/pull/36456))\r\n* Updated `torch.angle` to return float tensors for complex tensors. ([#36896](https://github.com/pytorch/pytorch/pull/36896))\r\n* Enabled `requires_grad` for complex tensors. ([#36932](https://github.com/pytorch/pytorch/pull/36932))\r\n* Fixed reciprocal divide for complex tensors. ([#37193](https://github.com/pytorch/pytorch/pull/37193))\r\n* Added `torch.reciprocal` for complex tensors on CUDA. ([#36749](https://github.com/pytorch/pytorch/pull/36749))\r\n* Added Python API for `Complex Storage`. ([#35771](https://github.com/pytorch/pytorch/pull/35771))\r\n* Added `torch.addmv` for complex tensors. ([#37924](https://github.com/pytorch/pytorch/pull/37924), [#40238](https://github.com/pytorch/pytorch/pull/40238))\r\n* Updated dtype inference for `torch.tensor` . ([#38030](https://github.com/pytorch/pytorch/pull/38030))\r\n* Added `torch.pow` for complex tensors on CUDA. ([#36793](https://github.com/pytorch/pytorch/pull/36793))\r\n* Added support for complex values as exponents in `torch.pow` .([#36793](https://github.com/pytorch/pytorch/pull/36793), [#39117](https://github.com/pytorch/pytorch/pull/39117))\r\n* Added `torch.roll` for complex tensors on CUDA. ([#38664](https://github.com/pytorch/pytorch/pull/38664))\r\n* Added `torch.gather` for complex tensors on CPU. ([#36430](https://github.com/pytorch/pytorch/pull/36430))\r\n* Added `torch.tanh` for complex tensors on CUDA. ([#38786](https://github.com/pytorch/pytorch/pull/38786))\r\n* Added complex dtypes to list of supported types in autograd. ([#38325](https://github.com/pytorch/pytorch/pull/38325))\r\n* Added `torch.cumsum, torch.cumprod` for complex tensors on CUDA. ([#39063](https://github.com/pytorch/pytorch/pull/39063))\r\n* Added `real` and `imag` views as tensor attributes. ([#39033](https://github.com/pytorch/pytorch/pull/39033))\r\n* Added `torch.flip` and `torch.rot90` for complex tensors. ([#37826](https://github.com/pytorch/pytorch/pull/37826))\r\n* Added `torch.view_as_real`, `torch.view_as_complex` for complex tensors. ([#39099](https://github.com/pytorch/pytorch/pull/39099))\r\n* Added printing logic for complex tensors ([#40513](https://github.com/pytorch/pytorch/pull/40513), [#38031](https://github.com/pytorch/pytorch/pull/38031))\r\n* Add `torch.tan` for complex tensors on CUDA ([#38400](https://github.com/pytorch/pytorch/pull/38400))\r\n* Added support for complex tensors for `torch.tanh` backward function ([#37791](https://github.com/pytorch/pytorch/pull/37791), [#38786](https://github.com/pytorch/pytorch/pull/38786))\r\n\r\n\r\n**C++ API:**\r\n\r\n* Added core of c10::complex. ([#36626](https://github.com/pytorch/pytorch/pull/36626))\r\n* Added overloads of std:: math functions in c10::complex ([#37468](https://github.com/pytorch/pytorch/pull/37468), [#37689](https://github.com/pytorch/pytorch/pull/37689))\r\n* Added c10::complex as the C++ type for complex tensors ([#37421](https://github.com/pytorch/pytorch/pull/37421), [#39306](https://github.com/pytorch/pytorch/pull/39306))\r\n* Added support for operations on c10::complex and integer scalars ([#38418](https://github.com/pytorch/pytorch/pull/38418))\r\n* Added overloads for complex math functions in both :: and std:: to fix ROCm bugs ([#39829](https://github.com/pytorch/pytorch/pull/39829))\r\n* Added `at::tensor()` and `torch::tensor()` for complex numbers ([#39793](https://github.com/pytorch/pytorch/pull/39793))\r\n\r\n### Distributed\r\n\r\n* `torch.distributed`: Add `all_to_all` API to the MPI backend in the distributed module ([#32361](https://github.com/pytorch/pytorch/pull/32361)).\r\n* `torch.distributed`: Add `c10d` dynamic loading mechanism to support 3rd-party `c10d` implementations ([#28068](https://github.com/pytorch/pytorch/pull/28068)).\r\n* `torch.nn.parallel.DistributedDataParallel`: Add distributed data parallel benchmark tool ([#35198](https://github.com/pytorch/pytorch/pull/35198)).\r\n* `torch.nn.parallel.DistributedDataParallel` and `torch.distributed.rpc`: allow DDP to work with RPC ([#37998](https://github.com/pytorch/pytorch/pull/37998), [#39916](https://github.com/pytorch/pytorch/pull/39916), [#40130](https://github.com/pytorch/pytorch/pull/40130), [#40139](https://github.com/pytorch/pytorch/pull/40139), [#40495](https://github.com/pytorch/pytorch/pull/40495)).\r\n\r\n### Mobile\r\n\r\n* Add `torch.utils.mobile_optimizer.optimize_for_mobile` to encapsulate several model optimizations appropriate for mobile models.  (Note: currently broken on Windows.)  ([#35227](https://github.com/pytorch/pytorch/pull/35227)) ([#36357](https://github.com/pytorch/pytorch/pull/36357))\r\n\r\n### New operator registration API\r\n\r\nPyTorch 1.6 has a new, pybind11-based operator registration API which replaces the torch::RegisterOperators() class.\r\n\r\nBefore:\r\n\r\n```cpp\r\nstatic auto registry =\r\n  torch::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective);\r\n```\r\n\r\n\r\nAfter:\r\n\r\n```cpp\r\nTORCH_LIBRARY(my_ops, m) {\r\n  m.def(\"warp_perspective\", warp_perspective);\r\n}\r\n```\r\n\r\n\r\nYou can read more about this API in the [custom C++ operators tutorial](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html) or the [reference documentation](https://pytorch.org/cppdocs/library.html).\r\n\r\nThe new API was developed in PRs #35061, [#35629](https://github.com/pytorch/pytorch/pull/35629), [#35706](https://github.com/pytorch/pytorch/pull/35706), [#36222](https://github.com/pytorch/pytorch/pull/36222), [#36223](https://github.com/pytorch/pytorch/pull/36223), [#36258](https://github.com/pytorch/pytorch/pull/36258), [#36742](https://github.com/pytorch/pytorch/pull/36742), [#37019](https://github.com/pytorch/pytorch/pull/37019). Internal code was ported to this API in [#36799](https://github.com/pytorch/pytorch/pull/36799), [#36800](https://github.com/pytorch/pytorch/pull/36800), [#36389](https://github.com/pytorch/pytorch/pull/36389), [#37834](https://github.com/pytorch/pytorch/pull/37834), [#38014](https://github.com/pytorch/pytorch/pull/38014); you may find the code examples in these PRs helpful for your ports.\r\n\r\n\r\n### ONNX\r\n\r\nIn PyTorch 1.6, we have added support for ONNX Opset 12. We have also enhanced export of torchvision models, such as FasterRCNN, MaskRCNN, and KeypointRCNN to support dynamic input image size. Export support for several new ops have also been added. A new operator export mode, ONNX_FALLTHROUGH, has been added to the export API that allows exporting the model with non-standard ONNX operators. For large (> 2 GB) model export (using `external_data_format=True` argument), we now support models with large tensor data in attributes (not just model parameters). \r\n\r\nNew ONNX operator support:\r\n\r\n* Update Dropout Export ([#37641](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37641&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607483755&sdata=gqOLWvDUVWCyRgsXR4K3eXjfe3LmQ5sV9%2BR8hIgmlx0%3D&reserved=0))\r\n*  Update Argmin/Argmax ONNX Export ([#38329](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38329&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607493754&sdata=tAsOMeoN%2BCPeHcyzOtbA86HnHxT76czQndM%2BkgT4RGQ%3D&reserved=0))\r\n* Fix pow op export ([#38065](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38065&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607493754&sdata=9kV75mFBBAztmbhXphDIZw0FmfMvFYIuNfuAX8Bouxc%3D&reserved=0))\r\n* Export Support for Celu ([#38243](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38243&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607503748&sdata=wenYmcUvMBgE1Yyc%2FTnvJURrkBS4%2Ff8MQ7a%2Bn5RMHc8%3D&reserved=0))\r\n* Add GreaterOrEqual and LessOrEqual to opset 12 ONNX export ([#38311](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38311&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607503748&sdata=IgTJ8aFhEDYjUnI0SKzqvT4yfMZNh87mmpeFxLTxrOQ%3D&reserved=0))\r\n*  ONNX Export Support for CrossEntropyLoss ([#34830](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F34830&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607513740&sdata=EmaydXdyXfrJ7GjLherR4A6IiLBJH2AMjquYdUJv8G4%3D&reserved=0))\r\n* Adding 'numel' and 'to' export for script module ([#36501](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36501&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607513740&sdata=h7r6%2FofjNaqfQ2k2kL%2FX7E7DdShuw2n%2Fu2tpEnHeexo%3D&reserved=0))\r\n*  Support clamp_min and clamp_max ([#37872](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37872&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607523734&sdata=IpUM4SNqUOr%2Ft5UXALT66FEBkk8qaOzSVLWVtp3FBkc%3D&reserved=0))\r\n* Quantization: Add `aten::max_pool2d` to onnx jit pass ([#34912](https://github.com/pytorch/pytorch/pull/34912))\r\n* Quantization: Mark `upsample_nearest2d`, sigmoid and reshape as no scale in onnx ([#36325](https://github.com/pytorch/pytorch/pull/36325))\r\n* Quantization: export of quantized models with new conv and linear API in onnx ([#38736](https://github.com/pytorch/pytorch/pull/38736))\r\n\r\n### Quantization\r\n\r\nNew quantization operators:\r\n\r\n* quantized Conv1d ([#35093](https://github.com/pytorch/pytorch/pull/35093), [#36352](https://github.com/pytorch/pytorch/pull/36352), [#38248](https://github.com/pytorch/pytorch/pull/38248), [#38283](https://github.com/pytorch/pytorch/pull/38283), [#38438](https://github.com/pytorch/pytorch/pull/38438), [#38449](https://github.com/pytorch/pytorch/pull/38449), [#38749](https://github.com/pytorch/pytorch/pull/38749))\r\n* quantized hardsigmoid ([#34959,](https://github.com/pytorch/pytorch/pull/34959)[#36351](https://github.com/pytorch/pytorch/pull/36351), [#36698](https://github.com/pytorch/pytorch/pull/36698), [#36699](https://github.com/pytorch/pytorch/pull/36699))\r\n* quantized hardswish ([#34820,](https://github.com/pytorch/pytorch/pull/34820)[#36350](https://github.com/pytorch/pytorch/pull/36350), [#36252](https://github.com/pytorch/pytorch/pull/36252), [#36320](https://github.com/pytorch/pytorch/pull/36320), [#36545](https://github.com/pytorch/pytorch/pull/36545))\r\n* quantized layernorm ([#36593](https://github.com/pytorch/pytorch/pull/36593), [#36690](https://github.com/pytorch/pytorch/pull/36690), [#35693](https://github.com/pytorch/pytorch/pull/35693))\r\n* quantized groupnorm ([#36835](https://github.com/pytorch/pytorch/pull/36835), [#39090](https://github.com/pytorch/pytorch/pull/39090))\r\n* quantized instancenorm ([#36847](https://github.com/pytorch/pytorch/pull/36847), [#39091](https://github.com/pytorch/pytorch/pull/39091))\r\n* quantized reflection_pad1d ([#37452](https://github.com/pytorch/pytorch/pull/37452))\r\n* quantized adaptive avgpool. ([#36813](https://github.com/pytorch/pytorch/pull/36813))\r\n* channel shuffle op fp32 + quantized. ([#36815](https://github.com/pytorch/pytorch/pull/36815))\r\n* qnnpack path for hardtanh ([#35779](https://github.com/pytorch/pytorch/pull/35779))\r\n* Quantized Threshold ([#39352](https://github.com/pytorch/pytorch/pull/39352))\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc`: Add TensorPipe RPC backend ([#36197](https://github.com/pytorch/pytorch/pull/36197), [#35483](https://github.com/pytorch/pytorch/pull/35483), [#37839](https://github.com/pytorch/pytorch/pull/37839), [#37918](https://github.com/pytorch/pytorch/pull/37918), [#37919,](https://github.com/pytorch/pytorch/pull/37919)[#37850,](https://github.com/pytorch/pytorch/pull/37850)[#37851](https://github.com/pytorch/pytorch/pull/37851), [#37852,](https://github.com/pytorch/pytorch/pull/37852)[#37980](https://github.com/pytorch/pytorch/pull/37980), [#38052](https://github.com/pytorch/pytorch/pull/38052), [#38265](https://github.com/pytorch/pytorch/pull/38265), [#38266](https://github.com/pytorch/pytorch/pull/38266), [#40162](https://github.com/pytorch/pytorch/pull/40162), [#40389](https://github.com/pytorch/pytorch/pull/40389), [#37910](https://github.com/pytorch/pytorch/pull/37910), [#38448](https://github.com/pytorch/pytorch/pull/38448), [#38818](https://github.com/pytorch/pytorch/pull/38818), [#38819](https://github.com/pytorch/pytorch/pull/38819), [#38926](https://github.com/pytorch/pytorch/pull/38926), [#38931](https://github.com/pytorch/pytorch/pull/38931), [#38930](https://github.com/pytorch/pytorch/pull/38930), [#38933](https://github.com/pytorch/pytorch/pull/38933), [#38934](https://github.com/pytorch/pytorch/pull/38934), [#39010](https://github.com/pytorch/pytorch/pull/39010), [#39011](https://github.com/pytorch/pytorch/pull/39011), [#39397](https://github.com/pytorch/pytorch/pull/39397))\r\n* `torch.distributed.rpc`: Support per-RPC timeouts for `rpc_sync` and `rpc_async` ([#34650](https://github.com/pytorch/pytorch/pull/34650))\r\n* `torch.distributed.rpc.functions.async_execution`: Add an `@async_execution` decorator to allow pause and resume executions in RPC target functions ([#39216](https://github.com/pytorch/pytorch/pull/39216), [#39267](https://github.com/pytorch/pytorch/pull/39267), [#39485](https://github.com/pytorch/pytorch/pull/39485), [#39486](https://github.com/pytorch/pytorch/pull/39486), [#39758](https://github.com/pytorch/pytorch/pull/39758)).\r\n* `torch.futures.Future`:Expose a `Future` type to Python API ([#39008](https://github.com/pytorch/pytorch/pull/39008), [#37311](https://github.com/pytorch/pytorch/pull/37311), [#39119](https://github.com/pytorch/pytorch/pull/39119), [#39597](https://github.com/pytorch/pytorch/pull/39597), [#39964](https://github.com/pytorch/pytorch/pull/39964), [#39950](https://github.com/pytorch/pytorch/pull/39950))\r\n* `torch.distributed.rpc`: Allow profiler to be enabled remotely with RPC ([#38748](https://github.com/pytorch/pytorch/pull/38748), [#40066](https://github.com/pytorch/pytorch/pull/40066))\r\n* `torch.distributed.rpc`: Implement TorchScript-compatible `RemoteModule` API ([#37139](https://github.com/pytorch/pytorch/pull/37139), [#40173](https://github.com/pytorch/pytorch/pull/40173))\r\n* `torch.distributed.rpc.RRef`: enable retrying RRef control messages on communication failures ([#33636](https://github.com/pytorch/pytorch/pull/33636))\r\n* `torch.distributed.rpc`: Let RPC use `torch._C.Future` instead of exposing a dedicated future type. No impact on user side ([#35039](https://github.com/pytorch/pytorch/pull/35039))\r\n* `torch.distributed.autograd`: Add profiler support for `backward` of the distributed autograd engine ([#35261](https://github.com/pytorch/pytorch/pull/35261))\r\n* `torch.distributed.rpc.RRef`: Add TorchScript support for `RRef.local_value()` ([#35433](https://github.com/pytorch/pytorch/pull/35433))\r\n* `torch.distributed.rpc.WorkerInfo`: Add TorchScript support for `WorkerInfo` ([#35447](https://github.com/pytorch/pytorch/pull/35447))\r\n* `torch.distributed.rpc`: Allow profiling RPC with TorchScript target functions ([#36275](https://github.com/pytorch/pytorch/pull/36275))\r\n* `torch.distributed.rpc.RRef`: Add RRef Python Helper to launch function on the remotely referenced object ([#36619](https://github.com/pytorch/pytorch/pull/36619))\r\n* `torch.distributed.rpc`: Add timeout argument to TorchScriptable `rpc_async` ([#37884](https://github.com/pytorch/pytorch/pull/37884))\r\n* `torch.distributed.rpc`: Enable RPC Server Global Profiler ([#38847](https://github.com/pytorch/pytorch/pull/38847))\r\n* `torch.distributed.rpc`: Implement timeout support for `rpc.remote` and `RRef.to_here()` ([#38590](https://github.com/pytorch/pytorch/pull/38590))\r\n* `torch.distributed.rpc`: Enable RRef timeout for TensorPipe ([#39531](https://github.com/pytorch/pytorch/pull/39531))\r\n* `torch.distributed.rpc.WorkerInfo`: Add `WorkerInfo` python `__repr__` magic method ([#40004](https://github.com/pytorch/pytorch/pull/40004))\r\n\r\n### TorchScript\r\n\r\n* Fork / Join Async Parallelism  ([#40438](https://github.com/pytorch/pytorch/pull/40438/files))\r\n* ScriptModule Freezing ([#40409](https://github.com/pytorch/pytorch/pull/40409), [#37044](https://github.com/pytorch/pytorch/pull/37044), [#38830](https://github.com/pytorch/pytorch/pull/38830), [#34786](https://github.com/pytorch/pytorch/pull/34786), [#34787](https://github.com/pytorch/pytorch/pull/34787))\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Added long description to wheel packages ([#39676](https://github.com/pytorch/pytorch/pull/39676))\r\n* `torch.add`: Prevent unbounded growth while adding sparse tensors ([#36030](https://github.com/pytorch/pytorch/pull/36030))\r\n* `torch.mv`: enabled for sparse tensors ([#21782](https://github.com/pytorch/pytorch/pull/21782))\r\n* `torch.bmm`: enabled for sparse x dense tensor operations ([#33430](https://github.com/pytorch/pytorch/pull/33430))\r\n* `torch.cat`: improved error message ([#38978](https://github.com/pytorch/pytorch/pull/38978))\r\n* `torch.masked_select`: enabled bfloat16 support  ([#36859](https://github.com/pytorch/pytorch/pull/36859))\r\n* `torch.absolute`: added as an alias for `torch.abs`  ([#36597](https://github.com/pytorch/pytorch/pull/36597))\r\n* `torch.device`: improved error message to include `xla` as an acceptable device ([#36446](https://github.com/pytorch/pytorch/pull/36446))\r\n* `torch.linspace`, `torch.logspace`: improved precision ([#35461](https://github.com/pytorch/pytorch/pull/35461))\r\n* `Tensor.true_divide` method variant added ([#34794](https://github.com/pytorch/pytorch/pull/34794))\r\n* `Tensor.isnan()`, `Tensor.isinf()`, `Tensor.isfinite()` method variants added ([#37942](https://github.com/pytorch/pytorch/pull/37942))\r\n* `Tensor.is_nonzero`: improved error message  ([#38150](https://github.com/pytorch/pytorch/pull/38150))\r\n* `Tensor.cauchy_`, T`ensor.log_normal_`, `Tensor.exponential_`: added support for bfloat16 ([#38427](https://github.com/pytorch/pytorch/pull/38427))\r\n* `Tensor.as_subclass` method added. ([#34369](https://github.com/pytorch/pytorch/pull/34369))\r\n* `collect_env.py`: improved to detect relevant conda-installed numpy and cudatoolkit ([#35646](https://github.com/pytorch/pytorch/pull/35646))\r\n* `collect_env.py`: made it more robust on Windows ([#39136](https://github.com/pytorch/pytorch/pull/39136))\r\n* `torch.utils.data`: Add `generator=` kwarg for DataLoader & random samplers ([#39737](https://github.com/pytorch/pytorch/pull/39737))\r\n* `torch.utils.data.DataLoader`: properly diagnose exceeding file descriptor limit ([#34768](https://github.com/pytorch/pytorch/pull/34768))\r\n* `torch.utils.data.DataLoader`: added repr for WorkerInfo ([#39975](https://github.com/pytorch/pytorch/pull/39975))\r\n* `torch.utils.data.random_split`: added option to pass a generator for determinism ([#34043](https://github.com/pytorch/pytorch/pull/34043))\r\n* `torch.utils.data.IterableDataset`: make the warning for when a DataLoader holds an IterableDataset clearer (#41185)\r\n* `torch.nn`: Added support for non-persistent buffers that do not show up in a Module\u2019s state dict ([#37191](https://github.com/pytorch/pytorch/pull/37191))\r\n* `nn.Fold`, `nn.Unfold`: added double backwards support ([#36379](https://github.com/pytorch/pytorch/pull/36379))\r\n* `nn.MultiheadAttention`: added support for bool/byte `attn_mask` tensor ([#33763](https://github.com/pytorch/pytorch/pull/33763))\r\n* `nn.functional.upsample`: enabled uint8 sampling support ([#35029](https://github.com/pytorch/pytorch/pull/35029))\r\n* `nn.functional.kl_div`: added option to accept target in log space ([#34586](https://github.com/pytorch/pytorch/pull/34586))\r\n* `nn.functional.softmax`: added support for sparse tensors (CPU) ([#36305](https://github.com/pytorch/pytorch/pull/36305))\r\n* `nn.Softmin`, `nn.Softmax`: improved repr ([#39084](https://github.com/pytorch/pytorch/pull/39084))\r\n* warnings: Changed warnings generated in cpp to show point of Python origination ([#36052](https://github.com/pytorch/pytorch/pull/36052))\r\n* warnings: Improve warnings to actually point at user code ([#39143](https://github.com/pytorch/pytorch/pull/39143))\r\n* Extend some of the basic ops to kHalf ([#37121](https://github.com/pytorch/pytorch/pull/37121))\r\n* Added a warning to a known autograd issue on XLA backend. ([#35449](https://github.com/pytorch/pytorch/pull/35449), [#35543](https://github.com/pytorch/pytorch/pull/35543))\r\n* `torch.cuda`: Change DeprecationWarning to FutureWarning ([#32142](https://github.com/pytorch/pytorch/pull/32142))\r\n* Added `torch.utils.cmake_prefix_path` pointing to `share/cmake` folder ([#38559](https://github.com/pytorch/pytorch/pull/38559))\r\n* `torch.hub`: Added `file_name` argument to `load_state_dict_from_url` ([#39749](https://github.com/pytorch/pytorch/pull/39749))\r\n* Disable autograd while preparing Tensor for printing ([#39420](https://github.com/pytorch/pytorch/pull/39420))\r\n* Improved CUDA error message for MSVC ([#39987](https://github.com/pytorch/pytorch/pull/39987))\r\n* Improved reentrant autograd error message ([#38625](https://github.com/pytorch/pytorch/pull/38625))\r\n* Let >> and << support half on CUDA ([#37670](https://github.com/pytorch/pytorch/pull/37670))\r\n* dockerfile: Update miniconda installer download location & remove unnecessary flag ([#37082](https://github.com/pytorch/pytorch/pull/37082))\r\n* `torch.cuda.get_arch_list()` and `torch.cuda.get_gencode_flags()` added. These return the architecture list and gencode flags PyTorch was compiled with.  (#41212)\r\n* `torch.min, torch.max`: significantly improved CUDA performance (#38440, #39029)\r\n* `torch.multinomial` with `replacement=False:` significantly improved performance (#39742)\r\n\r\n### Python Type Annotations\r\n\r\n* `torch.autograd`: add type hints in-line ([#38080](https://github.com/pytorch/pytorch/pull/38080))\r\n* `torch.finfo`, `torch.iinfo` type annotations added ([#38220](https://github.com/pytorch/pytorch/pull/38220))\r\n* Moved `torch.cuda` annotations inline ([#40075](https://github.com/pytorch/pytorch/pull/40075))\r\n* Add typing for `torch.cuda._CudaStreamBase` and `torch.cuda._CudaEventBase` classes ([#40256](https://github.com/pytorch/pytorch/pull/40256))\r\n* Introduced `torch.types.Device` and stubbed all `torch._C` functions comprehensively ([#38173](https://github.com/pytorch/pytorch/pull/38173))\r\n* Move all `torch.nn` modules type annotations inline ([#38211](https://github.com/pytorch/pytorch/pull/38211))\r\n* Fixes type annotations for named tensors ([#36890](https://github.com/pytorch/pytorch/pull/36890))\r\n* Fix minor issue in type stub for Optimizer ([#38067](https://github.com/pytorch/pytorch/pull/38067))\r\n* Fixed some miscellaneous type hints ([#36584](https://github.com/pytorch/pytorch/pull/36584))\r\n* Fix multiple issues with type annotations ([#36358](https://github.com/pytorch/pytorch/pull/36358))\r\n* `torch.autograd.anomaly_mode`: fixed type hints stub ([#39324](https://github.com/pytorch/pytorch/pull/39324))\r\n* `torch.backends.cudnn` added type annotations ([#38947](https://github.com/pytorch/pytorch/pull/38947))\r\n* `torch.channels_last`, `torch.preserve_format`: added annotations ([#39120](https://github.com/pytorch/pytorch/pull/39120))\r\n\r\n### AMD/ROCm\r\n\r\n* `torch.topk`: enabled support for BFloat16 type on ROCm. ([#34849](https://github.com/pytorch/pytorch/pull/34849))\r\n* `torch.dot`: enabled fp16 support on ROCm ([#30431](https://github.com/pytorch/pytorch/pull/30431), [#30432](https://github.com/pytorch/pytorch/pull/30432))\r\n* `torch.add`: enabled support for BFloat16 type on ROCm for sparse tensors([#35978](https://github.com/pytorch/pytorch/pull/35978))\r\n* Enabled bfloat16 for operators in BERT model ([#37634](https://github.com/pytorch/pytorch/pull/37634))\r\n* `torch.log`: improved ROCm support ([#40079](https://github.com/pytorch/pytorch/pull/40079))\r\n* `torch.pow`, `torch.exp`, `torch.erf`: enabled support for BFloat16 type on ROCm ([#40236](https://github.com/pytorch/pytorch/pull/40236))\r\n\r\n### C++ API\r\n\r\n* Eliminate warnings for cpp extensions on Windows ([#37400](https://github.com/pytorch/pytorch/pull/37400))\r\n* Disable C4251 when compiling `cpp_extensions` on Windows ([#35272](https://github.com/pytorch/pytorch/pull/35272)) \r\n    Note: Above two PRs eliminate unnecessary compile warnings for windows build, make build log more readable.\r\n\r\n### Distributed\r\n\r\n* `torch.distributed`: Enhance error message for MPI unavailability. ([#36781](https://github.com/pytorch/pytorch/pull/36781)).\r\n* `torch.distributed`: Expose `torch.distributed.is_available()` API ([#37021](https://github.com/pytorch/pytorch/pull/37021)).\r\n* `torch.utils.data`: Only create `torch.generator` and seed in `DistributedSampler` when shuffling ([#37604](https://github.com/pytorch/pytorch/pull/37604)).\r\n* `ProcessGroup`: Log incorrect device in `ProcessGroupGloo` ([#38844](https://github.com/pytorch/pytorch/pull/38844)).\r\n* `torch.utils.data`: Improve `DistributedSampler` docs and add seed option ([#39628](https://github.com/pytorch/pytorch/pull/39628)).\r\n* `torch.cuda.comm.reduce`: Avoid initializing unnecessary tensors in `nccl.reduce` ([#39688](https://github.com/pytorch/pytorch/pull/39688)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Remove obsolete warning message from DDP ([#40190](https://github.com/pytorch/pytorch/pull/40190)).\r\n\r\n### Distributions\r\n\r\n* `distributions.Cauchy`: Implemented kl divergence ([#36477](https://github.com/pytorch/pytorch/pull/36477))\r\n* `distributions.Transform`: Add a `.with_cache()` method ([#36882](https://github.com/pytorch/pytorch/pull/36882))\r\n* `distributions.Binary`: Implemented BTRS algorithm for fast/efficient binomial sampling ([#36858](https://github.com/pytorch/pytorch/pull/36858))\r\n\r\n### Internals\r\n\r\n* New macro `TORCH_FN` for passing in compile time function pointers as regular function arguments rather than template arguments ([#39823](https://github.com/pytorch/pytorch/pull/39823), [#40110](https://github.com/pytorch/pytorch/pull/40110))\r\n* Improved support for more types in registered custom kernels\r\n    * Allow std::array as kernel argument and return ([#34399](https://github.com/pytorch/pytorch/pull/34399))\r\n    * Allow ArrayRef as kernel argument ([#34335](https://github.com/pytorch/pytorch/pull/34335))\r\n* Added FPGA DispatchKey, DeviceType, Backend for out-of-tree experimentation ([#38938](https://github.com/pytorch/pytorch/pull/38938))\r\n* Better type safety for calling the dispatcher; we now do a runtime test when casting OperatorHandle to TypedOperatorHandle that you\u2019ve provided the correct type for kernels  ([#40251](https://github.com/pytorch/pytorch/pull/40251))\r\n* OperatorHandle::callBoxed now works on all operators, you no longer need to manually go through JIT registry ([#36010](https://github.com/pytorch/pytorch/pull/36010), [#36850](https://github.com/pytorch/pytorch/pull/36850))\r\n* Added Dispatcher::redispatch for performing a dispatch that bypasses the current key and all keys before it ([#35476](https://github.com/pytorch/pytorch/pull/35476), subsequently renamed)\r\n* More operators are fully supported by the dispatcher ([#37273](https://github.com/pytorch/pytorch/pull/37273), [#36564](https://github.com/pytorch/pytorch/pull/36564), [#36398](https://github.com/pytorch/pytorch/pull/36398), [#36666](https://github.com/pytorch/pytorch/pull/36666), [#36838](https://github.com/pytorch/pytorch/pull/36838))\r\n* Tracing is no longer done inside our autograd code; instead it has been factored into a separate Tracing dispatch key ([#39514](https://github.com/pytorch/pytorch/pull/39514), [#38467](https://github.com/pytorch/pytorch/pull/38467))\r\n* DispatchKey computation no longer relies on TensorOptions; instead, factory functions and other functions with special dispatch key computation needs can register a BackendSelect kernel to compute the required key. (#36290, [#36562](https://github.com/pytorch/pytorch/pull/36562), [#37257](https://github.com/pytorch/pytorch/pull/37257))\r\n\r\n### ONNX\r\n\r\n* Enable Constant Folding for ONNX Opset 12 ([#34823](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F34823&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607523734&sdata=mE9sjvn0hbzbW043n95fhXI%2F0%2F0A9LQX2I%2BSw4XlAfQ%3D&reserved=0))\r\n* ONNX Update training ops and training amenable export API ([#35567](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35567&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607533731&sdata=PCPdWbtr4LR658huourTjSPnMurlpHx30py6AZ4sWrw%3D&reserved=0))\r\n* Fix for constant folding: Slice, Added ReduceL1 and ReduceL2 ([#35280](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35280&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607533731&sdata=%2B4yXdlL5yD5ml7eoaB59u8%2FLCoknOMQqitYMupnFL48%3D&reserved=0))\r\n*  Added support for constant folding onnx::Add and onnx::Sub ([#35869](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35869&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607543728&sdata=VEcm1q7SAAPdmYI0EJxWoSJxwL7Ad8Kh5aEzQgYf0KE%3D&reserved=0))\r\n* Enable constant folding for Shape ([#35386](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35386&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607543728&sdata=EEnX%2FvBKmXXEPaFMiA%2BdKgRx1g0xfkOw7%2Fl23svPnYc%3D&reserved=0))\r\n* Improve error checking for large model export ([#37798](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37798&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607553719&sdata=yGaeZExiqA8bUKocrYRhQiZSlWUu3PnbdVDUJuzpNBo%3D&reserved=0))\r\n* Remove Aten ops from ONNX export ([#37239](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37239&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607553719&sdata=4sOYH6Agwm2BpQsX9HvuTYwlPEeQOsaD9NDZoDked9U%3D&reserved=0))\r\n* Update pytoch/onnx doc ([#39480](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39480&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607563711&sdata=39%2FTkmCQd%2BvHt0TvIVKJoenvZPofsR1WhJdkMdzJtRU%3D&reserved=0))\r\n* Update pytorch/onnx docs for new export API args ([#39802](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39802&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607563711&sdata=iSBv23qZ9LyWRcK%2B%2FWNk5PldN%2Bc7Y7WR0PLhhA4NS3w%3D&reserved=0))\r\n* Support large attribute and subgraph for large model ([#38793](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38793&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607573710&sdata=8XqGYuUyFD5LCC%2BGxHr7yTd0GeU70dwySb4kaTXZUfc%3D&reserved=0))\r\n\r\n### Operator Benchmark\r\n\r\n* Added benchmark for quantized batchnorm ([#35389](https://github.com/pytorch/pytorch/pull/35389))\r\n* Added more quantized activation benchmarks and input sizes ([#35729](https://github.com/pytorch/pytorch/pull/35729))\r\n* Added `__torch_function__` benchmarks ([#36138](https://github.com/pytorch/pytorch/pull/36138))\r\n* Aligned qconv benchmark to conv ([#36673](https://github.com/pytorch/pytorch/pull/36673))\r\n* Aligned the qlinear benchmark to linear ([#36674](https://github.com/pytorch/pytorch/pull/36674))\r\n* Added CUDA support for the observer benchmark ([#39360](https://github.com/pytorch/pytorch/pull/39360))\r\n\r\n### Profiler\r\n\r\n* `torch.autograd.profiler`: Make RecordFunction callbacks thread local and modernize interface ([#37491](https://github.com/pytorch/pytorch/pull/37491))\r\n* `torch.autograd.profiler`: Make profiler thread local ([#36291](https://github.com/pytorch/pytorch/pull/36291))\r\n\r\n### Quantization\r\n\r\n* Add ConvBn3d, ConvBnReLU3d, BNReLU2d, BNReLU3d to eager mode quantization ([#33540](https://github.com/pytorch/pytorch/pull/33540))\r\n* Enabled per channel quantized static linear/conv in QNNPACK ([#37622](https://github.com/pytorch/pytorch/pull/37622))\r\n* Enable per-channel quantization for LSTM Modules ([#39666](https://github.com/pytorch/pytorch/pull/39666), [#39041](https://github.com/pytorch/pytorch/pull/39041))\r\n* Dynamic quantization support for LSTMCell, RNNCell and GRUCell ([#40102](https://github.com/pytorch/pytorch/pull/40102))\r\n* Quantization aware training now works with nn.DataParallel and nn.DistributedDataParallel\r\n    * Make quantization modules work with nn.DataParallel ([#37032](https://github.com/pytorch/pytorch/pull/37032))\r\n    * fake_quant: move observer and fake_quant flags into buffers ([#38368](https://github.com/pytorch/pytorch/pull/38368))\r\n    * Make QAT Conv-BN work with nn.DistributedDataParallel and nn.SyncBatchNorm ([#38478](https://github.com/pytorch/pytorch/pull/38478))\r\n    * fake_quantize: respect device affinity ([#39031](https://github.com/pytorch/pytorch/pull/39031))\r\n* Add quantized tensor support on CUDA ([#37081](https://github.com/pytorch/pytorch/pull/37081))\r\n* Add reduce_range params for quantized_lstm ([#39604](https://github.com/pytorch/pytorch/pull/39604))\r\n* Use TorchBind for ConvPackedParams ([#35923](https://github.com/pytorch/pytorch/pull/35923))\r\n* Use TorchBind for Linear PackedParams\" ([#38101](https://github.com/pytorch/pytorch/pull/38101))\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc.RRef`: Throw an actionable error message on user call `RRef.to_here()` in TorchScript ([#35369](https://github.com/pytorch/pytorch/pull/35369))\r\n* `torch.distributed.rpc.RRef`: Handle exceptions returned via `remote()` calls ([#35331](https://github.com/pytorch/pytorch/pull/35331))\r\n* `torch.distributed.rpc.RRef`: Make RRef type hint mismatch exception message more actionable to users ([#35943](https://github.com/pytorch/pytorch/pull/35943))\r\n* `torch.distributed.rpc`:Allow abort `RecvWork::wait()` in `ProcessGroupAgent::listenLoop` ([#36084](https://github.com/pytorch/pytorch/pull/36084))\r\n* `torch.distributed.autograd`: Appropriately handle exceptions in autograd engine. ([#36019](https://github.com/pytorch/pytorch/pull/36019))\r\n* `torch.distributed.autograd`: Catch exception in distributed engine callbacks. ([#36118](https://github.com/pytorch/pytorch/pull/36118))\r\n* `torch.distributed.autograd`: Avoid some future callback self-captures. ([#36502](https://github.com/pytorch/pytorch/pull/36502))\r\n* `torch.distributed.rpc`: Propagate error from RPC retries to the original attempt ([#35263](https://github.com/pytorch/pytorch/pull/35263))\r\n* `torch.distributed.autograd`: Ensure future is complete when exiting `Engine::mark_graph_task_completed()` ([#36856](https://github.com/pytorch/pytorch/pull/36856))\r\n* `torch.distributed.autograd`: Trigger pre/post hooks of output function nodes under distributed autograd ([#34501](https://github.com/pytorch/pytorch/pull/34501))\r\n* `torch.distributed.rpc`: Supporting create an RPC gang of world size 1 ([#32731](https://github.com/pytorch/pytorch/pull/32731))\r\n* `torch.distributed.autograd`: Improve Error Message for Dist Autograd Context Cleanup Failure ([#37255](https://github.com/pytorch/pytorch/pull/37255))\r\n* `torch.distributed.rpc`: Guard against negative `rpcTimeout` being passed in to `RpcBackendOptions` ([#38267](https://github.com/pytorch/pytorch/pull/38267))\r\n* `torch.distributed.rpc`: Use infinite timeout for operations in ProcessGroup RPC backend ([#38577](https://github.com/pytorch/pytorch/pull/38577))\r\n* `torch.distributed.rpc.WorkerInfo`: Add stringify `WorkerInfo` ([#39974](https://github.com/pytorch/pytorch/pull/39974))\r\n* `torch.distributed.rpc`: Avoid using default process group in ProcessGroupAgent. ([#39909](https://github.com/pytorch/pytorch/pull/39909))\r\n* `torch.distributed.rpc`: Ignore expected errors in TensorPipe RPC backend ([#39182](https://github.com/pytorch/pytorch/pull/39182))\r\n* `torch.distributed.rpc`: Don't use separate heap allocation for metrics  in TensorPipe RPC backend ([#39183](https://github.com/pytorch/pytorch/pull/39183))\r\n* `torch.distributed.rpc`: Bind to hostname's IP address instead of localhost in TensorPipe RPC backend ([#39184](https://github.com/pytorch/pytorch/pull/39184))\r\n* `torch.distributed.rpc`: Use PrefixStore to avoid conflicting keys in TensorPipe RPC backend ([#39185](https://github.com/pytorch/pytorch/pull/39185))\r\n\r\n### TorchScript\r\n\r\n## Improvements\r\n\r\n* Add `id` function ([#34975](https://github.com/pytorch/pytorch/pull/34975))\r\n* Add lazy script decorator ([#34935](https://github.com/pytorch/pytorch/pull/34935))\r\n* Make Future type annotation available in Python ([#27637](https://github.com/pytorch/pytorch/pull/27637))\r\n* Support converting `str` to `float` ([#35352](https://github.com/pytorch/pytorch/pull/35352))\r\n* Enable recording of TorchScript functions ([#34710](https://github.com/pytorch/pytorch/pull/34710))\r\n* Improve the error message when registering a custom class twice ([#35568](https://github.com/pytorch/pytorch/pull/35568))\r\n* Improve optimization of `if` statements with statically determinable predicates ([#35834](https://github.com/pytorch/pytorch/pull/35834))\r\n* Fix reporting of error message in `toBool` ([#35570](https://github.com/pytorch/pytorch/pull/35570))\r\n* Better error when types of default value and parameter do not match ([#35888](https://github.com/pytorch/pytorch/pull/35888))\r\n* Improve serialization for lists and dictionary ([#35741](https://github.com/pytorch/pytorch/pull/35741))\r\n* Add type hints on `hardsigmoid`, `hardswish`, and `elu` to make them scriptable ([#35885](https://github.com/pytorch/pytorch/pull/35885))\r\n* Add `strict` tracer flag to guard against risky behaviors ([#36277](https://github.com/pytorch/pytorch/pull/36277))\r\n* Add support of `Dict` as output when connecting script and tracing ([_#36265_](https://github.com/pytorch/pytorch/pull/36265))\r\n* Use current default `dtype` with `torch.tensor` when `dtype` is not specified ([_#36587_](https://github.com/pytorch/pytorch/pull/36587))\r\n* Add dictionary as output of tracer ([_#36696_](https://github.com/pytorch/pytorch/pull/36696))\r\n* Allowing casting `str` to `int` ([_#36016_](https://github.com/pytorch/pytorch/pull/36016))\r\n* Convert float Tensor argument to double in `Tensor.tolist` ([_#37465_](https://github.com/pytorch/pytorch/pull/37465))\r\n* Add a `code_with_constants` method to module printing ([_#37586_](https://github.com/pytorch/pytorch/pull/37586))\r\n* Support indexing using list literal as index ([_#37848_](https://github.com/pytorch/pytorch/pull/37848))\r\n* Support indexing using list variable as index ([_#37966_](https://github.com/pytorch/pytorch/pull/37966))\r\n* Support `del` statements with variables as targets in TorchScript ([_#37608_](https://github.com/pytorch/pytorch/pull/37608))\r\n* Recursively compile TorchScript class types ([_#38050_](https://github.com/pytorch/pytorch/pull/38050))\r\n* Better error message when missing `init` on custom C++ classes ([_#37474_](https://github.com/pytorch/pytorch/pull/37474))\r\n* Fix `@staticmethod` access from `self` on modules ([_#37702_](https://github.com/pytorch/pytorch/pull/37702))\r\n* Allow `@torch.jit.unused` to be used on TorchScript classes ([_#38522_](https://github.com/pytorch/pytorch/pull/38522), [_#39336_](https://github.com/pytorch/pytorch/pull/39336))\r\n* Add support for `%=` operator in TorchScript ([_#38983_](https://github.com/pytorch/pytorch/pull/38983))\r\n* Provide error messages when JIT infers the type of an argument as `Tensor` ([_#38527_](https://github.com/pytorch/pytorch/pull/38527))\r\n* Allow self-referential type annotations in TorchScript classes ([_#39821_](https://github.com/pytorch/pytorch/pull/39821))\r\n* Support having a different forward method when not in scripting mode ([_#38158_](https://github.com/pytorch/pytorch/pull/38158))\r\n* Fix `index_put_` error in subscript assignment ([_#38378_](https://github.com/pytorch/pytorch/pull/38378))\r\n* Refactor attributes to support buffers and parameters as first class citizens, add support for iterating over named_buffers() ([_#37905_](https://github.com/pytorch/pytorch/pull/37905))\r\n* Add ROCm-specific `half_support_literal` ([_#38899_](https://github.com/pytorch/pytorch/pull/38899))\r\n* Make `torch.unique_consecutive` compilable ([_#39339_](https://github.com/pytorch/pytorch/pull/39339))\r\n* Make `deepcopy()` of Objects call **`g/setstate`** if present ([_#39500_](https://github.com/pytorch/pytorch/pull/39500))\r\n* Allow slicing sequential container (fe45c2c986)\r\n* Support `torch.Tensor` subclasses (like `Parameter`) as inputs to functions ([_#39487_](https://github.com/pytorch/pytorch/pull/39487))\r\n* Add `dtype` as supported type annotation ([_#39741_](https://github.com/pytorch/pytorch/pull/39741))\r\n* Improve error message when type annotation Future without a contained type ([_#39751_](https://github.com/pytorch/pytorch/pull/39751))\r\n* Fix inconsistent results of string `split` func ([_#38772_](https://github.com/pytorch/pytorch/pull/38772))\r\n* Support `pad_sequence/pack_sequence` ([_#39844_](https://github.com/pytorch/pytorch/pull/39844))\r\n* Enable `copy.deepcopy` and `copy.copy` for `RecursiveScriptModule` ([_#32685_](https://github.com/pytorch/pytorch/pull/32685))\r\n* Fix zip serialization for file > 2GiB (0c90b6da5c)\r\n* Fix `dictConstruct` ordering and enable dict mix (41816dc97f)\r\n* Fix delegating to `jit.load` from `torch.load` (#41013)\r\n* Add distributed `backward` support ([#38494](https://github.com/pytorch/pytorch/pull/38494))\r\n\r\n# Bug Fixes\r\n\r\n### Python API\r\n\r\n* `torch.cat`: fixed missing type promotion ([#35030](https://github.com/pytorch/pytorch/pull/35030), [#39777](https://github.com/pytorch/pytorch/pull/39777))\r\n* `torch.gather`: fixed silently incorrect results when in-place gather tries to use incorrect shapes  ([#37102](https://github.com/pytorch/pytorch/pull/37102))\r\n* `torch.median`: fixed `NaN` comparison ([#38216](https://github.com/pytorch/pytorch/pull/38216))\r\n* `torch.cdist`: fixed backward calculation for `p=2` ([#37337](https://github.com/pytorch/pytorch/pull/37337))\r\n* `torch.eig`: fixed segfault when input has NaNs and infs ([#37642](https://github.com/pytorch/pytorch/pull/37642))\r\n* `torch.irfft`: stopped modifying the input in-place ([#35219](https://github.com/pytorch/pytorch/pull/35219))\r\n* `torch.max`, `torch.min`, `torch.median`: fixed incorrect backwards implementation ([#36316](https://github.com/pytorch/pytorch/pull/36316))\r\n* `torch.fmod`: fixed crash on division by zero ([#38919](https://github.com/pytorch/pytorch/pull/38919))\r\n* `torch.multinomial`: fixed support for tensors with empty batch ([#39873](https://github.com/pytorch/pytorch/pull/39873))\r\n* `torch.einsum`: fixed incorrect `__torch_function__` handling ([#38741](https://github.com/pytorch/pytorch/pull/38741))\r\n* `torch.remainder`: fixed overflow when dividend is very large ([#37758](https://github.com/pytorch/pytorch/pull/37758))\r\n* `torch.remainder`: fixed precision issues for CPU tensors ([#38293](https://github.com/pytorch/pytorch/pull/38293))\r\n* `torch.argmax`, `torch.argmin`: fixed bug for big CPU tensors with `dim=2` ([#39576](https://github.com/pytorch/pytorch/pull/39576))\r\n* `torch.histc:` fixed support when passed empty tensor ([#38987](https://github.com/pytorch/pytorch/pull/38987))\r\n* `torch.as_strided`: added error message when passed a negative stric=de ([#39508](https://github.com/pytorch/pytorch/pull/39508))\r\n* `torch.argmax`, `torch.argmin`: fixed bogus returns when called on a scalar tensor ([#37214](https://github.com/pytorch/pytorch/pull/37214))\r\n* `torch.topk`: Fixed bogus results with 4d+ input tensors with topk dimension >= 1024/2048 on CUDA (depending on GPU) ([#40349](https://github.com/pytorch/pytorch/pull/40349))\r\n* `torch.mv`: Fixed bug when grad has  `stride=0` on GPU in the backward pass ([#38321](https://github.com/pytorch/pytorch/pull/38321))\r\n* `>>`, `<<` on CUDA changed to match the behavior on CPU for certain compiler variants ([#35339](https://github.com/pytorch/pytorch/pull/35339))\r\n* `Tensor.exponential_(0)` fixed to return a Tensor filled with `inf` ([#36837](https://github.com/pytorch/pytorch/pull/36837))\r\n* `Tensor.to(..., non_blocking=True)`: fixed regression where `non_blocking` is ignored ([#35144](https://github.com/pytorch/pytorch/pull/35144))\r\n* `Tensor.to`: fixed CUDA negative float to uint8 cast to be consistent with CPU ([#36832](https://github.com/pytorch/pytorch/pull/36832))\r\n* Fixed incorrect binary pointwise operations when the first argument is a scalar ([#39956](https://github.com/pytorch/pytorch/pull/39956))\r\n* `Tensor.copy_`: Fixed error when used with AMD devices ([#38003](https://github.com/pytorch/pytorch/pull/38003))\r\n* `torch.tensor`: fix segfault in error checking in Tensor constructor ([#40106](https://github.com/pytorch/pytorch/pull/40106))\r\n* Fix overflow issues when constructing tensors with large numbers ([#39140](https://github.com/pytorch/pytorch/pull/39140))\r\n* Fixed regression in unary ops casting to output dtype (#41097)\r\n* `nn.Module`: fixed AttributeError reporting for `nn.Module`'s properties ([#34324](https://github.com/pytorch/pytorch/pull/34324))\r\n* `nn.MaxPool2d`: fix for returning wrong shape with `return_indices=True` on CUDA ([#38992](https://github.com/pytorch/pytorch/pull/38992))\r\n* `nn.MaxPool2d`: fix NCHW backward bug ([#38953](https://github.com/pytorch/pytorch/pull/38953))\r\n* `nn.MaxPool2d`: fixed dilated case ([#36288](https://github.com/pytorch/pytorch/pull/36288))\r\n* `nn.MultiheadAttention`: Removed weights from `__constants__` to fix warnings when converting to TorchScript.\r\n* `nn.ConvTranspose2d`: fixed error in backward pass for fp16 inputs. ([#37569](https://github.com/pytorch/pytorch/pull/37569))\r\n* `nn.ConvTranspose3d`: fixed index overflow ([#39198](https://github.com/pytorch/pytorch/pull/39198))\r\n* `nn.RReLU`: fixed memory leak ([#39347](https://github.com/pytorch/pytorch/pull/39347))\r\n* `nn.PReLU`: fixed stack overflow in backward pass ([#36134](https://github.com/pytorch/pytorch/pull/36134))\r\n* `nn.MultiheadAttention`: fixed assertion to support FP16 training ([#37539](https://github.com/pytorch/pytorch/pull/37539))\r\n* `nn.MultiheadAttention`: Updated assert to remove check on 3rd dim for MHA ([#39402](https://github.com/pytorch/pytorch/pull/39402))\r\n* `nn.ModuleDict`, `nn.ParameterDict`: fixed bug in updating with another `ModuleDict/ParameterDict`, respectively ([#27814](https://github.com/pytorch/pytorch/pull/27814))\r\n* `nn.BatchNorm`: fixed buffer update when `track_running_stats` is set to False ([#38084](https://github.com/pytorch/pytorch/pull/38084))\r\n* `nn.MaxPool3d`: fixed incorrect CUDA backward results for non-square output ([#36820](https://github.com/pytorch/pytorch/pull/36820))\r\n* `nn.DataParallel`: fixed support for empty tensors ([#35965](https://github.com/pytorch/pytorch/pull/35965))\r\n* `nn.functional.grid_sample`: fixed out of boundary bug when grid contains large numbers ([#35506](https://github.com/pytorch/pytorch/pull/35506))\r\n* `nn.functional.max_pool2d`, `nn.functional.avg_pool2d`: fixed issue when stride=None ([#39221](https://github.com/pytorch/pytorch/pull/39221))\r\n* `nn.functional.max_pool2d`: fixed erroneous dimension out of range on CUDA ([#36095](https://github.com/pytorch/pytorch/pull/36095))\r\n* `nn.grad._grad_input_padding`: fixed support for dilation argument ([#33872](https://github.com/pytorch/pytorch/pull/33872))\r\n* `nn.functional.log_softmax`: improved accuracy on CUDA ([#38945](https://github.com/pytorch/pytorch/pull/38945))\r\n* `nn.utils.prune`, `nn.utils.weight_norm`: fixed problems when used with RNNs ([#34170](https://github.com/pytorch/pytorch/pull/34170))\r\n* Fixed nan, inf in GPU {fractional,adaptive} max_pool{2,3}d ([#39903](https://github.com/pytorch/pytorch/pull/39903))\r\n* `nn.functional.interpolation`: nearest interpolation implementation fix for CUDA ([#39055](https://github.com/pytorch/pytorch/pull/39055))\r\n* `torch.utils.mkldnn.to_mkdnn`: cover `nn.Conv1d` in mkldnn model conversion logic ([#38528](https://github.com/pytorch/pytorch/pull/38528))\r\n* `torch.utils.data.DataLoader`: Relax sampler check in BatchSampler ([#38403](https://github.com/pytorch/pytorch/pull/38403))\r\n* `torch.utils.data.DataLoader`: The exception raised when RandomSampler.replacement is non-boolean should be TypeError ([#36547](https://github.com/pytorch/pytorch/pull/36547))\r\n* `torch.utils.data.DataLoader`: Correct a ValueError in dataloader to TypeError ([#36244](https://github.com/pytorch/pytorch/pull/36244))\r\n* `torch.utils.data.DataLoader`: Allow shuffle when auto-batching is disabled ([#39865](https://github.com/pytorch/pytorch/pull/39865))\r\n* `torch.utils.data.DataLoader`: Kill DataLoader workers when we can't join to clean up gracefully ([#39869](https://github.com/pytorch/pytorch/pull/39869))\r\n* `torch.utils.data.Dataloader`: Added error when using `default_collate` on lists of unequal size ([#38492](https://github.com/pytorch/pytorch/pull/38492))\r\n* Fixed crashes on `import torch` related to defining static data in Vec256 ([#37767](https://github.com/pytorch/pytorch/pull/37767), [#38088](https://github.com/pytorch/pytorch/pull/38088)) \r\n* For `out=` operations, preserve output tensor's strides if it is correctly sized ([#38895](https://github.com/pytorch/pytorch/pull/38895))\r\n* `cuda`: fixed a bug where it was possible to incorrectly access the CUDA device before it was initialized ([#36714](https://github.com/pytorch/pytorch/pull/36714))\r\n* `torch.device`: Added better device idx parse checks ([#37376](https://github.com/pytorch/pytorch/pull/37376))\r\n* `torch.autograd`: fixed init-shutdown race condition in autograd engine ([#39194](https://github.com/pytorch/pytorch/pull/39194))\r\n* `torch.autograd`: Fixed error when using hooks with no `__name__` attribute\r\n* `torch.autograd`: Fixed error message ([#39729](https://github.com/pytorch/pytorch/pull/39729))\r\n* `torch.autograd`: wait for non-reentrant threads to shutdown ([#34529](https://github.com/pytorch/pytorch/pull/34529))\r\n* `torch.autograd`: Add undefined tensor gradient support to all backward functions ([#39400](https://github.com/pytorch/pytorch/pull/39400))\r\n* `torch.autograd`: fixed engine flakiness ([#35599](https://github.com/pytorch/pytorch/pull/35599))\r\n* `torch.autograd.Function`: fixed ability to report error messages inside ([#34845](https://github.com/pytorch/pytorch/pull/34845))\r\n* `torch.autograd`: move scalar input to a different device when needed; fixes backward passes of binary-pointwise operators with scalar inputs ([#35286](https://github.com/pytorch/pytorch/pull/35286))\r\n* `torch.autograd.gradcheck`: Fixed behavior for `stride=0` ([#38774](https://github.com/pytorch/pytorch/pull/38774))\r\n* `torch.autograd.Function`: prevent custom Functions from creating non differentiable type that requires grad ([#38326](https://github.com/pytorch/pytorch/pull/38326))\r\n* `torch.no_grad`: Fixed bad interaction between `torch.no_grad` and `tensor.numpy()` conversion  ([#38906](https://github.com/pytorch/pytorch/pull/38906))\r\n* `torch.optim.AdamW`: fixed error message ([#36088](https://github.com/pytorch/pytorch/pull/36088))\r\n* `torch.optim.Optimizer.state_dict()` fixed non-determinism ([#37347](https://github.com/pytorch/pytorch/pull/37347))\r\n* `torch.hub`: added optional request headers to avoid \u201cconnection refused\u201d errors ([#39740](https://github.com/pytorch/pytorch/pull/39740))\r\n* `torch.hub.hub_dir`: fixed inconsistencies ([#38969](https://github.com/pytorch/pytorch/pull/38969))\r\n* OpenMP: fixed memory leak for `num_threads==1` with operations that use OpenMP ([#39533](https://github.com/pytorch/pytorch/pull/39533))\r\n* `torch.multiprocessing`: Fixed deadlock when sharing CUDA tensors ([#40347](https://github.com/pytorch/pytorch/pull/40347))\r\n* `torch.distributions.Binomial`: fix bug where there is a small chance of incorrectly returning -1 ([#38456](https://github.com/pytorch/pytorch/pull/38456))\r\n* `torch.cuda.amp.GradScalar`: fixed bug where `GradScalar` was not pickle-able ([#38296](https://github.com/pytorch/pytorch/pull/38296))\r\n* Fixed uninitialized value in helper function `vec_reduce_all` ([#37853](https://github.com/pytorch/pytorch/pull/37853))\r\n* Fixed potential memory corruption in helper function `cpu_serial_kernel` ([#37869](https://github.com/pytorch/pytorch/pull/37869))\r\n* Synchronize MAGMA functions with the current CUDA stream ([#36605](https://github.com/pytorch/pytorch/pull/36605))\r\n* Windows support: Fix openmp detection with the clang-cl compiler ([#35365](https://github.com/pytorch/pytorch/pull/35365))\r\n* Windows support: Use `ProgramFiles` environment variable on Windows for portability ([#39707](https://github.com/pytorch/pytorch/pull/39707))\r\n* Windows support: Fix AVX detection with clang-cl ([#35653](https://github.com/pytorch/pytorch/pull/35653))\r\n* Windows support: Delay loading the cuda library until it is necessary ([#37811](https://github.com/pytorch/pytorch/pull/37811))\r\n* Windows support: Fix `_copysign` is not a member of std ([#35199](https://github.com/pytorch/pytorch/pull/35199))\r\n* Windows support: Fix zip serialization for files > 2GiB (#40783)\r\n* Windows support: Add runtime check for MSVC redist, fixed `import torch` errors ([#39841](https://github.com/pytorch/pytorch/pull/39841))\r\n* Windows support: More fixes about using Windows API through ctypes ([#39376](https://github.com/pytorch/pytorch/pull/39376))\r\n* Windows support: fixed `import torch` errors ([#39334](https://github.com/pytorch/pytorch/pull/39334))\r\n* Windows support: Fix wrong MSVC version constraint for CUDA 9.2 ([#40794](https://github.com/pytorch/pytorch/pull/40794))\r\n* Windows support: Use LoadLibraryEX, fix problems when loading dlls ([#38302](https://github.com/pytorch/pytorch/pull/38302))\r\n* Windows support: Fix dll load failure in virtual environments ([#39622](https://github.com/pytorch/pytorch/pull/39622))\r\n* Windows support: Make `find_first_set` work on x86 MSVC ([#38637](https://github.com/pytorch/pytorch/pull/38637), [#38706](https://github.com/pytorch/pytorch/pull/38706))\r\n* Removes pickle deprecation warning ([#39003](https://github.com/pytorch/pytorch/pull/39003))\r\n* dockerfile: Sync submodules ([#35423](https://github.com/pytorch/pytorch/pull/35423))\r\n* Fix crashes in `manywheels` builds related to having special `CUDNN` search path rules for `torch_python` ([#37349](https://github.com/pytorch/pytorch/pull/37349))\r\n* *`torch._six.PY37` should be true for Python-3.8 as well (#40868) *\r\n\r\n### AMD/ROCm\r\n\r\n* Stopped erroneously warning about CUDA compute capabilities ([#35949](https://github.com/pytorch/pytorch/pull/35949))\r\n* Stopped using MIOpen for tensors with more than `INT_MAX` number of elements ([#37110](https://github.com/pytorch/pytorch/pull/37110))\r\n* Enable HgemmBatched for ROCm ([#37483](https://github.com/pytorch/pytorch/pull/37483))\r\n* Fix encoding errors for hipify tool ([#37906](https://github.com/pytorch/pytorch/pull/37906))\r\n* Added HIP version guard for occupancy API compatibility ([#38551](https://github.com/pytorch/pytorch/pull/38551))\r\n* Fix the processing logic of `bernoulli` ([#40001](https://github.com/pytorch/pytorch/pull/40001))\r\n* Use correct device type when exporting tensors to DLPack ([#40124](https://github.com/pytorch/pytorch/pull/40124))\r\n\r\n### C++ API\r\n\r\n* Fixed the crash problem when using `BuildExtension.with_options `([#40121](https://github.com/pytorch/pytorch/pull/40121)) \r\n* Fixed the dir permission denied problem when multiple user building cpp_ext on the same machine ([#34239](https://github.com/pytorch/pytorch/pull/34239)) \r\n\r\n### Distributed\r\n\r\n* `torch.nn.SyncBatchNorm`: Fix batch size check. ([#37133](https://github.com/pytorch/pytorch/pull/37133)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Fix DDP error checking for unused parameters ([#36054](https://github.com/pytorch/pytorch/pull/36054)).\r\n* `torch.nn.DataParallel`: Ensure `DataParallel` replicas can be pickled ([#37307](https://github.com/pytorch/pytorch/pull/37307)).\r\n* `torch.distributed`: Ensure `NCCL_BLOCKING_WAIT=1` works for `dist.barrier()` ([#40249](https://github.com/pytorch/pytorch/pull/40249)).\r\n* `torch.nn.SyncBatchNorm`: Avoid blocking host thread when using  `SyncBatchNorm` ([#36659](https://github.com/pytorch/pytorch/pull/36659)).\r\n* `torch.cuda.comm.gather`: Fix `Gather::apply` to avoid accessing moved tensors ([#39733](https://github.com/pytorch/pytorch/pull/39733)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Add a guard to allow DDP\u2019s autograd engine callback to work in a with non-default CUDA streams ([#40115](https://github.com/pytorch/pytorch/pull/40115)).\r\n\r\n### Internals\r\n\r\n* Add missing mutex for listener removal ([#35486](https://github.com/pytorch/pytorch/pull/35486))\r\n* Add missing mutex for fallback register/deregister ([#36628](https://github.com/pytorch/pytorch/pull/36628))\r\n* Improved boxed dispatch performance ([#33313](https://github.com/pytorch/pytorch/pull/33313))\r\n* Refactored jit::Operator to more clearly distinguish the two possible states: c10 vs jit ([#33905](https://github.com/pytorch/pytorch/pull/33905), [#36634](https://github.com/pytorch/pytorch/pull/36634))\r\n* Per device initialization now occurs in backend kernels via code generation, rather than during backend selection ([#37402](https://github.com/pytorch/pytorch/pull/37402))\r\n* Improved support for dispatcher on mobile\r\n    * Unconditionally register schema even on mobile build (61b680c012, [#36250](https://github.com/pytorch/pytorch/pull/36250), [#35148](https://github.com/pytorch/pytorch/pull/35148), [#35193](https://github.com/pytorch/pytorch/pull/35193))\r\n    * Forced schema registration output now generated into a separate file ([#36284](https://github.com/pytorch/pytorch/pull/36284))\r\n* Improved error messages\r\n    * Print the class name when a custom class in kernel signature is invalid ([#39491](https://github.com/pytorch/pytorch/pull/39491))\r\n    * Add operator name to callBoxed() error message ([#39562](https://github.com/pytorch/pytorch/pull/39562))\r\n\r\n### ONNX\r\n\r\n* Fixes default dtype value for onnx hardtanh export (opset11) ([#35467](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35467&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607573710&sdata=lkFXOuIzi1OHmLfA8UA7sGXx6n%2FKhEBG%2F6aqmbZu9JU%3D&reserved=0))\r\n* disable size optimizations for onnx ([#36243](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36243&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607583699&sdata=l4xM3lRRXDMf5sZHq7mZzo7k21nkN9nyFaogqrsLv40%3D&reserved=0))\r\n* Adding a pass to replace interpolate function with `aten::__interpolate` ([#35744](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35744&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607583699&sdata=uhPb%2FgDMJusIp%2BGA9eXzpwqLPy%2FXAT2Jl2zLCJh3ZSA%3D&reserved=0))\r\n* fix `provider_version` and add consistency test ([#36797](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36797&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607593694&sdata=O4HsT%2BiyJ5i9ddZZOaH04zxntDt0ZXrlV1t2RWJ%2FzdA%3D&reserved=0))\r\n*  Fix numerical errors in softmax when dim is not last dimension ([#37326](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37326&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607593694&sdata=%2FdLdlHCzoPuKmBb4p%2Bp50U7tKRV54CAzXDmyqwcKey0%3D&reserved=0))\r\n*  make onnx expect tests resilient to producer_version changes ([#39002](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39002&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607603698&sdata=ILk%2FKvQELZbvfqQ8RFejGY8wkcGGh30qB3xRAZQ0L4M%3D&reserved=0))\r\n* Enable models tests ([#38791](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38791&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607603698&sdata=HsUuISwEDItvGutFLR5Pjdl%2Fr2dqWe6Gonc8XN2sk4A%3D&reserved=0))\r\n*  Enable Constant Folding Tests ([#38751](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38751&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607613689&sdata=Uu5iNJACcpg0V67u4POepfzMYenbslAr0pORiHe6iwE%3D&reserved=0))\r\n* Bump up ONNX submodule to a82c6a7010e2e332d8f74ad5b0c726fd47c85376 ([#39372](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39372&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607613689&sdata=YiRARqN0Ck5SeMCwi84vllPydPsi1qQ2EJwi1jXfK8w%3D&reserved=0))\r\n* Fix type casting for reduce ops ([#38829](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38829&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607623677&sdata=CLbIPtrcC%2BoFuFj7tFF52hwWafH6b86h6ANOL842hn4%3D&reserved=0))\r\n* Fix ONNX export of RNNs with no bias ([#36894](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36894&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607623677&sdata=NNh1t3IOy3IyecUnR4MiiuqRG5VtdNVVtVbR%2Fe0OTMA%3D&reserved=0))\r\n* Fix regression disabling checker ([#39073](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39073&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607633671&sdata=Pr8FqnUI0SnQXSbEibwLnzHMEm7gyfEbwmZX8UyXWLk%3D&reserved=0))\r\n* Fix KeypointRCNN test ([#39589](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39589&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607633671&sdata=MpMkw8vK9c2dYnfhUL5xxMtO9G5Nr%2BctU%2Fqp8H8p6Vc%3D&reserved=0))\r\n* Fix bug in export of ops involving torch.bool type ([#40006](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F40006&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607643667&sdata=qenOAbymKJ0hVHND3cQUYNW%2Fff4%2FKntBUrWStn3Cgno%3D&reserved=0))\r\n* Fix bug in export of cumsum operator ([#40044](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F40044&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607643667&sdata=GdfZcHFg9EoA8QQCdyXMdtsKfKJM%2Ff2qf%2BhIQfkqHZE%3D&reserved=0))\r\n* Set onnx opset version before model select ([#37466](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37466&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607653661&sdata=NjHYlOkTKEidVr2fa%2FJAAFI671fUhf617%2FJ6L8mLqNw%3D&reserved=0))\r\n* Enable tests for opset 12 ([#37846](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37846&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607653661&sdata=WJY2Cyhh3LG902%2BDLGztnzCG77XUe87h9rRvQQx1qYU%3D&reserved=0))\r\n* Enable tests in `test_pytorch_onnx_onnxruntime` ([#37868](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37868&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607663657&sdata=SJQeNlkkqnTyK3DPJqopSBklEHiFlr8668kMEIoRPuE%3D&reserved=0))\r\n* Enable tests in test_operators.py ([#39431](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39431&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607663657&sdata=tnEHdrvEjIbPfWIxzAic%2FC0Lnll9UPiu3ZMGKalLVzI%3D&reserved=0))\r\n\r\n### Operator Benchmark\r\n\r\n* Fixed missing comma in activation benchmarks ([#35104](https://github.com/pytorch/pytorch/pull/35104))\r\n* Fixed bug where activation benchmarks didn\u2019t run anything ([#35731](https://github.com/pytorch/pytorch/pull/35731))\r\n* Replaced `import cpp_benchmark` with `torch.utils.cpp_benchmark` ([#38832](https://github.com/pytorch/pytorch/pull/38832))\r\n\r\n### Profiler\r\n\r\n* `torch.autograd.profiler`: Use `high_resolution_clock` for profiling on Mac ([#37280](https://github.com/pytorch/pytorch/pull/37280))\r\n* `torch.autograd.profiler`: Fixes for profiling JIT code ([#38453](https://github.com/pytorch/pytorch/pull/38453))\r\n* `torch.autograd.profiler`: Destroy CUDA events after profiling ([#39962](https://github.com/pytorch/pytorch/pull/39962))\r\n\r\n### Quantization\r\n\r\n* Fix a bug for convolution bias in QAT Conv-BN ([#36173](https://github.com/pytorch/pytorch/pull/36173))\r\n* Ensure that histogram observers have zero-point of zero for post ReLU activations ([#37107](https://github.com/pytorch/pytorch/pull/37107))\r\n* Unify numerics between fakequant and quant/dequant ([#37188](https://github.com/pytorch/pytorch/pull/37188))\r\n* Release qnnpack original weights for conv/linear ([#37595](https://github.com/pytorch/pytorch/pull/37595))\r\n* Fix histogram observer with 0 input ([#40191](https://github.com/pytorch/pytorch/pull/40191))\r\n* Histogram observer bug fix with min == max ([#40310](https://github.com/pytorch/pytorch/pull/40310))\r\n* Add save/load state_dict to quantized dynamic RNNs ([#39105](https://github.com/pytorch/pytorch/pull/39105))\r\n* Ensure qconv doesn't assert with empty batch ([#38252](https://github.com/pytorch/pytorch/pull/38252))\r\n* Support empty batch input for quantized ops ([#38508](https://github.com/pytorch/pytorch/pull/38508))\r\n* Fixed CUDA memory pinning (#41139)\r\n\r\n### RPC\r\n\r\n* `torch.distributed.autograd`: Respect dist autograd context in `torch.jit._fork`. ([#34360](https://github.com/pytorch/pytorch/pull/34360))\r\n* `torch.distributed.autograd`: Continue trying `send()` even if one `send()` failed when cleanup distributed autograd contexts ([#34943](https://github.com/pytorch/pytorch/pull/34943))\r\n* `torch.distributed.rpc`: In ProcessGroup RPC backend, avoid read-after-free ([#35252](https://github.com/pytorch/pytorch/pull/35252))\r\n* `torch.distributed.rpc`: Fix `aten::wait` for RPC futures([#35695](https://github.com/pytorch/pytorch/pull/35695))\r\n* `torch.distributed.rpc`: Fix `prim::rpc_async` for RPC futures ([#35994](https://github.com/pytorch/pytorch/pull/35994))\r\n* `torch.distributed.rpc`: Only Schedule Retries before Agent Shutdown ([#35554](https://github.com/pytorch/pytorch/pull/35554))\r\n* `torch.distributed.rpc`: Call `threadPool.waitWorkComplete` after `listenerThread.join()` to fix graceful shutdown ([#35394](https://github.com/pytorch/pytorch/pull/35394))\r\n* `torch.distributed.rpc`: Fixing Potential TSAN issue with joining RPC helper threads ([#36094](https://github.com/pytorch/pytorch/pull/36094))\r\n* `torch.distributed.rpc`: Fix race during RPC shutdown. ([#36113](https://github.com/pytorch/pytorch/pull/36113))\r\n* `torch.distributed.rpc`: Fixing RPC shutdown and thread joining ([#36239](https://github.com/pytorch/pytorch/pull/36239))\r\n* `torch.distributed.autograd`: Capture global state, distributed autograd current context id, before thread switching triggered by JIT `future.wait()` ([#36395](https://github.com/pytorch/pytorch/pull/36395))\r\n* `torch.distributed.autograd`: Fix race in `mark_graph_task_completed`. ([#36640](https://github.com/pytorch/pytorch/pull/36640))\r\n* `torch.distributed.rpc`: Acquire GIL when constructing/destructing `ConcretePyObjectHolder` ([#37870](https://github.com/pytorch/pytorch/pull/37870))\r\n* `torch.distributed.rpc`: Explicitly decref `py::object` in `ConcretePyObjectHolder` and `PythonFunctionGuard` ([#38364](https://github.com/pytorch/pytorch/pull/38364))\r\n* `torch.distributed.rpc`: Explicitly decref `py::object` in `PythonRpcHandler` ([#38366](https://github.com/pytorch/pytorch/pull/38366))\r\n* `torch.distributed.rpc`: Keep `py::object` alive until `jit::toIValue` returns ([#38348](https://github.com/pytorch/pytorch/pull/38348))\r\n* `torch.distributed.rpc`: Use GIL to guard decref of `jit::toPyObj` return value in `processRpc` ([#38376](https://github.com/pytorch/pytorch/pull/38376))\r\n* `torch.distributed.rpc`: Use Future's `then()` API to make sure profiling logic is completed when the Future completes ([#38352](https://github.com/pytorch/pytorch/pull/38352))\r\n* `torch.distributed.rpc`: Fix timeout computation in TensorPipe agent([#38928](https://github.com/pytorch/pytorch/pull/38928))\r\n* `torch.distributed.rpc`: Fix lock inversion upon response read error handling ([#38929](https://github.com/pytorch/pytorch/pull/38929))\r\n* `torch.distributed.rpc`: Acquire lock when adding message to timeout map to fix race in TensorPipe RPC backend ([#39398](https://github.com/pytorch/pytorch/pull/39398))\r\n* `torch.distributed.rpc`: Explicitly decref in `UnpickledPythonCall` dtor ([#38398](https://github.com/pytorch/pytorch/pull/38398))\r\n* `torch.distributed.rpc`: Fix possible deadlock in `_wait_all_workers` ([#39535](https://github.com/pytorch/pytorch/pull/39535))\r\n* `torch.distributed.rpc`: Release GIL when deleting users and unforked owners ([#39555](https://github.com/pytorch/pytorch/pull/39555))\r\n* `torch.distributed.rpc`: Fix error handling for `rpc.remote` ([#39605](https://github.com/pytorch/pytorch/pull/39605))\r\n* `torch.distributed.rpc`: Fix RRef alias annotation ([#39933](https://github.com/pytorch/pytorch/pull/39933))\r\n* `torch.distributed.rpc`: Fix TensorPipeAgent shutdown to ensure it drains all outstanding work. ([#40060](https://github.com/pytorch/pytorch/pull/40060))\r\n* `torch.futures`: Let `torch.futures.wait_all()` re-throw errors ([#40291](https://github.com/pytorch/pytorch/pull/40291))\r\n* `torch.distributed.autograd`: Add basic GPU support to distributed autograd. ([#40312](https://github.com/pytorch/pytorch/pull/40312))\r\n\r\n### TensorBoard\r\n\r\n* `summary.hparams`: Support `None` in `hparams_dict` ([#36497](https://github.com/pytorch/pytorch/pull/36497))\r\n* `SummaryWriter.add_scalars()`: Removed incorrect documentation ([#36495](https://github.com/pytorch/pytorch/pull/36495))\r\n* `SummaryWriter.add_embedding`: Fix error where NaN appears in some cases ([#36496](https://github.com/pytorch/pytorch/pull/36496))\r\n* `SummaryWriter.add_hparams`: Fix input parameters ([#31301](https://github.com/pytorch/pytorch/pull/31301))\r\n* `SummaryWriter.add_image_with_boxes`: Added option to add strings to image boxes ([#30941](https://github.com/pytorch/pytorch/pull/30941))\r\n* `SummaryWriter.add_graph`: Fixed missing documentation ([#37504](https://github.com/pytorch/pytorch/pull/37504))\r\n* `SummaryWriter.add_hparams` Let hparam render values correctly ([#31544](https://github.com/pytorch/pytorch/pull/31544))\r\n* Enforce tensorboard minimum version as 1.15 ([#35952](https://github.com/pytorch/pytorch/pull/35952))\r\n\r\n### TorchScript\r\n\r\n* Fix scope of writes in comprehensions ([#36105](https://github.com/pytorch/pytorch/pull/36105))\r\n* Fix name collision during module loading ([#35720](https://github.com/pytorch/pytorch/pull/35720))\r\n* Fix `NamedTuple` resolution ([#35409](https://github.com/pytorch/pytorch/pull/35409))\r\n* Fix copying of bound method from `Module` to `ScriptModule` ([_#36546_](https://github.com/pytorch/pytorch/pull/36546))\r\n* Fix lifting bug in tracing module calls ([_#37189_](https://github.com/pytorch/pytorch/pull/37189))\r\n* Fix tracing of return types for modules that return heterogenous tuples ([_#37190_](https://github.com/pytorch/pytorch/pull/37190))\r\n* Add type-hint check for default arguments in TorchScript C++ frontend ([_#39021_](https://github.com/pytorch/pytorch/pull/39021))\r\n* Fix recursive compilation of function annotated with `@torch.jit._script_if_tracing`` (#40468) ([_#40468_](https://github.com/pytorch/pytorch/pull/40468))\r\n* Fix parsing of subscript expressions using python resolver ([_#39269_](https://github.com/pytorch/pytorch/pull/39269))\r\n* Fix compilation error with gcc 5.5 ([#38112](https://github.com/pytorch/pytorch/pull/38112))\r\n* Fix handling of `aten::masked_select`, properly update type of the `aten::unsqueeze`'s output in shape analysis ([#40716](https://github.com/pytorch/pytorch/pull/40716))\r\n* Fix handling of `aten::unfold`, properly handle default dtype, and fix a gradient thrashing issue in shape analysis ([#41044](https://github.com/pytorch/pytorch/pull/41044))\r\n* Fix a bug with incorrect handling of `aten::view` in autodiff graph construction ([#42029](https://github.com/pytorch/pytorch/pull/42029))\r\n* Fix a bug with incorrect handling of constructor operations with tensor inputs tensor properties based on an input tensor rather than defaults in shape analysis ([#41016](https://github.com/pytorch/pytorch/pull/41016))\r\n* Fix  bug with incorrect  handling of `prim::grad` operation for `Undefined` values in shape analysis ([#41015](https://github.com/pytorch/pytorch/pull/41015))\r\n* Fix the incorrect requires_grad property propagation on loop\u2019s block inputs ([#41014](https://github.com/pytorch/pytorch/pull/41014))\r\n\r\n# Performance\r\n\r\n### Misc\r\n\r\n* `F.avg_pool2d`: added specialized kernel for channels-last ([#35855](https://github.com/pytorch/pytorch/pull/35855))\r\n* Relax cudnn conditions for channels-last convolutions ([#38904](https://github.com/pytorch/pytorch/pull/38904))\r\n* `torch.cat`: Enabled fast path for channels-last inputs ([#39448](https://github.com/pytorch/pytorch/pull/39448))\r\n* `torch.index_put` parallelized accumulate CPU float path with `cpu_atomic_add_float` ([#29705](https://github.com/pytorch/pytorch/pull/29705))\r\n* Make discontiguous tensors also benefit from unrolling ([#34708](https://github.com/pytorch/pytorch/pull/34708))\r\n* `torch.scatter`, `torch.gather`: removed some redundant checks to achieve some speedups ([#34690](https://github.com/pytorch/pytorch/pull/34690))\r\n* `torch.scatter`, `torch.gather` improved performance on CUDA ([#36181](https://github.com/pytorch/pytorch/pull/36181))\r\n* `torch.min(tensor, dim)`, `torch.max(tensor, dim)`: Optimize performance on CPU ([#34875](https://github.com/pytorch/pytorch/pull/34875))\r\n* `torch.index_select`: Optimize performance for 1D inputs ([#35243](https://github.com/pytorch/pytorch/pull/35243))\r\n* Vectorize (CPU) generic types for binary bitwise operators ([#34338](https://github.com/pytorch/pytorch/pull/34338))\r\n* `torch.linspace` vectorized on CPU. ([#27957](https://github.com/pytorch/pytorch/pull/27957), [#34555](https://github.com/pytorch/pytorch/pull/34555), [#35842](https://github.com/pytorch/pytorch/pull/35842), ([#37981](https://github.com/pytorch/pytorch/pull/37981), [#38093](https://github.com/pytorch/pytorch/pull/38093))\r\n* Set device only when device index are different ([#35438](https://github.com/pytorch/pytorch/pull/35438))\r\n* Don't replace TensorImpl for inplace min/max dim ([#35591](https://github.com/pytorch/pytorch/pull/35591), [#39696](https://github.com/pytorch/pytorch/pull/39696))\r\n* `torch.clamp` vectorized for bfloat16 ([#35082](https://github.com/pytorch/pytorch/pull/35082))\r\n* bfloat16: vectorized many unary ops ([#35092](https://github.com/pytorch/pytorch/pull/35092))\r\n* `torch.bincount` optimized for CPU by removing extra `size()` calls ([#35822](https://github.com/pytorch/pytorch/pull/35822))\r\n* Improve reduction op performance on CUDA for large tensors ([#35997](https://github.com/pytorch/pytorch/pull/35997), [#36014](https://github.com/pytorch/pytorch/pull/36014))\r\n* Vectorize in-place comparison operators ([#35117](https://github.com/pytorch/pytorch/pull/35117))\r\n* Vectorize reduction when reducing on fastest striding dimension ([#36873](https://github.com/pytorch/pytorch/pull/36873))\r\n* `nn.EmbeddingBag`: add a fast path that calls FBGEMM ([#36679](https://github.com/pytorch/pytorch/pull/36679))\r\n* `nn.Conv3d`: Optimized grouped Conv3d performance ([#36355](https://github.com/pytorch/pytorch/pull/36355))\r\n* Reduce overheads on several CPU kernels by avoiding restrides. ([#36875](https://github.com/pytorch/pytorch/pull/36875))\r\n* `nn.EmbeddingBag`: uninitialize output and `bag_size` in the fast path to save overhead ([#36681](https://github.com/pytorch/pytorch/pull/36681))\r\n* `nn.SmoothL1Loss`: vectorize forward (CPU) ([#37114](https://github.com/pytorch/pytorch/pull/37114), [#37115](https://github.com/pytorch/pytorch/pull/37115))\r\n* `nn.Unfold`: optimized backward pass ([#36612](https://github.com/pytorch/pytorch/pull/36612), [#38871](https://github.com/pytorch/pytorch/pull/38871))\r\n* Add per-device allocator object in CUDACachingAllocator, reducing lock contention between operations on different devices. ([#37567](https://github.com/pytorch/pytorch/pull/37567))\r\n* Lazily initialize thread local num_threads value ([#37461](https://github.com/pytorch/pytorch/pull/37461))\r\n* Vectorize non-persistent Softmax ([#38557](https://github.com/pytorch/pytorch/pull/38557))\r\n* `nn.GroupNorm`: performance optimized on CPU and CUDA ([#28203](https://github.com/pytorch/pytorch/pull/28203), [#28204](https://github.com/pytorch/pytorch/pull/28204))\r\n* `torch.cumsum`, `torch.cumprod`: Restore thrust path for 1d tensors cumulative ops ([#39180](https://github.com/pytorch/pytorch/pull/39180))\r\n* TensorIterator: Remove unnecessary `!op.is_read_write` test  ([#39747](https://github.com/pytorch/pytorch/pull/39747))\r\n* `torch.multinomial` : fast-path for replacement=False ([#39742](https://github.com/pytorch/pytorch/pull/39742))\r\n* Vectorize on output for reduction kernels ([#37206](https://github.com/pytorch/pytorch/pull/37206))\r\n* `nn.UpSample`: optimized performance for linear modes on CPU ([#34864](https://github.com/pytorch/pytorch/pull/34864))\r\n* Make dynamic casting case also benefit from unrolling ([#34749](https://github.com/pytorch/pytorch/pull/34749))\r\n* `torch.sinh`, `torch.cosh`: vectorized on CPU ([#36396](https://github.com/pytorch/pytorch/pull/36396))\r\n* Speed up sparse tensor gradient accumulation ([#36292](https://github.com/pytorch/pytorch/pull/36292))\r\n* `torch.masked_select` sped up ([#36539](https://github.com/pytorch/pytorch/pull/36539), [#33269](https://github.com/pytorch/pytorch/pull/33269))\r\n* `torch.var`, `torch.std` sped up ([#39967](https://github.com/pytorch/pytorch/pull/39967))\r\n* `torch.max(tensor, dim)` , `torch.min(tensor, dim)` sped up ([#39029](https://github.com/pytorch/pytorch/pull/39029))\r\n\r\n### Distributed\r\n\r\n* `torch.nn.SyncBatchNorm`: Speed up `SyncBatchNorm` by batching distributed communication ([#38246](https://github.com/pytorch/pytorch/pull/38246)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Dynamically adjust DDP bucketing order using the signals collected from the first iteration ([#35137](https://github.com/pytorch/pytorch/pull/35137)).\r\n\r\n### Mobile\r\n\r\n* Use XNNPACK to improve performance for some instances of convolution and linear. ([#35790](https://github.com/pytorch/pytorch/pull/35790)) ([#35791](https://github.com/pytorch/pytorch/pull/35791))\r\n* Use a custom allocator on mobile to automatically include padding for {Q,X}NNPACK, reducing reallocation costs.  ([#36032](https://github.com/pytorch/pytorch/pull/36032))\r\n* Use updated open-source pthreadpool to improve multi-threading performance. (#40951)\r\n\r\n### Quantization\r\n\r\n* qmul and qadd should preserve input memory format ([#34834](https://github.com/pytorch/pytorch/pull/34834))\r\n* remove the slow path(NCHW) for avg_pool3d ([#34994](https://github.com/pytorch/pytorch/pull/34994))\r\n* Optimized qadd_scalar ([#34925](https://github.com/pytorch/pytorch/pull/34925))\r\n* Optimize qavg_pool3d_nhwc ([#35740](https://github.com/pytorch/pytorch/pull/35740))\r\n* Changes to qadd for perf improvement. (602b51eb30)\r\n* improve the quantized batch_norm performance ([#35639](https://github.com/pytorch/pytorch/pull/35639))\r\n* Add vector path to copy kernel for quantized data types ([#36189](https://github.com/pytorch/pytorch/pull/36189))\r\n* Speed up calculate Qparams for per-channel observers ([#30485](https://github.com/pytorch/pytorch/pull/30485))\r\n* Enable float requantization for avgpool/gavgpool ops. ([#37037](https://github.com/pytorch/pytorch/pull/37037))\r\n* Move to using MemoryFormat::ChannelsLast for avgpool2d. ([#36812](https://github.com/pytorch/pytorch/pull/36812))\r\n* Use `gpu_kernel` in Affine Quantizer ([#37312](https://github.com/pytorch/pytorch/pull/37312))\r\n* Perf optimization for conv and gemm kernels. ([#37626](https://github.com/pytorch/pytorch/pull/37626))\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc`: In RPC Server, handle TorchScript continuations asynchronously ([#34109](https://github.com/pytorch/pytorch/pull/34109))\r\n* `torch.distributed.autograd`: Avoid holding lock when completing GraphTask futureResult ([#35101](https://github.com/pytorch/pytorch/pull/35101))\r\n* `torch.distributed.autograd`: Lock optimizations for `DistAutogradContainer` ([#36529](https://github.com/pytorch/pytorch/pull/36529))\r\n* `torch.distributed.rpc.RRef`:Prevent `RRef.to_here()` to block an RPC thread on the callee using Future callbacks ([#36805](https://github.com/pytorch/pytorch/pull/36805))\r\n* `torch.distributed.rpc.RRef`:Prevent `RRef` unpickle to block waiting for `OwnerRRef` creation ([#36785](https://github.com/pytorch/pytorch/pull/36785))\r\n* `torch.distributed.autograd`: Remove spinning for dist engine ([#36606](https://github.com/pytorch/pytorch/pull/36606))\r\n* `torch.distributed.rpc`: Avoid Releasing, Reacquiring lock per iteration in RPC Retry Thread ([#38521](https://github.com/pytorch/pytorch/pull/38521))\r\n\r\n### TorchScript\r\n\r\n* Add vectorized load/store support for JIT generated CUDA kernel ([_#36555_](https://github.com/pytorch/pytorch/pull/36555))\r\n* Speed up alias analysis ([_#36345_](https://github.com/pytorch/pytorch/pull/36345))\r\n* Make new zip serialization for torch save/load significantly (~70%) faster ([_#38379_](https://github.com/pytorch/pytorch/pull/38379))\r\n* Run extra optimizations after inlining ([#35562](https://github.com/pytorch/pytorch/pull/35562))\r\n\r\n# Documentation\r\n\r\n* Split up documentation into subpages, greatly improving performance and search-ability ([#37419](https://github.com/pytorch/pytorch/pull/37419))\r\n* Rename `torch._C.Generator` to `torch.Generator` ([#38773](https://github.com/pytorch/pytorch/pull/38773))\r\n* FAQ: Add note about recovering from OOM ([#35214](https://github.com/pytorch/pytorch/pull/35214))\r\n* `torch.histc`: Add a note on elements outside of given bounds ([#34889](https://github.com/pytorch/pytorch/pull/34889))\r\n* `functional.hardswish`, `functional.hardsigmoid`: improve docs ([#35431](https://github.com/pytorch/pytorch/pull/35431))\r\n* `Tensor.is_complex` doc fix ([#35680](https://github.com/pytorch/pytorch/pull/35680))\r\n* `nn.KLDivLoss` doc fix ([#36137](https://github.com/pytorch/pytorch/pull/36137))\r\n* `torch.min`, `torch.max`, `torch.median`: added note on deterministic/non-deterministic gradient ([#36481](https://github.com/pytorch/pytorch/pull/36481))\r\n* Amp gradient accumulation example ([#36601](https://github.com/pytorch/pytorch/pull/36601))\r\n* `functional.softmax` doc fix ([#36600](https://github.com/pytorch/pytorch/pull/36600))\r\n* Update `contribution_guide.rst` ([#36438](https://github.com/pytorch/pytorch/pull/36438))\r\n* Documentation LU Decomposition: deriving L, U, and P ([#36907](https://github.com/pytorch/pytorch/pull/36907))\r\n* CONTRIBUTING.md: Fixed missing links ([#37131](https://github.com/pytorch/pytorch/pull/37131))\r\n* Add links to more subdir READMEs in CONTRIBUTING.md ([#38049](https://github.com/pytorch/pytorch/pull/38049))\r\n* `torch.isclose`: Adds missing documentation . ([#37295](https://github.com/pytorch/pytorch/pull/37295))\r\n* Improve checkpoint docs to warn users about detached gradient issues ([#37266](https://github.com/pytorch/pytorch/pull/37266))\r\n* Add documentation about multithread autograd ([#37020](https://github.com/pytorch/pytorch/pull/37020))\r\n* `tensor.view` doc improved ([#36728](https://github.com/pytorch/pytorch/pull/36728))\r\n* `nn.MultiheadAttention`: Fixed typo in documentation ([#37496](https://github.com/pytorch/pytorch/pull/37496))\r\n*  `contribution_guide.rst` and `governance.rst` : fixed broken links ([#37820](https://github.com/pytorch/pytorch/pull/37820))\r\n*  `nn.Hardsigmoid` and `nn.functional.hardsigmoid` documentation added ([#38120](https://github.com/pytorch/pytorch/pull/38120))\r\n* `nn.FeatureAlphaDropout` documentation added ([#36295](https://github.com/pytorch/pytorch/pull/36295))\r\n* `nn.functional.hardwish` documentation added ([#37989](https://github.com/pytorch/pytorch/pull/37989))\r\n* `torch.bucketize`, `torch.searchsorted` documentation added ([#38119](https://github.com/pytorch/pytorch/pull/38119))\r\n* Documented bfloat16 dtype and BFloat16Tensor ([#37051](https://github.com/pytorch/pytorch/pull/37051))\r\n* `nn.Linear` fix sample code ([#38002](https://github.com/pytorch/pytorch/pull/38002))\r\n* `torch.index_add`: add missing args for index_add ([#38213](https://github.com/pytorch/pytorch/pull/38213))\r\n* `Tensor.is_nonzero` doc added ([#37845](https://github.com/pytorch/pytorch/pull/37845))\r\n* `functional.upsample`: Correct upsample doc to match interpolation ([#38455](https://github.com/pytorch/pytorch/pull/38455))\r\n* `nn.CTCLoss`: added target un-pad example ([#38393](https://github.com/pytorch/pytorch/pull/38393))\r\n* `nn.SyncBatchNorm`: improved doc ([#38423](https://github.com/pytorch/pytorch/pull/38423), [#38890](https://github.com/pytorch/pytorch/pull/38890), [#39646](https://github.com/pytorch/pytorch/pull/39646))\r\n* `torch.utils.cmake_prefix_path` documented ([#38727](https://github.com/pytorch/pytorch/pull/38727))\r\n*  `torch.logcumsumexp`: fixed formula ([#38952](https://github.com/pytorch/pytorch/pull/38952))\r\n* Add C++ Landing Page ([#38450](https://github.com/pytorch/pytorch/pull/38450))\r\n* use version number instead of 'master' in html header title ([#38149](https://github.com/pytorch/pytorch/pull/38149))\r\n* Docs fix: Added missing indent ([#35017](https://github.com/pytorch/pytorch/pull/35017))\r\n* Fix many doc issues ([#37099](https://github.com/pytorch/pytorch/pull/37099))\r\n* docs: Fixed docstring indentation for documentation ([#37739](https://github.com/pytorch/pytorch/pull/37739))\r\n* Remove old section of the aten doc that is not true anymore ([#35807](https://github.com/pytorch/pytorch/pull/35807))\r\n* Removed Python 2 references ([#36336](https://github.com/pytorch/pytorch/pull/36336))\r\n* Fix multiline signatures in docstring ([#38768](https://github.com/pytorch/pytorch/pull/38768))\r\n* Removed java documentation ([#38920](https://github.com/pytorch/pytorch/pull/38920))\r\n* Fix missing code in 'Installing C++ distribution of Pytorch' ([#39237](https://github.com/pytorch/pytorch/pull/39237))\r\n* `nn.MultiheadAttention`: Update `key_padding_mask` arg docs ([#39321](https://github.com/pytorch/pytorch/pull/39321))\r\n* `torch.squeeze`, `torch.split`, `torch.set_printoption`, `torch.save` docs updated. ([#39303](https://github.com/pytorch/pytorch/pull/39303))\r\n* `utils.cpp_extension`: correct some usages and docs ([#39766](https://github.com/pytorch/pytorch/pull/39766))\r\n* `nn.BatchNorm`, `nn.InstanceNorm`: Clarify that variance estimator is biased for normalization layers ([#39752](https://github.com/pytorch/pytorch/pull/39752))\r\n* Remove duplicated entries in `random.rst` ([#39725](https://github.com/pytorch/pytorch/pull/39725))\r\n* Fix `Tensor.tolist` signature in the docstring ([#39732](https://github.com/pytorch/pytorch/pull/39732)\r\n* `torch.save`: added note ([#40394](https://github.com/pytorch/pytorch/pull/40394))\r\n* Update docs feature classifications (#40539)\r\n* Fix autograd doc subsubsection display issue (#40796)\r\n* Add specific list of supported types in autograd ([#38325](https://github.com/pytorch/pytorch/pull/38325))\r\n\r\n### C++ API\r\n\r\n* Added C++ autograd APIs and C++ Tensor indexing docs ([#35777](https://github.com/pytorch/pytorch/pull/35777))\r\n\r\n### Distributed\r\n\r\n* `torch.distributed.launch`: Include launcher script docs in the distributed doc page ([#40963](https://github.com/pytorch/pytorch/pull/40963))\r\n* `torch.nn.parallel.DistributedDataparallel`: Enhance DDP doc strings for DDP + RPC support. ([#39916](https://github.com/pytorch/pytorch/pull/39916))\r\n\r\n### Quantization\r\n\r\n* Minor fix to quantized conv docstring ([#35134](https://github.com/pytorch/pytorch/pull/35134))\r\n* quant docs: clean up hardswish ([#40323](https://github.com/pytorch/pytorch/pull/40323))\r\n* quant docs: add and clean up hardsigmoid ([#40340](https://github.com/pytorch/pytorch/pull/40340))\r\n* quant docs: add and clean up hardtanh ([#40341](https://github.com/pytorch/pytorch/pull/40341))\r\n* quant docs: add and clean up LayerNorm ([#40342](https://github.com/pytorch/pytorch/pull/40342))\r\n* quant docs: add and clean up GroupNorm ([#40343](https://github.com/pytorch/pytorch/pull/40343))\r\n* quant docs: add and clean up InstanceNorm{n}d ([#40345](https://github.com/pytorch/pytorch/pull/40345))\r\n* quant docs: add and clean up BatchNorm{n}d ([#40346](https://github.com/pytorch/pytorch/pull/40346))\r\n* quant docs: add and clean up ELU ([#40377](https://github.com/pytorch/pytorch/pull/40377))\r\n* Docstring changes for dynamic quantized classes (#40931)\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc.RRef`:Updated RRef docs to indicate RPC Retries ([#36678](https://github.com/pytorch/pytorch/pull/36678))\r\n* `torch.distributed.rpc`: Add pointer to RPC parameter server tutorial ([#37667](https://github.com/pytorch/pytorch/pull/37667))\r\n* `torch.distributed.rpc`: Add TensorPipe RPC Backend documents ([#39467](https://github.com/pytorch/pytorch/pull/39467), [#40222](https://github.com/pytorch/pytorch/pull/40222))\r\n* `torch.distributed.rpc`: Fix `ProcessGroupRpcBackendOptions` Doc ([#39787](https://github.com/pytorch/pytorch/pull/39787))\r\n* `torch.distributed.rpc`: fix RPC reference in top-level index ([#40077](https://github.com/pytorch/pytorch/pull/40077))\r\n* `torch.futures`: Add torch.futures to API docs ([#40051](https://github.com/pytorch/pytorch/pull/40051))\r\n* `torch.distributed.rpc`: Fix typos in RPC Docs ([#40219](https://github.com/pytorch/pytorch/pull/40219))\r\n* `torch.futures`: Improve torch.futures docs ([#40245](https://github.com/pytorch/pytorch/pull/40245))\r\n* `torch.distributed.rpc`: Minor improvements for RPC documents ([#40296](https://github.com/pytorch/pytorch/pull/40296), [#40298](https://github.com/pytorch/pytorch/pull/40298), [#40299](https://github.com/pytorch/pytorch/pull/40299), [#40300](https://github.com/pytorch/pytorch/pull/40300), [#40305](https://github.com/pytorch/pytorch/pull/40305), [#35809](https://github.com/pytorch/pytorch/pull/35809))\r\n* `torch.distributed.rpc.functions.async_execution`: Add a warning to mention that async_execution does not work with autograd profiler ([#40309](https://github.com/pytorch/pytorch/pull/40309))\r\n* `torch.distributed.rpc.functions.async_execution`: Add examples and tests for combining static/class method with async execution (#40619) ([#40619](https://github.com/pytorch/pytorch/pull/40619))\r\n* `torch.distributed`: Add a link in RPC doc page to point to PT Distributed overview ([#41108](https://github.com/pytorch/pytorch/pull/41108)) \r\n\r\n### TorchScript\r\n\r\n* Remove import statement reference in serialization docs ([#38578](https://github.com/pytorch/pytorch/pull/38578))\r\n* Fix Markdown in overview.md for proper rendering of `Node*` ([#37686](https://github.com/pytorch/pytorch/pull/37686))\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.6.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.6.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.6.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/29030094", "release_id": 29030094, "date_created": "2020-07-24T22:18:09Z", "date_published": "2020-07-28T17:13:18Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/27656022", "tag": "v1.5.1", "name": "Bug Fix release", "author": {"name": "zou3519", "type": "User"}, "description": "# PyTorch 1.5.1 Release Notes\r\n- Backwards Incompatible Changes\r\n- Known Issues and Workarounds\r\n- Critical Fixes\r\n- Crashes and Error Fixes\r\n- Other Fixes\r\n\r\n# Backwards Incompatible Changes\r\n\r\n### Autograd: Operations that return integer-type tensors now always returns tensors that don\u2019t require grad  (#37789).\r\n\r\nThis most notably affects `torch.argmin`, `torch.argmax`, and `torch.argsort`. This change is BC-Breaking because previously one could obtain an integer-type tensor that requires grad in 1.5.0. However, said tensors were not usable by autograd; calling `.backward()` on them resulted in an error, so most users are likely to not have been relying on this behavior.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> torch.argmax(tensor).requires_grad\r\nTrue\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> torch.argmax(tensor).requires_grad\r\nFalse\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n# Known Issues and Workarounds\r\n\r\n### When using multiprocessing, PyTorch 1.5.1 and 1.5.0 may error out with complaints about incompatibility between MKL and libgomp (#37377)\r\n\r\nYou may see error messages like the following when using the `torch.multiprocessing` package. This bug has primarily affected users with AMD CPUs.\r\n\r\n```\r\n`Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\r\n        Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.`\r\n```\r\n\r\nYou can get rid of the error and the error message by setting the environment `MKL_THREADING_LAYER=GNU`. This can be done either by including the following in your python code:\r\n\r\n```\r\nimport os\r\nos.environ['MKL_THREADING_LAYER'] = 'GNU'\r\n```\r\n\r\nor by specifying the environment variable when running your script:\r\n\r\n```\r\nMKL_THREADING_LAYER=GNU python my_script.py\r\n```\r\n\r\nTo learn more about what triggers this bug and other workarounds if the above isn\u2019t working, please [read this comment on the issue](https://github.com/pytorch/pytorch/issues/37377#issuecomment-629610327).\r\n\r\n\r\n# Critical Fixes\r\n\r\n### [`torch.multinomial`:](https://pytorch.org/docs/stable/torch.html#torch.multinomial) Fixed a bug where CUDA `multinomial` generated the same sequence over and over again with a shift of 4. (#38046)\r\n\r\n### [`nn.Conv2d`](https://pytorch.org/docs/stable/nn.html#conv2d): Fixed a bug where circular padding applied padding across the wrong dimension (#37881)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> circular = nn.Conv2d(6, 1, (3, 3), padding=(0, 1), padding_mode='circular')\r\n>>> circular(torch.zeros(1, 6, 10, 10)).shape\r\n# Notice the padding is incorrectly on the H dimension, not the W dimension.\r\ntorch.Size([1, 1, 10, 8])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> other = tensor + 1\r\n>>> output = nn.LeakyReLU(0, inplace=True)(other)\r\n>>> output.sum().backward()\r\ntorch.Size([1, 1, 8, 10])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Fixed bug where asserts in CUDA kernels were mistakingly disabled, leading to many silent kernel errors. (#38943, #39047, #39218)\r\n\r\n### [`torch.gather`](https://pytorch.org/docs/stable/torch.html#torch.gather), [`torch.scatter`](https://pytorch.org/docs/stable/torch.html#torch.scatter): added checks for illegal input dtypes that caused silently incorrect behaviors (#38025, #38646)\r\n\r\n### [`torch.argmin`](https://pytorch.org/docs/stable/torch.html#torch.argmin), [`torch.argmax`](https://pytorch.org/docs/stable/torch.html?highlight=argmax#torch.argmax): Fixed silently incorrect result for inputs with more than 2^32 elements (#39212)\r\n\r\n### C++ Custom Operators: fixed a bug where custom operators stopped working with autograd and ignored the `requires_grad=True` flag. (#37355)\r\n\r\n\r\n\r\n## Crashes and Error Fixes\r\n\r\n### Fixed CUDA reduction operations on inputs with more than 2^32 elements (#37788)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> `torch.zeros(5, 14400, 14400, device='cuda').sum(0)`\r\n`RuntimeError: sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Reduce.cuh:706, please report a bug to PyTorch.`      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.zeros(5, 14400, 14400, device='cuda').sum(0)\r\n# No problem\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### Fixed pickling of PyTorch operators (#38033)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> `pickle.dumps(torch.tanh)`\r\nPicklingError: Can't pickle <class 'torch._C._VariableFunctions'>: it's not the same object as torch._C._VariableFunctions\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> pickle.dumps(torch.tanh)\r\n# No problem\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### [`nn.LeakyReLU`](https://pytorch.org/docs/stable/nn.html?highlight=leaky#torch.nn.LeakyReLU): Fixed a bug where using autograd with in-place `nn.LeakyReLu` with a slope of 0 incorrectly errored out. (#37453, #37559)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> other = tensor + 1\r\n>>> output = nn.LeakyReLU(0, inplace=True)(other)\r\n>>> output.sum().backward()\r\nRuntimeError: In-place leakyReLu backward calculation is triggered with a non-positive slope which is not supported. This is caused by calling in-place forward function with a non-positive slope, please call out-of-place version instead.\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> other = tensor + 1\r\n>>> output = nn.LeakyReLU(0, inplace=True)(other)\r\n>>> output.sum().backward()\r\n# No error\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### [`torch.as_strided`](https://pytorch.org/docs/stable/torch.html#torch.as_strided) : Fixed crash when passed `sizes` and `strides` of different lengths. (#39301)\r\n\r\n### [`nn.SyncBatchNorm.convert_sync_batchnorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm): Fixed bug where it did not respect the devices of the original BatchNorm module, resulting in device mismatch errors (#39344)\r\n\r\n### [`nn.utils.clip_grad_norm_`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_): Fixed ability to operate on tensors on different devices (#38615)\r\n\r\n### [`torch.min`](https://pytorch.org/docs/stable/torch.html#torch.min), [`torch.max`](https://pytorch.org/docs/stable/torch.html#torch.max): added check for illegal output dtypes (#38850)\r\n\r\n### MacOS: Fixed `import torch` error (#36941).\r\n\r\n### C++ Extensions: fixed compilation error when building with older versions of nvcc (#37221)\r\n\r\nThis bug mainly affected users of ubuntu 16.04. We\u2019re certain it affected the following configurations:\r\n\r\n* ubuntu 16.04 + cuda 9.2 + gcc 5\r\n* ubuntu 16.04 + cuda 9.2 + gcc 7\r\n* ubuntu 16.04 + cuda 10.0 + gcc 5\r\n\r\n### C++ Extensions: fixed ability to compile with paths that include spaces (#38860, #38670)\r\n\r\n### C++ Extensions: fixed ability to compile with relative `include_dirs` for ahead-of-time compilation (#38264)\r\n\r\n\r\n\r\n# Other Fixes\r\n\r\n### [`nn.Conv1d`](https://pytorch.org/docs/stable/nn.html#conv1d), [`nn.Conv2d`](https://pytorch.org/docs/stable/nn.html#conv2d), [`nn.Conv3d`](https://pytorch.org/docs/stable/nn.html#conv3d): Fixed a bug where convolutions were using more memory than previous versions of PyTorch. (#38674)\r\n\r\n### Fixed in-place floor division magic method (#38695)\r\n\r\nIn 1.5.0, the in-place floor division magic method mistakingly performed the floor division out-of-place. We\u2019ve fixed this in 1.5.1. \r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.ones(1)\r\n>>> expected_data_ptr = tensor.data_ptr()\r\n>>> tensor //= 1\r\n>>> tensor.data_ptr() == expected_data_ptr\r\nFalse\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.ones(1)\r\n>>> expected_data_ptr = tensor.data_ptr()\r\n>>> tensor //= 1\r\n>>> tensor.data_ptr() == expected_data_ptr\r\nTrue\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Documentation: fixed link to java docs. (#39039)\r\n\r\n### Quantization: Fixed weight quantization inaccuracies for LSTM (#35961)\r\n\r\nWeight quantization was done incorrectly for LSTMs, the statistics for all weights (across layers) were combined in the observer. This meant that weights for later layers in a LSTM would use sub-optimal scales impacting accuracy. The problem gets worse as the number of layers increases.\r\n\r\n### DistributedDataParallel: Fixed single-process multi-GPU use case (#36503)\r\n\r\n### RPC: Fixed future callbacks not capturing and restoring autograd context id (#38512)\r\n\r\n### TorchScript: Fixed support with [`torch.unique`](https://pytorch.org/docs/stable/torch.html#torch.unique) (#38156)\r\n\r\n### ONNX: Fix `pow` operator export (#39791)\r\n\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.5.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.5.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.5.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/27656022", "release_id": 27656022, "date_created": "2020-06-11T22:26:46Z", "date_published": "2020-06-18T16:43:46Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/25725370", "tag": "v1.5.0", "name": "Stable C++ Frontend, Distributed RPC framework, and more. New experimental higher-level autograd API, Channels Last memory format, and more.", "author": {"name": "zou3519", "type": "User"}, "description": "# PyTorch 1.5.0 Release Notes\r\n\r\n* Highlights\r\n* Known Issues\r\n* Backwards Incompatible Changes\r\n    * Python\r\n    * C++ API\r\n    * JIT\r\n    * Quantization\r\n    * RPC\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Performance\r\n* Documentation\r\n* Deprecations\r\n    * Python\r\n    * C++ API\r\n* Miscellaneous\r\n\r\n# Highlights\r\n\r\nThis release includes several major new API additions and improvements. These include new APIs for autograd allowing for easy computation of hessians and jacobians, a significant update to the C++ frontend, \u2018channels last\u2019 memory format for more performant computer vision models, a stable release of the distributed RPC framework used for model parallel training, and a new API that allows for the creation of Custom C++ Classes that was inspired by PyBind. Additionally `torch_xla` 1.5 is now available and tested with the PyTorch 1.5 release providing a mature Cloud TPU experience. \r\n\r\n### C++ Frontend API [Now Stable]\r\n\r\nThe C++ frontend API is now at parity with Python and the features overall has been moved to \u2018stable\u2019. (previously tagged as experimental). Some of the major highlights include:\r\n\r\n* C++ torch::nn module/functional are now at ~100% parity with Python API, with appropriate documentation. Now users can easily translate their model from Python API to C++ API, making the model authoring experience much smoother.\r\n* C++ optimizers now behave identically to the Python API. In the past, optimizers in C++ had deviated from the Python equivalent: C++ optimizers couldn\u2019t take parameter groups as input while the Python ones could. Also step function implementations were not exactly the same. With the 1.5 release, C++ optimizers will always behave the same as the Python equivalent.\r\n* New C++ tensor multi-dim indexing API which looks and behaves the similar to the Python API. The previous workaround was to use a combination of `narrow` / `select` / `index_select` / `masked_select`, which is clunky and error-prone compared to the Python API\u2019s elegant `tensor[:, 0, ..., mask]` syntax. With the 1.5 release users can use `tensor.index({Slice(), 0, \"...\", mask})` to achieve the same result.\r\n\r\n### Channels last memory format for Computer Vision models [Experimental]\r\n\r\nChannels Last memory format is an alternative way of ordering NCHW tensors in memory while preserving the NCHW semantic dimensions ordering. Channels Last tensors are ordered in memory in such a way that channels become the densest dimension (aka storing images pixel-per-pixel).\r\n\r\nChannels Last memory format unlocks the ability to use performance efficient convolution algorithms and hardware (NVidia\u2019s Tensor Cores, FBGEMM, QNNPACK). Additionally it was designed to automatically propagate through the operators, which allows easy switching between memory layouts.\r\n\r\nLearn more [here](https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators) on how to write memory format aware operators.\r\n\r\n### Custom C++ Classes [Experimental]\r\n\r\nThis release adds a new API for binding custom C++ classes into TorchScript and Python simultaneously. This API is almost identical in syntax to [pybind11](https://pybind11.readthedocs.io/en/stable/). It allows users to expose their C++ class and its methods to the TorchScript type system and runtime system such that they can instantiate and manipulate arbitrary C++ objects from TorchScript and Python. An example C++ binding:\r\n\r\n```\r\ntemplate <class T>\r\nstruct MyStackClass : torch::CustomClassHolder {\r\n  std::vector<T> stack_;\r\n  MyStackClass(std::vector<T> init) : stack_(std::move(init)) {}\r\n\r\n  void push(T x) {\r\n    stack_.push_back(x);\r\n  }\r\n  T pop() {\r\n    auto val = stack_.back();\r\n    stack_.pop_back();\r\n    return val;\r\n  }\r\n};\r\n\r\nstatic auto testStack =\r\n  torch::class_<MyStackClass<std::string>>(\"myclasses\", \"MyStackClass\")\r\n      .def(torch::init<std::vector<std::string>>())\r\n      .def(\"push\", &MyStackClass<std::string>::push)\r\n      .def(\"pop\", &MyStackClass<std::string>::pop)\r\n      .def(\"size\", [](const c10::intrusive_ptr<MyStackClass>& self) {\r\n        return self->stack_.size();\r\n      });\r\n```\r\n\r\nWhich exposes a class you can use in Python and TorchScript like so:\r\n\r\n```\r\n@torch.jit.script\r\ndef do_stacks(s : torch.classes.myclasses.MyStackClass):\r\n    s2 = torch.classes.myclasses.MyStackClass([\"hi\", \"mom\"])\r\n    print(s2.pop()) # \"mom\"\r\n    s2.push(\"foobar\")\r\n    return s2 # [\"hi\", \"foobar\"]\r\n```\r\n\r\nYou can try it out in the tutorial [here](https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html).\r\n\r\n\r\n### Distributed RPC framework APIs [Now Stable]\r\n\r\nThe `torch.distributed.rpc` package aims at supporting a wide range of distributed training paradigms that do not fit into `DistributedDataParallel`. Examples include parameter server training, distributed model parallelism, and distributed pipeline parallelism. Features in the `torch.distributed.rpc` package can be categorized into four main sets of APIs.\r\n\r\n* The **RPC** API allows running a function on a specified destination worker with given arguments and fetches the return value or creates a distributed reference to the return value. \r\n* The **RRef** (Remote REFerence) serves as a reference to an object on another worker. A worker holding an RRef can explicitly request copies of the object, and it can also share the light-weight RRef with other workers without worrying about reference counting. This is especially useful when multiple workers need to repeatedly access different versions of the same remote object. \r\n* With **Distributed Autograd**, applications can automatically compute gradients even if a model is split on multiple workers using RPC. This is achieved by stitching together local autograd graphs at RPC boundaries in the forward pass and reaching out to participants to transparently launch local autograd in the backward pass. \r\n* The **Distributed Optimizer** uses gradients computed by Distributed Autograd to update model parameters. Its constructor takes a local optimizer (e.g., `SGD`, `Adagrad`, etc.) and a list of parameter RRefs, and its `step()` function automatically uses the local optimizer to update parameters on all distinct RRef owner workers.\r\n\r\nLearn more [here](https://pytorch.org/docs/stable/rpc.html).\r\n\r\n\r\n### **torch_xla 1.5 now available**\r\n\r\n[torch_xla](http://pytorch.org/xla/) is a Python package that uses the [XLA linear algebra compiler](https://www.tensorflow.org/xla) to accelerate the [PyTorch deep learning framework](https://pytorch.org/) on [Cloud TPUs](https://cloud.google.com/tpu/) and [Cloud TPU Pods](https://cloud.google.com/tpu/docs/tutorials/pytorch-pod). torch_xla aims to give PyTorch users the ability to do everything they can do on GPUs on Cloud TPUs as well while minimizing changes to the user experience. This release of [torch_xla](http://pytorch.org/xla/) is aligned and tested with PyTorch 1.5 to reduce friction for developers and to provide a stable and mature PyTorch/XLA stack for training models using Cloud TPU hardware. You can [try it for free](https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc) in your browser on an 8-core Cloud TPU device with [Google Colab](https://colab.research.google.com/), and you can use it at a much larger scale [on Google Cloud](https://cloud.google.com/gcp).\r\n\r\nSee the full torch_xla release notes [here](https://github.com/pytorch/xla/releases) and the full docs [here](https://pytorch.org/xla/).\r\n\r\n\r\n### **New High level autograd API [Experimental]**\r\n\r\nPyTorch 1.5 brings new functions including jacobian, hessian, jvp, vjp, hvp and vhp to the `torch.autograd.functional.*` submodule. This feature builds on the current API and allow the user to easily perform these functions. \r\n\r\nSee the full docs [here](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api).\r\n\r\n\r\n### Python 2 no longer supported\r\n\r\nFor PyTorch 1.5.0 we will no longer support Python 2, specifically version 2.7. Going forward support for Python will be limited to Python 3, specifically Python 3.5, 3.6, 3.7 and 3.8 (first enabled in PyTorch 1.4.0). \r\n\r\n\r\n# Known Issues\r\n\r\n### `torch.nn.parallel.DistributedDataParallel` does not work in Single-Process Multi-GPU mode.\r\n\r\n`DistributedDataParallel` (DDP) used to support two modes \r\n\r\n1. Single-Process Multi-GPU (SPMG): In this mode, each DDP process replicates the input `module` to all specified devices and trains on all `module` replicas. This mode is enabled when application passes in a `device_ids` argument that contains multiple devices. Or if `device_ids` is not presented, DDP will try to use all available devices. \r\n2. Multi-Process Single-GPU (MPSG): This is the **recommended** mode, as it is faster than SPMG. In this mode, each DDP process directly works on the provided `module` without creating additional replicas. This mode is enabled when `device_ids` only contains a single device or if there is only one visible device (e.g., by setting `CUDA_VISIBLE_DEVICES`). \r\n\r\nA recent change (#33907)  in `torch.nn.parallel.replicate` breaks DDP\u2019s assumption on replicated modules and leads to failures in the SPMG mode. However, since SPMG is known to be slower due to GIL contention and additional overhead caused by scattering input and gathering output, we are planning to retire this mode in future releases and make MPSG the only supported mode in DDP. The code below shows an example of the recommended way to construct DDP.\r\n\r\n\r\n```\r\nimport torch\r\nfrom torch.nn.parallel import DistributedDataParallel as DDP\r\n\r\n# use \"cuda:1\" as the target device\r\ntarget_device = 1 \r\nlocal_model = torch.nn.Linear(2, 2).to(target_device)\r\nddp_model = DDP(local_model, device_ids=[target_device])\r\n```\r\n\r\n\r\nSee [#36268](https://github.com/pytorch/pytorch/issues/36268) for more discussion.\r\n\r\n\r\n### `Tensor.exponential_(0)` used to return `Inf`, now it incorrectly returns `0` \r\n\r\nPreviously in 1.4, `x.exponential_(0)` gives a tensor full of `inf`. On 1.5.0, it wrongly gives a tensor full of zeros.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.randn(3).exponential_(0)\r\ntensor([inf, inf, inf])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.randn(3).exponential_(0)\r\n# This is wrong!\r\ntensor([0., 0., 0.])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nSee [#36798](https://github.com/pytorch/pytorch/issues/36798) for more details\r\n\r\n\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## Python\r\n\r\n### `Tensor.clone`, `Tensor.to`, `Tensor.empty_like`, and similar functions preserve stride information instead of returning contiguous tensors\r\n\r\n`clone`, `to`, `type`, `cuda`, `cpu`, `byte`, `char`, `double`, `bool`, `half`, `int`, `long`, `short`, `float`, `bfloat16`, `empty_like`, `full_like`, `ones_like`, `zeros_like`, `rand_like`, `randn_like`, `randint_like` operators now propagate memory format (roughly, the strides) of the input tensor to the output tensor.\r\n\r\nSince PyTorch operators generally support non-contiguous tensors, this should have no functional effect on most PyTorch programs.\r\n\r\nThe most common incompatibility with Python programs is with the `view` operator, which has specific stride requirements. If these requirements are no longer met as a result of this change, you will get an error message indicating that you should use reshape instead, i.e. \"RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\"\r\n\r\nAnother possible exception incompatibility is if you have a (usually) C++ operator implementation that works directly on memory (i.e. calls data_ptr and relies on the strides being contiguous).\r\n\r\n In the following example, we go through the implementation of a simple `clone` operation and see how it needs to change between versions.\r\n\r\n```\r\n# Version 1.4.0\r\nTensor simple_clone(const Tensor& input) {\r\n    TORCH_CHECK(input.dim() == 1);\r\n    auto output = at::empty_like(input);\r\n    auto input_stride = input.strides()[0];\r\n    auto* output_ptr = output.data_ptr<float>();\r\n    auto* input_ptr = input.data_ptr<float>();\r\n    // Before 1.5.0, the result of `empty_like` is always contiguous.\r\n    for (int64_t idx = 0; idx < input.size(); idx++) {\r\n        output[idx] = input[idx * input_stride]\r\n    }\r\n}\r\n```\r\n\r\n```\r\n# Version 1.5.0\r\nTensor simple_clone(const Tensor& input) {\r\n    TORCH_CHECK(input.dim() == 1);\r\n    // From 1.5.0 on, the result of `empty_like` may not be contiguous.\r\n    auto output = at::empty_like(input);\r\n    \r\n    // As a result, we need to keep track of the output stride.\r\n    auto input_stride = input.strides()[0];\r\n    auto output_stride = output.strides()[0];\r\n    auto* output_ptr = output.data_ptr<float>();\r\n    auto* input_ptr = input.data_ptr<float>();\r\n    for (int64_t idx = 0; idx < input.size(); idx++) {\r\n        output[idx * output_stride] = input[idx * input_stride]\r\n    }\r\n}\r\n\r\n```\r\n\r\n\r\n### The inferred dtype of np.float_, np.float64 scalars in tensor constructors (e.g. torch.tensor(...), torch.as_tensor(...) is now torch.float64 instead of the default dtype (usually torch.float32). (#30486 (https://github.com/pytorch/pytorch/pull/30486))\r\n\r\nPlease explicitly pass in the desired dtype when constructing tensors with NumPy float64 scalars to get the old behavior.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# Old behavior: return torch.float32 tensor (by default)\r\n>>> torch.tensor(np.float64(0))\r\ntensor(0.)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# To keep the old behavior, please explicitly pass the dtype\r\n >>> torch.tensor(np.float64(0), dtype=torch.get_default_dtype())\r\ntensor(0.)\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThis can cause your program to execute in torch.float64, potentially slowing down your program or can lead to errors for operators that don't support torch.float64 or mixed-dtypes.\r\n\r\nnumpy integer scalars are now treated as integers for the purposes of type promotion (#30486 (https://github.com/pytorch/pytorch/pull/30486))\r\n\r\nPreviously, in 1.4.0, they were mistakenly treated as floats (so for example, torch.ones(3) * np.int64(3) would return a float32 tensor. In 1.5.0, we\u2019ve fixed that behavior; torch.ones(3) * np.int64(3) returns an int32 tensor.\r\n\r\nThis can cause your code to fail if you performed operations between PyTorch tensors and numpy scalars and then passed the result into an operation that does not support integral types or mixed types. To fix your code, please cast the resulting tensor to the desired dtype.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.ones(3) * np.int64(3)\r\ntensor([3., 3., 3.])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> (torch.ones(3) * np.int64(3)).float()\r\ntensor([3., 3., 3.])\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n###  numpy integer scalars are now treated as integers for the purposes of type promotion (#30486)\r\n\r\nPreviously, in 1.4.0, they were mistakenly treated as floats (so for example, `torch.ones(3) * np.int64(3)` would return a float32 tensor. In 1.5.0, we\u2019ve fixed that behavior; `torch.ones(3) * np.int64(3)` returns an int32 tensor.\r\n\r\nThis can cause your code to fail if you performed operations between PyTorch tensors and numpy scalars and then passed the result into an operation that does not support integral types or mixed types. To fix your code, please cast the resulting tensor to the desired dtype.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.ones(3) * np.int64(3)\r\ntensor([3., 3., 3.])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> (torch.ones(3) * np.int64(3)).float()\r\ntensor([3., 3., 3.])\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `torch.autograd.Function`: dropped support for old-style Functions ([#33956](https://github.com/pytorch/pytorch/pull/33956)).\r\n\r\nIn previous versions of PyTorch, there were two ways to write autograd Functions. We deprecated one of them in 1.3.0 and dropped support for it entirely in 1.5.0. Old-style autograd Functions will no longer work in user code.\r\n\r\nThese Functions be identified by not having `staticmethod` `forward` and `backward` functions (see the example below) Please see [the current documentation](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) for how to write new-style Functions.\r\n\r\n```\r\n# Version 1.4.0\r\nclass Exp(torch.autograd.Function):\r\n    def forward(self, i):\r\n        result = i.exp()\r\n        self.save_for_backward(result)\r\n        return result\r\n\r\n    def backward(self, grad_output):\r\n        result, = self.saved_tensors\r\n        return grad_output * result\r\n\r\nExp()(torch.tensor(1.))\r\n```\r\n\r\n```\r\n# Version 1.5.0\r\nclass Exp(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, i):\r\n        result = i.exp()\r\n        ctx.save_for_backward(result)\r\n        return result\r\n        \r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        result, = ctx.saved_tensors\r\n        return grad_output * result\r\n\r\nExp.apply(torch.tensor(1.))   \r\n```\r\n\r\n\r\n\r\n### `torch.optim` optimizers changed to fix in-place checks for the changes made by the optimizer  ([#33640](https://github.com/pytorch/pytorch/pull/33640), [#34211](https://github.com/pytorch/pytorch/pull/34211))\r\n\r\nIf this causes your code to fail, there are two possible reasons:\r\n\r\nReason 1: The value of that parameter was actually saved and used and we were computing incorrect gradients in previous versions of PyTorch. This would result in an error message mentioning incorrect version numbers. You should replace code that uses `self.my_param` by `self.my_param.clone()` to make sure the saved version is different from the one that is modified by the optimizer. For example:\r\n\r\nBefore 1.5.0, the following may have worked.\r\n\r\n```\r\ndef model(input, target, param):\r\n    return `(input * param ** 2 - target).norm()`\r\n\r\nparam = torch.randn(2, requires_grad=True)\r\ninput = torch.randn(2)\r\ntarget = torch.randn(2)\r\nsgd = optim.SGD([param], lr=0.001)\r\nloss = model(input, target, param)\r\nloss.backward(retain_graph=True)\r\nsgd.step()\r\nloss.backward()\r\nparam.grad\r\n```\r\n\r\n\r\nIf after upgrading to 1.5.0, the above fails due to a version counter error, then that means the gradient computed was incorrect. To remedy this, clone `param` before using it in the model:\r\n\r\n```\r\ndef model(input, target, param):\r\n    return (input * param ** 2 - target).norm()\r\n\r\nparam = torch.randn(2, requires_grad=True)\r\ninput = torch.randn(2)\r\ntarget = torch.randn(2)\r\nsgd = optim.SGD([param], lr=0.001)\r\nloss = model(input, target, param.clone())\r\nloss.backward(retain_graph=True)\r\nsgd.step()\r\nloss.backward()\r\nparam.grad\r\n```\r\n\r\n\r\nReason 2: You know what you're doing and change the values back to the right thing before the next backward. However, you're running into an error because the version counter cannot be decremented. Open an issue with your particular use case and we will help you to work around the version counter issue.\r\n\r\n\r\n### `utils.cpp_extensions` now use `ninja` as the default compilation backend ([#32495](https://github.com/pytorch/pytorch/pull/32495))\r\n\r\n`ninja` enables parallel compilation of your C++ extension, greatly speeding up compilation. This change will not break most user code; if you do not have `ninja` installed, we fallback to the old `distutils` backend.\r\n\r\nHowever, if you do have `ninja` installed, it is possible that this change will cause your C++ extension build to fail by oversubscribing your system with too many worker processes. There are two potential workarounds to this.\r\n\r\nMethod 1: If a previously succeeding `python setup.py install` now fails, try setting the `MAX_JOBS` environment variable.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"sh\">\r\npython setup.py install\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"sh\">\r\nMAX_JOBS=2 python setup.py install\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nMethod 2: Switch back to the old `distutils` backend inside your `setup.py`\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ncmdclass={'clean': clean,\r\n          'build_ext': BuildExtension},\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ncmdclass={'clean': clean,\r\n          'build_ext': BuildExtension.with_options(use_ninja=False)},\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `torch.optim.Adam`, `torch.optim.SGD`  changed to not modify gradients in-place ([#30257](https://github.com/pytorch/pytorch/pull/30257))\r\n\r\nIn previous versions of PyTorch, the Adam and SGD optimizers modified gradients (e.g. `param.grad`) in-place via in-place addition of `params.grad += weight_decay * param`. To make this consistent with the behavior of other optimizers and to prevent surprises about the behavior, we\u2019ve changed them to stop modifying gradients in-place.\r\n\r\nThis should not have an effect on most PyTorch programs unless they relied on this behavior. The easiest way to replicate the old behavior is to create a custom optimizer that implements it.\r\n\r\n\r\n###  `torch.masked_select` now always returns a 1D tensor ([#29923](https://github.com/pytorch/pytorch/pull/29923))\r\n\r\nThe behavior of `torch.masked_select` when both \"self\" and \"mask\" are 0-dimensional was changed. In previous versions of PyTorch, this would return a 0-dimensional tensor. Now, we return a 1-dimensional tensor to be consistent with other input sizes and our documentation.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.masked_select(torch.tensor(0), torch.tensor(True))\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.masked_select(torch.tensor(0), torch.tensor(True))\r\ntensor([0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `torch.index_select` on a 0-d tensor now returns a 0-d tensor. ([#30790](https://github.com/pytorch/pytorch/pull/30790))\r\n\r\nIn previous versions of PyTorch, the output of `torch.index_select` on a 0D input tensor produced a 1D tensor. This was inconsistent with our documentation on it, which stated \"The returned tensor has the same number of dimensions as the original tensor (input).\" Now, we return a 0D tensor.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.index_select(torch.tensor(5), 0, torch.tensor([0]))\r\ntensor([5])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.index_select(torch.tensor(5), 0, torch.tensor([0]))\r\ntensor(5)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `nn.MultiLabelMarginLoss:` 'none' reduction on 1D tensor now returns a 0D tensor ([#30768](https://github.com/pytorch/pytorch/pull/30768))\r\n\r\nIn previous versions of PyTorch, the output of `nn.MultiLabelMarginLoss` on 1D and 0D tensors incorrectly produced 1-D tensors. Now, those cases return a 0D tensor to be consistent with the 2-D tensor case.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiLabelMarginLoss(reduction='none')(torch.randn(3), torch.zeros(3, dtype=torch.long))\r\ntensor([0.2959])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiLabelMarginLoss(reduction='none')(torch.randn(3), torch.zeros(3, dtype=torch.long))\r\ntensor(0.2959)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `nn.MultiMarginLoss:` \u2018none' reduction on 1D target now returns a 1D tensor ([#30826](https://github.com/pytorch/pytorch/pull/30826))\r\n\r\nIn previous versions of PyTorch, the output of `nn.MultiMarginLoss` on a 1D `target` tensor produced a 0D output. We changed this to return a 1D `target` tensor to make it consistent with other input sizes which return an output that matches the target shape.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiMarginLoss(reduction='none')(torch.tensor([1.]), torch.tensor([0]))\r\ntensor(0.)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiMarginLoss(reduction='none')(torch.tensor([1.]), torch.tensor([0]))\r\ntensor([0.])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `Tensor.exponential_(lambda)` no longer supports `lambda < 0` ([#32501](https://github.com/pytorch/pytorch/pull/32501))\r\n\r\n`lambda`, the rate parameter of the exponential distribution, mathematically should be greater than 0. We\u2019ve disabled support `lambda < 0` to be mathematically correct; most users will not have used a lambda less than zero.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntensor = torch.empty(3).exponential_(-1.5)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# Negative lambda not supported!\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `nn.BCELoss`, `nn.functional.binary_cross_entropy` no longer accept inputs with the same number of elements that are not broadcastable ([#31365](https://github.com/pytorch/pytorch/pull/31365))\r\n\r\nPreviously, we supported accepting inputs with the same number of elements. However, this behavior was deprecated and we removed it in 1.5.0. In order to replicate the old behavior, please explicitly `reshape` your input and target tensors to have the same shape.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.rand(3, 3)\r\n>>> target = torch.randn(9)\r\n>>> torch.nn.functional.binary_cross_entropy(input, target)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.rand(3, 3)\r\n>>> target = torch.randn(9)\r\n>>> torch.nn.functional.binary_cross_entropy(input, target.reshape_as(input))\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `torch.normal` out argument is now required to have the same size as the computed output ([#32031](https://github.com/pytorch/pytorch/pull/32031))\r\n\r\nPreviously, on CPU devices, `torch.normal(mean, std, out=out)`  would resize `out` to the correct size. To be consistent with the CUDA implementation, we\u2019ve changed it so that `out` must either already have the correct size, or be an empty tensor with size `[0]`. To work around this, please ensure that your `out` tensor has the correct size.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.normal(torch.zeros(3), torch.ones(3), out=torch.randn(2))\r\ntensor([ 0.0300,  0.7830, -1.3579])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.normal(torch.zeros(3), torch.ones(3), out=torch.randn(2))\r\nRuntimeError: inconsistent tensor, output size ([2]) is not the same as broadcasted mean and std size (3)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### `Tensor.geometric_` no longer supports integral Tensors ([#31878](https://github.com/pytorch/pytorch/pull/31878))\r\n\r\nPreviously, on CPU devices, `Tensor.geometric_` supported Tensors with integral dtype. Now, it only supports floating point. We removed support for this because it doesn\u2019t make sense for `geometric_` to operate on integral dtypes.\r\n\r\n\r\n### Changed `torch.floor_divide` `input` positional argument name to `self`  ([#34552](https://github.com/pytorch/pytorch/pull/34552))\r\n\r\nBefore PyTorch 1.5, `torch.floor_divide` took two positional arguments: `torch.floor_divide(input, other)`. We\u2019ve changed the name of the `input` argument to `self`; this will break code that called `torch.floor_divide` via keyword argument. For example:\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.floor_divide(input=x, other=y)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# Either of the following works.\r\ntorch.floor_divide(self=x, other=y)\r\ntorch.floor_divide(x, y)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n\r\n## C++ API\r\n\r\n### RNN / GRU / LSTM layers ([#34322](https://github.com/pytorch/pytorch/pull/34322))\r\n\r\n* Instead of returning `RNNOutput`, RNN / GRU `forward` method now returns `std::tuple<Tensor, Tensor>`, and LSTM `forward` method now returns `std::tuple<Tensor, std::tuple<Tensor, Tensor>>`, matching Python API.\r\n* LSTM forward method\u2019s hidden state parameter now has type `torch::optional<std::tuple<Tensor, Tensor>>`, matching Python API.\r\n* RNN / LSTM / GRU layers now have `forward_with_packed_input` method which accepts `PackedSequence` as input and optionally hidden state, matching the `forward(PackedSequence, ...)` variant in Python API.\r\n* RNN / LSTM / GRU layers no longer have these fields: `w_ih` / `w_hh` / `b_ih` / `b_hh`. Instead, to access the weights and biases of the gates, users should do e.g. `rnn->named_parameters()[\"weight_ih_l0\"]`, which mirrors the Python API `rnn.weight_ih_l0`.\r\n* In `RNNOptions`\r\n    * `tanh()` / `relu()` / `activation` are removed. Instead, `nonlinearity` is added which takes either `torch::kTanh` or `torch::kReLU`\r\n    * `layers` is renamed to `num_layers`\r\n    * `with_bias` is renamed to `bias`\r\n* In `LSTMOptions`\r\n    * `layers` is renamed to `num_layers`\r\n    * `with_bias` is renamed to `bias`\r\n* In `GRUOptions`\r\n    * `layers` is renamed to `num_layers`\r\n    * `with_bias` is renamed to `bias`\r\n\r\n\r\n\r\n### Upsample layer / F::interpolate function ([#35025](https://github.com/pytorch/pytorch/pull/35025))\r\n\r\n* There are changes to `UpsampleOptions` and `InterpolateFuncOptions`:\r\n    * `size` is changed from `std::vector<int64_t>` to `c10::optional<std::vector<int64_t>>`. If you want to pass a list of `int64_t` to this argument, you must pass it as `std::vector<int64_t>`.\r\n    * `scale_factor` is changed from `std::vector<double>` to `c10::optional<std::vector<double>>`. If you want to pass a list of `double` to this argument, you must pass it as `std::vector<double>`.\r\n* F::multilabel_margin_loss / F::multilabel_soft_margin_loss functions ([#35163](https://github.com/pytorch/pytorch/pull/35163))\r\n* `torch::nn::functional::MultiLabelMarginLossFuncOptions` is renamed to `torch::nn::functional::MultilabelMarginLossFuncOptions`\r\n* `torch::nn::functional::MultiLabelSoftMarginLossFuncOptions` is renamed to `torch::nn::functional::MultilabelSoftMarginLossFuncOptions`\r\n* The deprecated `torch::nn::BatchNorm` is removed in favor of `torch::nn::BatchNorm{1,2,3}d`\r\n* The deprecated `torch::nn::FeatureDropout` is removed in favor of `torch::nn::Dropout{2,3}d` \r\n* The deprecated `torch::nn::modules_ordered_dict` is removed. User should do `Sequential sequential({{\"m1\", MyModule(1)}, {\"m2\", MyModule(2)}})` instead.\r\n* The deprecated `torch::nn::init::Nonlinearity` is removed, in favor of these enums: `torch::kLinear `/ `torch::kConv1D` / `torch::kConv2D` / `torch::kConv3D` / `torch::kConvTranspose1D` / `torch::kConvTranspose2D` / `torch::kConvTranspose3D` / `torch::kSigmoid` / `torch::kTanh` / `torch::kReLU` / `torch::kLeakyReLU`\r\n* The deprecated `torch::nn::init::FanMode` is removed, in favor of these enums: `torch::kFanIn` / `torch::kFanOut`\r\n\r\n\r\n\r\n### Optimizers\r\n\r\n* `Optimizer::step` now accepts closure function as optional input and returns a tensor, and `LossClosureOptimizer` is removed (#34790) (#34957). If you had a custom optimizer class defined as:\r\n\r\n```\r\nstruct MyOptimizer : Optimizer {\r\n  using Optimizer::Optimizer;\r\n  void step() override {...}\r\n};\r\n```\r\n\r\n    * you would need to update your optimizer class definition as follows:\r\n\r\n```\r\nstruct MyOptimizer : Optimizer {\r\n  using Optimizer::Optimizer;\r\n  torch::Tensor step(LossClosure closure = nullptr) override {\r\n    ...\r\n    // return `torch::Tensor()` if `closure` is nullptr\r\n    // (i.e. we are not computing the loss)\r\n    return torch::Tensor();\r\n  }\r\n};\r\n```\r\n\r\n* Adagrad ([#29335](https://github.com/pytorch/pytorch/pull/29335))\r\n    * In `AdagradOptions`, `learning_rate` is renamed to `lr`.\r\n    * In `Adagrad`, `sum_buffers` and `step_buffers` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<AdagradParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.sum()\r\n// param_state.step()\r\n```\r\n\r\n* SGD ([#32592](https://github.com/pytorch/pytorch/pull/32592))\r\n    * In `SGDOptions`, `learning_rate` is renamed to `lr`.\r\n    * In `SGD`, `momentum_buffers` is now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<SGDParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.momentum_buffer()\r\n```\r\n\r\n* Adam ([#33730](https://github.com/pytorch/pytorch/pull/33730))\r\n    * In `AdamOptions`:\r\n        * `learning_rate` is renamed to `lr`\r\n        * `beta1` and `beta2` are replaced by a tuple `betas`\r\n    * In `Adam`, `step_buffers`, `exp_average_buffers`, `exp_average_sq_buffers` and `max_exp_average_sq_buffers` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<AdamParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.step()\r\n// param_state.exp_avg()\r\n// param_state.exp_avg_sq()\r\n// param_state.max_exp_avg_sq()\r\n```\r\n\r\n* RMSprop ([#33450](https://github.com/pytorch/pytorch/pull/33450))\r\n    * In `RMSpropOptions`:\r\n        * `learning_rate` is renamed to `lr`\r\n    * In `RMSprop`, `square_average_buffers`, `momentum_buffers` and `grad_average_buffers` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<RMSpropParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.square_avg()\r\n// param_state.momentum_buffer()\r\n// param_state.grad_avg()\r\n```\r\n\r\n* LBFGS ([#34564](https://github.com/pytorch/pytorch/pull/34564)) ([#34957](https://github.com/pytorch/pytorch/pull/34957))\r\n\r\n    * In `LBFGSOptions`:\r\n        * `learning_rate` is renamed to `lr`\r\n        * `max_eval`\u2018s type is changed from `int64_t` to `c10::optional<int64_t>`\r\n        * `tolerance_grads type` is changed from `float` to `double`\r\n        * `tolerance_change type` is changed from `float` to `double`\r\n        * `history_size type` is changed from `size_t` to `int64_t`\r\n    * In `LBFGS`, `d`, `H_diag`, `prev_flat_grad`, `t`, `prev_loss`, `ro`, `al`, `old_dirs`, `old_stps`, `func_evals` and `state_n_iter` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<LBFGSParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.d()\r\n// param_state.H_diag()\r\n// param_state.prev_flat_grad()\r\n// param_state.t()\r\n// param_state.prev_loss()\r\n// param_state.ro()\r\n// param_state.al()\r\n// param_state.old_dirs()\r\n// param_state.old_stps()\r\n// param_state.func_evals()\r\n// param_state.n_iter()\r\n```\r\n\r\n\r\n\r\n### Removed `AutoGIL/AutoNoGIL` in favor of `pybind11::gil_scoped_*` functions (#[34301](https://github.com/pytorch/pytorch/pull/34301))\r\n\r\nIf your code released or acquired the GIL via AutoNoGIL or AutoGIL, please change the invocations to `pybind11::gil_scoped_release` or `pybind11::gil_scoped_release`, respectively.\r\n\r\n\r\n### Others\r\n\r\n* `torch::tensor(floating-point values)` will always produce tensor of default dtype, and `torch::tensor(integer values)` will always produce tensor of `torch::kLong` dtype, matching Python API behavior ([#32367](https://github.com/pytorch/pytorch/pull/32367)).\r\n* `torch::Tensor::base()` is renamed to `torch::Tensor::_base()` , matching Python API. (#33316)\r\n* Renamed TensorTypeId to DispatchKey ([#32154](https://github.com/pytorch/pytorch/pull/32154))\r\n* Throw an error if nbytes is called on a sparse tensor. ([#33897](https://github.com/pytorch/pytorch/pull/33897))\r\n\r\n\r\n\r\n## JIT \r\n\r\n### Simple Executor Is Now On By Default\r\n\r\nThe simple executor skips the number of fusion-related passes and analyses that are very time-consuming. Disabling these optimizations fixes pathologically long compilation times. The users that rely on GPU fusion to have their desired performance profile, should turn on the profiling executor. We provide C++ and python API to enable the profiling executor:\r\n\r\n* in python, call `torch._C._jit_set_profiling_mode(True)` before you call your model for the first time.\r\n* in C++, include `#include <torch/csrc/jit/runtime/graph_executor.h>` and set `getProfilingMode() = true` before you invoke your model for the first time.\r\n\r\n\r\n\r\n## Quantization\r\n\r\n### **Remove qconfig_dict in top level eager mode quantization API** ([#31972](https://github.com/pytorch/pytorch/pull/31972)).\r\n\r\nIn eager mode quantization, one needs to manually insert quant and dequant stubs in a model to specify where activations are quantized. Having a qconfig_dict that specifies the quantization configuration for each module is not useful as one needs to manually modify the model with quant/dequant stubs. The new API makes it explicit that the model needs to be manually modified for quantization.\r\n\r\n```\r\n# previously qconfig_dict was an optional argument to prepare\r\ndef prepare(model, qconfig_dict=None, inplace=False):\r\n\r\n# now replaced with\r\ndef prepare(model, inplace=False):\r\n```\r\n\r\n## RPC\r\n\r\n### Functional API for Distributed Autograd and Distributed Optimizer\r\n\r\nMore specifically, callers must pass `context_id` to `torch.distributed.autograd.backward()` and `torch.distributed.optim.step()`.  ([#33711](https://github.com/pytorch/pytorch/pull/33711))\r\n\r\n\r\n```\r\n# Before\r\nimport torch.distributed.autograd as dist_autograd\r\nimport torch.distributed.rpc as rpc\r\nfrom torch import optim\r\nfrom torch.distributed.optim import DistributedOptimizer\r\n\r\nwith dist_autograd.context() as context_id:\r\n    # Forward pass.\r\n    rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n    rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\r\n    loss = rref1.to_here() + rref2.to_here()\r\n    # Backward pass.\r\n    dist_autograd.backward([loss.sum()])\r\n    # Optimizer.\r\n    dist_optim = DistributedOptimizer(\r\n        optim.SGD,\r\n        [rref1, rref2],\r\n        lr=0.05,\r\n    )\r\n```\r\n```\r\n# After\r\nimport torch.distributed.autograd as dist_autograd\r\nimport torch.distributed.rpc as rpc\r\nfrom torch import optim\r\nfrom torch.distributed.optim import DistributedOptimizer\r\n\r\nwith dist_autograd.context() as context_id:\r\n    # Forward pass.\r\n    rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n    rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\r\n    loss = rref1.to_here() + rref2.to_here()\r\n    # Backward pass.\r\n    dist_autograd.backward(context_id, [loss.sum()])\r\n    # Optimizer.\r\n    dist_optim = DistributedOptimizer(\r\n        optim.SGD,\r\n        [rref1, rref2],\r\n        lr=0.05,\r\n    )\r\n    \r\n    dist_optim.step(context_id)    \r\n```\r\n\r\n### Disallow sending CUDA tensors over RPC\r\n\r\nThe motivation is to prevent potential invalid device errors when the number of devices on the sender and the receiver does not match. However applications, can always move CUDA tensors to CPU before sending (#33604).\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.distributed.rpc as rpc\r\nrpc.init_rpc(\"worker0\", rank=0, world_size=2)\r\nx = torch.zeros(2, device=0)\r\nret = rpc.rpc_sync(\"worker1\", torch.add, args=(x, 3))\r\nrpc.shutdown()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.distributed.rpc as rpc\r\nrpc.init_rpc(\"worker0\", rank=0, world_size=2)\r\nx = torch.zeros(2, device=0)\r\nret = rpc.rpc_sync(\"worker1\", torch.add, args=(x.cpu(), 3))\r\nrpc.shutdown()\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n# New Features\r\n\r\n## Python\r\n\r\n### Added new functional autograd API ([#34066](https://github.com/pytorch/pytorch/pull/34066))\r\n\r\n* See Highlights for more details\r\n\r\n\r\n### New `__torch_function__` API Override Mechanism ([#30730](https://github.com/pytorch/pytorch/pull/30730), [#32194](https://github.com/pytorch/pytorch/pull/32194), [#32799](https://github.com/pytorch/pytorch/pull/32799), [#34240](https://github.com/pytorch/pytorch/pull/34240), [#34303](https://github.com/pytorch/pytorch/pull/34303)).\r\n\r\nWe introduced `__torch_function__`, an API override mechanism for subclassing `torch.Tensor` in Python. This is useful for creating custom objects that implement the `torch.*` APIs. These currently support overriding most `torch.*`, and `torch.nn.functional` APIs; we\u2019ve also planned future support for subclassing `torch.Tensor` (see tracking issue [#22402](https://github.com/pytorch/pytorch/issues/22402)).\r\n\r\n\r\n## New Operators\r\n\r\n* `torch.logical_and` and `torch.logical_or` operations added ([#30521](https://github.com/pytorch/pytorch/pull/30521)).\r\n* `torch.square` added ([#30719](https://github.com/pytorch/pytorch/pull/30719)).\r\n* `torch.bitwise_and` added ([#31104](https://github.com/pytorch/pytorch/pull/31104)).\r\n* `torch.cummax`, `torch.cummin` added ([#32169](https://github.com/pytorch/pytorch/pull/32169), [#32238](https://github.com/pytorch/pytorch/pull/32238), [#32537](https://github.com/pytorch/pytorch/pull/32537), [#33492](https://github.com/pytorch/pytorch/pull/33492)).\r\n* `torch.floor_divide` ,  `Tensor.floor_divide` added ([#30493](https://github.com/pytorch/pytorch/pull/30493), [#34552](https://github.com/pytorch/pytorch/pull/34552)).\r\n* `torch.true_divide` , `Tensor.true_divide` added, analogous to Python 's, and NumPy's (true) division ([#34236](https://github.com/pytorch/pytorch/pull/34236), [#34794](https://github.com/pytorch/pytorch/pull/34794))\r\n* `nn.functional.hardsigmoid` added([#34545](https://github.com/pytorch/pytorch/pull/34545)).\r\n* Added PCA and SVD for low-rank matrices (`torch.pca_lowrank`,  `torch.svd_lowrank`), `torch.lobpcg` for positive-defined generalized eigenvalue problem ([#34721](https://github.com/pytorch/pytorch/pull/34721)).\r\n\r\n\r\n\r\n## Distributions\r\n\r\n* `distributions.von_mises` added ([#33418](https://github.com/pytorch/pytorch/pull/33418)).\r\n* `distributions.mixture_same_family` : Added support for mixture distributions ([#22742](https://github.com/pytorch/pytorch/pull/22742), [#33408](https://github.com/pytorch/pytorch/pull/33408)).\r\n* `distributions.transforms.TanhTransform`  added([#19785](https://github.com/pytorch/pytorch/pull/19785)).\r\n* `distributions.continuous_bernoulli` added ([#34619](https://github.com/pytorch/pytorch/pull/34619)).\r\n\r\n\r\n\r\n## C++ API\r\n\r\n* NN modules / functionals\r\n    * `torch::nn::MultiheadAttention` ([#27309](https://github.com/pytorch/pytorch/pull/27309))\r\n    * `torch::nn::RNNCell` / `LSTMCell` / `GRUCell` ([#34400](https://github.com/pytorch/pytorch/pull/34400))\r\n    * `torch::nn::AdaptiveLogSoftmaxWithLoss` ([#29076](https://github.com/pytorch/pytorch/pull/29076)).\r\n    * `torch::nn::utils::rnn::PackedSequence` / `pack_padded_sequence` / `pad_packed_sequence` / `pack_sequence` / `pad_sequence` ([#32387](https://github.com/pytorch/pytorch/pull/32387), [#33652](https://github.com/pytorch/pytorch/pull/33652), [#34185](https://github.com/pytorch/pytorch/pull/34185))\r\n* C++ tensor indexing ([#30424](https://github.com/pytorch/pytorch/pull/30424), [#32841](https://github.com/pytorch/pytorch/pull/32841), [#30427](https://github.com/pytorch/pytorch/pull/30427), [#34255](https://github.com/pytorch/pytorch/pull/34255))\r\n    * Please see docs: https://pytorch.org/cppdocs/notes/tensor_indexing.html\r\n* Operators\r\n    * C++ API parity: `isinf` ([#31099](https://github.com/pytorch/pytorch/pull/31099)).\r\n* Autograd\r\n    * Add `at::Tensor::retain_grad` API ([#33349](https://github.com/pytorch/pytorch/pull/33349)).\r\n* C++ extensions\r\n    * Add option to use ninja to compile ahead-of-time `cpp_extensions` (#32495, [#33084](https://github.com/pytorch/pytorch/pull/33084))\r\n    * Added support for Pytorch C++ extensions to use HIP ([#32669](https://github.com/pytorch/pytorch/pull/32669)).\r\n\r\n\r\n\r\n## Distributed\r\n\r\n* Allows Python application to create subclass of C++ `c10d.Store` using pybind11 trampoline class  [#30415](https://github.com/pytorch/pytorch/pull/30415).\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Loading module from android asset ([#30378](https://github.com/pytorch/pytorch/pull/30378)).\r\n* Torchscript print to logcat ([#31456](https://github.com/pytorch/pytorch/pull/31456)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* qnnpack TanH ([#31013](https://github.com/pytorch/pytorch/pull/31013)).\r\n* Adding quantized clamp kernel ([#30541](https://github.com/pytorch/pytorch/pull/30541)).\r\n* Quantized H Tangent function ([#31031](https://github.com/pytorch/pytorch/pull/31031)).\r\n* QNNPACK: Add support for dynamic quantization. ([#31896](https://github.com/pytorch/pytorch/pull/31896)).\r\n* Add operator support for dynamic quant on mobile ([#32479](https://github.com/pytorch/pytorch/pull/32479)).\r\n* Adding native qconcat ([#32252](https://github.com/pytorch/pytorch/pull/32252)).\r\n* FP16 dynamic quantized Linear ([#32331](https://github.com/pytorch/pytorch/pull/32331)).\r\n* Add support for Dynamic LSTM quantization on Mobile ([#32757](https://github.com/pytorch/pytorch/pull/32757)).\r\n* Quantized sigmoid function ([#31851](https://github.com/pytorch/pytorch/pull/31851)).\r\n* Quantized leaky relu ([#33004](https://github.com/pytorch/pytorch/pull/33004)).\r\n* Add a quantized batch_norm operator ([#33080](https://github.com/pytorch/pytorch/pull/33080)).\r\n* Add Quantized BatchNorm2d module ([#33109](https://github.com/pytorch/pytorch/pull/33109)).\r\n* Add the 3d avg pool for video related model ([#33339](https://github.com/pytorch/pytorch/pull/33339)).\r\n* Add quantized_hardtanh ([#34097](https://github.com/pytorch/pytorch/pull/34097)).\r\n* Add quantized ELU activation ([#34267](https://github.com/pytorch/pytorch/pull/34267)).\r\n* Add the 3d upsample quantized op for video model ([#34594](https://github.com/pytorch/pytorch/pull/34594)).\r\n* Add the quantized batch_norm3d and also batch_norm3d fused with relu operators ([#34702](https://github.com/pytorch/pytorch/pull/34702)).\r\n* Add quantized implementation of hard sigmoid ([#34607](https://github.com/pytorch/pytorch/pull/34607)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* [Experimental] Enable autograd profiler to work with RPC ([#31381](https://github.com/pytorch/pytorch/pull/31381), [#34398](https://github.com/pytorch/pytorch/pull/34398), [#30677](https://github.com/pytorch/pytorch/pull/30677), [#31346](https://github.com/pytorch/pytorch/pull/31346), [#31380](https://github.com/pytorch/pytorch/pull/31380)). \r\n* [Experimental] Allow calling remote TorchScript functions using RPC ([#32466](https://github.com/pytorch/pytorch/pull/32466), [#33190](https://github.com/pytorch/pytorch/pull/33190), [#32990](https://github.com/pytorch/pytorch/pull/32990), [#32959](https://github.com/pytorch/pytorch/pull/32959),  [#33526](https://github.com/pytorch/pytorch/pull/33526), [#33992](https://github.com/pytorch/pytorch/pull/33992), [#33582](https://github.com/pytorch/pytorch/pull/33582), [#32197](https://github.com/pytorch/pytorch/pull/32197), [#33329](https://github.com/pytorch/pytorch/pull/33329), [#34183](https://github.com/pytorch/pytorch/pull/34183)).\r\n\r\n\r\n\r\n# Improvements\r\n\r\n## AMD/ROCm\r\n\r\n*  `nn.RNN`: Ensure MIOpen is called on same stream as operator ([#30672](https://github.com/pytorch/pytorch/pull/30672))\r\n* Fixed asserts in CUDA kernels ([#31276](https://github.com/pytorch/pytorch/pull/31276), [#31297](https://github.com/pytorch/pytorch/pull/31297)).\r\n* Enable BFloat16 support for convolutions ([#30948](https://github.com/pytorch/pytorch/pull/30948)).\r\n* Abstract atomic add calls ([#31992](https://github.com/pytorch/pytorch/pull/31992)).\r\n* Install complete set of headers for ROCm build ([#32076](https://github.com/pytorch/pytorch/pull/32076)).\r\n* Adjust `elementwise_kernel` settings on ROCm ([#32609](https://github.com/pytorch/pytorch/pull/32609)).\r\n* `nn.BatchNorm{1,2,3}d`: Use `C10_WARP_SIZE` to fix functionality on HIP vs CUDA for gradient computation ([#33098](https://github.com/pytorch/pytorch/pull/33098)).\r\n* Enabled Bfloat16 type for activation functions and `batch_norm` ([#32065](https://github.com/pytorch/pytorch/pull/32065)).\r\n* Added ability to enable/disable MIOpen at runtime ([#33118](https://github.com/pytorch/pytorch/pull/33118)).\r\n* Enable BFloat16 type for pooling ops ([#34166](https://github.com/pytorch/pytorch/pull/34166)).\r\n* `torch.pdist`: improved precision by enabling double `__shfl_down` ([#34103](https://github.com/pytorch/pytorch/pull/34103)).\r\n* Enabled BFloat16 type for loss functions and few misc ops required for resnet50 ([#34469](https://github.com/pytorch/pytorch/pull/34469)).\r\n* Enabled BFloat16 type for EmbeddingBag, Index, and Sigmoid ops ([#34630](https://github.com/pytorch/pytorch/pull/34630)).\r\n* Enabled 3D batch norms through MIOpen ([#33262](https://github.com/pytorch/pytorch/pull/33262)).\r\n* Enabled 3D convolutions through ROCm ([#33067](https://github.com/pytorch/pytorch/pull/33067)).\r\n* `nn.RNN`: Check if weights need to be flattened ([#34265](https://github.com/pytorch/pytorch/pull/34265)).\r\n\r\n\r\n\r\n## C++ API\r\n\r\n* NN modules / functionals\r\n    * Allow skipping default arguments in module's forward method when module is used in `torch::nn::Sequential` ([#33027](https://github.com/pytorch/pytorch/pull/33027)) ([#33718](https://github.com/pytorch/pytorch/pull/33718))\r\n    * Make `torch::nn::Sequential::push_back(AnyModule)` methods public ([#34208](https://github.com/pytorch/pytorch/pull/34208)).\r\n    * Refactor RNN / GRU / LSTM layers to match Python API ([#34322](https://github.com/pytorch/pytorch/pull/34322)).\r\n    * For `Conv{1,2,3}d`, `padding_mode` now accepts `torch::kZeros` / `torch::kReflect` / `torch::kReplicate` / `torch::kCircular`, matching Python API behavior. ([#35023](https://github.com/pytorch/pytorch/pull/35023))\r\n    * Fix `F::interpolate` and `torch::nn::Upsample` implementation to match Python API behavior ([#35025](https://github.com/pytorch/pytorch/pull/35025)) ([#36274](https://github.com/pytorch/pytorch/pull/36274))\r\n    * Renaming: MultiLabelMarginLossFuncOptions -> MultilabelMarginLossFuncOptions, MultiLabelSoftMarginLossFuncOptions -> MultilabelSoftMarginLossFuncOptions ([#35163](https://github.com/pytorch/pytorch/pull/35163))\r\n* Optimizers\r\n    * All existing optimizers in the C++ API (Adagrad / SGD / Adam / RMSprop / LBFGS) have the following changes to achieve parity with the Python API: ([#29335](https://github.com/pytorch/pytorch/pull/29335)) ([#30739](https://github.com/pytorch/pytorch/pull/30739)) ([#32592](https://github.com/pytorch/pytorch/pull/32592)) ([#33730](https://github.com/pytorch/pytorch/pull/33730)) ([#33450](https://github.com/pytorch/pytorch/pull/33450)) ([#34790](https://github.com/pytorch/pytorch/pull/34790)) ([#34564](https://github.com/pytorch/pytorch/pull/34564)) ([#34957](https://github.com/pytorch/pytorch/pull/34957)) ([#35001](https://github.com/pytorch/pytorch/pull/35001)) ([#36033](https://github.com/pytorch/pytorch/pull/36033)) (#36245)\r\n        * step function implementation is changed to behave the same as Python equivalent\r\n        * Constructor now accepts `std::vector<OptimizerParamGroup>` as input\r\n        * `optimizer.add_param_group(...)` can be used to add parameter group to an existing optimizer\r\n        * `optimizer.state()` should be used to access parameter state\r\n* autograd\r\n    * Renamed `at::Tensor::base()` to `_base()`, matching Python API (#33316)\r\n\r\n\r\n\r\n## Distributed\r\n\r\n* Allow TCPStore to pick a port to bind to ([#31674](https://github.com/pytorch/pytorch/pull/31674)).\r\n* Enhance NCCL watchdog to actively abort communicators for timed out ops ([#32338](https://github.com/pytorch/pytorch/pull/32338)).\r\n* Adding DDP Design Note ([#32158](https://github.com/pytorch/pytorch/pull/32158)).\r\n* Recommend using DDP over DataParallel ([#35063](https://github.com/pytorch/pytorch/pull/35063/files))\r\n\r\n\r\n\r\n## Distributions\r\n\r\n* `distributions.independent`: added explicit string representation ([#33676](https://github.com/pytorch/pytorch/pull/33676)).\r\n* `categorical.sample`: Reduced memory overhead ([#34900](https://github.com/pytorch/pytorch/pull/34900)).\r\n* `distributions.MultivariateNormal`: improved numeric stability and performance ([#32092](https://github.com/pytorch/pytorch/pull/32092)).\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Add module level qpl logging. ([#30906](https://github.com/pytorch/pytorch/pull/30906)).\r\n* Expose setNumThreads to android api ([#31033](https://github.com/pytorch/pytorch/pull/31033)).\r\n* remove unused SparseCPUType from mobile build ([#33517](https://github.com/pytorch/pytorch/pull/33517)).\r\n* make sure mobile build work with dynamic dispatch ([#34038](https://github.com/pytorch/pytorch/pull/34038)).\r\n* support for custom mobile build with dynamic dispatch ([#34055](https://github.com/pytorch/pytorch/pull/34055)).\r\n* Add watchOS support ([#33318](https://github.com/pytorch/pytorch/pull/33318)).\r\n* speed_benchmark_torch switch to log latency from dataset level to row level ([#34598](https://github.com/pytorch/pytorch/pull/34598)).\r\n\r\n\r\n\r\n## ONNX\r\n\r\n### ****Exporting More Torch Operators to ONNX****\r\n\r\nIn PyTorch 1.5, we have added support for 10 additional operators and also enhanced support for another set of 10+ existing operators. We have also added support for exporting large models (> 2GB) to ONNX. Additionally, we have made enhancements and optimizations to the export of ScriptModules and will continue to do that in the next release. We have also made improvements to the custom op export experience. \r\n\r\n* Export dynamic unbind, split and getitem ([#29136](https://github.com/pytorch/pytorch/pull/29136)).\r\n* Export torch.new_zeros ([#34077](https://github.com/pytorch/pytorch/pull/34077)).\r\n* Export Im2col ([#30972](https://github.com/pytorch/pytorch/pull/30972)).\r\n* Export bitwise_not for bool ([#28439](https://github.com/pytorch/pytorch/pull/28439)).\r\n* Export logsoftmax with dim != -1 ([#30433](https://github.com/pytorch/pytorch/pull/30433)).\r\n* Export einsum ([#32716](https://github.com/pytorch/pytorch/pull/32716)).\r\n* Export aten::copy_ and aten::index_put to ONNX opset 11 ([#26941](https://github.com/pytorch/pytorch/pull/26941)).\r\n* Export floor_divide ([#31081](https://github.com/pytorch/pytorch/pull/31081)).\r\n* Export one_hot ([#34454](https://github.com/pytorch/pytorch/pull/34454)).\r\n* Export torch.take ([#33061](https://github.com/pytorch/pytorch/pull/33061)).\r\n* Export bool type index mask ([#32445](https://github.com/pytorch/pytorch/pull/32445)).\r\n* Export split with list of sizes ([#33161](https://github.com/pytorch/pytorch/pull/33161)).\r\n* Export scalar tensor for split ([#32493](https://github.com/pytorch/pytorch/pull/32493)).\r\n* Export flatten to accept negative indices in opset 11 ([#30751](https://github.com/pytorch/pytorch/pull/30751)).\r\n* Export sort with negative axes ([#31971](https://github.com/pytorch/pytorch/pull/31971)).\r\n* Export Interpolate to support scale ([#28324](https://github.com/pytorch/pytorch/pull/28324), [#31526](https://github.com/pytorch/pytorch/pull/31526), [#32554](https://github.com/pytorch/pytorch/pull/32554)).\r\n* Export quantized concat ([#30887](https://github.com/pytorch/pytorch/pull/30887)).\r\n\r\n### ****Enhancing the Support for ScriptModule****\r\n\r\n* Fixed access to element in size tensor for scripting  ([#32652](https://github.com/pytorch/pytorch/pull/32652)).\r\n* Export Conv in TorchScript module ([#30618](https://github.com/pytorch/pytorch/pull/30618)).\r\n* Export Dim operation in TorchScript module ([#31928](https://github.com/pytorch/pytorch/pull/31928)).\r\n* Export randnlike in TorchScript module ([#32830](https://github.com/pytorch/pytorch/pull/32830)).\r\n* Partially support tensor lists in loop/concat/stack ([#30126](https://github.com/pytorch/pytorch/pull/30126))\r\n\r\n### ****Enhancing Existing Export Logic****\r\n\r\n* Updating ONNX checker logic. ([#33522](https://github.com/pytorch/pytorch/pull/33522)).\r\n* Adding ONNX large model export support in  exporter ([#33062](https://github.com/pytorch/pytorch/pull/33062)).\r\n* Extend op registration ([#32943](https://github.com/pytorch/pytorch/pull/32943)).\r\n* Support op registration if name starts with underscore ([#32017](https://github.com/pytorch/pytorch/pull/32017)).\r\n\r\n### ****Optimizing Exported ONNX Graph****\r\n\r\n* Try exporting ONNX with force_outplace=False ([#29466](https://github.com/pytorch/pytorch/pull/29466)).\r\n* Enable constant folding ([#29834](https://github.com/pytorch/pytorch/pull/29834)).\r\n* Added cons folding for ONNX mul, div, sqrt ops ([#32077](https://github.com/pytorch/pytorch/pull/32077)).\r\n* Enable constant folding for Reshape ([#31054](https://github.com/pytorch/pytorch/pull/31054)).\r\n\r\n### ****Adding Utility Functions and Refactoring****\r\n\r\n* Added ONNX model checker to ONNX export ([#32298](https://github.com/pytorch/pytorch/pull/32298)).\r\n* Export custom ops ([#29752](https://github.com/pytorch/pytorch/pull/29752)).\r\n* Upgrade exported ONNX IR version to 6 ([#31025](https://github.com/pytorch/pytorch/pull/31025)).\r\n* Provide names for operator nodes in ONNX exported graph ([#27342](https://github.com/pytorch/pytorch/pull/27342)).\r\n* Update ONNX landing page since 1.3 ([#32805](https://github.com/pytorch/pytorch/pull/32805)).\r\n* Turn ONNX_ML into a proper build option ([#33424](https://github.com/pytorch/pytorch/pull/33424)).\r\n\r\n\r\n\r\n## Operator Benchmark\r\n\r\n* Added small input shapes to test operator overhead ([#30617](https://github.com/pytorch/pytorch/pull/30617)).\r\n* Added `binary_test` to benchmark binary ops ([#31326](https://github.com/pytorch/pytorch/pull/31326)).\r\n* Added `Tensor.copy_` operator ([#31327](https://github.com/pytorch/pytorch/pull/31327)).\r\n* Removed option to wipe cache because it did not help with variance ([#31334](https://github.com/pytorch/pytorch/pull/31334)).\r\n* Added `torch.diag` ([#32597](https://github.com/pytorch/pytorch/pull/32597)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Guard against copying from quantized Tensor to non-quantized Tensor ([#29660](https://github.com/pytorch/pytorch/pull/29660)).\r\n* Add assert for min, max, qmin, qmax for ChooseQuantizationParams ([#32739](https://github.com/pytorch/pytorch/pull/32739)).\r\n* Support broadcast for quantized mul kernel ([#30442](https://github.com/pytorch/pytorch/pull/30442)).\r\n* Make FakeQuant use `REGISTER_DISPATCH` ([#33682](https://github.com/pytorch/pytorch/pull/33682)).\r\n* Set alias analysis kind to `FROM_SCHEMA` for qadd, qmul, qclamp, qconcat ([#33359](https://github.com/pytorch/pytorch/pull/33359)).\r\n* Migrate `fake_quant_slice` to TensorIterator ([#33744](https://github.com/pytorch/pytorch/pull/33744)).\r\n* Parallelize quantize and dequantize ([#33765](https://github.com/pytorch/pytorch/pull/33765)).\r\n* Make FP16 RNN use new prepack op ([#34339](https://github.com/pytorch/pytorch/pull/34339)).\r\n* Refactor QAT Conv module for better extensibility ([#30362](https://github.com/pytorch/pytorch/pull/30362)).\r\n* Use non-inplace for insert observer pass ([#34190](https://github.com/pytorch/pytorch/pull/34190)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Add default arguments for `init_method` ([#30208](https://github.com/pytorch/pytorch/pull/30208)).\r\n* By default ignore RRef leaks during shutdown ([#30217](https://github.com/pytorch/pytorch/pull/30217)).\r\n* Robustify `rpc_agent` handlers with generic Future ([#31224](https://github.com/pytorch/pytorch/pull/31224)).\r\n* Fix error message in incorrect `rref.localValue()` call ([#31199](https://github.com/pytorch/pytorch/pull/31199)).\r\n* Add `RpcAgent::getWorkerInfos()` API to return all `WorkInfo`s in the group ([#30241](https://github.com/pytorch/pytorch/pull/30241)).\r\n* Add local shutdown to process group agent ([#30330](https://github.com/pytorch/pytorch/pull/30330)).\r\n* Add `RRef.str()` API to return a string representation of the RRef ([#30609](https://github.com/pytorch/pytorch/pull/30609)).\r\n* Adding Debug Info for RRef Context ([#30610](https://github.com/pytorch/pytorch/pull/30610)).\r\n* Add `get_metrics` and `get_debug_info` to RPC agent ([#30833](https://github.com/pytorch/pytorch/pull/30833)).\r\n* Adding debugging metrics to process group agent ([#30884](https://github.com/pytorch/pytorch/pull/30884)).\r\n* Add glue code to collect debug info from all components ([#30888](https://github.com/pytorch/pytorch/pull/30888)).\r\n* Make RRef leak detection always print a warning log ([#31922](https://github.com/pytorch/pytorch/pull/31922)).\r\n* Allow multiple backward passes to accumulate gradients. ([#32506](https://github.com/pytorch/pytorch/pull/32506)).\r\n* Allow RRef local creation with IValue objects ([#33263](https://github.com/pytorch/pytorch/pull/33263)).\r\n* Improve ProcessGroup `RpcBackendOptions` Constructor API ([#34081](https://github.com/pytorch/pytorch/pull/34081)).\r\n* Enhanced Error Reporting in Dist Autograd/RPC ([#34179](https://github.com/pytorch/pytorch/pull/34179)).\r\n* Delete all user forks tracked in `RRefContext` before graceful shutdown ([#31893](https://github.com/pytorch/pytorch/pull/31893)).\r\n* Best-effort Error Detection for Using Deleted UserRRefs ([#34673](https://github.com/pytorch/pytorch/pull/34673)).\r\n* Don't run user function until all UserRRefs in the args are confirmed ([#34497](https://github.com/pytorch/pytorch/pull/34497)).\r\n* Support using self as the destination in `rpc.remote` for builtin operators ([#34931](https://github.com/pytorch/pytorch/pull/34931)).\r\n* Add debug info API for distributed autograd. ([#30642](https://github.com/pytorch/pytorch/pull/30642)).\r\n* Propagate errors in `clearAndWaitForOutstandingRpcsAsync`. ([#32952](https://github.com/pytorch/pytorch/pull/32952)).\r\n\r\n\r\n\r\n## Type Hints\r\n\r\n* DataLoader `default_collate` type hint added ([#28935](https://github.com/pytorch/pytorch/pull/28935)).\r\n* `Tensor.rsub, Tensor.rpow, Tensor.rtruediv, Tensor.map_` type hints were added ([#30576](https://github.com/pytorch/pytorch/pull/30576)).\r\n* `torch.optim`: added more missing type hints ([#31130](https://github.com/pytorch/pytorch/pull/31130)).\r\n* `nn.functional.grid_sample`, `nn.functional.affine_grid`: added missing align_corners annotation ([#32492](https://github.com/pytorch/pytorch/pull/32492)).\r\n* `torch.nn.Parameter` constructor type hint was fixed ([#32617](https://github.com/pytorch/pytorch/pull/32617)).\r\n* `nn.MultiheadAttention`, `nn.Transformer`: added type hints ([#28396](https://github.com/pytorch/pytorch/pull/28396)).\r\n* `torch.optim.LambdaLR` constructor type hint was fixed ([#33271](https://github.com/pytorch/pytorch/pull/33271)).\r\n* `torch.optim`: added missing default value for `LRScheduler.step()` ([#32411](https://github.com/pytorch/pytorch/pull/32411)).\r\n* Make type of `Tensor.type()` more specific ([#32353](https://github.com/pytorch/pytorch/pull/32353)).\r\n* `torch.optim.optimizer.Optimizer`  type hints were fixed ([#32900](https://github.com/pytorch/pytorch/pull/32900)).\r\n* `optim.AdamW` type hints were fixed ([#34299](https://github.com/pytorch/pytorch/pull/34299)).\r\n* `torch.utils.data.Sampler`  subclasses type hints were added ([#33679](https://github.com/pytorch/pytorch/pull/33679)).\r\n* `nn.Sequential`, `nn.ModuleList`, `nn.ParameterList`, `nn.ParameterDict` type hints were fixed ([#33686](https://github.com/pytorch/pytorch/pull/33686)).\r\n* `Tensor.bfloat16()` type hint was added ([#33747](https://github.com/pytorch/pytorch/pull/33747)).\r\n* Binary operator type hints were fixed ([#33748](https://github.com/pytorch/pytorch/pull/33748)).\r\n* `torch.bfloat16`, `nn.Module.training`, `Tensor.cuda`, and 10s of other type hints added ([#33762](https://github.com/pytorch/pytorch/pull/33762)).\r\n* `torch.add` type hint was fixed([#33935](https://github.com/pytorch/pytorch/pull/33935)).\r\n* `Tensor.shape` type hint was fixed ([#34595](https://github.com/pytorch/pytorch/pull/34595)).\r\n* Fixed `utils.data` imports ([#33543](https://github.com/pytorch/pytorch/pull/33543)).\r\n* `Tensor.__radd__` type hint was fixed ([#35231](https://github.com/pytorch/pytorch/pull/35231))\r\n\r\n\r\n\r\n## Other\r\n\r\n* `autograd.detect_anomaly`: added support for Sparse Tensors ([#29803](https://github.com/pytorch/pytorch/pull/29803)).\r\n* `autograd.detect_anomaly`: Error messages now print the current Node name ([#33875](https://github.com/pytorch/pytorch/pull/33875)).\r\n* `autograd.profiler`: added better error message when crashing while profiling multi-worker DataLoader ([#31473](https://github.com/pytorch/pytorch/pull/31473)).\r\n* `autograd.profiler` Enable using `torch.autograd.profiler.record_function` as decorator ([#30861](https://github.com/pytorch/pytorch/pull/30861)).\r\n* `autograd.profiler` Speed up `export_chrome_trace` by up to 4x ([#30724](https://github.com/pytorch/pytorch/pull/30724)).\r\n* `torch.autograd`: added better error message when attempting to fork ([#33885](https://github.com/pytorch/pytorch/pull/33885)).\r\n* `torch.cuda.memory.caching_allocator_alloc`, `torch.cuda.memory.caching_allocator_delete` exposed in Python API ([#33860](https://github.com/pytorch/pytorch/pull/33860)).\r\n* `torch.roll`: added bool tensor support ([#31194](https://github.com/pytorch/pytorch/pull/31194)).\r\n* `torch.flip`: added support for bool tensors ([#31267](https://github.com/pytorch/pytorch/pull/31267)).\r\n* `torch.equal`: added support for bfloat16 CPU scalar types ([#30817](https://github.com/pytorch/pytorch/pull/30817)).\r\n* `torch.save`, `torch.load`: added error message for minimum dill version support ([#30985](https://github.com/pytorch/pytorch/pull/30985)).\r\n* `torch.diagonal`: added named tensor support([#30193](https://github.com/pytorch/pytorch/pull/30193)).\r\n* `torch.linspace`: added support for integral types on CPU ([#32218](https://github.com/pytorch/pytorch/pull/32218)).\r\n* `torch.eig`: Added autograd support in the case where eigenvalues are real ([#33090](https://github.com/pytorch/pytorch/pull/33090)).\r\n* `torch.mvlgamma`: improved error message ([#32665](https://github.com/pytorch/pytorch/pull/32665)).\r\n* `torch.no_grad`, `torch.enable_grad`: added support for decorating generator functions ([#31792](https://github.com/pytorch/pytorch/pull/31792)).\r\n* `torch.narrow`: added Tensor overload for `start` ([#34317](https://github.com/pytorch/pytorch/pull/34317)).\r\n* `Tensor.random_`: enabled support for half on CPU ([#34030](https://github.com/pytorch/pytorch/pull/34030)).\r\n* `Tensor.grad`: added warnings when accessing it if it won't be populated for known reasons ([#30531](https://github.com/pytorch/pytorch/pull/30531)).\r\n* `torch.cuda.comm.gather`: improved error message ([#27456](https://github.com/pytorch/pytorch/pull/27456)).\r\n* `nn.functional.max_pool{1,2,3}d`: added named tensor support ([#31669](https://github.com/pytorch/pytorch/pull/31669)).\r\n* `nn.Module.load_state_dict`: Include the contents of the exception in error messages ([#32693](https://github.com/pytorch/pytorch/pull/32693)).\r\n* `nn.MultiheadAttention`: add support for 3D attention mask ([#31996](https://github.com/pytorch/pytorch/pull/31996)).\r\n* `nn.MSELoss` : Added performance warning for using CPU Half ([#33021](https://github.com/pytorch/pytorch/pull/33021)).\r\n* `nn.ModuleList`, `nn.ParameterDict`, `nn.ParameterDict`: added more descriptive error messages when attempting to call these like Modules ([#29991](https://github.com/pytorch/pytorch/pull/29991)).\r\n* `nn.init.dirac_`: Added `groups` option for compatibility with initializing group convolutions ([#32825](https://github.com/pytorch/pytorch/pull/32825)).\r\n* Added error message to indicate that reduction operations are not supported for dim >= 64 ([#31476](https://github.com/pytorch/pytorch/pull/31476)).\r\n* Type Promotion: added supports for sparse tensors and arithmetic operations ([#30429](https://github.com/pytorch/pytorch/pull/30429)).\r\n* Enabled indexing for bfloat16 tensors ([#31692](https://github.com/pytorch/pytorch/pull/31692)).\r\n* Add 64-bit indexing support for CUDA Tensors ([#33405](https://github.com/pytorch/pytorch/pull/33405)).\r\n* Added warning when converting a read-only NumPy array to `torch.Tensor` ([#33615](https://github.com/pytorch/pytorch/pull/33615)).\r\n* Set rpath for JNI library on Mac ([#32247](https://github.com/pytorch/pytorch/pull/32247)).\r\n* Updated MAGMA to 2.5.2 for Windows ([#30513](https://github.com/pytorch/pytorch/pull/30513), [#34205](https://github.com/pytorch/pytorch/pull/34205)).\r\n* Marked PyTorch incompatible with Python-3.6.0 ([#34724](https://github.com/pytorch/pytorch/pull/34724)).\r\n* Consider `hub_dir` alongside `TORCH_HOME` env variable for storing hub models ([#32844](https://github.com/pytorch/pytorch/pull/32844)).\r\n* Improved dll loading logic on Windows ([#33856](https://github.com/pytorch/pytorch/pull/33856)).\r\n* Error out if legacy `Tensor.new ` is called on alternate layouts or dtypes ([#31485](https://github.com/pytorch/pytorch/pull/31485)).\r\n* `utils.checkpoint.checkpoint_sequential`: Removed deprecated variadic arguments behavior ([#25985](https://github.com/pytorch/pytorch/pull/25985)).\r\n\r\n# Bug Fixes\r\n\r\n## C++ API\r\n\r\n* NN modules / functionals\r\n    * `output_ratio` for `FractionalMaxPool{2,3}d `module and `fractional_max_pool{2,3}d` functional should accept double as data type ([#33304](https://github.com/pytorch/pytorch/pull/33304))\r\n    * For `AdaptiveAvgPool{2,3}d `and `AdaptiveMaxPool{2,3}d`, `output_size` is changed to accept `c10::nullopt` in its elements, matching Python API behavior. ([#35022](https://github.com/pytorch/pytorch/pull/35022))\r\n    * Fix bug in `fractional_max_pool3d_with_indices` implementation ([#35024](https://github.com/pytorch/pytorch/pull/35024))\r\n    * Remove `namespace F = torch::nn::functional` from torch/nn/modules/batchhnorm.h, so that people don't have to use `F` to alias `torch::nn::functional` if they don't want to ([#30684](https://github.com/pytorch/pytorch/pull/30684))\r\n* autograd\r\n    * For `AutogradContext`, `get_dirty()` is removed and `get_and_bump_dirty()` is added, and the latter always bumps the version counter of the returned tensors (#33068)\r\n    * Fix allow_unused checking for C++ API (#34035)\r\n    * Remove `using namespace torch::autograd` from `torch/csrc/api/include/torch/nn/modules/_functions.h` ([#34423](https://github.com/pytorch/pytorch/pull/34423))\r\n* Operators\r\n    * `torch::tensor(floating-point values)` will always produce tensor of default dtype, and `torch::tensor(integer values)` will always produce tensor of `torch::kLong` dtype, matching Python API behavior ([#32367](https://github.com/pytorch/pytorch/pull/32367))\r\n    * Fix `torch::allclose` to handle `std::numeric_limits::lowest()` for integral types ([#32978](https://github.com/pytorch/pytorch/pull/32978))\r\n    * Switch `torch::empty_like` to use `merge_in` to process TensorOptions (#33505)\r\n\r\n\r\n\r\n## Distributed\r\n\r\n* Allow DDP to detect globally unused parameters ([#28883](https://github.com/pytorch/pytorch/pull/28883)).\r\n* Accept url query when `rank` or `world_size` is specified in Process Group `init_method` URL ([#32016](https://github.com/pytorch/pytorch/pull/32016)).\r\n* Add ability to abort NCCL communicators from the store. ([#32895](https://github.com/pytorch/pytorch/pull/32895)).\r\n* Fix timeout support when initializing process group with TCP store ([#33434](https://github.com/pytorch/pytorch/pull/33434)).\r\n* Abort NCCL communicators before throwing operation timed out ([#31128](https://github.com/pytorch/pytorch/pull/31128)).\r\n* Fix logging for aborted communicators in ProcessGroupNCCL ([#33147](https://github.com/pytorch/pytorch/pull/33147)).\r\n* Fix handling of replica parameters in DataParallel ([#33907](https://github.com/pytorch/pytorch/pull/33907)).\r\n* Specify `requires_grad` for Parameter replica so it's not always set to True by default ([#32356](https://github.com/pytorch/pytorch/pull/32356))\r\n* Put sparse `allreduce` results to input tensors ([#32226](https://github.com/pytorch/pytorch/pull/32226))\r\n* Issue a warning when `zero_grad` is used in `DataParallel` ([#33064](https://github.com/pytorch/pytorch/pull/33064))\r\n\r\n\r\n\r\n## JIT\r\n\r\n* TorchScript compilation fixed for ([#33783](https://github.com/pytorch/pytorch/pull/33783)):  \r\n    * `torch.stft`\r\n    * `torch.lu`, \r\n    * `torch.lu_unpack`\r\n    * `torch.cdist`\r\n    * `torch.norm`\r\n* `tensor.tolist()` compilation now supported, requires output type annotation ([#33472](https://github.com/pytorch/pytorch/pull/34554))\r\n```\r\ndef foo(float_matrix, scalar_ten):\r\n    # type: (Tensor, Tensor) -> Tuple[List[List[float]], bool]\r\n    out1 : List[List[float]] = float_matrix.tolist()\r\n    out2 = torch.jit.annotate(bool, scalar_ten.tolist())\r\n    return out1, out2\r\n```\r\n* `torch.rand_like` and other `_like` constructors no longer require additional arguments in TorchScript\r\n* Compilation for `nn.Module` APIs added [(#29495)](https://github.com/pytorch/pytorch/pull/29495):\r\n    * `children`\r\n    * `named_children`\r\n    * `modules`\r\n    * `named_modules`\r\n* Support for ModuleList Indexing with Integer Literal ([#29236)](https://github.com/pytorch/pytorch/pull/29236/)\r\n* Fixed flipped outputs for `PackedSequence`  ([#32955)](https://github.com/pytorch/pytorch/pull/32955)\r\n* Support `index` and `type` properties on `Device` ([#32953](https://github.com/pytorch/pytorch/pull/32953))\r\n    * `device.index`\r\n    * `device.type`\r\n* Add remaining `Tensor` properties ([#33906](https://github.com/pytorch/pytorch/pull/33906))\r\n    * `tensor.ndim`\r\n    * `tensor.T`\r\n    * `tensor.name`\r\n    * `tensor.is_leaf`\r\n* Fix augmented assignment to non-tensor attributes [#32993](https://github.com/pytorch/pytorch/pull/32993)\r\n* Fixed type resolution for function arguments [#29623](https://github.com/pytorch/pytorch/pull/29623)\r\n    * Previously we resolved types by parsing their names directly, but now TorchScript uses the value of the type directly from Python\r\n    * This allows types types like `torch.device` to be used\r\n* `len` on tuples containing different types [#35768](https://github.com/pytorch/pytorch/pull/35768)\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Fix exception message in Java Tensor ([#30205](https://github.com/pytorch/pytorch/pull/30205)).\r\n* Fix the crashes for c++ not able to find java class through Jni ([#30390](https://github.com/pytorch/pytorch/pull/30390)).\r\n* Add @DoNotStrip to nativeNewTensor method. ([#30472](https://github.com/pytorch/pytorch/pull/30472)).\r\n* GenericDict/List type use unshapedType() ([#30428](https://github.com/pytorch/pytorch/pull/30428)).\r\n* Support tensors with a storage offset in Java ([#31584](https://github.com/pytorch/pytorch/pull/31584)).\r\n* Fix SIGABORT caused by double exception in PyTorchStreamReader when file not found. ([#33243](https://github.com/pytorch/pytorch/pull/33243)).\r\n* Fix `SELECTED_OP_LIST` file path issue ([#33942](https://github.com/pytorch/pytorch/pull/33942)).\r\n* Fix for handling batch size 0. ([#34599](https://github.com/pytorch/pytorch/pull/34599)).\r\n* fixed AutoGradMode/AutoNonVariableTypeMode uses for mobile callsites\r\n* Use `gettimeofday` on iOS ([#30361](https://github.com/pytorch/pytorch/pull/30361)).\r\n\r\n\r\n\r\n## ONNX\r\n\r\n* Fix `weight_norm` export for dim=0 ([#31015](https://github.com/pytorch/pytorch/pull/31015)).\r\n* Fix for constant folding flaky tests ([#32546](https://github.com/pytorch/pytorch/pull/32546)).\r\n* Fix export for avg_pool with default stride  ([#33017](https://github.com/pytorch/pytorch/pull/33017)).\r\n* Fix ONNX CI by moving test data to aws ([#33200](https://github.com/pytorch/pytorch/pull/33200)).\r\n* Fix for random generators export ([#33789](https://github.com/pytorch/pytorch/pull/33789)).\r\n* Fix export of index_put  ([#31552](https://github.com/pytorch/pytorch/pull/31552)).\r\n* Fix for expand -1 dim value ([#34069](https://github.com/pytorch/pytorch/pull/34069)).\r\n* Reduce ONNX test time on CI ([#33242](https://github.com/pytorch/pytorch/pull/33242)).\r\n* ONNX Error Message on Missing Op ([#33593](https://github.com/pytorch/pytorch/pull/33593)).\r\n* Fix exporting `copy_` with index as tensor input ([#32801](https://github.com/pytorch/pytorch/pull/32801)).\r\n* Fix for `rand_like` as well ([#33095](https://github.com/pytorch/pytorch/pull/33095)).\r\n* Added torchvision tests as part of ORT tests ([#31835](https://github.com/pytorch/pytorch/pull/31835)).\r\n* Remove non-ascii character from `torch/onnx/symbolic_opset11.py` ([#31814](https://github.com/pytorch/pytorch/pull/31814)).\r\n* Add flag to enable script tests ([#32654](https://github.com/pytorch/pytorch/pull/32654)).\r\n* Skip same tests in ONNX Python3 CI as in Python2 ([#31827](https://github.com/pytorch/pytorch/pull/31827)).\r\n* Fixed `torch.mm` export ([#34794](https://github.com/pytorch/pytorch/pull/34661))\r\n* Fixed `aten::size` for opset 11 ([#35984](https://github.com/pytorch/pytorch/pull/35984))\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Bug fix: Handle missing keys in observer state dict during load ([#30357](https://github.com/pytorch/pytorch/pull/30357)).\r\n* Fix BC for quantized linear ([#30481](https://github.com/pytorch/pytorch/pull/30481)).\r\n* Fix mapping white list to avoid attaching qconfig for DeQuantStub ([#30636](https://github.com/pytorch/pytorch/pull/30636)).\r\n* Fix default instantation of dynamic quantized LSTM ([#31433](https://github.com/pytorch/pytorch/pull/31433)).\r\n* Use default scale/zero_point in fake_quantize module instead of None ([#32318](https://github.com/pytorch/pytorch/pull/32318)).\r\n* Fix ASAN / potential segfault in quantized Tensor memory allocations. ([#29882](https://github.com/pytorch/pytorch/pull/29882)).\r\n* Don't serialize None values in observer ([#32733](https://github.com/pytorch/pytorch/pull/32733)).\r\n* Enable inplace relu fusion for training ([#33105](https://github.com/pytorch/pytorch/pull/33105)).\r\n* Bug fix in dynamic quantization kernels + better test coverage. ([#33320](https://github.com/pytorch/pytorch/pull/33320)).\r\n* Run weight_post_process for QAT ([#33852](https://github.com/pytorch/pytorch/pull/33852)).\r\n* Fix histogram observer to work with QAT on GPU ([#34232](https://github.com/pytorch/pytorch/pull/34232)).\r\n* Fix the quantized batchnorm2d ([#34579](https://github.com/pytorch/pytorch/pull/34579)).\r\n* Move QScheme ops to c10 ([#30134](https://github.com/pytorch/pytorch/pull/30134)).\r\n* Remove incorrect fp16 dynamic linear/relu op ([#32774](https://github.com/pytorch/pytorch/pull/32774)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Fix serialization memory lifetime issue. ([#30603](https://github.com/pytorch/pytorch/pull/30603)).\r\n* Don't crash callee when function does not exist on it, instead return an Exception ([#32726](https://github.com/pytorch/pytorch/pull/32726)).\r\n* Throw the correct Exception on local client based on the `RemoteException` ([#32936](https://github.com/pytorch/pytorch/pull/32936)).\r\n* Attach autograd edges only for tensors requiring grad. ([#30904](https://github.com/pytorch/pytorch/pull/30904)).\r\n* `WireSerializer` should check `has_storage()` ([#34626](https://github.com/pytorch/pytorch/pull/34626)).\r\n* Fixed potential deadlock in python exception handling ([#35283](https://github.com/pytorch/pytorch/pull/35283/))\r\n\r\n\r\n\r\n## Other\r\n\r\n* `torch.split`: Fixed incorrect gradient computation that assumed the output was not a view ([#32044](https://github.com/pytorch/pytorch/pull/32044)).\r\n* Allowed numpy integer types to be used where we accept Python integers ([#30486](https://github.com/pytorch/pytorch/pull/30486)).\r\n* `torch.unique`, `torch.unique_consecutive`:  fixed bug with zero-element input support ([#31211](https://github.com/pytorch/pytorch/pull/31211)).\r\n* `Tensor.to_sparse`: fixed backward in the non-contiguous tensor case ([#31223](https://github.com/pytorch/pytorch/pull/31223)).\r\n* `torch.index_put`: Added error checks for input tensors\u2019 devices (#31280) ([#31280](https://github.com/pytorch/pytorch/pull/31280)).\r\n* Ensure we switch the CUDA stream correctly in CUDA operations ([#31537](https://github.com/pytorch/pytorch/pull/31537), [#31538](https://github.com/pytorch/pytorch/pull/31538), [#31541](https://github.com/pytorch/pytorch/pull/31541)).\r\n* `torch.SparseTensor`: ensure the legacy sparse constructor doesn't interpret Python data as tensor data. ([#31490](https://github.com/pytorch/pytorch/pull/31490)).\r\n* `torch.argmax`, `torch.argmin`: Fixed incorrect behavior on large tensors ([#33310](https://github.com/pytorch/pytorch/pull/33310)).\r\n* `torch.div`: Fixed to throw an error when dividing by integer zero on CPU  ([#32629](https://github.com/pytorch/pytorch/pull/32629)).\r\n* `torch.cos`: Fixed incorrect gradient computation caused by not properly initializing temporary vectors in avx2 code ([#32722](https://github.com/pytorch/pytorch/pull/32722), [#34281](https://github.com/pytorch/pytorch/pull/34281)).\r\n* `torch.logspace`: Added missing integer dtype support, fixed precision issues in floating-point implementation ([#32744](https://github.com/pytorch/pytorch/pull/32744)).\r\n* `torch.prod`: Fixed behavior when passed a `torch.half` input tensor and `torch.float` output tensor ([#32831](https://github.com/pytorch/pytorch/pull/32831)).\r\n* `torch.max`, `torch.min`: Fixed NaN handling ([#32541](https://github.com/pytorch/pytorch/pull/32541)).\r\n* `torch.max`, `torch.min`: Added error check that operand and outputs are on the same device type ([#32862](https://github.com/pytorch/pytorch/pull/32862)).\r\n*  `torch.stack`: Added missing input size checks ([#32931](https://github.com/pytorch/pytorch/pull/32931)).\r\n* `torch.add`: Fixed memory leak on certain platforms ([#32478](https://github.com/pytorch/pytorch/pull/32478)).\r\n* `torch.normal`: Fixed shape checks ([#33050](https://github.com/pytorch/pytorch/pull/33050)).\r\n* `torch.cumsum`: fixed to handle inputs with zero-sized dimensions correctly ([#31694](https://github.com/pytorch/pytorch/pull/31694)).\r\n* `torch.device`: Disallow incorrectly formatted device strings ([#29087](https://github.com/pytorch/pytorch/pull/29087)).\r\n* `torch.cat`: Disallow passing `out` as one of the input tensors ([#30577](https://github.com/pytorch/pytorch/pull/30577)).\r\n* `torch.pdist`: Added support for large batch sizes ([#31593](https://github.com/pytorch/pytorch/pull/31593)).\r\n* `torch.stft`: Fixed crash when used with `nn.DataParallel` ([#31861](https://github.com/pytorch/pytorch/pull/31861)).\r\n* `torch.autograd`: Ensure the original grad mode is restored during backward ([#31884](https://github.com/pytorch/pytorch/pull/31884)).\r\n* `torch.autograd`: Fixed a race condition by locking graph_task before writing leaf_streams. (#31995) ([#31995](https://github.com/pytorch/pytorch/pull/31995)).\r\n* `torch.tensordot`: Fixed support for negative dimensions ([#31954](https://github.com/pytorch/pytorch/pull/31954)).\r\n* `torch.cumprod`: Fixed to handle inputs with zero-sized dimensions correctly ([#32070](https://github.com/pytorch/pytorch/pull/32070)).\r\n* `torch.pow`: Fixed the gradient computation when the base is a Tensor or Scalar of zeros ([#32062](https://github.com/pytorch/pytorch/pull/32062), [#32063](https://github.com/pytorch/pytorch/pull/32063)).\r\n* `torch.baddbmm`: Fixed bug in corner case ([#33538](https://github.com/pytorch/pytorch/pull/33538)).\r\n* `torch.where`: Added check for consistent devices ([#33432](https://github.com/pytorch/pytorch/pull/33432)).\r\n* `torch.cdist`: Fixed gradient computation for `p=2` and large inputs ([#31167](https://github.com/pytorch/pytorch/pull/31167)).\r\n* `torch.mv`: Fixed NaN handling ([#31666](https://github.com/pytorch/pytorch/pull/31666)).\r\n* `torch.index_put`: Added handling for large input tensors ([#33753](https://github.com/pytorch/pytorch/pull/33753)).\r\n* `torch.addmm`: Fixed incorrect output when using BLAS backend ([#33819](https://github.com/pytorch/pytorch/pull/33819)).\r\n* `torch.topk` fixed double backward when input has non-finite values ([#35253](https://github.com/pytorch/pytorch/pull/35253))\r\n* `torch.load`: Avoid problematic pickle usages on Python 3.8.0 and 3.8.1 ([#33824](https://github.com/pytorch/pytorch/pull/33824)).\r\n* `Tensor.to`: Fixed race condition for gradient computation that spans CUDA devices ([#31930](https://github.com/pytorch/pytorch/pull/31930)).\r\n* `Tensor.random_` added check that `from` and `to` are within the Tensor\u2019s dtype bounds ([#34033](https://github.com/pytorch/pytorch/pull/34033)).\r\n* `Tensor.copy_`: Fixed memory overlap check and allowed outputs to be zero-strided tensors if the size is <= 1 along that dimension ([#34100](https://github.com/pytorch/pytorch/pull/34100)).\r\n* `nn.BatchNorm{1,2,3}d`: fixed gradient computation for empty inputs ([#32820](https://github.com/pytorch/pytorch/pull/32820)).\r\n* `nn.BatchNorm`: Fixed behavior for inputs with large batch sizes ([#32763](https://github.com/pytorch/pytorch/pull/32763)).\r\n* `nn.Conv2d`: Fixed 5d weight handling with MKLDNN backend ([#34115](https://github.com/pytorch/pytorch/pull/34115)).\r\n* `nn.Conv3d`: Fixed unstable gradient computation ([#34358](https://github.com/pytorch/pytorch/pull/34358)).\r\n* `nn.Conv{1,2,3}d`: added support for empty batch size([#32709](https://github.com/pytorch/pytorch/pull/32709)).\r\n* `nn.Conv{1,2,3}d`: fixed `CUDNN_STATUS_NOT_SUPPORTED` errors by trying multiple algorithms ([#33073](https://github.com/pytorch/pytorch/pull/33073)).\r\n* `nn.Conv{1,2,3}d`: fixed padding mode support and added additional padding modes (reflection and replication) ([#31784](https://github.com/pytorch/pytorch/pull/31784)).\r\n* `nn.Conv2d`, `nn.Conv3d`, `nn.Conv1d`, `nn.ConvTranspose2d`: Fixed support for batch sizes greater than 2^32 ([#31383](https://github.com/pytorch/pytorch/pull/31383), [#31379](https://github.com/pytorch/pytorch/pull/31379), [#31889](https://github.com/pytorch/pytorch/pull/31889), [#34407,](https://github.com/pytorch/pytorch/pull/34407)[#31510](https://github.com/pytorch/pytorch/pull/31510)).\r\n* `nn.InstanceNorm`, `nn.GroupNorm`: Added error check for input with exactly one element ([#29082](https://github.com/pytorch/pytorch/pull/29082)).\r\n* `nn.RNN`: Fixed moving RNNs to a device after applying weight norm ([#32563](https://github.com/pytorch/pytorch/pull/32563), [#32989](https://github.com/pytorch/pytorch/pull/32989)).\r\n* `nn.MultiLabelMarginLoss`: added support for 0-d tensors ([#30765](https://github.com/pytorch/pytorch/pull/30765)).\r\n* `nn.GroupNorm`: added support for empty batch ([#32401](https://github.com/pytorch/pytorch/pull/32401)).\r\n* `nn.NLLLoss`: fixed to support empty tensors on CUDA ([#31491](https://github.com/pytorch/pytorch/pull/31491)).\r\n* `nn.GroupNorm`: corrected input size check ([#33008](https://github.com/pytorch/pytorch/pull/33008))\r\n* `nn.MultiLabelMarginLoss`: fixed memory leak on CUDA ([#30767](https://github.com/pytorch/pytorch/pull/30767)).\r\n* `nn.MultiMarginLoss`: fixed error checking on CUDA for the 1D case.  ([#30825](https://github.com/pytorch/pytorch/pull/30825)).\r\n* `nn.Softmax`: Fixed half->float case of softmax backward ([#30838](https://github.com/pytorch/pytorch/pull/30838)).\r\n* `nn.Softshrink`: Added check that lambda is no less than zero ([#33201](https://github.com/pytorch/pytorch/pull/33201)).\r\n* `nn.functional.interpolate`: added support for empty batch size input for interpolate. ([#32400](https://github.com/pytorch/pytorch/pull/32400)).\r\n* `nn.functional.pad`: Also return a new tensor instead of sometimes returning a view ([#32350](https://github.com/pytorch/pytorch/pull/32350)).\r\n* `nn.functional.grid_sample`: Fixed gradient computation at image borders ([#32829](https://github.com/pytorch/pytorch/pull/32829)).\r\n* `nn.functional.leaky_relu_`: disabled incorrect leaky_relu_ negative slope backward calculation ([#33639](https://github.com/pytorch/pytorch/pull/33639)).\r\n* `optim.LambdaLR`: removed unintentional side effects ([#32848](https://github.com/pytorch/pytorch/pull/32848)).\r\n* `optim.Adam`, `optim.AdamW`: Added missing `weight_decay` parameter validation ([#33126](https://github.com/pytorch/pytorch/pull/33126)).\r\n* `optim.MultiStepLR`: Fix \u201cunbound local variable\u201d error by removing return value for `__exit__` ([#32997](https://github.com/pytorch/pytorch/pull/32997)).\r\n* `optim.MultiStepLR`: Fixed broken `step()` method ([#33356](https://github.com/pytorch/pytorch/pull/33356)).\r\n* `torch.autograd`: added new error message if incorrect usage would cause a deadlock ([#32295](https://github.com/pytorch/pytorch/pull/32295)).\r\n* `torch.autograd`: Prohibited copying autograd engines ([#34567](https://github.com/pytorch/pytorch/pull/34567)).\r\n* `torch.autograd`: Fixed incorrect handling of functions that return multiple views ([#32790](https://github.com/pytorch/pytorch/pull/32790)).\r\n* `autograd.Function`: Fixed error if `Function` returned a view in a `torch.no_grad` block ([#33896](https://github.com/pytorch/pytorch/pull/33896)).\r\n* `autograd.Function`: Added more error checks for incorrect behavior ([#33069](https://github.com/pytorch/pytorch/pull/33069)).\r\n* `autograd.Function`: Added nice error message if missing overrides ([#33142](https://github.com/pytorch/pytorch/pull/33142)).\r\n* `autograd.Function`: Fixed version check for `grad_fn` for views ([#34145](https://github.com/pytorch/pytorch/pull/34145)).\r\n* `autograd.profiler`: Fix incorrect chrome trace formatting output for CUDA traces ([#33987](https://github.com/pytorch/pytorch/pull/33987)).\r\n* `multiprocessing.util.register_after_fork`: fixed crash on Windows  ([#30809](https://github.com/pytorch/pytorch/pull/30809)).\r\n* `utils.data.DataLoader`: Fixed potential hang when exiting main process ([#33721](https://github.com/pytorch/pytorch/pull/33721)).\r\n* `utils.tensorboard.SummaryWriter` fixed `scale_factor` calculation for uint8 tensor ([#31778](https://github.com/pytorch/pytorch/pull/31778)).\r\n* `utils.tensorboard` Fix for when PyTorch model trace has RecursiveScriptModules ([#30430](https://github.com/pytorch/pytorch/pull/30430)).\r\n* Fixed `CPU_INTEL` flag error on Windows ([#30564](https://github.com/pytorch/pytorch/pull/30564)).\r\n* Don't use `RTLD_GLOBAL` to load `_C`, resolving a multitude of weird segfaults and crashes\r\n    when PyTorch is imported along with other packages ([#31162](https://github.com/pytorch/pytorch/pull/31162)).\r\n* Fixed dll load logic for Python 3.8 on Windows ([#32215](https://github.com/pytorch/pytorch/pull/32215)).\r\n* `quasirandom.SobolEngine`: Fixed crash when default tensor type is CUDA ([#32496](https://github.com/pytorch/pytorch/pull/32496)).\r\n\r\n* Fixed error message when converting NumPy array with negative strides to a `torch.Tensor` ([#33254](https://github.com/pytorch/pytorch/pull/33254)).\r\n* Fixed crash when indexing a `torch.Tensor` with a single-element array ([#33456](https://github.com/pytorch/pytorch/pull/33456)).\r\n* Fixed crash when converting CUDA tensors and non-strided tensors to NumPy arrays ([#33612](https://github.com/pytorch/pytorch/pull/33612)).\r\n* Prevented crash on exit from static destructor race on Windows ([#33955](https://github.com/pytorch/pytorch/pull/33955)).\r\n* Fixed uncaught `std::domain_error` on macOS ([#34301](https://github.com/pytorch/pytorch/pull/34301)).\r\n* Don\u2019t reset worker affinity when using operators that call into OpenMP ([#29006](https://github.com/pytorch/pytorch/pull/29006)).\r\n* `torch.backends.mkldnn`: changed to be usable without import ([#32055](https://github.com/pytorch/pytorch/pull/32055)).\r\n\r\n\r\n\r\n\r\n# Performance\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Java Tensor hybrid, owns at::Tensor, no memcopy for java outputs. ([#30501](https://github.com/pytorch/pytorch/pull/30501)).\r\n* Tensor prep from image in native ([#31426](https://github.com/pytorch/pytorch/pull/31426)).\r\n* Pass to remove prepacking ops. ([#34319](https://github.com/pytorch/pytorch/pull/34319)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Per channel quantization performance improvement ([#33772](https://github.com/pytorch/pytorch/pull/33772)).\r\n* Speed up per-channel min-max observer ([#34118](https://github.com/pytorch/pytorch/pull/34118)).\r\n* Vectorized qmul and more methods on qint data types ([#34376](https://github.com/pytorch/pytorch/pull/34376)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Improve `ProcessGroupAgent` serialization speed ([#29785](https://github.com/pytorch/pytorch/pull/29785)).\r\n* Avoid sending large unneeded data over wire in `ProcessGroupAgent`. ([#31357](https://github.com/pytorch/pytorch/pull/31357)).\r\n* Integrate async mode for autograd engine with distributed autograd. ([#31508](https://github.com/pytorch/pytorch/pull/31508)).\r\n* Make handling of `FORWARD_AUTOGRAD_REQ` in `request_callback_impl` nonblocking ([#32476](https://github.com/pytorch/pytorch/pull/32476)).\r\n\r\n\r\n\r\n## Other\r\n\r\n* Major multithreaded performance regression when doing operator calls resolved ([#30333](https://github.com/pytorch/pytorch/pull/30333))\r\n* Improved performance of comparison ops on CUDA ([#29743](https://github.com/pytorch/pytorch/pull/29743)).\r\n* `Tensor.view` improved performance ([#30554](https://github.com/pytorch/pytorch/pull/30554)).\r\n* Improved tensor creation overhead ([#30452](https://github.com/pytorch/pytorch/pull/30452), [#30709](https://github.com/pytorch/pytorch/pull/30709))\r\n* `nn.SmoothL1Loss`: vectorized gradient computation on CPU. ([#30046](https://github.com/pytorch/pytorch/pull/30046)).\r\n* `nn.EmbeddingBag`: improved performance on CPU ([#30701](https://github.com/pytorch/pytorch/pull/30701), [#27477](https://github.com/pytorch/pytorch/pull/27477)).\r\n* `nn.LayerNorm`: optimized with explicit vectorization using Vec256 ([#31127](https://github.com/pytorch/pytorch/pull/31127)).\r\n* `Tensor.copy_`: fixed kernel speed regression introduced in #29631 ([#31279](https://github.com/pytorch/pytorch/pull/31279)).\r\n* Moved a number of debug asserts to not compile in release builds ([#31240](https://github.com/pytorch/pytorch/pull/31240)).\r\n* `Tensor::has_names` sped up for unnamed tensors ([#31436](https://github.com/pytorch/pytorch/pull/31436)).\r\n* `torch.index_select`: optimized performance on CPU ([#30598](https://github.com/pytorch/pytorch/pull/30598)).\r\n* `nn.Conv{1,2,3}d`: Improved performance by refactoring `bias` handling for cuDNN backend ([#31524](https://github.com/pytorch/pytorch/pull/31524)).\r\n* `torch.norm`: Optimized case where `p = 2` ([#31903](https://github.com/pytorch/pytorch/pull/31903)).\r\n* `nn.utils.clip_grad_norm_`: Refactored the computation for more performance ([#32020](https://github.com/pytorch/pytorch/pull/32020)).\r\n* Made an assert on a hotpath trigger only in DEBUG mode ([#32117](https://github.com/pytorch/pytorch/pull/32117)).\r\n* First steps toward TensorIterator unrolling and vectorized load ([#31974](https://github.com/pytorch/pytorch/pull/31974)).\r\n* `nn.functional.normalize`: changed to use `clamp_min_`  ([#32360](https://github.com/pytorch/pytorch/pull/32360)).\r\n* Stopped refreshing numel on a stride update ([#32116](https://github.com/pytorch/pytorch/pull/32116)).\r\n* `nn.functional.softplus`: vectorized operator and gradient computation on CPU ([#32944](https://github.com/pytorch/pytorch/pull/32944)).\r\n* `torch.gather` regression fixed by not materializing loop vars in error message ([#33108](https://github.com/pytorch/pytorch/pull/33108)).\r\n* `nn.ELU` forward and backward vectorized on CPU ([#32985](https://github.com/pytorch/pytorch/pull/32985), [#32986](https://github.com/pytorch/pytorch/pull/32986))\r\n* `torch.cat`: optimized performance on CPU ([#30806](https://github.com/pytorch/pytorch/pull/30806), [#33534](https://github.com/pytorch/pytorch/pull/33534)).\r\n* `torch.conv3d`: optimized Unfold3d to improve performance ([#33191](https://github.com/pytorch/pytorch/pull/33191)).\r\n* Workaround performance bug and memory leak in GOMP for AMD CPUs ([#32875](https://github.com/pytorch/pytorch/pull/32875)).\r\n* Improved TensorIterator overhead ([#33165](https://github.com/pytorch/pytorch/pull/33165)).\r\n* `torch.conv3d`: optimized Unfold3dAcc to improve gradient computation performance ([#33317](https://github.com/pytorch/pytorch/pull/33317)).\r\n* `torch.roll` improved performance ([#33623](https://github.com/pytorch/pytorch/pull/33623)).\r\n* Bounds checking for functor execution in vectorized/unrolled kernels ([#33642](https://github.com/pytorch/pytorch/pull/33642)).\r\n* `nn.EmbeddingBag`: improved performance on CUDA ([#33589](https://github.com/pytorch/pytorch/pull/33589)).\r\n* Remove unnecessary tensor copies while calling operators ([#33732](https://github.com/pytorch/pytorch/pull/33732)).\r\n* clang intrinsics targeting on Windows ([#33958](https://github.com/pytorch/pytorch/pull/33958)).\r\n* `nn.Dropout`: added vectorized CUDA implementation ([#33879](https://github.com/pytorch/pytorch/pull/33879)).\r\n* `nn.UpSampleNearest{1, 2, 3}d` performance on CPU optimized (#31452) ([#31452](https://github.com/pytorch/pytorch/pull/31452)).\r\n* Remove `cudaMemcpy` on full memory overlap ([#34548](https://github.com/pytorch/pytorch/pull/34548)).\r\n* CUDA Loops: move address computation into policy, make `policy.load` load all arguments ([#33720](https://github.com/pytorch/pytorch/pull/33720)).\r\n* `nn.BatchNorm{1, 2, 3}d` contiguous case's performance improved ([#34530](https://github.com/pytorch/pytorch/pull/34530)).\r\n* Add the build for runtime dispatch for AVX, AVX2 instruction set ([#26125](https://github.com/pytorch/pytorch/pull/26125)).\r\n* `nn.RReLU` performance improved up to 5x for inference on CPU  ([#31094](https://github.com/pytorch/pytorch/pull/31094)).\r\n* `nn.LogSigmoid` performance improved up to 10x on CPU ([#30958](https://github.com/pytorch/pytorch/pull/30958)).\r\n* `torch.dist` performance improved up to 2x ([#29714](https://github.com/pytorch/pytorch/pull/29714)).\r\n* `torch.max`, `torch.min` performance improved up to 1.5x on CPU ([#33936](https://github.com/pytorch/pytorch/pull/33936)). \r\n* `nn.GLU` performance improved up to 1.5X on CPU  ([#33179](https://github.com/pytorch/pytorch/pull/33179)).\r\n* `nn.LeakyReLU` performance improved up to 4x ([#29899](https://github.com/pytorch/pytorch/pull/29899)).\r\n* `nn.HardTanh` performance improved up to 5x  ([#30152](https://github.com/pytorch/pytorch/pull/30152)). \r\n\r\n\r\n\r\n# Documentation\r\n\r\n## Python\r\n\r\n* Added documentation for `nn.functional.softplus` ([#30055](https://github.com/pytorch/pytorch/pull/30055), [#32945](https://github.com/pytorch/pytorch/pull/32945)).\r\n* `torch.max`: Added warning about different, nondeterministic behavior on CPU and CUDA ([#31115](https://github.com/pytorch/pytorch/pull/31115)).\r\n* Clarified the documentation for `nn.NLLLoss`  ([#31488](https://github.com/pytorch/pytorch/pull/31488)).\r\n* Exclude generated source docs from Google search indexing ([#31484](https://github.com/pytorch/pytorch/pull/31484)).\r\n* `torch.poisson` docstring added to documentation (#31667) ([#31667](https://github.com/pytorch/pytorch/pull/31667)).\r\n* `torch.eq` fixed incorrect examples in documentation ([#32399](https://github.com/pytorch/pytorch/pull/32399)).\r\n* `torch.load`: added warning regarding pickle insecurity ([#32593](https://github.com/pytorch/pytorch/pull/32593)).\r\n* `optim.CosineAnnealingLR`: fixed the usage in examples ([#31358](https://github.com/pytorch/pytorch/pull/31358)).\r\n* Added doc previewing instructions ([#31905](https://github.com/pytorch/pytorch/pull/31905)).\r\n* Removed legacy `.data` usages from the `torch.nn` documentation ([#31481](https://github.com/pytorch/pytorch/pull/31481)).\r\n* Fixed description of convolution modules ([#30079](https://github.com/pytorch/pytorch/pull/30079)).\r\n* `Tensor.t()`, `Tensor.permute()`, `Tensor.unfold()`, and `Tensor.select()` clarified to note that they return views ([#32512](https://github.com/pytorch/pytorch/pull/32512)).\r\n* `torch.multiprocessing` Updated documentation indicating that start_method is ignored for `mp.spawn()` ([#33070](https://github.com/pytorch/pytorch/pull/33070)).\r\n* Improved CPU threading documentation ([#33083](https://github.com/pytorch/pytorch/pull/33083)).\r\n* `nn.BCELoss`: documented how it avoids infinite results ([#33160](https://github.com/pytorch/pytorch/pull/33160)).\r\n* `nn.utils.rnn.pack_padded_sequence`: Improved the description of `enforce_sorted`  ([#33617](https://github.com/pytorch/pytorch/pull/33617)).\r\n* `nn.utils.pad_packed_sequence`: doc improvement ([#33768](https://github.com/pytorch/pytorch/pull/33768)).\r\n* `nn.LPPool{1,2}d` : removed nonexistent parameter ([#33714](https://github.com/pytorch/pytorch/pull/33714)).\r\n* Created a Tensor View documentation page that documents all PyTorch operations that return views ([#32560](https://github.com/pytorch/pytorch/pull/32560)).\r\n* Added grad context manager doc to top level torch module. ([#33877](https://github.com/pytorch/pytorch/pull/33877)).\r\n* Enhanced reproducibility documentation ([#33795](https://github.com/pytorch/pytorch/pull/33795)).\r\n* Numerous typo fixes ([#30448](https://github.com/pytorch/pytorch/pull/30448), [#30518](https://github.com/pytorch/pytorch/pull/30518), [#30614](https://github.com/pytorch/pytorch/pull/30614), [#30464](https://github.com/pytorch/pytorch/pull/30464), [#30608](https://github.com/pytorch/pytorch/pull/30608), [#24335](https://github.com/pytorch/pytorch/pull/24335), [#34581](https://github.com/pytorch/pytorch/pull/34581), [#34624](https://github.com/pytorch/pytorch/pull/34624), [#34008](https://github.com/pytorch/pytorch/pull/34008), [#31395](https://github.com/pytorch/pytorch/pull/31395), [#31677](https://github.com/pytorch/pytorch/pull/31677), [#31617](https://github.com/pytorch/pytorch/pull/31617), [#31973](https://github.com/pytorch/pytorch/pull/31973), [#32068](https://github.com/pytorch/pytorch/pull/32068), [#33689](https://github.com/pytorch/pytorch/pull/33689), [#30385](https://github.com/pytorch/pytorch/pull/30385), [#32003](https://github.com/pytorch/pytorch/pull/32003), [#31682](https://github.com/pytorch/pytorch/pull/31682), [#30846](https://github.com/pytorch/pytorch/pull/30846), [#33478](https://github.com/pytorch/pytorch/pull/33478), [#33549](https://github.com/pytorch/pytorch/pull/33549), [#32307](https://github.com/pytorch/pytorch/pull/32307), [#33144](https://github.com/pytorch/pytorch/pull/33144), [#33805](https://github.com/pytorch/pytorch/pull/33805), [#33836](https://github.com/pytorch/pytorch/pull/33836), [#34053](https://github.com/pytorch/pytorch/pull/34053)).\r\n* Numerous formatting and/or rendering fixes ([#30377](https://github.com/pytorch/pytorch/pull/30377), [#30779](https://github.com/pytorch/pytorch/pull/30779), [#32667](https://github.com/pytorch/pytorch/pull/32667), [#34027](https://github.com/pytorch/pytorch/pull/34027), [#32911](https://github.com/pytorch/pytorch/pull/32911), [#30814](https://github.com/pytorch/pytorch/pull/30814), [#30815](https://github.com/pytorch/pytorch/pull/30815), [#31760](https://github.com/pytorch/pytorch/pull/31760), [#34503](https://github.com/pytorch/pytorch/pull/34503)).\r\n\r\n\r\n\r\n## C++ API\r\n\r\n* Fix `at::Tensor` docs generation and make it accessible again at https://pytorch.org/cppdocs/api/classat_1_1_tensor.html ([#34467](https://github.com/pytorch/pytorch/pull/34467))\r\n* Add docs for all `torch::nn modules` and functionals ([#34522](https://github.com/pytorch/pytorch/pull/34522)) ([#34688](https://github.com/pytorch/pytorch/pull/34688)) ([#34752](https://github.com/pytorch/pytorch/pull/34752))\r\n* Improve C++ autograd and tensor indexing docs ([#35919](https://github.com/pytorch/pytorch/pull/35919))\r\n* Fix example in `torch::nn::ModuleList` docs ([#34463](https://github.com/pytorch/pytorch/pull/34463))\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Reorganize RPC API doc and add introduction ([#30491](https://github.com/pytorch/pytorch/pull/30491), [#35109](https://github.com/pytorch/pytorch/pull/35109)).\r\n* Make doc source format consistent in `rpc/init.cpp` ([#30515](https://github.com/pytorch/pytorch/pull/30515)).\r\n* Add examples to RRef doc ([#30516](https://github.com/pytorch/pytorch/pull/30516)).\r\n* Add more details to explain `rpc_backend_options` arg in `init_rpc` ([#30855](https://github.com/pytorch/pytorch/pull/30855)).\r\n* Fix examples in API doc ([#30856](https://github.com/pytorch/pytorch/pull/30856)).\r\n* Fix examples in RRef API doc ([#30857](https://github.com/pytorch/pytorch/pull/30857)).\r\n* Document WorkerInfo and `RpcBackendOptions` structures in RPC docs. ([#31077](https://github.com/pytorch/pytorch/pull/31077)).\r\n* Explain RPC behavior when using Tensor as arg or return value ([#31968](https://github.com/pytorch/pytorch/pull/31968)).\r\n* Update RPC docs to reflect correct use of dist_autograd backwards and dist_optim `step() `([#34670](https://github.com/pytorch/pytorch/pull/34670)).\r\n* Minor doc tweak to use mp.spawn in example ([#30381](https://github.com/pytorch/pytorch/pull/30381)).\r\n* Update distributed autograd note ([#34657](https://github.com/pytorch/pytorch/pull/34657)).\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Add info about transitive dependencies in case of using local aars ([#30128](https://github.com/pytorch/pytorch/pull/30128)).\r\n* Update Docs for building PyTorch for Android. ([#32578](https://github.com/pytorch/pytorch/pull/32578)).\r\n* Javadoc changes ([#31956](https://github.com/pytorch/pytorch/pull/31956)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Updates to quantization documentation ([#30288](https://github.com/pytorch/pytorch/pull/30288)).\r\n* Fix docs so that the example works ([#30120](https://github.com/pytorch/pytorch/pull/30120)).\r\n* Add the explicit per-tensor/per-channel quant info when we print the module ([#30591](https://github.com/pytorch/pytorch/pull/30591)).\r\n* Fixed typos in quantization docs / docstrings ([#34182](https://github.com/pytorch/pytorch/pull/34182)).\r\n* Docs entry for the `is_quantized` ([#32075](https://github.com/pytorch/pytorch/pull/32075)).\r\n\r\n\r\n\r\n# Deprecations\r\n\r\n## Python\r\n\r\n### How to figure out which line in your code is raising a warning\r\n\r\nAttempting to use deprecated behavior will raise warnings. Unfortunately, sometimes it is not entirely obvious what line of code the warning corresponds to, especially if the the warning comes from our C++ backend. For example, with a file named `foo.py` with the following contents,\r\n\r\n```\r\nimport torch\r\n# This is newly deprecated behavior, see the next section\r\ntorch.tensor(1) / torch.tensor(2)\r\n```\r\n\r\nrunning it doesn\u2019t give us the location of the warning:\r\n\r\n```\r\n> python foo.py\r\n../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true\r\n division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\r\n```\r\n\r\nWe can use the `warnings` module to tell us where the warning is by asking it to treat warnings as errors:\r\n\r\n```\r\nimport torch\r\nimport warnings\r\nwarnings.filterwarnings('error', message='Integer division')\r\n# This is newly deprecated behavior, see the next section\r\ntorch.tensor(1) / torch.tensor(2)\r\n```\r\n\r\nRunning the file now tells us exactly where the warning is:\r\n\r\n```\r\n> python foo.py\r\nTraceback (most recent call last):\r\n  File \"foo.py\", line 5, in <module>\r\n    torch.tensor(1) / torch.tensor(2)\r\nUserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide\r\nor floor_divide (// in Python) instead.\r\n```\r\n\r\n\r\n\r\n### Deprecated `torch.div` and `torch.addcdiv` integer floor division behavior ([#34570](https://github.com/pytorch/pytorch/pull/34570))\r\n\r\nIn 1.5.0 and older PyTorch releases `torch.div` and the `/` operator perform integer floor division. In a future PyTorch release, torch.div (including the `/` operator) will perform \"true\" division as in Python3 and NumPy.\r\n\r\nTo floor divide integer tensors, please use `torch.floor_divide` instead.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Before</th><th>After</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(3) / torch.tensor(2)\r\n../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\r\ntensor(1)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> NB: the following is equivalent to `torch.floor_divide(torch.tensor(3), torch.tensor(2))\r\n>>> torch.tensor(3) // torch.tensor(2)\r\ntensor(1)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThe fix for `torch.addcdiv` is similar.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Before</th><th>After</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> torch.addcdiv(input, tensor, other, value=value)\r\n../aten/src/ATen/native/PointwiseOps.cpp:81: UserWarning: Integer division with addcdiv is deprecated, and in a future  release addcdiv will perform a true division of tensor1 and tensor2. The current addcdiv behavior can be replicated using floor_divide for integral inputs (self + value * tensor1 // tensor2) and division for float inputs (self + value * tensor1 / tensor2). The new addcdiv behavior can be implemented with true_divide (self + value * torch.true_divide(tensor1, tensor2).\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> (input + torch.floor_divide(value * tensor, other))\r\ntensor(0)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Deprecated `torch.full` returning float tensors if no dtype is specified ([#34709](https://github.com/pytorch/pytorch/pull/34709)).\r\n\r\nIn a future PyTorch release, `torch.full` will infer its dtype from its fill value when the optional dtype and out parameters are unspecified, matching NumPy's inference for `numpy.full`. For example, `torch.full(size, 1)` will return a tensor of `torch.long` dtype, unlike today where it returns a tensor of `torch.float` dtype.\r\n\r\n\r\n### Deprecated `torch.nn.modules.conv._ConvTransposeMixin` ([#31784](https://github.com/pytorch/pytorch/pull/31784)).\r\n\r\nThis is an internal-facing class that is not a part of our public API. We\u2019ve refactored some PyTorch internals to work without it and will remove it in a future release. \r\n\r\n\r\n### Deprecated positional args in multiple `torch` function signatures ([#32009](https://github.com/pytorch/pytorch/pull/32009), [#33428](https://github.com/pytorch/pytorch/pull/33428))\r\n\r\nBelow please find a list of deprecated signatures and what to change them to.\r\n\r\n* `torch.add(self: Tensor, alpha: Scalar, other: Tensor)`, `torch.sub(self: Tensor, alpha: Scalar, other: Tensor)` please use `alpha` as a keyword-only arg instead of positional args\r\n* `torch.addbmm(beta: Scalar, self: Tensor, alpha: Scalar, batch1: Tensor, batch2: Tensor)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.addcdiv(self: Tensor, value: Scalar, tensor1: Tensor, tensor2: Tensor)`, `torch.addmdiv(self: Tensor, value: Scalar, tensor1: Tensor, tensor2: Tensor)`: please use `value` as a keyword-only arg\r\n* `torch.addmm(beta: Scalar, self: Tensor, alpha: Scalar, mat1: Tensor, mat2: Tensor)`, `torch.sspaddmm(beta: Scalar, self: Tensor, alpha: Scalar, mat1: Tensor, mat2: Tensor)` please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.addmv(beta: Scalar, self: Tensor, alpha: Scalar, mat: Tensor, vec: Tensor)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.addr(beta: Scalar, self: Tensor, alpha: Scalar, vec1: Tensor, vec2: Scalar)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.baddbmm(beta: Scalar, self: Tensor, alpha: Scalar, batch1: Tensor, batch2: Tensor)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Before</th><th>After</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.zeros(2,3).add(2, torch.ones(2, 3))\r\n../torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add is deprecated:\r\n        add(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n        add(Tensor other, Number alpha)\r\ntensor([[2., 2., 2.],\r\n        [2., 2., 2.]])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.zeros(2, 3).add(torch.ones(2, 3), alpha=2)\r\ntensor([[2., 2., 2.],\r\n        [2., 2., 2.]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Deprecate modifying in-place a view that returned by a custom autograd Function ([#32839](https://github.com/pytorch/pytorch/pull/32839)). \r\n\r\nModifying in-place a view that was created by a custom Function leads to the custom backward not being called or being called with a partial gradient. This behavior will be removed in 1.6.\r\n\r\nPlease clone() the output of the Function to avoid incorrect gradient computation.\r\n\r\n```\r\nclass Id(Function):\r\n    @staticmethod\r\n    def forward(ctx, input):\r\n        return input.view_as(input)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_input):\r\n        return grad_input\r\n```\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.randn(3, requires_grad=True)\r\n>>> other = torch.randn(3)\r\n>>> output = Id.apply(input)\r\n>>> output.copy_(other)\r\n# Warning: Incorrect gradients\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.randn(3, requires_grad=True)\r\n>>> other = torch.randn(3)\r\n>>> output = Id.apply(input).clone()\r\n>>> output.copy_(other)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Deprecate modifying in-place a view created inside a no_grad block (#32839)\r\n\r\nModifying in-place a view created inside a no_grad block is ambiguous and error-prone so we have deprecated it. \r\n\r\nHere is an example of some code that we\u2019ve deprecated. In previous versions of PyTorch, the following code throws a non-descriptive error message, but we've added a deprecation in 1.5.0.\r\n\r\n```\r\n>>> base = torch.rand(10, requires_grad=True)\r\n>>> var = torch.rand([], requires_grad=True)\r\n>>> with torch.no_grad():\r\n>>>     view = base[1]\r\n>>> view.copy_(var)\r\n>>> torch.autograd.grad(base.sum(), var)\r\nRuntimeError: A view was created in no_grad mode and is being modified inplace with grad mode enabled. Given that this use case is ambiguous and error-prone,\r\nit is deprecated and will be forbidden  starting 1.6 (see https://github.com/pytorch/pytorch/pull/32839 for more details about this). You can clarify your code and remove this warning by moving both the view and the inplace either both inside the no_grad block (if you don't want the inplace to be tracked) or both outside (if you want the inplace to be tracked).\r\n```\r\nIf you want to differentiate, you should change the above code to\r\n```\r\n>>> base = torch.rand(10, requires_grad=True)\r\n>>> var = torch.rand([], requires_grad=True)\r\n>>> view = base[1]\r\n>>> view.copy_(var)\r\n>>> torch.autograd.grad(base.sum(), var)\r\n(tensor(1.),)\r\n```\r\n\r\nIf you don\u2019t want to differentiate, you should change it to\r\n\r\n```\r\n>>> base = torch.rand(10, requires_grad=True)\r\n>>> var = torch.rand([], requires_grad=True)\r\n>>> with torch.no_grad():\r\n>>>     view = base[1]\r\n>>>     view.copy_(var)\r\n```\r\n\r\n\r\n## C++ API\r\n\r\n### Deprecated `Tensor.type()` [(#30281](https://github.com/pytorch/pytorch/pull/30281))\r\n\r\nPlease use `Tensor.options()` instead.\r\n\r\n\r\n# Miscellaneous\r\n\r\n* Part of an automated mixed-precision solution ([#33366](https://github.com/pytorch/pytorch/pull/33366), [#33832](https://github.com/pytorch/pytorch/pull/33832)).\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.5.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.5.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.5.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/25725370", "release_id": 25725370, "date_created": "2020-04-20T23:59:38Z", "date_published": "2020-04-21T16:26:30Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/22877176", "tag": "v1.4.0", "name": "Mobile build customization, Distributed model parallel training, Java bindings, and more", "author": {"name": "nairbv", "type": "User"}, "description": "# PyTorch 1.4.0 Release Notes\r\n- Highlights\r\n- Backwards Incompatible Changes\r\n  * Python\r\n  * JIT\r\n  * C++\r\n- New Features\r\n  * torch.optim\r\n  * Distributed\r\n  * RPC [Experimental]\r\n  * JIT\r\n  * Mobile\r\n- Improvements\r\n  * Distributed\r\n  * JIT\r\n  * Mobile\r\n  * Named Tensors\r\n  * C++ API\r\n  * AMD Support\r\n  * ONNX\r\n  * Quantization\r\n  * Visualization\r\n  * Other Improvements\r\n- Bug Fixes\r\n  * Distributed\r\n  * RPC\r\n  * C++ API\r\n  * JIT\r\n  * Quantization\r\n  * Mobile\r\n  * Other Bug fixes\r\n- Deprecations\r\n- Performance\r\n\r\nThe PyTorch v1.4.0 release is now available.\r\n\r\nThe release contains over 1,500 commits and a significant amount of effort in areas spanning existing areas like JIT, ONNX, Distributed, Performance and Eager Frontend Improvements and improvements to experimental areas like mobile and quantization. It also contains new experimental features including rpc-based model parallel distributed training and language bindings for the Java language (inference only). \r\n\r\n**PyTorch 1.4 is the last release that supports Python 2**.  For the C++ API, it is the last release that supports C++11: you should start migrating to Python 3 and building with C++14 to make the future transition from 1.4 to 1.5 easier.\r\n\r\n\r\n# Highlights\r\n\r\n## PyTorch Mobile - Build level customization \r\n\r\nFollowing the experimental release of [PyTorch Mobile in the 1.3 release](https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/), PyTorch 1.4 adds additional mobile support including the ability to customize build scripts at a fine-grain level. This allows mobile developers to optimize library size by only including the operators used by their models and, in the process, reduce their on device footprint significantly. Initial results show that, for example, a customized MobileNetV2 is 40% to 50% smaller than the prebuilt PyTorch mobile library. [Learn more](https://pytorch.org/mobile/home/) about how to create your own custom builds, and please engage with the community on the [PyTorch forums](https://discuss.pytorch.org/c/mobile) to provide any feedback you have.\r\n\r\n## Distributed Model Parallel Training [Experimental]\r\n\r\n With the scale of models, such as RoBERTa, continuing to increase into the billions of parameters, model parallel training has become ever more important to help researchers push the limits. This release provides a distributed RPC framework to support distributed model parallel training. It allows for running functions remotely and referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backwards and update parameters across RPC boundaries. \r\n\r\nTo learn more about the APIs and the design of this feature, see the links below:\r\n\r\n* [API documentation](https://pytorch.org/docs/stable/rpc.html)\r\n* [Distributed Autograd design doc](https://pytorch.org/docs/stable/notes/distributed_autograd.html)\r\n* [Remote Reference design doc](https://pytorch.org/docs/stable/notes/rref.html)\r\n\r\nFor the full tutorials, see the links below: \r\n\r\n* [A full RPC tutorial](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)\r\n* [Examples using model parallel training for reinforcement learning and with an LSTM](https://github.com/pytorch/examples/tree/master/distributed/rpc)\r\n\r\n As always, you can connect with community members and discuss more on the [forums](https://discuss.pytorch.org/c/distributed/distributed-rpc). \r\n\r\n\r\n## Java bindings [Experimental] \r\n\r\n In addition to supporting Python and C++, this release adds experimental support for Java bindings. Based on the interface developed for Android in PyTorch Mobile, the new bindings allow you to invoke TorchScript models from any Java program. Note that the Java bindings are only available for Linux for this release, and for inference only. We expect support to expand in subsequent releases. See the code snippet below for how to use PyTorch within Java:\r\n\r\nLearn more about how to use PyTorch from Java [here](https://github.com/pytorch/java-demo), and see the full Javadocs API documentation [here](https://pytorch.org/docs/stable/packages.html).\r\n\r\n## Pruning\r\n\r\nPruning functionalities have been added to PyTorch in the `nn.utils.prune` module. This provides out-of-the-box support for common magnitude-based and random pruning techniques, both structured and unstructured, both layer-wise and global, and it also enables custom pruning from user-provided masks.\r\n\r\nTo prune a tensor, first select a pruning technique among those available in `nn.utils.prune` (or implement your own by subclassing `BasePruningMethod`). \r\n```python\r\nfrom torch.nn.utils import prune\r\nt = torch.rand(2, 5)\r\np = prune.L1Unstructured(amount=0.7)\r\npruned_tensor = p.prune(t)\r\n```\r\n\r\nTo prune a module, select one of the pruning functions available in `nn.utils.prune` (or implement your own) and specify which module and which parameter within that module pruning should act on.\r\n```python\r\nm = nn.Conv2d(3, 1, 2)\r\nprune.ln_structured(module=m, name='weight', amount=5, n=2, dim=1)\r\n```\r\n\r\nPruning reparametrizes the module by turning `weight` (in the example above) from a parameter to an attribute, and replacing it with a new parameter called `weight_orig` (i.e. appending `\"_orig\"` to the initial parameter `name`) that stores the unpruned version of the tensor. The pruning mask is stored as a buffer named `weight_mask` (i.e. appending `\"_mask\"` to the initial parameter `name`). Pruning is applied prior to each forward pass by recomputing `weight` through a multiplication with the updated mask using PyTorch's `forward_pre_hooks`.\r\n\r\nIterative pruning is seamlessly enabled by repeatedly calling pruning functions on the same parameter (this automatically handles the combination of successive masks by making use of a `PruningContainer` under the hood).\r\n\r\n`nn.utils.prune` is easily extensible to support new pruning functions by subclassing the `BasePruningMethod` base class and implementing the `compute_mask` method with the instructions to compute the mask according to the logic of the new pruning technique.\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## Python\r\n\r\n### `torch.optim`: It is no longer supported to use `Scheduler.get_lr()` to obtain the last computed learning rate.  to get the last computed learning rate, call `Scheduler.get_last_lr()` instead.  ([26423](https://github.com/pytorch/pytorch/pull/26423))\r\n\r\nLearning rate schedulers are now \u201cchainable,\u201d as mentioned in the *New Features* section below.  `Scheduler.get_lr` was sometimes used for monitoring purposes to obtain the current learning rate.  But since `Scheduler.get_lr` is also used internally for computing new learning rates, this actually returns a value that is \u201cone step ahead.\u201d  To get the last computed learning rate, use `Scheduler.get_last_lr` instead.\r\n\r\nNote that `optimizer.param_groups[0]['lr']` was in version 1.3.1 and remains in 1.4.0 a way of getting the current learning rate used in the optimizer.\r\n\r\n### `Tensor.unfold` on a 0-dimensional Tensor now properly returns a 1-dimensional Tensor.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.1</th><th>Version 1.4.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5).unfold(dimension=0, size=1, step=1)\r\ntensor(5)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5).unfold(dimension=0, size=1, step=1)\r\ntensor([5])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.symeig` now return a 0-element eigenvectors tensor when `eigenvectors=False` (the default).\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.1</th><th>Version 1.4.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.symeig(torch.randn(3,3)).eigenvectors.shape\r\ntorch.Size([3, 3])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.symeig(torch.randn(3,3)).eigenvectors.shape\r\ntorch.Size([0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## JIT\r\n\r\n* Make `torch.jit.get_trace_graph` private (it is now `torch.jit._get_trace_graph`) ([29149](https://github.com/pytorch/pytorch/pull/29149))\r\n    * This function was intended only for ONNX integration; use `traced_module.graph` instead, like:\r\n    * traced_module = torch.jit.trace(my_module, example_inputs)\r\n        traced_graph = traced_module.graph\r\n* `@property` on `ScriptModule`s has been disabled ([28395](https://github.com/pytorch/pytorch/pull/28395))\r\n    * Scripted `@property` accesses were silently broken before, where we would evaluate the the `get` function once and store that as the attribute permanently. They properly error now; a workaround is to make your `@property` a regular method.\r\n* Custom ops: `torch::jit::RegisterOperators` has been removed, use `torch::RegisterOperators` instead ([28229](https://github.com/pytorch/pytorch/pull/28229)). The usage and behavior should remain the same.\r\n* Remove` torch.jit._register_*` bindings from Python (e.g. `torch.jit._register_attribute`). These were private functions that were not intended to be used.  ([29499](https://github.com/pytorch/pytorch/pull/29499))\r\n\r\n## C++\r\n\r\n### [C++] The distinction between Tensor and Variable has been eliminated at the C++ level. ([28287](https://github.com/pytorch/pytorch/pull/28287))\r\n\r\nThis change simplifies our C++ API and matches previous changes we did at the python level that merged Tensors and Variables into a single type.\r\n\r\nThis change is unlikely to affect user code; the most likely exceptions are:\r\n\r\n1) [Argument-dependent lookup](https://en.cppreference.com/w/cpp/language/adl) for `torch::autograd` may no longer work.  This can break because Variable is now defined as an alias for Tensor (`using Variable = Tensor;`).  In this case, you must explicitly qualify the calls to `torch::autograd` functions. \r\n\r\n2) Because `Variable` and `Tensor` are now the same type, code which assumes that they are different types (e.g., for the purposes of templating, or `std::enable_if` checks) will not work until you delete the (now) redundant overload/specialization.\r\n\r\n3) Some operators may trace differently.  If this happens, please [file a bug.](https://github.com/pytorch/pytorch/issues/new?template=bug-report.md)  The most likely situations are:\r\n\r\n1. There are now *more* operations in your trace than before (usually, calls to `aten::empty`)\r\n2. There are now *less* operations in your trace than before (e.g., the trace complains that `\"there is no observable dependence\"` with the inputs)\r\n\r\n### [C++] arguments in `torch::nn::LinearOptions` are renamed to match the Python API. ([27382](https://github.com/pytorch/pytorch/pull/27382))\r\n\r\n* Arguments that are renamed:\r\n    * `in` -> `in_features`\r\n    * `out` -> `out_features`\r\n    * `with_bias` -> `bias`\r\n\r\n### [C++] arguments in `torch::nn::Conv{1,2,3}dOptions` are renamed to match the Python API. ([28917](https://github.com/pytorch/pytorch/pull/28917)) ([29838](https://github.com/pytorch/pytorch/pull/29838))\r\n\r\n* Arguments that are renamed:\r\n    * `input_channels` -> `in_channels`\r\n    * `output_channels` -> `out_channels`\r\n    * `with_bias` -> `bias`\r\n\r\n### [C++] `torch::nn::Conv{1,2,3}dOptions` no longer has the `transposed` argument. ([31005](https://github.com/pytorch/pytorch/pull/31005))\r\n\r\n* If users have `transposed` originally set to `true` in `torch::nn::Conv{1,2,3}dOptions`, they should migrate their code to use `torch::nn::ConvTranspose{1,2,3}d` layers instead.\r\n\r\n### [C++] All Reduction enums for `torch::nn` layers and functionals are changed to have `torch::KEnumNAME` syntax. ([27942](https://github.com/pytorch/pytorch/pull/27942), [26837](https://github.com/pytorch/pytorch/pull/26837))\r\n\r\n* Example: previously, to specify \u201cmean\u201d as the reduction method in a torch::nn layer or functional, we would use `torch::Reduction::Mean`. Now, `torch::Reduction::Mean` has been renamed to the shorter `torch::kMean`.\r\n\r\n### [C++] `torch::tensor` constructor is improved to match Python API behavior. ([28523](https://github.com/pytorch/pytorch/pull/28523)) ([29632](https://github.com/pytorch/pytorch/pull/29632)) ([29066](https://github.com/pytorch/pytorch/pull/29066))\r\n\r\n* Shape checking fixes\r\n    * Example 1: previously, `torch::tensor({{1}, {2}})` produced a tensor of sizes `{2}`. Now, it produces a tensor of sizes `{2, 1}`.\r\n    * Example 2: previously, `torch::tensor(1.1)` produced a 1-dim tensor. Now it produces a 0-dim tensor.\r\n* Type inference improvements\r\n    * Example 1: previously, C++ `torch::tensor` with a double (e.g. `torch::tensor(1.1)`) or a (nested) braced-init-list of doubles (e.g. `torch::tensor({{1.1, 2.2}})` produces a tensor with dtype `torch::kDouble`. Now it produces a tensor with dtype `torch::get_default_dtype()`.\r\n    * Example 2: previously, C++ `torch::tensor` with an integer type (e.g. `torch::tensor(1)`) or a (nested) braced-init-list of integer types (e.g. `torch::tensor({{1, 2}})`) produces a tensor with the same dtype. Now it always produces a tensor of dtype `torch::kLong` (aka. `int64_t`).\r\n    * Example 3: previously, when passed a `TensorOptions` without a dtype set to the `torch::tensor` constructor, it always produces a tensor of dtype `torch::get_default_dtype()`. Now it produces a tensor of different dtypes based on the dtype of the braced-init-list and the default dtype.\r\n* Passing a `std::initializer_list` (NOT braced-init-list) to `torch::tensor` will no longer compile, and the user should pass the equivalent braced-init-list to `torch::tensor` instead. For example, write `torch::tensor({1.1, 1.2})` instead of `torch::tensor(std::initializer_list<double>({1.1, 1.2}))`.\r\n\r\n### [C++] Some activation modules\u2019 `forward` function now take `Tensor` instead of `Tensor&` as input. ([28501](https://github.com/pytorch/pytorch/pull/28501))\r\n\r\n`torch::nn` layers affected: `ELU` / `SELU` / `Hardtanh` / `LeakyReLU` / `ReLU` / `ReLU6` / `RReLU` / `CELU`\r\nThis change ensures that the above layers can be used in a `torch::nn::Sequential` module. If your C++ model uses any of the above layers, you must recompile your C++ code with the new libtorch binary.\r\n\r\n# New Features\r\n\r\n## torch.optim\r\n\r\nLearning rate schedulers (`torch.optim.lr_scheduler`) now support \u201cchaining.\u201d This means that two schedulers can be defined and stepped one after the other to compound their effect, see example below. Previously, the schedulers would overwrite each other.\r\n\r\n```\r\n>>> import torch\r\n>>> from torch.optim import SGD\r\n>>> from torch.optim.lr_scheduler import ExponentialLR, StepLR\r\n>>>\r\n>>> model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\r\n>>> optimizer = SGD(model, 0.1)\r\n>>>\r\n>>> scheduler1 = ExponentialLR(optimizer, gamma=0.9)\r\n>>> scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)\r\n>>>\r\n>>> for epoch in range(4):\r\n>>>     print(epoch, scheduler2.get_last_lr()[0])\r\n>>>\r\n>>>     optimizer.step()\r\n>>>     scheduler1.step()\r\n>>>     scheduler2.step()\r\n    \r\n0 0.1\r\n1 0.09000000000000001\r\n2 0.08100000000000002\r\n3 0.00729000000000002\r\n4 0.00656100000000002\r\n```\r\n\r\n## Distributed\r\n\r\n* Add `allgather_coalesced` API to `ProcessGroup` ([28634,](https://github.com/pytorch/pytorch/pull/28634)[29059](https://github.com/pytorch/pytorch/pull/29059))\r\n* Add `abort` API in `ProcessGroupGloo` Send/Recv Work ([29928](https://github.com/pytorch/pytorch/pull/29928)).\r\n* Add `--no_python` flag to allow using a bash script wrapper in the launch command ([29144](https://github.com/pytorch/pytorch/pull/29144)).\r\n\r\n\r\n\r\n## RPC [Experimental] \r\n\r\n`torch.distributed.rpc` is a newly introduced package. It contains basic building blocks to run functions remotely in model training and inference, which will be useful for scenarios like distributed model parallel or implementing parameter server frameworks. More specifically, it contains four pillars: RPC, Remote Reference, Distributed Autograd, and Distributed Optimizer. Please refer to the [documentation](https://pytorch.org/docs/master/rpc.html) and the [tutorial](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html) for more details.\r\n\r\n* Add `rpc_sync` and `rpc_async` for builtin operators and Python user functions ([23228](https://github.com/pytorch/pytorch/pull/23228), [23569](https://github.com/pytorch/pytorch/pull/23569), [28392](https://github.com/pytorch/pytorch/pull/28392)).\r\n* Add `remote` and `RRef` for builtin operators and Python user functions ([25169](https://github.com/pytorch/pytorch/pull/25169), [25499](https://github.com/pytorch/pytorch/pull/25499)).\r\n* Distributed Autograd - FAST mode backward pass implementation. ([27022](https://github.com/pytorch/pytorch/pull/27022), [27576](https://github.com/pytorch/pytorch/pull/27576)).\r\n* Integrate `remote` and `RRef` with distributed autograd ([28630](https://github.com/pytorch/pytorch/pull/28630), [28656](https://github.com/pytorch/pytorch/pull/28656)).\r\n* Add a distributed optimizer ([29304](https://github.com/pytorch/pytorch/pull/29304), [30062](https://github.com/pytorch/pytorch/pull/30062)).\r\n* Add python API for `get_gradients()` method to retrieve gradients from distributed autograd context. ([28926](https://github.com/pytorch/pytorch/pull/28926)).\r\n* Support creating local `RRef`s on local values and to-self `remote` calls ([28948](https://github.com/pytorch/pytorch/pull/28948), [29634](https://github.com/pytorch/pytorch/pull/29634)).\r\n* Support custom pickler for RPC ([30185](https://github.com/pytorch/pytorch/pull/30185)).\r\n* Add default RPC agent options based on the backend type ([30201](https://github.com/pytorch/pytorch/pull/30201)).\r\n* Add local `shutdown` to `ProcessGroup` agent ([30330](https://github.com/pytorch/pytorch/pull/30330)).\r\n\r\n## JIT\r\n\r\n* `script::Module`: implement more of of the nn.Module API ([28828](https://github.com/pytorch/pytorch/pull/28828))\r\n    * In particular, adds the (optionally recursive) methods that iterate over submodules, parameters, etc.\r\n    * Adds a pybind-like `attr()` method to simplify attribute access.\r\n* Add support for `@staticmethod` on `ScriptModule`s ([27163](https://github.com/pytorch/pytorch/pull/27163))\r\n* Support Module Containers as Iterables ([26465](https://github.com/pytorch/pytorch/pull/26465))\r\n* Support Iterables In List Comprehensions ([26768)](https://github.com/pytorch/pytorch/pull/26768)\r\n* Dictionaries now preserve insertion order, and `OrderedDict` is supported ([26465](https://github.com/pytorch/pytorch/pull/26465))\r\n* Add support for `hasattr()` ([29332](https://github.com/pytorch/pytorch/pull/29332))\r\n* TorchScript classes can now be callable ([26743](https://github.com/pytorch/pytorch/pull/26743))\r\n* Add `clone_instance` for `ScriptModule`s ([30168](https://github.com/pytorch/pytorch/pull/30168))\r\n* Add `torch.memory_format` support to the TorchScript ([28544](https://github.com/pytorch/pytorch/pull/28544))\r\n* Custom `forward()` is now allowed on container modules ([28988](https://github.com/pytorch/pytorch/pull/28988))\r\n* Calls to submodules are now preserved in the traced graph ([29261](https://github.com/pytorch/pytorch/pull/29261))\r\n* Add support for module containers to be used as iterables ([28255](https://github.com/pytorch/pytorch/pull/28255))\r\n* Make JIT Serialization support arbitrary std::function<> IO ([28039](https://github.com/pytorch/pytorch/pull/28039))\r\n* Support `layout() `in script ([27100](https://github.com/pytorch/pytorch/pull/27100))\r\n* Methods and functions are no longer inlined in the serialized file format ([26706](https://github.com/pytorch/pytorch/pull/26706))\r\n\r\n## Mobile\r\n\r\n* Build level customization\r\n    * Add custom build script to only include selected operators ([30144](https://github.com/pytorch/pytorch/pull/30144)).\r\n    * Dump operator names used by a script module ([29374](https://github.com/pytorch/pytorch/pull/29374), [30467](https://github.com/pytorch/pytorch/pull/30467)).\r\n    * Disable JIT optimizer in Android wrapper for mobile custom build ([30285](https://github.com/pytorch/pytorch/pull/30285)).\r\n    * FBJNI Gradle ABI_FILTERS parameter ([30135](https://github.com/pytorch/pytorch/pull/30135)).\r\n\r\n# Improvements\r\n\r\n## Distributed\r\n\r\n### Improvements\r\n\r\n* Add timeout support in `ProcessGroupNCCL` ([27224](https://github.com/pytorch/pytorch/pull/27224)).\r\n* Ensure that DDP wrapped module has parameters that require gradients ([25858](https://github.com/pytorch/pytorch/pull/25858)).\r\n* Making `torch/csrc/cuda` NCCL usage safe for NCCL 2.5 ([29014](https://github.com/pytorch/pytorch/pull/29014)).\r\n* Enable `test_distributed` for ROCm but only with NCCL backend ([28814](https://github.com/pytorch/pytorch/pull/28814)).\r\n\r\n### RPC Improvements\r\n\r\n* Separate out RPC to `rpc_sync` and `rpc_async` APIs ([26570](https://github.com/pytorch/pytorch/pull/26570)).\r\n* Make python user function serialization format to be consistent with builtin operators ([27136](https://github.com/pytorch/pytorch/pull/27136)).\r\n* Clean up distributed autograd context on all participants on exit ([27951](https://github.com/pytorch/pytorch/pull/27951)).\r\n* Improve error handling for distributed autograd engine. ([27940](https://github.com/pytorch/pytorch/pull/27940)).\r\n* Scope pybind11 functions to `torch.distributed.{autograd,rpc}` ([27529](https://github.com/pytorch/pytorch/pull/27529)).\r\n* Lift `rpc_timeout` to `RpcAgent` to make it reusable for other `RpcAgent` implementations. ([29341](https://github.com/pytorch/pytorch/pull/29341)).\r\n* Support sending message to self in `process_group_agent` ([29253](https://github.com/pytorch/pytorch/pull/29253)).\r\n* Properly shutdown RPC even in the case of `clean_shutdown=False`. ([29148](https://github.com/pytorch/pytorch/pull/29148)).\r\n* Ensure `initializedContextIds_` map is cleaned up appropriately in distributed autograd engine. ([29787](https://github.com/pytorch/pytorch/pull/29787)).\r\n* Add hash and equality operators for `WorkerInfo` ([29958](https://github.com/pytorch/pytorch/pull/29958)).\r\n* Add `RpcAgentOptions` struct type to bundle arguments for different `RpcAgent`s ([29972](https://github.com/pytorch/pytorch/pull/29972)).\r\n* Mark timeout `FutureMessage`s and throw exceptions in `ProcessGroupAgent` ([29601](https://github.com/pytorch/pytorch/pull/29601)).\r\n* Re-throw python remote exception when using remote reference to itself ([29930](https://github.com/pytorch/pytorch/pull/29930)).\r\n* By default ignore `RRef` leaks during shutdown ([30217](https://github.com/pytorch/pytorch/pull/30217)).\r\n\r\n### Documentation\r\n\r\n* Add Design doc for Distributed Autograd Engine ([29175](https://github.com/pytorch/pytorch/pull/29175), [30068](https://github.com/pytorch/pytorch/pull/30068), [29927](https://github.com/pytorch/pytorch/pull/29927))\r\n* Add Design doc for Remote Reference ([30066](https://github.com/pytorch/pytorch/pull/30066)).\r\n* Add documentation page for `torch.distrbuted.rpc` ([29276](https://github.com/pytorch/pytorch/pull/29276), [28030](https://github.com/pytorch/pytorch/pull/28030), [29971](https://github.com/pytorch/pytorch/pull/29971), [30160](https://github.com/pytorch/pytorch/pull/30160), [30050](https://github.com/pytorch/pytorch/pull/30050), [30069](https://github.com/pytorch/pytorch/pull/30069), [30179](https://github.com/pytorch/pytorch/pull/30179), [30218](https://github.com/pytorch/pytorch/pull/30218), [30240](https://github.com/pytorch/pytorch/pull/30240), [30243](https://github.com/pytorch/pytorch/pull/30243), [30259](https://github.com/pytorch/pytorch/pull/30259)).\r\n\r\n### MISC\r\n\r\n* Add known worker IDs to distributed autograd context ([26324](https://github.com/pytorch/pytorch/pull/26324)).\r\n* Minor tweaks to RPC message API ([28326](https://github.com/pytorch/pytorch/pull/28326)).\r\n* Rename `PythonUDF{Call,Resp}` ([27530](https://github.com/pytorch/pytorch/pull/27530)).\r\n* Use `std::shared_ptr` for `DistAutogradContext` ([29770](https://github.com/pytorch/pytorch/pull/29770)).\r\n* Mark `c10d::~NCCLUtils` as noexcept ([29118](https://github.com/pytorch/pytorch/pull/29118)).\r\n\r\n## JIT\r\n\r\n* Move custom passes to last optimization step ([29256](https://github.com/pytorch/pytorch/pull/29256))\r\n* Represent the original Python name of a module type the same way in traced and scripted modules. ([29912](https://github.com/pytorch/pytorch/pull/29912))\r\n* Only print original SourceRange on highlight ([29708](https://github.com/pytorch/pytorch/pull/29708))\r\n* Error message and ergonomic improvements:\r\n    * Show full call stack in TorchScript exception even when calls were inlined. ([29911](https://github.com/pytorch/pytorch/pull/29911))\r\n    * Reduce error context from 10 -> 3 ([26765](https://github.com/pytorch/pytorch/pull/26765))\r\n    * Fix error report highlight for unmatched type annotation ([27195](https://github.com/pytorch/pytorch/pull/27195))\r\n    * Make default string arguments in schemas human readable ([27088](https://github.com/pytorch/pytorch/pull/27088))\r\n    * Print which output didn't have dependence during trace checking. ([29047](https://github.com/pytorch/pytorch/pull/29047))\r\n* Improvements to save/load and serialization performance:\r\n    * Modules can now share JIT types if their implementation is the same, improving save/load performance ([26666](https://github.com/pytorch/pytorch/pull/26666))\r\n    * Improve float pickling speed. ([28553](https://github.com/pytorch/pytorch/pull/28553))\r\n    * Pickler: convert `std::stringstream` cases for improved performance. ([29351](https://github.com/pytorch/pytorch/pull/29351))\r\n    * Buffer to speed Unpickler ([27727](https://github.com/pytorch/pytorch/pull/27727))\r\n    * Buffer in Pickler to improve performance. ([27720](https://github.com/pytorch/pytorch/pull/27720))\r\n    * In `torch::save()` avoid zip compressing small header records. ([28180](https://github.com/pytorch/pytorch/pull/28180))\r\n    * String optimizations related to serialization. ([28230](https://github.com/pytorch/pytorch/pull/28230))\r\n* Clean up serialized source format ([28129](https://github.com/pytorch/pytorch/pull/28129))\r\n* API for finding a common ancestor block for a pair of nodes ([28864](https://github.com/pytorch/pytorch/pull/28864))\r\n* Make inserted child module names unique ([27237](https://github.com/pytorch/pytorch/pull/27237))\r\n* Better hashing for constant pool ([27733](https://github.com/pytorch/pytorch/pull/27733))\r\n* Improve error messages when a method or attribute is missing ([27110](https://github.com/pytorch/pytorch/pull/27110))\r\n* Display original source range in `Node::print` ([27524](https://github.com/pytorch/pytorch/pull/27524))\r\n* Always use the closure to resolve variable names ([27515](https://github.com/pytorch/pytorch/pull/27515))\r\n\r\n## Mobile\r\n\r\n* Improve Java API / JNI\r\n    * Add module method to allow explicitly destructing native part ([27090](https://github.com/pytorch/pytorch/pull/27090)).\r\n    * Add methods to write image tensor content to buffer ([27359](https://github.com/pytorch/pytorch/pull/27359)).\r\n    * Various improvements to Android API ([27454](https://github.com/pytorch/pytorch/pull/27454), [27455](https://github.com/pytorch/pytorch/pull/27455)).\r\n    * Add support for PyTorch JNI build ([29412](https://github.com/pytorch/pytorch/pull/29412), [42faf961c8](https://github.com/pytorch/pytorch/commit/42faf961c8), [d22f61432d](https://github.com/pytorch/pytorch/commit/d22f61432d)).\r\n    * Various fixes to PyTorch JNI ([29350](https://github.com/pytorch/pytorch/pull/29350), [29861](https://github.com/pytorch/pytorch/pull/29861), [30206](https://github.com/pytorch/pytorch/pull/30206), [30207](https://github.com/pytorch/pytorch/pull/30207)).\r\n* Improve support for older Android NDK\r\n    * Introduce math_compat.h for older Android versions ([28567](https://github.com/pytorch/pytorch/pull/28567)).\r\n    * Define std::strtoll for older Android ([28603](https://github.com/pytorch/pytorch/pull/28603)).\r\n* Improve error message, documentation, debuggability\r\n    * Enable full error message for mobile builds ([29926](https://github.com/pytorch/pytorch/pull/29926)).\r\n    * Update iOS README.md ([27145](https://github.com/pytorch/pytorch/pull/27145)).\r\n    * Update Android README.md ([28533](https://github.com/pytorch/pytorch/pull/28533)).\r\n    * Rename function parameters to avoid [-Werror,-Wshadow] ([30276](https://github.com/pytorch/pytorch/pull/30276)).\r\n    * Fix exception message in Java Tensor ([30776](https://github.com/pytorch/pytorch/pull/30776)).\r\n* Improve support for benchmark and profiling\r\n    * Add Android and iOS test app for benchmark and profiling ([28405](https://github.com/pytorch/pytorch/pull/28405), [28406](https://github.com/pytorch/pytorch/pull/28406), [28469](https://github.com/pytorch/pytorch/pull/28469), [28622](https://github.com/pytorch/pytorch/pull/28622)).\r\n    * Integration with mobile benchmark in PEP ([28437](https://github.com/pytorch/pytorch/pull/28437)).\r\n    * Subscribe for record function and if android do atrace ([28708](https://github.com/pytorch/pytorch/pull/28708)).\r\n* Improve build / CI\r\n    * Improve Android Gradle build and publishing ([26833](https://github.com/pytorch/pytorch/pull/26833), [27389](https://github.com/pytorch/pytorch/pull/27389), [29262](https://github.com/pytorch/pytorch/pull/29262), [29738](https://github.com/pytorch/pytorch/pull/29738)).\r\n    * Misc fixes to the Android test project ([27453](https://github.com/pytorch/pytorch/pull/27453)).\r\n    * Improve XCode build script ([27358](https://github.com/pytorch/pytorch/pull/27358), [28996](https://github.com/pytorch/pytorch/pull/28996), [29002](https://github.com/pytorch/pytorch/pull/29002)).\r\n    * Add testing code to iOS CI jobs ([27593](https://github.com/pytorch/pytorch/pull/27593), [27594](https://github.com/pytorch/pytorch/pull/27594), [27784](https://github.com/pytorch/pytorch/pull/27784), [30133](https://github.com/pytorch/pytorch/pull/30133)).\r\n    * Misc fixes to the iOS TestApp ([27591](https://github.com/pytorch/pytorch/pull/27591), [28356](https://github.com/pytorch/pytorch/pull/28356), [28809](https://github.com/pytorch/pytorch/pull/28809), [29247](https://github.com/pytorch/pytorch/pull/29247), [29962](https://github.com/pytorch/pytorch/pull/29962), [29963](https://github.com/pytorch/pytorch/pull/29963)).\r\n    * Add support for host build to pytorch_android ([27662,](https://github.com/pytorch/pytorch/pull/27662)[27664](https://github.com/pytorch/pytorch/pull/27664)).\r\n    * Add host build Gradle publishing ([29749](https://github.com/pytorch/pytorch/pull/29749)).\r\n    * Add mobile build CI with host toolchain ([30292](https://github.com/pytorch/pytorch/pull/30292)).\r\n\r\n## Named Tensors\r\n\r\n* `torch.addcdiv`, `torch.addcmul` Added named tensor support ([28975](https://github.com/pytorch/pytorch/pull/28975)).\r\n* `torch.{ones,zeros,full,rand,randn}_like` Added named tensor support ([28981](https://github.com/pytorch/pytorch/pull/28981)).\r\n* `torch.cdist` Added named tensor support ([29129](https://github.com/pytorch/pytorch/pull/29129)).\r\n* `torch.equal` Added named tensor support ([29322](https://github.com/pytorch/pytorch/pull/29322)).\r\n* Added named tensor support for comparison ops ([27162](https://github.com/pytorch/pytorch/pull/27162)).\r\n* `Tensor.align_to` Fixed error message ([27221](https://github.com/pytorch/pytorch/pull/27221)).\r\n* `Tensor.align_to` Make method-only. ([27304](https://github.com/pytorch/pytorch/pull/27304)).\r\n* `Tensor.align_to` Accept partially named tensors ([27308](https://github.com/pytorch/pytorch/pull/27308)).\r\n* `torch.mean(Tensor, Dimname)` Fixed autograd support ([29199](https://github.com/pytorch/pytorch/pull/29199)).\r\n* `Tensor.unflatten` Fix when dim is a negative integer (#31208) ([31432](https://github.com/pytorch/pytorch/pull/31432)).\r\n* Fix type errors in examples about Named Tensor ([27828](https://github.com/pytorch/pytorch/pull/27828)).\r\n\r\n## C++ API\r\n\r\n### New torch::nn modules\r\n\r\n* Convolution layers\r\n    * torch::nn::ConvTranspose{1,2,3}d / Unfold ([29721](https://github.com/pytorch/pytorch/pull/29721)) ([27809](https://github.com/pytorch/pytorch/pull/27809)).\r\n* Pooling layers\r\n    * torch::nn::AdaptiveAvgPool{1, 2, 3}d / MaxUnpool{1, 2, 3}d / LPPool{1, 2}d / FractionalMaxPool{2,3}d ([26808](https://github.com/pytorch/pytorch/pull/26808), [26818](https://github.com/pytorch/pytorch/pull/26818), [26819](https://github.com/pytorch/pytorch/pull/26819)) ([26896](https://github.com/pytorch/pytorch/pull/26896), [26915](https://github.com/pytorch/pytorch/pull/26915), [27027](https://github.com/pytorch/pytorch/pull/27027)) ([27800](https://github.com/pytorch/pytorch/pull/27800), [28492](https://github.com/pytorch/pytorch/pull/28492), [29584](https://github.com/pytorch/pytorch/pull/29584)) ([29933](https://github.com/pytorch/pytorch/pull/29933)).\r\n* Loss layers\r\n    * torch::nn::HingeEmbeddingLoss / CosineEmbeddingLoss /MultiMarginLoss ([27101](https://github.com/pytorch/pytorch/pull/27101)) ([27345](https://github.com/pytorch/pytorch/pull/27345)) ([27424](https://github.com/pytorch/pytorch/pull/27424)) ([27770](https://github.com/pytorch/pytorch/pull/27770)).\r\n    * torch::nn::TripletMarginLoss / SoftMarginloss / MultiLabelMargin / MarginRankingLoss / MultiLabelSoftMarginLoss ([27713](https://github.com/pytorch/pytorch/pull/27713), [27956](https://github.com/pytorch/pytorch/pull/27956)) ([27660](https://github.com/pytorch/pytorch/pull/27660)) ([27659](https://github.com/pytorch/pytorch/pull/27659)) ([29000](https://github.com/pytorch/pytorch/pull/29000)) ([27669](https://github.com/pytorch/pytorch/pull/27669)).\r\n    * torch::nn::MSELoss / KLDivLoss / BCELoss / SmoothL1Loss / PoissonNLLLoss / BCEWithLogitsLoss ([27156](https://github.com/pytorch/pytorch/pull/27156)) ([28806](https://github.com/pytorch/pytorch/pull/28806)) ([30146](https://github.com/pytorch/pytorch/pull/30146)) ([27661](https://github.com/pytorch/pytorch/pull/27661)) ([28755](https://github.com/pytorch/pytorch/pull/28755)) ([28783](https://github.com/pytorch/pytorch/pull/28783)).\r\n    * torch::nn::NLLLoss / CrossEntropyLoss / CTCLoss ([29812](https://github.com/pytorch/pytorch/pull/29812)) ([28654](https://github.com/pytorch/pytorch/pull/28654)).\r\n* Normalization Layers\r\n    * torch::nn::LayerNorm / InstanceNorm{1,2,3}d / BatchNorm{1,2,3}d / GroupNorm / LocalResponseNorm / CrossMapLRN2d ([28032](https://github.com/pytorch/pytorch/pull/28032)) ([28790](https://github.com/pytorch/pytorch/pull/28790)) ([28176](https://github.com/pytorch/pytorch/pull/28176), [28936](https://github.com/pytorch/pytorch/pull/28936)) ([29920](https://github.com/pytorch/pytorch/pull/29920)) ([28759](https://github.com/pytorch/pytorch/pull/28759)) ([29039](https://github.com/pytorch/pytorch/pull/29039)).\r\n* Activation Layers\r\n    * torch::nn::ELU / LeakyReLU / SELU / PReLU / ReLU / ReLU6 / RRelu / CELU / GLU ([27028)](https://github.com/pytorch/pytorch/pull/27028) ([27059](https://github.com/pytorch/pytorch/pull/27059)) ([27434](https://github.com/pytorch/pytorch/pull/27434)) ([27429](https://github.com/pytorch/pytorch/pull/27429)) ([27435](https://github.com/pytorch/pytorch/pull/27435)) ([27436](https://github.com/pytorch/pytorch/pull/27436)) ([27437](https://github.com/pytorch/pytorch/pull/27437)) ([27487](https://github.com/pytorch/pytorch/pull/27487)) ([29922](https://github.com/pytorch/pytorch/pull/29922)).\r\n    * torch::nn::Sigmoid / LogSigmoid / LogSoftmax / Softmax / Softmax2d / Softplus / Softmin / Softsign / Softshrink / Hardshrink / Hardtanh / Tanh / Threshold ([27488](https://github.com/pytorch/pytorch/pull/27488)) ([27060](https://github.com/pytorch/pytorch/pull/27060)) ([27462](https://github.com/pytorch/pytorch/pull/27462)) ([27446](https://github.com/pytorch/pytorch/pull/27446)) ([27509](https://github.com/pytorch/pytorch/pull/27509)) ([27489](https://github.com/pytorch/pytorch/pull/27489)) ([27459](https://github.com/pytorch/pytorch/pull/27459)) ([27535](https://github.com/pytorch/pytorch/pull/27535)) ([27534](https://github.com/pytorch/pytorch/pull/27534)) ([27035](https://github.com/pytorch/pytorch/pull/27035)) ([27537](https://github.com/pytorch/pytorch/pull/27537)) ([27038](https://github.com/pytorch/pytorch/pull/27038)) ([27536](https://github.com/pytorch/pytorch/pull/27536)) ([27538](https://github.com/pytorch/pytorch/pull/27538)).\r\n* Dropout Layers\r\n    * torch::nn::Dropout / Dropout{2, 3}d / AlphaDropout / FeatureAlphaDropout ([29761](https://github.com/pytorch/pytorch/pull/29761)) ([28424](https://github.com/pytorch/pytorch/pull/28424)).\r\n* Padding Layers\r\n    * torch::nn::ReflectionPad{1, 2}d / ReplicationPad{1,2,3}d / ZeroPad2d / ConstantPad{1,2,3}d ([28538](https://github.com/pytorch/pytorch/pull/28538)) ([28539](https://github.com/pytorch/pytorch/pull/28539)) ([28540](https://github.com/pytorch/pytorch/pull/28540)) ([28541](https://github.com/pytorch/pytorch/pull/28541)).\r\n* Embedding layers\r\n    * torch::nn::Embedding / EmbeddingBag ([26358](https://github.com/pytorch/pytorch/pull/26358)).\r\n* Linear layers\r\n    * torch::nn::Bilinear / Flatten ([26082](https://github.com/pytorch/pytorch/pull/26082)) ([28072](https://github.com/pytorch/pytorch/pull/28072)).\r\n* Vision layers\r\n    * torch::nn::Upsample / PixelShuffle ([28413](https://github.com/pytorch/pytorch/pull/28413)) ([28140](https://github.com/pytorch/pytorch/pull/28140)).\r\n\r\n### New torch::nn::functional functions\r\n\r\n* Convolution functions\r\n    * torch::nn::functional::conv{1,2,3}d / conv_transpose{1,2,3}d / fold / unfold ([28917](https://github.com/pytorch/pytorch/pull/28917)) ([29721](https://github.com/pytorch/pytorch/pull/29721)) ([28732](https://github.com/pytorch/pytorch/pull/28732)) ([27809](https://github.com/pytorch/pytorch/pull/27809)).\r\n* Pooling functions\r\n    * torch::nn::functional::adaptive_avg_pool{1, 2, 3}d / lp_pool{1, 2}d / fractional_max_pool{2, 3}d / fractional_max_pool{2, 3}d_with_indices ([26808](https://github.com/pytorch/pytorch/pull/26808), [26818](https://github.com/pytorch/pytorch/pull/26818), [26819](https://github.com/pytorch/pytorch/pull/26819)) ([27800](https://github.com/pytorch/pytorch/pull/27800), [28492](https://github.com/pytorch/pytorch/pull/28492)) ([29584](https://github.com/pytorch/pytorch/pull/29584)) ([29933](https://github.com/pytorch/pytorch/pull/29933)).\r\n* Loss functions\r\n    * torch::nn::functional::hinge_embedding_loss / multi_margin_loss / multilabel_soft_margin_loss / triplet_margin_loss / soft_margin_loss / margin_ranking_loss ([27101](https://github.com/pytorch/pytorch/pull/27101)) ([27424](https://github.com/pytorch/pytorch/pull/27424)) ([27669](https://github.com/pytorch/pytorch/pull/27669)) ([27713](https://github.com/pytorch/pytorch/pull/27713)) ([27660](https://github.com/pytorch/pytorch/pull/27660)) ([29000](https://github.com/pytorch/pytorch/pull/29000)).\r\n    * torch::nn::functional::poisson_nll_loss / nll_loss / cross_entropy / binary_cross_entropy_with_logits ([28755](https://github.com/pytorch/pytorch/pull/28755)) ([29812](https://github.com/pytorch/pytorch/pull/29812)) ([28783](https://github.com/pytorch/pytorch/pull/28783)).\r\n    * torch::nn::functional::l1_loss / kl_div / mse_loss / binary_cross_entropy / smooth_l1_loss / ctc_loss ([27156](https://github.com/pytorch/pytorch/pull/27156)) ([28806](https://github.com/pytorch/pytorch/pull/28806)) ([30146](https://github.com/pytorch/pytorch/pull/30146)) ([27661](https://github.com/pytorch/pytorch/pull/27661)) ([28654](https://github.com/pytorch/pytorch/pull/28654)).\r\n* Normalization functions\r\n    * torch::nn::functional::layer_norm / instance_norm / clip_grad_norm_ / batch_norm / group_norm / local_response_norm / normalize ([28032](https://github.com/pytorch/pytorch/pull/28032)) ([28790](https://github.com/pytorch/pytorch/pull/28790), [30684](https://github.com/pytorch/pytorch/pull/30684)) ([26140](https://github.com/pytorch/pytorch/pull/26140), [29584](https://github.com/pytorch/pytorch/pull/29584), [30216](https://github.com/pytorch/pytorch/pull/30216)) ([28176](https://github.com/pytorch/pytorch/pull/28176), [28936](https://github.com/pytorch/pytorch/pull/28936)) ([29920](https://github.com/pytorch/pytorch/pull/29920)) ([28759](https://github.com/pytorch/pytorch/pull/28759)) ([27280](https://github.com/pytorch/pytorch/pull/27280)).\r\n* Activation functions\r\n    * torch::nn::functional::elu / leaky_relu / selu / prelu / relu / relu6 / rrelu / celu / glu / gelu ([27028](https://github.com/pytorch/pytorch/pull/27028)) ([27059](https://github.com/pytorch/pytorch/pull/27059)) ([27434](https://github.com/pytorch/pytorch/pull/27434)) ([27429](https://github.com/pytorch/pytorch/pull/27429)) ([27435](https://github.com/pytorch/pytorch/pull/27435)) ([27436](https://github.com/pytorch/pytorch/pull/27436)) ([27437](https://github.com/pytorch/pytorch/pull/27437)) ([27487](https://github.com/pytorch/pytorch/pull/27487)) ([29922](https://github.com/pytorch/pytorch/pull/29922)) ([28433](https://github.com/pytorch/pytorch/pull/28433)).\r\n    * torch::nn::functional:: log_sigmoid/ log_softmax / softmax / softplus / softmin / softsign / softshrink / hardshrink / tanhshrink / hardtanh / gumbel_softmax / threshold ([27060](https://github.com/pytorch/pytorch/pull/27060)) ([27462](https://github.com/pytorch/pytorch/pull/27462)) ([27446](https://github.com/pytorch/pytorch/pull/27446)) ([27489](https://github.com/pytorch/pytorch/pull/27489)) ([27459](https://github.com/pytorch/pytorch/pull/27459)) ([27535](https://github.com/pytorch/pytorch/pull/27535)) ([27534](https://github.com/pytorch/pytorch/pull/27534)) ([27035](https://github.com/pytorch/pytorch/pull/27035)) ([27537](https://github.com/pytorch/pytorch/pull/27537)) ([27038](https://github.com/pytorch/pytorch/pull/27038)) ([28121](https://github.com/pytorch/pytorch/pull/28121)) ([27538](https://github.com/pytorch/pytorch/pull/27538)).\r\n* Embedding functions\r\n    * torch::nn::functional::embedding  / embedding_bag / one_hot ([28669](https://github.com/pytorch/pytorch/pull/28669)) ([29673](https://github.com/pytorch/pytorch/pull/29673)) ([27177](https://github.com/pytorch/pytorch/pull/27177)).\r\n* Linear functions\r\n    * torch::nn::functional::linear / bilinear ([27382](https://github.com/pytorch/pytorch/pull/27382)) ([26082](https://github.com/pytorch/pytorch/pull/26082)).\r\n* Padding functions\r\n    * torch::nn::functional::pad ([26601](https://github.com/pytorch/pytorch/pull/26601), [28760](https://github.com/pytorch/pytorch/pull/28760)).\r\n* Vision functions\r\n    * torch::nn::functional::affine_grid / grid_sample / interpolate / pixel_shuffle ([27263](https://github.com/pytorch/pytorch/pull/27263)) ([28354](https://github.com/pytorch/pytorch/pull/28354)) ([28413](https://github.com/pytorch/pytorch/pull/28413)) ([28140](https://github.com/pytorch/pytorch/pull/28140)).\r\n* Distance functions\r\n    * torch::nn::functional::pdist ([27122](https://github.com/pytorch/pytorch/pull/27122)).\r\n* Utility functions\r\n    * torch::nn::utils::clip_grad_value_ / parameters_to_vector / vector_to_parameters ([28736](https://github.com/pytorch/pytorch/pull/28736), [29584](https://github.com/pytorch/pytorch/pull/29584)) ([30216](https://github.com/pytorch/pytorch/pull/30216)) ([29267](https://github.com/pytorch/pytorch/pull/29267)).\r\n\r\n\r\n\r\n## AMD Support\r\n\r\n* New features integration\r\n    * Enabled RCCL Integration ([23884](https://github.com/pytorch/pytorch/pull/23884), [27383](https://github.com/pytorch/pytorch/pull/27383), [27518](https://github.com/pytorch/pytorch/pull/27518), [29385](https://github.com/pytorch/pytorch/pull/29385))\r\n    * Enabled rocTX and rocTracer Integration ([27416](https://github.com/pytorch/pytorch/pull/27416))\r\n    * Improved hiprtc integration ([27390](https://github.com/pytorch/pytorch/pull/27390))\r\n    * bfloat16 enablement (initial) on ROCm ([27719](https://github.com/pytorch/pytorch/pull/27719))\r\n* Build/CI\r\n    * Upgrade to ROCm 2.9 ([27417](https://github.com/pytorch/pytorch/pull/27417))\r\n    * Upgrade ROCm CI to Python3.6 ([30119](https://github.com/pytorch/pytorch/pull/30119), [27353](https://github.com/pytorch/pytorch/pull/27353))\r\n    * Distribute hipify scripts as part of torch package ([27425](https://github.com/pytorch/pytorch/pull/27425))\r\n    * Build and test gfx908 architecture ([27388](https://github.com/pytorch/pytorch/pull/27388))\r\n    * Add `torch.version.hip` ([29815](https://github.com/pytorch/pytorch/pull/29815)).\r\n    * Build fixes ([29547](https://github.com/pytorch/pytorch/pull/29547), [29009](https://github.com/pytorch/pytorch/pull/29009))\r\n\r\n## ONNX\r\n\r\nIn PyTorch 1.4, we have mainly focused on expanding the coverage for ONNX Opset 11, and enabling exporting torchvision models. Most of the torchvision models can be exported to ONNX (Opset 11, with fixed input size), including FasterRCNN, MaskRCNN, and KeypointRCNN. We have also enhanced export support for some tensor indexing scenarios, with more enhancements to come in the next release. In addition, 20+ new PyTorch operators are enabled in ONNX exporter.\r\n\r\n### Expanding Coverage for ONNX Opset 11\r\n\r\n* `torch.sort/torch.topk` are supported in Opset 11 ([25739](https://github.com/pytorch/pytorch/pull/25739))\r\n* `torch.size/torch.squeeze/torch.unsqueeze/torch.mm/torch.index_fill/torch.index_copy` are supported in Opset 11 ([27578](https://github.com/pytorch/pytorch/pull/27578))\r\n* `torch.masked_select/torch.masked_scatter` are supported in Opset 11 ([25949](https://github.com/pytorch/pytorch/pull/25949))\r\n* `torch.arange` is supported in Opset 11 ([26875](https://github.com/pytorch/pytorch/pull/26875))\r\n* `avg_pool, constant_pad_nd, reflection_pad, replication_pad` Support enhanced in Opset 11 ([28225](https://github.com/pytorch/pytorch/pull/28225))\r\n* `torch.hardtanh` is supported in Opset 11 ([30169](https://github.com/pytorch/pytorch/pull/30169))\r\n* Enable ONNX constant folding for opset 11 ([29011](https://github.com/pytorch/pytorch/pull/29011))\r\n\r\n### Exporting More Torch Operators/Models to ONNX\r\n\r\n* `torch.remainder` is enabled in exporter ([24410](https://github.com/pytorch/pytorch/pull/24410))\r\n* `torch.unfold` is enabled in exporter ([24970](https://github.com/pytorch/pytorch/pull/24970))\r\n* `torch.slice/torch.select` with negative index are enabled in exporter ([25273](https://github.com/pytorch/pytorch/pull/25273), [26549](https://github.com/pytorch/pytorch/pull/26549))\r\n* `torch.ones/torch.ones_like/torch.zeros/torch.zeros_like/torch.full/torch.full_like` with default dtype are enabled in exporter ([27577](https://github.com/pytorch/pytorch/pull/27577))\r\n* `torch.unbind` is enabled in exporter ([27247](https://github.com/pytorch/pytorch/pull/27247))\r\n* `torch.nn.functional.interpolate` export is enhanced ([27179](https://github.com/pytorch/pytorch/pull/27179), [27566](https://github.com/pytorch/pytorch/pull/27566), [28560](https://github.com/pytorch/pytorch/pull/28560), [29489](https://github.com/pytorch/pytorch/pull/29489))\r\n* `torch.det` is enabled in exporter ([26958](https://github.com/pytorch/pytorch/pull/26958))\r\n* `torch.group_norm` is enabled in exporter ([27071](https://github.com/pytorch/pytorch/pull/27071))\r\n* `torch.meshgrid` is enabled in exporter ([26037](https://github.com/pytorch/pytorch/pull/26037))\r\n* `torch.randn/torch.randn_like` are enabled in exporter ([28470](https://github.com/pytorch/pytorch/pull/28470), [29354](https://github.com/pytorch/pytorch/pull/29354))\r\n* `torch.weight_norm` enabled in exporter ([28618](https://github.com/pytorch/pytorch/pull/28618))\r\n* `torch.scalar_tensor` is enabled in exporter ([28713](https://github.com/pytorch/pytorch/pull/28713))\r\n* `torch.logdet` is enabled in exporter ([29767](https://github.com/pytorch/pytorch/pull/29767))\r\n* `torch.batch_norm` 2D with affine=False is enabled in exporter ([29458](https://github.com/pytorch/pytorch/pull/29458))\r\n* `torch.bitshift` is enabled in exporter ([28210](https://github.com/pytorch/pytorch/pull/28210))\r\n\r\n### Enhancing Export/Test Infra\r\n\r\n* Use deepcopy inputs in ONNX ORT test cases ([27186](https://github.com/pytorch/pytorch/pull/27186))\r\n* Return NotImplemented from all binary math ops ([27423](https://github.com/pytorch/pytorch/pull/27423)).\r\n* Disabling ONNX IR v4 sematics for opset 8 or lower ([28990](https://github.com/pytorch/pytorch/pull/28990))\r\n* Add ONNX tests for torchvision models ([30121](https://github.com/pytorch/pytorch/pull/30121))\r\n* Keep output type information while exporting ONNX graph ([25906](https://github.com/pytorch/pytorch/pull/25906))\r\n\r\n## Quantization\r\n\r\nQuantization updates correspond to a mix of bug-fixes and feature improvements, with feature improvements adding improved operator coverage and performance improvements.   We have also made a lot of progress towards enabling graph mode quantization support.\r\n\r\n* Feature improvements:\r\n    * Enabling intra-op parallelism ([26692](https://github.com/pytorch/pytorch/pull/26692)).\r\n    * Enabling inplace relu ([28710](https://github.com/pytorch/pytorch/pull/28710)).\r\n    * Quantized Tensor support copy ([28612](https://github.com/pytorch/pytorch/pull/28612)).\r\n    * Add quantized torch mean implementation ([27675](https://github.com/pytorch/pytorch/pull/27675)).\r\n    * Add quantized avg_pool2d for pytorch mobile ([27631](https://github.com/pytorch/pytorch/pull/27631)).\r\n    * Add nn.quantized.Conv3d ([29813](https://github.com/pytorch/pytorch/pull/29813)).\r\n    * Adding inplace quantized relu6 ([29245](https://github.com/pytorch/pytorch/pull/29245)).\r\n    * Fast histogram observer ([29790](https://github.com/pytorch/pytorch/pull/29790)).\r\n    * PackedSequence support for quantized LSTM ([29585](https://github.com/pytorch/pytorch/pull/29585)).\r\n    * Improve legacy QuantizedLinear functions to reduce overhead ([29773](https://github.com/pytorch/pytorch/pull/29773)).\r\n    * Add support for quantized operator conversion from PT to C2 via ONNX ([29694](https://github.com/pytorch/pytorch/pull/29694)).\r\n    * enable per channel dynamic quantization ([30122](https://github.com/pytorch/pytorch/pull/30122)).\r\n* Scripting support:\r\n    * Make PerChannelMinMaxObserver scriptable using `torch.jit.ignore` ([29416](https://github.com/pytorch/pytorch/pull/29416)).\r\n    * Make HistogramObserver scriptable with `@torch.jit.ignore` ([27950](https://github.com/pytorch/pytorch/pull/27950)).\r\n    * Fix tracing for dynamic quantized LSTM ([29331](https://github.com/pytorch/pytorch/pull/29331)).\r\n\r\n## Visualization\r\n\r\n* Fixed graph visualization: displaying proper names after recent JIT changes ([30244](https://github.com/pytorch/pytorch/pull/30244))\r\n* Support logging embedding for TensorBoard visualizations to generic filesystem ([27716](https://github.com/pytorch/pytorch/pull/27716))\r\n\r\n## Other Improvements\r\n\r\n* `torch.argmax/argmin` Allow half type ([28787](https://github.com/pytorch/pytorch/pull/28787)).\r\n* `torch.cuda.memory_stats / memory_summary` instrumentation for CUDA memory allocator ([27361](https://github.com/pytorch/pytorch/pull/27361)).\r\n* `torch.set_num_threads` Allow calling multiple times with TBB ([27190](https://github.com/pytorch/pytorch/pull/27190)).\r\n* `torch.set_num_threads` Allow calling multiple times in parallel native ([27947](https://github.com/pytorch/pytorch/pull/27947)).\r\n* `torch.logical_xor` Allow non-bool tensors ([27248](https://github.com/pytorch/pytorch/pull/27248)).\r\n* `torch.promote_types` Nicer error message. ([27941](https://github.com/pytorch/pytorch/pull/27941)).\r\n* `torch.batch_norm_elemt` Add an out-variant ([27621](https://github.com/pytorch/pytorch/pull/27621)).\r\n* `torch.lerp` Implement derivative with respect to weight ([28219](https://github.com/pytorch/pytorch/pull/28219)).\r\n* `torch.complex32` Add type promotion support ([27929](https://github.com/pytorch/pytorch/pull/27929)).\r\n* `torch.unique` Support bool tensors ([28374](https://github.com/pytorch/pytorch/pull/28374)).\r\n* `torch.reshape` Improve backward for viewable geometries ([28901](https://github.com/pytorch/pytorch/pull/28901)).\r\n* `torch.lu` Generalized factorization ([28608](https://github.com/pytorch/pytorch/pull/28608)).\r\n* `torch.equal` Add the intra-op parallelism ([28810](https://github.com/pytorch/pytorch/pull/28810)).\r\n* `torch.randint` Accept generator=None ([29748](https://github.com/pytorch/pytorch/pull/29748)).\r\n* `torch.bfloat16` Enabled for cuda ([27259](https://github.com/pytorch/pytorch/pull/27259)).\r\n* `torch.multinomial` Enable for torch.half ([29266](https://github.com/pytorch/pytorch/pull/29266)).\r\n* `nn.RNN` Respect the current stream in cudnn ([27026](https://github.com/pytorch/pytorch/pull/27026)).\r\n* `nn.RNN` Preserve nonlinearity attribute ([28058](https://github.com/pytorch/pytorch/pull/28058)).\r\n* `nn.Linear` Support 0-batch size. ([27211](https://github.com/pytorch/pytorch/pull/27211)).\r\n* `nn.functional.binary_cross_entropy` implement double backwards ([26983](https://github.com/pytorch/pytorch/pull/26983)).\r\n* `nn.AdaptiveAvgPool2d` Add support for NHWC memory format ([24396](https://github.com/pytorch/pytorch/pull/24396)).\r\n* `nn.GELU` Add GELU activation ([28944](https://github.com/pytorch/pytorch/pull/28944)).\r\n* `nn.LayerNorm` Handle batch size of zero ([28614](https://github.com/pytorch/pytorch/pull/28614)).\r\n* `nn.BatchNorm` Add NHWC support on cudnn ([23861](https://github.com/pytorch/pytorch/pull/23861)).\r\n* `nn.BatchNorm2d` support torch.channels_last ([28982](https://github.com/pytorch/pytorch/pull/28982)).\r\n* `nn.BatchNorm2d` Handle empty inputs ([30035](https://github.com/pytorch/pytorch/pull/30035)).\r\n* `nn.LayerNorm` Enable the intra-op parallelism ([28464](https://github.com/pytorch/pytorch/pull/28464)).\r\n* `nn.utils.prune` Add pruning functionality ([24076](https://github.com/pytorch/pytorch/pull/24076)).\r\n* `nn.Sequential` Make iterable ([28987](https://github.com/pytorch/pytorch/pull/28987)).\r\n* `dtype.is_signed` Ability to differentiate signed dtypes ([29511](https://github.com/pytorch/pytorch/pull/29511)).\r\n* `optim.lr_scheduler.MultiplicativeLR `Add new multiplicative learning rate scheduler. ([27254](https://github.com/pytorch/pytorch/pull/27254)).\r\n* `cuda.comm.scatter, gather` Add channel-last support ([28077](https://github.com/pytorch/pytorch/pull/28077)).\r\n* `at::parallel_for` Choose number of OMP threads based on GRAIN_SIZE ([26963](https://github.com/pytorch/pytorch/pull/26963)).\r\n* Return NotImplemented from unsupported tensor arithmetic operators ([26507](https://github.com/pytorch/pytorch/pull/26507)).\r\n* Automatically select proper tqdm submodule ([27108](https://github.com/pytorch/pytorch/pull/27108)).\r\n* Pickle support for sparse tensors ([27062](https://github.com/pytorch/pytorch/pull/27062)).\r\n* Vectorized complex unary and binary op support. ([26500](https://github.com/pytorch/pytorch/pull/26500)).\r\n* Complex support for reduce and linpack ops on CPU ([27653](https://github.com/pytorch/pytorch/pull/27653)).\r\n* Complex support for compare and pointwise ops on CPU ([28735](https://github.com/pytorch/pytorch/pull/28735)).\r\n* Make PyTorch Python 3.8 compatible ([29302](https://github.com/pytorch/pytorch/pull/29302)).\r\n* Buffer python warning to avoid deadlocks ([26613](https://github.com/pytorch/pytorch/pull/26613)).\r\n* Use NNPACK for strided convolutions. ([29084](https://github.com/pytorch/pytorch/pull/29084)).\r\n\r\n\r\n\r\n# Bug Fixes\r\n\r\n## Distributed\r\n\r\n* Ensure NCCL error handling code is disabled for NCCL versions < 2.4 ([27124](https://github.com/pytorch/pytorch/pull/27124)).\r\n* Fix segmentation fault in `FileStore` with concurrent accesses. ([28812](https://github.com/pytorch/pytorch/pull/28812)).\r\n* Fix DDP incompatibility issue with `nn.MultiheadAttention` ([26826](https://github.com/pytorch/pytorch/pull/26826)).\r\n\r\n## RPC\r\n\r\n* Add `ProcessGroupAgent` termination detection algorithm ([26984](https://github.com/pytorch/pytorch/pull/26984)).\r\n* Fix pybind11 warnings in Python RPC handler implementation ([27284](https://github.com/pytorch/pytorch/pull/27284)).\r\n* Defer creating `ProcessGroupAgent` listener thread until contexts are initialized ([28013](https://github.com/pytorch/pytorch/pull/28013)).\r\n* Fix Python RPC handler exit crash ([27251](https://github.com/pytorch/pytorch/pull/27251)).\r\n* Fix distributed autograd initialization ([29069](https://github.com/pytorch/pytorch/pull/29069)).\r\n* Always include autograd context id in `rpc_*` / `remote` requests ([29781](https://github.com/pytorch/pytorch/pull/29781)).\r\n* Make `RRefContext` singleton leaky, deal with module destruct order race. ([30172](https://github.com/pytorch/pytorch/pull/30172)).\r\n\r\n## C++ API Bug Fixes\r\n\r\n* at::Tensor::requires_grad_ now supported ([26332](https://github.com/pytorch/pytorch/pull/26332)).\r\n* torch::isfinite now supported ([30083](https://github.com/pytorch/pytorch/pull/30083)).\r\n* torch::nn::modules_ordered_dict is deprecated ([28774](https://github.com/pytorch/pytorch/pull/28774)).\r\n* Add reset_parameters to torch::nn modules ([29832](https://github.com/pytorch/pytorch/pull/29832)).\r\n* Allow passing undefined Tensor to Module::register_parameter ([27948](https://github.com/pytorch/pytorch/pull/27948)).\r\n* Exclude undefined tensors in the result of Module::parameters() / named_paramters() / buffers() / named_buffers() ([30626](https://github.com/pytorch/pytorch/pull/30626)).\r\n* Include hierarchy information in C++ API loading error messages ([28499](https://github.com/pytorch/pytorch/pull/28499)).\r\n* Fix a bug: the C++ L-BFGS optimizer does not work properly if there are one or more registered tensors with no grad in the model ([27606](https://github.com/pytorch/pytorch/pull/27606)).\r\n* Use c10::variant-based enums for Nonlinearity and FanMode ([27933](https://github.com/pytorch/pytorch/pull/27933)). Support for `torch::nn::init::Nonlinearity` and `torch::nn::init::FanMode` will be removed in 1.5.\r\n\r\n## JIT\r\n\r\n* Make dropout properly condition on training. ([29436](https://github.com/pytorch/pytorch/pull/29436))\r\n* Fix aten::grad to return optional list ([29577](https://github.com/pytorch/pytorch/pull/29577))\r\n* Fix `torch.arange` dtype\r\n* Fix type sharing on loaded ScriptModules ([29826](https://github.com/pytorch/pytorch/pull/29826))\r\n* Fix type sharing between traced modules ([29583](https://github.com/pytorch/pytorch/pull/29583))\r\n* Check for mutable default parameters ([29833](https://github.com/pytorch/pytorch/pull/29833))\r\n* Fix tracing of autograd functions ([29791](https://github.com/pytorch/pytorch/pull/29791))\r\n* Check for unrolled loop in break & continue ([29474](https://github.com/pytorch/pytorch/pull/29474))\r\n* Fix negative string indexing ([22700](https://github.com/pytorch/pytorch/pull/22700))\r\n* Make jit.trace_module reentrant ([29411](https://github.com/pytorch/pytorch/pull/29411))\r\n* Fix jit outplace tracing and reapply changes to _like operators. ([28839](https://github.com/pytorch/pytorch/pull/28839))\r\n* Properly guard against inheritance on TorchScript classes ([28407](https://github.com/pytorch/pytorch/pull/28407))\r\n* Fix when giving jit format warning about unsupported options ([28616](https://github.com/pytorch/pytorch/pull/28616))\r\n* Fix handling of function attributes. ([28569](https://github.com/pytorch/pytorch/pull/28569))\r\n* Fix pushLong() issue in pickler. ([28057](https://github.com/pytorch/pytorch/pull/28057))\r\n* Fix broken name mangling ([27511](https://github.com/pytorch/pytorch/pull/27511))\r\n* Fix segfault while printing value type for an error msg in emitListComprehension ([27261](https://github.com/pytorch/pytorch/pull/27261))\r\n* Fix `toIValue` dict iteration ([26856](https://github.com/pytorch/pytorch/pull/26856))\r\n* Fix race condition in Function::optimized_graph(). ([27012](https://github.com/pytorch/pytorch/pull/27012))\r\n* Sanitize module names on legacy import ([27764](https://github.com/pytorch/pytorch/pull/27764))\r\n* Python None should have its type inferred as NoneType ([26665](https://github.com/pytorch/pytorch/pull/26665))\r\n* Properly set existing attributes under recursive script ([27514](https://github.com/pytorch/pytorch/pull/27514))\r\n\r\n## Quantization\r\n\r\n* Skip copy_same_type_transpose_ for quantized tensor ([29609](https://github.com/pytorch/pytorch/pull/29609)).\r\n* Add note that cuda quantization is not supported ([27829](https://github.com/pytorch/pytorch/pull/27829)).\r\n* Rename _intrinsic to intrinsic ([27194](https://github.com/pytorch/pytorch/pull/27194)).\r\n* Better error message for quantized dispatch ([28635](https://github.com/pytorch/pytorch/pull/28635)).\r\n* Update the misleading comments for zero_points and scale in dynamic quant linear module [1/2] ([28767](https://github.com/pytorch/pytorch/pull/28767)).\r\n* Avoid the misleading zero_point and scale [2/2] ([28827](https://github.com/pytorch/pytorch/pull/28827)).\r\n* Add the warning message for API with linear modules ([28766](https://github.com/pytorch/pytorch/pull/28766)).\r\n* Do not insert observers for empty sequential modules ([28384](https://github.com/pytorch/pytorch/pull/28384)).\r\n* Fix the padding issue of quantized average pool operator ([28260](https://github.com/pytorch/pytorch/pull/28260)).\r\n\r\n## Mobile\r\n\r\n* Fix deadlock issues in ThreadPool ([29885](https://github.com/pytorch/pytorch/pull/29885)).\r\n* Disable ProfilingGraphExecutorImpl for mobile ([30067](https://github.com/pytorch/pytorch/pull/30067)).\r\n\r\n## Other Bug fixes\r\n\r\n* `torch.kthvalue` Fix CUDA shared memory out of bound access in findPattern ([28989](https://github.com/pytorch/pytorch/pull/28989)).\r\n* `torch.save` Fix source files not being saved ([28965](https://github.com/pytorch/pytorch/pull/28965)).\r\n* `torch.load` Fix OSError loading files larger than 2GB. ([27125](https://github.com/pytorch/pytorch/pull/27125)).\r\n* `torch.linspace` clearer error message for negative step sizes. ([28274](https://github.com/pytorch/pytorch/pull/28274)).\r\n* `torch.histc` Add range checks to avoid segfaults ([27712](https://github.com/pytorch/pytorch/pull/27712)).\r\n* `torch.lu` Fix thread` `local issue on cpu ([28546](https://github.com/pytorch/pytorch/pull/28546)).\r\n* `torch.max_pool2d` Limit tensor size to max CUDA grid size ([28931](https://github.com/pytorch/pytorch/pull/28931)).\r\n* `torch.renorm` Fix a memory leak in CUDA renorm. ([29873](https://github.com/pytorch/pytorch/pull/29873)).\r\n* `torch.index_add` Fix bug in atomicAdd on CUDA for some dtypes ([29231](https://github.com/pytorch/pytorch/pull/29231)).\r\n* `torch.addmm` Fix handling of empty tensors ([28613](https://github.com/pytorch/pytorch/pull/28613)).\r\n* `nn.CTCLoss` Fix incorrect gradient for large target sizes ([27460](https://github.com/pytorch/pytorch/pull/27460)).\r\n* `nn.functional.ctc_loss` Fix incorrect gradient on cudnn ([27039](https://github.com/pytorch/pytorch/pull/27039)).\r\n* `nn.Embedding` Incorrect gradient at padding_idx in cuda kernel. ([27731](https://github.com/pytorch/pytorch/pull/27731)).\r\n\r\n* `nn.LayerNorm` Fix an illegal memory access error ([28196](https://github.com/pytorch/pytorch/pull/28196)).\r\n* `nn.Conv2d` handle zero stride ([28784](https://github.com/pytorch/pytorch/pull/28784)).\r\n* `nn.PoissonNLLLoss` Fix incorrect result with `full=True` ([28637](https://github.com/pytorch/pytorch/pull/28637)).\r\n* `nn.AvgPool2d` fix an overflow for 2^31-1 sized inputs ([30793](https://github.com/pytorch/pytorch/pull/30793)).\r\n* `nn.RNNBase` Fix an issue with use of children of RNN third party device types ([28562](https://github.com/pytorch/pytorch/pull/28562)).\r\n* `nn.Upsample` Fix \u201cinvalid configuration argument\u201d error ([28927](https://github.com/pytorch/pytorch/pull/28927)).\r\n* `nn.Upsample` Fix a CUDA launch config failure ([29016](https://github.com/pytorch/pytorch/pull/29016)).\r\n* `optim.lr_scheduler.OneCycleLR` Correctly handle div_factor parameter ([28217](https://github.com/pytorch/pytorch/pull/28217)).\r\n* `PackedSequence.to` Ensure all tensors are moved ([27245](https://github.com/pytorch/pytorch/pull/27245)).\r\n* `EventList.total_average` Fix a regression caused by missing __iadd__ ([27498](https://github.com/pytorch/pytorch/pull/27498)).\r\n* `Tensor.record_stream` Ensure stream is recorded for shifted view tensors ([27371](https://github.com/pytorch/pytorch/pull/27371)).\r\n* `torch.hub` Handle branch names containing a slash. ([27960](https://github.com/pytorch/pytorch/pull/27960)).\r\n* Fix error handling in Magma kernels ([29003](https://github.com/pytorch/pytorch/pull/29003)).\r\n* Fix avx for c++14 ([28207](https://github.com/pytorch/pytorch/pull/28207)).\r\n* Fix illegal memory access thread safety issue in sparse CUDA ([29426](https://github.com/pytorch/pytorch/pull/29426)).\r\n* `__cuda_array_interface__` Fix stride calculation ([31450](https://github.com/pytorch/pytorch/pull/31450)).\r\n\r\n# Deprecations\r\n\r\n### **Python 2 support is deprecated and will not be supported in the 1.5 release.**\r\n\r\n### `torch.optim`: `Scheduler.step(epoch)` is now deprecated; use `Scheduler.step()` instead.  ([26432](https://github.com/pytorch/pytorch/pull/26423))\r\n\r\nFor example:\r\n\r\n```\r\n>>> for epoch in range(10):\r\n>>>    optimizer.step()\r\n>>>    scheduler.step(epoch)\r\nDeprecationWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\r\n  warnings.warn(EPOCH_DEPRECATION_WARNING, DeprecationWarning)\r\n```\r\n\r\n### **[C++]** C++11 is deprecated and will not be supported in the 1.5 release.\r\n\r\n### **[C++]** `Tensor::is_variable()` has been deprecated.  As noted in the **Backwards Incompatible Changes** section, the distinction between variable and non-variable has been eliminated, so this check is no longer meaningful.  Generally, `is_variable()` will now return true except in some special circumstances (see [29653](https://github.com/pytorch/pytorch/pull/29653) for more details).  ([29653](https://github.com/pytorch/pytorch/pull/29653))\r\n\r\n### **[C++]** `torch::nn::modules_ordered_dict` has been deprecated.  It is generally no longer necessary and can just be removed.  ([28774](https://github.com/pytorch/pytorch/pull/28774/))\r\n\r\n### `torch.jit.quantized` API has been deprecated in favor of  `torch.quantization.quantize_dynamic` ([28766](https://github.com/pytorch/pytorch/pull/28766))\r\n\r\n# Performance\r\n\r\nA benchmark suite is available to easily measure the performance of operators with a range of input shapes. The generated benchmark data fully characterize the performance of operators in terms of execution time. For more details see README.md in the benchmarks/operator_benchmark directory.\r\n\r\n\r\n* `torch.nn.functional.threshold, torch.nn.functional.layer_norm, torch.cdist` Performance of threshold (CPU), layer norm (CUDA) and cdist operations was improved ([27155,](https://github.com/pytorch/pytorch/pull/27155)[27634](https://github.com/pytorch/pytorch/pull/27634), [25799](https://github.com/pytorch/pytorch/pull/25799))\r\n* `torch.Tensor.fill_` Performance for half and bfloat16 types on CPU was improved  ([28397](https://github.com/pytorch/pytorch/pull/28397)).\r\n* `torch.nn.MaxPool2d` implementation for channels_last format was added ([24872](https://github.com/pytorch/pytorch/pull/24872))\r\n* There is a fast pass reducing the overheads of pointwise operations relying on TensorIterator under certain conditions (contiguous inputs, no broadcast) ([29180](https://github.com/pytorch/pytorch/pull/29180)).\r\n* Overheads of operations with scalars/number literals was improved ([29915](https://github.com/pytorch/pytorch/pull/29915)).\r\n* In case of type promotion on the GPU, the values are converted on the fly, without explicit casting of the full tensor ([30018](https://github.com/pytorch/pytorch/pull/30018)).\r\n* reorder_dimensions in TensorIterator favors output write locality, improving overall performance when operating on discontiguous tensors ([28615](https://github.com/pytorch/pytorch/pull/28615)).\r\n* Float pickling speed was improved ([28553](https://github.com/pytorch/pytorch/pull/28553)).\r\n* GRAIN_SIZE for intra-op parallelization was unified between TH and ATen operations ([28770](https://github.com/pytorch/pytorch/pull/28770))\r\n* `tensor.numel`  devirtualized, improving performance ([27294](https://github.com/pytorch/pytorch/pull/27294))\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.4.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.4.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.4.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/22877176", "release_id": 22877176, "date_created": "2020-01-14T17:05:04Z", "date_published": "2020-01-16T00:03:49Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/21295658", "tag": "v1.3.1", "name": "Bug Fix Release", "author": {"name": "gchanan", "type": "User"}, "description": "### Significant Fixes\r\n\r\n#### [Type Promotion](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype): fixed a bug where type promotion, combined with non-contiguous tensors could compute incorrect results.  ([28253](https://github.com/pytorch/pytorch/pull/28253/))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.0</th><th>Version 1.3.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([[True,  True],\r\n                      [False, True]])\r\n# get a non-contiguous tensor\r\n>>> a_transpose = a.t()\r\n# type promote by comparing across dtypes (bool -> long)\r\n>>> a_transpose == 0\r\n# POTENTIALLY INCORRECT VALUES\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([[True,  True],\r\n                      [False, True]])\r\n# get a non-contiguous tensor\r\n>>> a_transpose = a.t()\r\n# type promote by comparing across dtypes (bool -> long)\r\n>>> a_transpose == 0\r\ntensor([[False,  True],\r\n        [False, False]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### [Type Promotion](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype) / Indexing: Fixed a Bug that Allowed Mixed-Dtype Indexing and assignment could lead to incorrect results.  Mixed dtype operations of this form are currently disabled, as they were in 1.2.  ([28231](https://github.com/pytorch/pytorch/pull/28231))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.0</th><th>Version 1.3.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.ones(5, 2, dtype=torch.float)\r\n>>> b = torch.zeros(5, dtype=torch.long)\r\n>>> a[:, [1]] = b.unsqueeze(-1)\r\n>>> a\r\n# POTENTIALLY INCORRECT VALUES\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.ones(5, 2, dtype=torch.float)\r\n>>> b = torch.zeros(5, dtype=torch.long)\r\n>>> a[:, [1]] = b.unsqueeze(-1)\r\nRuntimeError: expected dtype Float but got dtype Long\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### [torch.where(condition, x, y)](https://pytorch.org/docs/stable/torch.html#torch.where): fixed a bug on CPU where incorrect results could be returned if `x` and `y` were of different dtypes.  Mixed dtype operations of this form are currently disabled, as they were in version 1.2.  ([29078](https://github.com/pytorch/pytorch/pull/29078))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.0</th><th>Version 1.3.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(2, 3)\r\n>>> y = torch.randint(0, 10, (2, 3))\r\n>>> torch.where(x < 0, x, y)\r\ntensor(...)\r\n# POTENTIALLY INCORRECT VALUES\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(2, 3)\r\n>>> y = torch.randint(0, 10, (2, 3))\r\n>>> torch.where(x < 0, x, y)\r\nRuntimeError: expected scalar type Float but found Long\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Other Fixes\r\n\r\n* `torch.argmax`: fix regression on CUDA that disabled support for `torch.float16` inputs.  ([28915](https://github.com/pytorch/pytorch/pull/28915/))\r\n* NamedTensor: fix Python refcounting bug with `Tensor.names`.  ([28922](https://github.com/pytorch/pytorch/pull/28922))\r\n* Quantization: support `deepcopy` for quantized tensors.  ([28612](https://github.com/pytorch/pytorch/pull/28612))\r\n* Quantization: support `nn.quantized.ReLU` with `inplace=True`.  ([28710](https://github.com/pytorch/pytorch/pull/28710))\r\n* Documentation: `torch.lgamma` and `torch.polygamma` are now documented.  ([28964](https://github.com/pytorch/pytorch/pull/28964))", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.3.1", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.3.1", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.3.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/21295658", "release_id": 21295658, "date_created": "2019-11-04T23:37:02Z", "date_published": "2019-11-07T17:19:44Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/20597721", "tag": "v1.3.0", "name": "Mobile Support, Named Tensors, Quantization, Type Promotion and many more", "author": {"name": "nairbv", "type": "User"}, "description": "## Table of Contents\r\n\r\n- Breaking Changes\r\n- Highlights\r\n  * [Experimental]: Mobile Support\r\n  * [Experimental]: Named Tensor Support\r\n  * [Experimental]: Quantization support\r\n  * Type Promotion\r\n  * Deprecations\r\n- New Features\r\n  * TensorBoard: 3D Mesh and Hyperparameter Support\r\n  * Distributed\r\n  * Libtorch Binaries with C++11 ABI\r\n  * New TorchScript features\r\n- Improvements\r\n  * C++ Frontend Improvements\r\n    + Autograd\r\n    + New torch::nn modules\r\n    + New torch::nn::functional functions\r\n    + tensor Construction API\r\n    + Other C++ Improvements\r\n  * Distributed Improvements\r\n  * Performance Improvements\r\n  * JIT Improvements\r\n  * ONNX Exporter Improvements\r\n    + Adding Support for ONNX IR v4\r\n    + Adding Support for ONNX Opset 11\r\n    + Exporting More Torch Operators/Models to ONNX\r\n    + Enhancing ONNX Export Infra\r\n  * Other Improvements\r\n- Bug Fixes\r\n    + TensorBoard Bug Fixes\r\n    + C++ API Bug fixes\r\n    + JIT\r\n    + Other Bug Fixes\r\n- Documentation Updates\r\n    + Distributed\r\n    + JIT\r\n    + Other documentation improvements\r\n\r\n# Breaking Changes\r\n\r\n#### Type Promotion: Mixed dtype operations may return a different dtype and value than in previous versions.  ([22273](https://github.com/pytorch/pytorch/pull/22273), [26981](https://github.com/pytorch/pytorch/pull/26981))\r\n\r\nPrevious versions of PyTorch supported a limited number of mixed dtype operations. These operations could result in loss of precision by, for example, truncating floating-point zero-dimensional tensors or Python numbers.\r\n\r\nIn Version 1.3, PyTorch supports NumPy-style type promotion (with slightly modified rules, see [full documentation](https://pytorch.org/docs/master/tensor_attributes.html#torch-dtype)).  These rules generally will retain precision and be less surprising to users.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.2</th><th>Version 1.3</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(1) + 2.5\r\ntensor(3)\r\n>>> torch.tensor([1]) + torch.tensor(2.5)\r\ntensor([3])\r\n>>> torch.tensor(**True**) + 5\r\ntensor(True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(1) + 2.5\r\ntensor(3.5000)\r\n>>> torch.tensor([1]) + torch.tensor(2.5)\r\ntensor([3.5000])\r\n>>> torch.tensor(True) + 5\r\ntensor(6)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### Type Promotion: in-place operations whose result_type is a lower dtype category (bool < integer < floating-point) than the in-place operand now throw an Error.  ([22273](https://github.com/pytorch/pytorch/pull/22273), [26981](https://github.com/pytorch/pytorch/pull/26981))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.2</th><th>Version 1.3</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> int_tensor = torch.tensor(1)\r\n>>> int_tensor.add_(1.5)\r\ntensor(2)\r\n>>> bool_tensor = torch.tensor(True)\r\n>>> bool_tensor.add_(5)\r\ntensor(True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> int_tensor = torch.tensor(1)\r\n>>> int_tensor.add_(1.5)\r\nRuntimeError: result type Float cannot be cast to the desired output type Long\r\n>>> bool_tensor = torch.tensor(True)\r\n>>> bool_tensor.add_(5)\r\nRuntimeError: result type Long cannot be cast to the desired output type Bool\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThese rules can be checked at runtime via [torch.can_cast](https://pytorch.org/docs/master/torch.html#torch.can_cast).\r\n\r\n#### `torch.flatten`: 0-dimensional inputs now return a 1-dim tensor.  ([25406](https://github.com/pytorch/pytorch/pull/25406)).\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.2</th><th>Version 1.3</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.flatten(torch.tensor(0))\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.flatten(torch.tensor(0))\r\ntensor([0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### `nn.functional.affine_grid`: when `align_corners = True`, changed the behavior of 2D affine transforms on 1D data and 3D affine transforms on 2D data (i.e., when one of the spatial dimensions has unit size).\r\n\r\nPreviously, all grid points along a unit dimension were considered arbitrarily to be at -1, now they are considered to be at 0 (the center of the input image).\r\n\r\n#### `torch.gels:` removed deprecated operator, use `torch.lstsq` instead.  ([26480](https://github.com/pytorch/pytorch/pull/26480)).\r\n\r\n#### `utils.data.DataLoader:` made a number of Iterator attributes private (e.g. `num_workers`, `pin_memory`).  ([22273](https://github.com/pytorch/pytorch/pull/22273))\r\n\r\n#### **[C++]** `Variable::backward` will no longer implicitly create a gradient for non-1-element Variables.  Previously, a gradient tensor of all 1s would be implicitly created . This behavior matches the Python API.  ([26150](https://github.com/pytorch/pytorch/pull/26150))\r\n\r\n```\r\nauto x = torch::randn({5, 5}, torch::requires_grad());\r\nauto y = x * x;\r\ny.backward()\r\n// ERROR: \"grad can be implicitly created only for scalar outputs\"\r\n```\r\n\r\n#### [C++] All option specifiers (e.g. `GRUOptions::bidirectional_`) are now private, use the function variants (`GRUOptions::bidirectional(...))` instead. ([26419](https://github.com/pytorch/pytorch/pull/26419)).\r\n\r\n# Highlights\r\n\r\n## [Experimental]: Mobile Support \r\n\r\nIn PyTorch 1.3, we are launching experimental support for mobile. Now you can run any TorchScript model directly without any conversion. Here are the full list of features in this release:\r\n\r\n* Support for full TorchScript inference on mobile;\r\n* Prebuilt LibTorch libraries for Android/iOS on JCenter/CocoaPods;\r\n* Java wrapper for Android with functionality to cover common inference cases (loading and invoking the model);\r\n* Support for all forward ops on mobile CPU (backward ops are not supported yet);\r\n* Some optimized fp32 operator implementations for ARM CPUs (based on Caffe2Go);\r\n* Some optimized int8 operator implementations for ARM CPUs (based on QNNPACK);\r\n\r\nWe decided not to create a new framework for mobile so that you can use the same APIs you are already familiar with to run the same TorchScript models on Android/iOS devices without any format conversion. This way you can have the shortest path from research ideas to production-ready mobile apps.\r\n\r\nThe tutorials, demo apps and download links for prebuilt libraries can be found at: https://pytorch.org/mobile/\r\n\r\nThis is an experimental release. We are working on other features like customized builds to make PyTorch smaller, faster and better for your specific use cases. Stay tuned and give us your feedback!\r\n\r\n## [Experimental]: Named Tensor Support\r\n\r\nNamed Tensors aim to make tensors easier to use by allowing users to associate explicit names with tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support \"broadcasting by name\" rather than \"broadcasting by position\".\r\n\r\nCreate a named tensor by passing a `names` argument into most tensor factory function.\r\n\r\n```python\r\n>>> tensor = torch.zeros(2, 3, names=('C', 'N'))\r\n    tensor([[0., 0., 0.],\r\n            [0., 0., 0.]], names=('C', 'N'))\r\n```\r\n\r\nNamed tensors propagate names across operations.\r\n\r\n```python\r\n>>> tensor.abs()\r\n    tensor([[0., 0., 0.],\r\n            [0., 0., 0.]], names=('C', 'N'))\r\n```\r\n\r\nRearrange to a desired ordering by using `align_to` .\r\n\r\n```python\r\n>>> tensor = tensor.align_to('N', 'C', 'H', 'W')\r\n>>> tensor.names, tensor.shape\r\n    (('N', 'C', 'H', 'W'), torch.Size([3, 2, 1, 1]))\r\n```\r\n\r\nAnd more! [Please see our documentation on named tensors.](https://pytorch.org/docs/master/named_tensor.html)\r\n\r\n## [Experimental]: Quantization support\r\n\r\nPyTorch now supports quantization from the ground up, starting with support for quantized tensors. Convert a float tensor to a quantized tensor and back by:\r\n\r\n```\r\nx = torch.rand(10,1, dtype=torch.float32)\r\nxq = torch.quantize_per_tensor(x, scale = 0.5, zero_point = 8, dtype=torch.quint8)\r\n# xq is a quantized tensor with data represented as quint8\r\nxdq = x.dequantize()\r\n# convert back to floating point\r\n```\r\n\r\nWe also support 8 bit quantized implementations of most common operators in CNNs, including:\r\n\r\n* Tensor operations:\r\n    * view, clone, resize, slice\r\n    * add, multiply, cat, mean, max, sort, topk\r\n* Modules/Functionals (in torch.nn.quantized)\r\n    * Conv2d\r\n    * Linear\r\n    * Avgpool2d, AdaptiveAvgpool2d, MaxPool2d, AdaptiveMaxPool2d\r\n    * Interpolate\r\n    * Upsample\r\n* Fused operations for preserving better accuracy (in torch.nn.intrinsic)\r\n    * ConvReLU2d, ConvBnReLU2d, ConvBn2d\r\n    * LinearReLU\r\n    * add_relu\r\n\r\nWe also support dynamic quantized operators, which take in floating point activations, but use quantized weights (in torch.nn.quantized.dynamic).\r\n\r\n* LSTM\r\n* Linear\r\n\r\nQuantization also requires support for methods to collect statistics from tensors and calculate quantization parameters (implementing interface torch.quantization.Observer). We support several methods to do so:\r\n\r\n* MinMaxObserver\r\n* MovingAverageMinMaxObserver\r\n* PerChannelMinMaxObserver\r\n* MovingAveragePerChannelMinMaxObserver\r\n* HistogramObserver\r\n\r\nFor quantization aware training, we support fake-quantization operators and modules to mimic quantization during training:\r\n\r\n* `torch.fake_quantize_per_tensor_affine`, `torch.fake_quantize_per_channel_affine`\r\n* `torch.quantization.FakeQuantize`\r\n\r\nIn addition, we also support workflows in torch.quantization for:\r\n\r\n* post-training dynamic quantization\r\n* static post training quantization\r\n* quantization aware training \r\n\r\nAll quantized operators are compatible with TorchScript.\r\n\r\nFor more details, see the documentation at: https://pytorch.org/docs/master/quantization.html\r\n\r\n## Type Promotion\r\n\r\nArithmetic and comparison operations may now perform mixed-type operations that promote to a common dtype. \r\n\r\nThis below example was not allowed in version 1.2. In version 1.3, the same code returns a tensor with `dtype=torch.float32`.\r\n\r\n```\r\n>>> torch.tensor([1], dtype=torch.int) + torch.tensor([1], dtype=torch.float32)\r\n```\r\n\r\nSee the full [documentation](https://github.com/pytorch/pytorch/blob/master/docs/source/tensor_attributes.rst#type-promotion-doc) for more details.\r\n\r\n* `torch.result_type` Provide function to determine result of mixed-type operations ([26012](https://github.com/pytorch/pytorch/pull/26012)).\r\n* `torch.can_cast` Expose casting rules for type promotion ([26805](https://github.com/pytorch/pytorch/pull/26805)).\r\n* `torch.promote_types` Expose promotion logic ([26655](https://github.com/pytorch/pytorch/pull/26655)).\r\n\r\n\r\n\r\n\r\n## Deprecations\r\n\r\n\r\n### `nn.functional.affine_grid` / `nn.functional.grid_sample`: USING The Align_CORNER Default value is now deprecated, because it will be changed in 1.4 release.\r\n\r\nThe `align_corner` parameter was added in this release; the behavior in the previous release was equivalent to setting the parameter to `True`.  This is also the current default value but it will be changed to `False` from 1.4 release. Note that using the default will trigger a warning as demonstrated below; set the value explicitly to remove the warning. \r\n\r\n    >>> torch.nn.functional.affine_grid(torch.randn(1,2,3),\r\n                                        (1,3,2,2))\r\n    UserWarning: Default grid_sample and affine_grid behavior will be changed\r\n    to align_corners=False from 1.4.0. \r\n    See the documentation of grid_sample for details.\r\n    ...\r\n    \r\n    >>> torch.nn.functional.affine_grid(torch.randn(1,2,3),\r\n                                        (1,3,2,2),\r\n                                        align_corners=True)\r\n    # NO WARNING!\r\n    ...\r\n\r\n### [C++] Deprecate `torch::Tensor::data<T>()` in favor of `torch::Tensor::data_ptr<T>()` ([24847](https://github.com/pytorch/pytorch/pull/24847), [24886](https://github.com/pytorch/pytorch/pull/24886)).\r\n\r\n# New Features\r\n\r\n## TensorBoard: 3D Mesh and Hyperparameter Support\r\n\r\n`torch.utils.tensorboard` supports 3D mesh and points plus hyperparameter logging. More details can be found in [the documentation](https://pytorch.org/docs/stable/tensorboard.html) for `SummaryWriter` with `add_mesh` and `add_hparams`.\r\n\r\nA simple example exercising both methods:\r\n\r\n```\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nvertices_tensor = torch.as_tensor([\r\n    [1, 1, 1],\r\n    [-1, -1, 1],\r\n    [1, -1, -1],\r\n    [-1, 1, -1],\r\n], dtype=torch.float).unsqueeze(0)\r\ncolors_tensor = torch.as_tensor([\r\n    [255, 0, 0],\r\n    [0, 255, 0],\r\n    [0, 0, 255],\r\n    [255, 0, 255],\r\n], dtype=torch.int).unsqueeze(0)\r\nfaces_tensor = torch.as_tensor([\r\n    [0, 2, 3],\r\n    [0, 3, 1],\r\n    [0, 1, 2],\r\n    [1, 3, 2],\r\n], dtype=torch.int).unsqueeze(0)\r\n\r\nwith SummaryWriter() as w:\r\n    w.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\r\n    for i in range(5):\r\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\r\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\r\n\r\n```\r\n\r\n## Distributed\r\n\r\nThis release adds macOS support for `torch.distributed` with the Gloo backend. You can more easily switch from development (e.g. on macOS) to deployment (e.g. on Linux) without having to change a single line of code. The prebuilt binaries for macOS (stable and nightly) include support out of the box.\r\n\r\n\r\n* `torch.distributed.all_reduce_coalesced` Support allreduce of a list of same-device tensors ([24949](https://github.com/pytorch/pytorch/pull/24949), [25470](https://github.com/pytorch/pytorch/pull/25470), [24876](https://github.com/pytorch/pytorch/pull/24876))\r\n* `torch.distributed.all_reduce` Add bitwise reduction ops (BAND, BOR, BXOR) ([26824](https://github.com/pytorch/pytorch/pull/26824))\r\n\r\n## Libtorch Binaries with C++11 ABI\r\n\r\nWe now provide Libtorch binaries for building applications compatible with the C++11 ABI. The download links for libtorch binaries with C++11 ABI can be found in https://pytorch.org/ \u201cQUICK START LOCALLY\u201d.\r\n\r\n\r\n## New TorchScript features\r\n\r\n* Add `not in` support for TorchScript ([23637](https://github.com/pytorch/pytorch/pull/23637)).\r\n* You can now raise exceptions in one side of an if branch ([23565](https://github.com/pytorch/pytorch/pull/23565)).\r\n* Add `torch.jit.is_scripting()` API ([25955](https://github.com/pytorch/pytorch/pull/25955)).\r\n* Make assertions like `x is not None` unwrap the optional type of `x` ([23949](https://github.com/pytorch/pytorch/pull/23949)).\r\n* Add dictionary augmented assignment (`+=`) support to TorchScript ([23639](https://github.com/pytorch/pytorch/pull/23639)).\r\n* Support `grad` and `data` attribute for tensor in TorchScript ([23842](https://github.com/pytorch/pytorch/pull/23842)).\r\n* Add `@ignore` for TorchScript classes ([23614](https://github.com/pytorch/pytorch/pull/23614)).\r\n* Support nn.GRU in script ([23266](https://github.com/pytorch/pytorch/pull/23266)).\r\n* Support tensor as a key type in TorchScript ([23638](https://github.com/pytorch/pytorch/pull/23638)).\r\n* Add support for ModuleDict ([25715](https://github.com/pytorch/pytorch/pull/25715)).\r\n* Bind `set_grad_enabled()` into TorchScript ([25350](https://github.com/pytorch/pytorch/pull/25350)).\r\n* Add `in` membership checks for lists ([25796](https://github.com/pytorch/pytorch/pull/25796)).\r\n* Add `tuple` keyword ([25474](https://github.com/pytorch/pytorch/pull/25474)).\r\n* Add `__getitem__` to class types ([25664](https://github.com/pytorch/pytorch/pull/25664)).\r\n* Add `__setitem__` to class types ([25750](https://github.com/pytorch/pytorch/pull/25750)).\r\n* Make JIT dicts ordered, matching Python 3.6+ semantics ([26465](https://github.com/pytorch/pytorch/pull/26465)).\r\n* Added invert bitwise operation to TorchScript ([22324](https://github.com/pytorch/pytorch/pull/22324)).\r\n* Add `min()` and `max()` for lists to TorchScript ([26351](https://github.com/pytorch/pytorch/pull/26351)).\r\n* Support iterables and ranges in list comprehensions ([26768](https://github.com/pytorch/pytorch/pull/26768)).\r\n\r\n# Improvements\r\n\r\n\r\n\r\n## C++ Frontend Improvements\r\n\r\nWe are on our way to better API parity between our Python and C++ frontends. Specifically, we made the following improvements:\r\n\r\n### Autograd\r\n\r\n* Tensor autograd APIs\r\n    * `torch::Tensor::data` Added ([26008](https://github.com/pytorch/pytorch/pull/26008)).\r\n    * `torch::Tensor::grad` Don\u2019t create a gradient for non-1-element Variables [BC-breaking] ([26150](https://github.com/pytorch/pytorch/pull/26150)).\r\n    * `torch::Tensor::is_leaf` Added ([26186](https://github.com/pytorch/pytorch/pull/26186)). \r\n    * `torch::Tensor::output_nr` Added ([26216](https://github.com/pytorch/pytorch/pull/26216)). \r\n    * `torch::Tensor::_version` Added ([26217](https://github.com/pytorch/pytorch/pull/26217)).\r\n* Add support for custom autograd functions in C++ API\r\n    * For example usage, please see the PR description and test cases in ([23572](https://github.com/pytorch/pytorch/pull/23572), [23628](https://github.com/pytorch/pytorch/pull/23628), and [23803](https://github.com/pytorch/pytorch/pull/23803))\r\n* `torch::autograd::backward` and `torch::autograd::grad` ([24342](https://github.com/pytorch/pytorch/pull/24342))\r\n* `torch::autograd::Variable::register_hook` ([24393](https://github.com/pytorch/pytorch/pull/24393)).\r\n\r\n### New torch::nn modules\r\n\r\n* Containers\r\n    * torch::nn::ModuleList ([24317](https://github.com/pytorch/pytorch/pull/24317)).\r\n* Linear layers\r\n    * torch::nn::Identity ([26713](https://github.com/pytorch/pytorch/pull/26713)).\r\n* Convolution layers\r\n    * torch::nn::Fold ([24160](https://github.com/pytorch/pytorch/pull/24160)).\r\n* Pooling layers\r\n    * torch::nn::MaxPool1d / MaxPool2d / MaxPool3d ([24860](https://github.com/pytorch/pytorch/pull/24860), [26521](https://github.com/pytorch/pytorch/pull/26521)).\r\n    * torch::nn::AvgPool1d / AvgPool2d / AvgPool3d ([25800](https://github.com/pytorch/pytorch/pull/25800)).\r\n    * torch::nn::AdaptiveMaxPool1d / AdaptiveMaxPool2d / AdaptiveMaxPool3d ([26755](https://github.com/pytorch/pytorch/pull/26755), [26772](https://github.com/pytorch/pytorch/pull/26772), [26775](https://github.com/pytorch/pytorch/pull/26775)).\r\n* Loss functions\r\n    * torch::nn::L1Loss ([25902](https://github.com/pytorch/pytorch/pull/25902)).\r\n* Distance functions\r\n    * torch::nn::CosineSimilarity ([26424](https://github.com/pytorch/pytorch/pull/26424))\r\n    * torch::nn::PairwiseDistance ([26424](https://github.com/pytorch/pytorch/pull/26424))\r\n\r\n### New torch::nn::functional functions\r\n\r\n* Pooling functions\r\n    * torch::nn::functional::max_pool1d / max_pool2d / max_pool3d ([26262](https://github.com/pytorch/pytorch/pull/26262)).\r\n    * torch::nn::functional::max_pool1d_with_indices / max_pool2d_with_indices / max_pool3d_with_indices ([26521](https://github.com/pytorch/pytorch/pull/26521)).\r\n    * torch::nn::functional::avg_pool1d / avg_pool2d / avg_pool3d ([26262](https://github.com/pytorch/pytorch/pull/26262)).\r\n    * torch::nn::functional::adaptive_max_pool1d / adaptive_max_pool2d / adaptive_max_pool3d ([26755](https://github.com/pytorch/pytorch/pull/26755), [26772](https://github.com/pytorch/pytorch/pull/26772), [26775](https://github.com/pytorch/pytorch/pull/26775)).\r\n    * torch::nn::functional::adaptive_max_pool1d_with_indices / adaptive_max_pool2d_with_indices / adaptive_max_pool3d_with_indices ([26755](https://github.com/pytorch/pytorch/pull/26755), [26772](https://github.com/pytorch/pytorch/pull/26772), [26775](https://github.com/pytorch/pytorch/pull/26775)).\r\n* Distance functions\r\n    * torch::nn::functional::cosine_similarity ([26424](https://github.com/pytorch/pytorch/pull/26424)).\r\n    * torch::nn::functional::pairwise_distance ([26424](https://github.com/pytorch/pytorch/pull/26424)).\r\n\r\n### tensor Construction API\r\n\r\n* Add support for multidimensional inputs to `torch::tensor` ([26210](https://github.com/pytorch/pytorch/pull/26210), [26890](https://github.com/pytorch/pytorch/pull/26890), [26756](https://github.com/pytorch/pytorch/pull/26756)).\r\n    * From now on, we can use `torch::tensor({{1, 2}, {3, 4}})` in C++ to construct the same tensor as `torch.tensor([[1, 2], [3, 4]])` in Python. Some caveats are noted in [this comment](https://github.com/pytorch/pytorch/blob/e0ae3ce5e4b5a98c8dd67b9ec1ea0f81dfc52fef/tools/autograd/templates/variable_factories.h#L184-L194).\r\n* Add support for bool and BFloat16 dtypes to `torch::tensor` ([23337](https://github.com/pytorch/pytorch/pull/23337)).\r\n\r\n### Other C++ Improvements\r\n\r\n* Add `torch::nn::Module::unregister_module` function, for unregistering a submodule from a `torch::nn::Module` ([26088](https://github.com/pytorch/pytorch/pull/26088)).\r\n\r\n## Distributed Improvements\r\n\r\n* `torch.distributed` Detect and handle NCCL errors appropriately instead of blocking peers until timeout in `ProcessGroupNCCL` ([25012](https://github.com/pytorch/pytorch/pull/25012), [25905](https://github.com/pytorch/pytorch/pull/25905))\r\n* `torch.distributed` Make scatter/gather arguments optional ([25575](https://github.com/pytorch/pytorch/pull/25575))\r\n* `torch.distributed.launch` Add a -m flag to allow users to launch python modules ([24910](https://github.com/pytorch/pytorch/pull/24910)).\r\n* `torch.distributed` Add function to get NCCL version for logging ([26583](https://github.com/pytorch/pytorch/pull/26583)).\r\n* `torch.distributed` Add timeout parameter to connect function in TCPStore ([26554](https://github.com/pytorch/pytorch/pull/26554)).\r\n* `torch.distributed` use timeout in connect function to prevent against infinite loop ([26364](https://github.com/pytorch/pytorch/pull/26364)).\r\n* `torch.nn.modules.batchnorm` Allow SyncBatchNorm to run without DDP in inference mode ([24815](https://github.com/pytorch/pytorch/pull/24815))\r\n\r\n## Performance Improvements\r\n\r\n* `torch.argmax/argmin` Rewrite as TensorIterator reductions ([26181](https://github.com/pytorch/pytorch/pull/26181)).\r\n* `torch.erfinv` Vectorize unary operator ([26629](https://github.com/pytorch/pytorch/pull/26629)).\r\n* `torch.sin/cos/tan` Use intrinsics for trigonometric functions on CPU ([26431](https://github.com/pytorch/pytorch/pull/26431)).\r\n* Fix possible deadlock in SharedCache inside a forked child proc ([25158](https://github.com/pytorch/pytorch/pull/25158)).\r\n* `torch.qr` Fix a regression ([23591](https://github.com/pytorch/pytorch/pull/23591)).\r\n* `nn.Conv` Use Caffe2's implementation of grouped depthwise 3x3 convolutions ([26556](https://github.com/pytorch/pytorch/pull/26556)).\r\n* `nn.Conv` Use parallel_for in DepthwiseConvKernel ([26879](https://github.com/pytorch/pytorch/pull/26879)).\r\n* `nn.Conv` Change shape for conv and unary ops ([25477](https://github.com/pytorch/pytorch/pull/25477)).\r\n* Fix pin_memory_thread not exiting quickly ([23646](https://github.com/pytorch/pytorch/pull/23646)).\r\n* Increase predefined_minimum_secs to reduce variation ([23734](https://github.com/pytorch/pytorch/pull/23734)).\r\n* Enhance Tensor indexSelect performance ([23055](https://github.com/pytorch/pytorch/pull/23055)).\r\n* Separate input shapes to reduce default execution time ([24136](https://github.com/pytorch/pytorch/pull/24136)).\r\n* constraints.lower_cholesky Vectorize LowerCholeskyTransform ([24131](https://github.com/pytorch/pytorch/pull/24131)).\r\n* Speed up an integer to the power of a positive integer on CPU ([26020](https://github.com/pytorch/pytorch/pull/26020)).\r\n* [ROCm] Enable jit fusion ([22872](https://github.com/pytorch/pytorch/pull/22872)).\r\n* [ROCm] Use MIOpen for transpose convolutions ([26172](https://github.com/pytorch/pytorch/pull/26172)).\r\n\r\n## JIT Improvements\r\n\r\n* Enable CPU fused kernel on Windows ([25578](https://github.com/pytorch/pytorch/pull/25578)).\r\n* Expose an API to iterate all the registered operators ([23207](https://github.com/pytorch/pytorch/pull/23207)).\r\n* Include recursive class compilations in error call stack ([23454](https://github.com/pytorch/pytorch/pull/23454)).\r\n* Substantial improvements to saved model format speed and size.\r\n    * Compress debug symbols when serializing TorchScript models. ([23659](https://github.com/pytorch/pytorch/pull/23659)).\r\n    * Compress all non-Tensor components of a serialized TorchScript model. ([23723](https://github.com/pytorch/pytorch/pull/23723)).\r\n    * Perform string uniquing by value in pickle serialization. ([23741](https://github.com/pytorch/pytorch/pull/23741)).\r\n    * Implement a bunch of pickle serialization features that optimize for size. ([23759](https://github.com/pytorch/pytorch/pull/23759)).\r\n    * Implement more size-oriented opcodes in the depickler. ([26454](https://github.com/pytorch/pytorch/pull/26454)).\r\n* Cache node operators to speed up optimization ([24827](https://github.com/pytorch/pytorch/pull/24827)).\r\n* Allow forward hooks in tracing ([23613](https://github.com/pytorch/pytorch/pull/23613)).\r\n* Add Pickler C++ API ([23241](https://github.com/pytorch/pytorch/pull/23241)).\r\n* Open up AliasAnalysisKind for any ops ([23810](https://github.com/pytorch/pytorch/pull/23810)).\r\n* Add the ability to compile exports on traced modules ([24298](https://github.com/pytorch/pytorch/pull/24298)).\r\n* Make `NoneType` a subtype of `Optional[T]` ([25361](https://github.com/pytorch/pytorch/pull/25361)).\r\n\r\n## ONNX Exporter Improvements\r\n\r\nIn PyTorch 1.3, we have added support for exporting graphs with ONNX IR v4 semantics, and set it as default. We have achieved good initial coverage for ONNX Opset 11, which was released recently with ONNX 1.6. Further enhancement to Opset 11 coverage will follow in the next release. We have enabled export for about 20 new PyTorch operators. Also, we have focused on enabling the export for all models in torchvision. We have introduced some necessary groundwork for that in this release, e.g., accepting PyTorch models with inputs/outputs of Dict or String. We continue to work on torchvision models, such as FasterRCNN and MaskRCNN, to enable their export.\r\n\r\n### Adding Support for ONNX IR v4\r\n\r\n* Provide an option to exclude the weights from model inputs ([#23284](https://github.com/pytorch/pytorch/pull/26146))\r\n* Make graph inputs without weights as default ([#26146](https://github.com/pytorch/pytorch/pull/26146))\r\n\r\n### Adding Support for ONNX Opset 11\r\n\r\n* Introduce ONNX Opset 11 support ([#23739](https://github.com/pytorch/pytorch/pull/23739))\r\n* Add export for torch.Interpolate in Opset 11 ([#24805](https://github.com/pytorch/pytorch/pull/24805), [#27179](https://github.com/pytorch/pytorch/pull/27179))\r\n* Add export for tensor.gather, tensor.scatter and tensor.scatter_add in Opset 11 ([#24790](https://github.com/pytorch/pytorch/pull/24790))\r\n* Add export for tensor.clamp in Opset 11 ([#25797](https://github.com/pytorch/pytorch/pull/25797))\r\n* Add export for torch.topk and torch.sort in Opset 11 ([#25739](https://github.com/pytorch/pytorch/pull/25739))\r\n\r\n### Exporting More Torch Operators/Models to ONNX\r\n\r\n* Export torch.pixel_shuffle ([#23739](https://github.com/pytorch/pytorch/pull/23739))\r\n* Export torch.multinomial ([#23581](https://github.com/pytorch/pytorch/pull/23581))\r\n* Export torch.norm\u2019s frobenius_norm ([#23536](https://github.com/pytorch/pytorch/pull/23536))\r\n* Export torch.std ([#22310](https://github.com/pytorch/pytorch/pull/22310))\r\n* Export torch.empty and torch.empty_like ([#24166](https://github.com/pytorch/pytorch/pull/24166))\r\n* Export torch.rsqrt ([#24153](https://github.com/pytorch/pytorch/pull/24153))\r\n* Export torch.log1p ([#25808](https://github.com/pytorch/pytorch/pull/25808))\r\n* Export torch.unique ([#25050](https://github.com/pytorch/pytorch/pull/25050))\r\n* Export torch.gelu ([#24475](https://github.com/pytorch/pytorch/pull/24475))\r\n* Export tensor.index_fill and tensor.index_copy ([#23052](https://github.com/pytorch/pytorch/pull/23052))\r\n* Export torch.round ([#26126](https://github.com/pytorch/pytorch/pull/26126))\r\n* Export torch.baddbmm ([#25738](https://github.com/pytorch/pytorch/pull/25738))\r\n* Export torch.remainder ([#24410](https://github.com/pytorch/pytorch/pull/24410))\r\n* Export torch.cumsum ([#24476](https://github.com/pytorch/pytorch/pull/24476))\r\n* Export tensor.size with negative axis ([#26436](https://github.com/pytorch/pytorch/pull/26436))\r\n* Export RNN/LSTM with h0/c0 initial state ([#22813](https://github.com/pytorch/pytorch/pull/22813))\r\n\r\n### Enhancing ONNX Export Infra\r\n\r\n* Enable exporting PyTorch models which have Dict and String as inputs and outputs ([#25889](https://github.com/pytorch/pytorch/pull/25889))\r\n* Systematically solving mismatched types caused by implicit type conversion for binary arithmetic operators by adding an ONNX type conversions pass. ([#24378](https://github.com/pytorch/pytorch/pull/24378))\r\n* Correctly validate dynamic axes names. ([#23974](https://github.com/pytorch/pytorch/pull/23974))\r\n* Enable ONNX Runtime tests for Opset 10 and partially for Opset 11 ([#22993](https://github.com/pytorch/pytorch/pull/22993))\r\n\r\n## Other Improvements\r\n\r\n* Error checking: many operators now perform strides check of the output tensor and errors if it contains inner overlaps that would result in incorrect result ([23063](https://github.com/pytorch/pytorch/issues/23063)).\r\n* `torch.det/logdet/slogdet` Allowing batching ([22909](https://github.com/pytorch/pytorch/pull/22909)).\r\n* `torch.logical_not` Add new operator ([23839](https://github.com/pytorch/pytorch/pull/23839)).\r\n* `torch.logical_xor` Add new operator ([23847](https://github.com/pytorch/pytorch/pull/23847)).\r\n* `torch.symeig` Improve the stability of gradient updates ([23018](https://github.com/pytorch/pytorch/pull/23018)).\r\n* `torch.eye` Enable for bool and half ([24148](https://github.com/pytorch/pytorch/pull/24148)).\r\n* `torch.tril / triu` Enable for bool and half ([24163](https://github.com/pytorch/pytorch/pull/24163)).\r\n* `torch.logical_not/xor` support non-bool tensors. ([23916](https://github.com/pytorch/pytorch/pull/23916), [23978](https://github.com/pytorch/pytorch/pull/23978)).\r\n* `torch.index_select` Implement indexing methods for sparse tensors ([24937](https://github.com/pytorch/pytorch/pull/24937)).\r\n* `torch.lu_solve` Enable broadcasting of batch dimensions ([24333](https://github.com/pytorch/pytorch/pull/24333)).\r\n* `torch.cholesky` Enable batches greater than 262140 ([24438](https://github.com/pytorch/pytorch/pull/24438)).\r\n* `torch.det` Simplify generation of singular matrices to avoid numerical issue on PowerPC ([25773](https://github.com/pytorch/pytorch/pull/25773)).\r\n* `torch.erfinv` In the CUDA implementation, use erfinv() for double to preserve accuracy ([25337](https://github.com/pytorch/pytorch/pull/25337)).\r\n* `torch.erfinv` Add a float version of erfinv on CPU ([26070](https://github.com/pytorch/pytorch/pull/26070)).\r\n* `torch.cuda.stream` Updates autograd engine to respect streams set in forward ([8354](https://github.com/pytorch/pytorch/pull/8354)).\r\n* `torch.backends.mkldnn.enabled` Allow disabling MKLDNN at runtime ([25459](https://github.com/pytorch/pytorch/pull/25459)).\r\n* `torch.cholesky_solve` Add derivative ([26185](https://github.com/pytorch/pytorch/pull/26185)).\r\n* `torch.cholesky_inverse` Add derivative ([26451](https://github.com/pytorch/pytorch/pull/26451)).\r\n* `torch.polygamma` Ensure that n is non-negativ`e` ([26294](https://github.com/pytorch/pytorch/pull/26294)).\r\n* `torch.pinverse` Enable batching ([26095](https://github.com/pytorch/pytorch/pull/26095)).\r\n* `torch.digamma/trigamma` Fix type mismatches on CUDA ([25791](https://github.com/pytorch/pytorch/pull/25791)).\r\n* `torch.where` Enable for bool tensor on CUDA ([26430](https://github.com/pytorch/pytorch/pull/26430)).\r\n* `torch.load` default encoding change to 'utf-8' ([26421](https://github.com/pytorch/pytorch/pull/26421)).\r\n* `torch.repeat_interleave` Respect the current stream ([26946](https://github.com/pytorch/pytorch/pull/26946)).\r\n* `torch.bernoulli_` Implement for bool tensors ([25076](https://github.com/pytorch/pytorch/pull/25076)).\r\n* `torch.norm` Fix nuclear norm with requires_grad=True ([26303](https://github.com/pytorch/pytorch/pull/26303)).\r\n* `torch.hub.download_url_to_file` Make function public ([26723](https://github.com/pytorch/pytorch/pull/26723)).\r\n* `nn.modules.conv` add padding_mode to repr ([23996](https://github.com/pytorch/pytorch/pull/23996)).\r\n* `nn.Transformer` Extend to support BERT (gelu) ([24181](https://github.com/pytorch/pytorch/pull/24181)).\r\n* `nn.BatchNorm2d` Add support for non-affine batch norm with float stats and half inputs ([22750](https://github.com/pytorch/pytorch/pull/22750)).\r\n* `nn.Parameter` Fix type hints ([25586](https://github.com/pytorch/pytorch/pull/25586)).\r\n* `nn.CTCLoss` Improve error message ([26325](https://github.com/pytorch/pytorch/pull/26325)).\r\n* `nn.Conv` Allow batch size of 0 ([26214](https://github.com/pytorch/pytorch/pull/26214)).\r\n* `nn.LSTM/GRU` enable double backward for non-cudnn ([26660](https://github.com/pytorch/pytorch/pull/26660)).\r\n* `optim.Adagrad` Add epsilon argument ([24980](https://github.com/pytorch/pytorch/pull/24980)).\r\n* `optim.LBFGS`  Change default tolerance_grad to 1e-7 ([25240](https://github.com/pytorch/pytorch/pull/25240)).\r\n* `optim.lr_scheduler.OneCycleLR` Add new 1cycle learning rate scheduler ([25324](https://github.com/pytorch/pytorch/pull/25324)).\r\n* `optimizer.step` Fix type annotation ([26930](https://github.com/pytorch/pytorch/pull/26930)).\r\n* `bfloat16` Add support for sub, mul, and div on CPU ([22851](https://github.com/pytorch/pytorch/pull/22851)).\r\n* `bfloat16` Enabled comparison ops on CPU ([24182](https://github.com/pytorch/pytorch/pull/24182)).\r\n* `bfloat16` Enabled masked methods ([24183](https://github.com/pytorch/pytorch/pull/24183)).\r\n* `bfloat16` Enabled torch.mm and torch.mv ([24224](https://github.com/pytorch/pytorch/pull/24224)).\r\n* `bfloat16` Enable log_softmax and CrossEntropyLoss ([24457](https://github.com/pytorch/pytorch/pull/24457)).\r\n* `bfloat16` Enabled conv methods ([26167](https://github.com/pytorch/pytorch/pull/26167)).\r\n* `bfloat16` Enabled dtype on CUDA ([26407](https://github.com/pytorch/pytorch/pull/26407)).\r\n* `quasirandom.SobolEngine` Use random seed if not specified ([24884](https://github.com/pytorch/pytorch/pull/24884)).\r\n* `utils.data.dataloader` Add possible out of shared memory error message ([25730](https://github.com/pytorch/pytorch/pull/25730)).\r\n* `cuda.set_rng_state` Add type hint ([26200](https://github.com/pytorch/pytorch/pull/26200)).\r\n* Zero sized tensor support for repeat_interleave ([23717](https://github.com/pytorch/pytorch/pull/23717)).\r\n* Recommend `~` and `bitwise_not()` when user tries to apply neg (`-`) on a bool tensor. ([23621](https://github.com/pytorch/pytorch/pull/23621)).\r\n* Fix double backward of inplace op on view ([23502](https://github.com/pytorch/pytorch/pull/23502)).\r\n* `autograd.grad` Validate shapes of outputs ([25349](https://github.com/pytorch/pytorch/pull/25349)).\r\n* Enable libflame as a LAPACK choice ([25795](https://github.com/pytorch/pytorch/pull/25795)).\r\n* Fix race condition in CUDA initialization ([25788](https://github.com/pytorch/pytorch/pull/25788)).\r\n* Include `iteration_` in SGD optimizer serialization ([26906](https://github.com/pytorch/pytorch/pull/26906)).\r\n* [C++] `torch::tensor` Fix an ambiguous overload issues in constructor ([26890](https://github.com/pytorch/pytorch/pull/26890)).\r\n* [XLA] Check device before accessing data_ptr in PackLayer ([26056](https://github.com/pytorch/pytorch/pull/26056)).\r\n* [XLA] Allow overwriting catch-all kernels ([25947](https://github.com/pytorch/pytorch/pull/25947)).\r\n\r\n\r\n\r\n# Bug Fixes\r\n\r\n### TensorBoard Bug Fixes\r\n\r\n* `SummaryWriter.add_graph`: Fix empty graph output in some cases ([25599](https://github.com/pytorch/pytorch/pull/25599)).\r\n* Update Caffe2 contrib TensorBoard logging to not require TensorFlow ([25259](https://github.com/pytorch/pytorch/pull/25259)).\r\n* `SummaryWriter.make_video`: Fix write_gif call to moviepy for newer lib ([21218](https://github.com/pytorch/pytorch/pull/21218)).\r\n\r\n### C++ API Bug fixes\r\n\r\n* Fixes mismatch of device and data type when computing `step_size` in LBFGS optimizer ([25909](https://github.com/pytorch/pytorch/pull/25909)).\r\n\r\n### JIT\r\n\r\n* Fix list comprehension that change the type of the original iterable ([24271](https://github.com/pytorch/pytorch/pull/24271)).\r\n* Fix double copying of constants during recursive scripting ([24412](https://github.com/pytorch/pytorch/pull/24412)).\r\n* Fix frontend error message ([23576](https://github.com/pytorch/pytorch/pull/23576)).\r\n* Clear recursive error stack on each compilation ([23458](https://github.com/pytorch/pytorch/pull/23458)).\r\n* Fix bugs in assignment to optionals ([25059](https://github.com/pytorch/pytorch/pull/25059)).\r\n* Make `torch.jit.Attribute` work when `PYTORCH_ENABLED=0` ([23851](https://github.com/pytorch/pytorch/pull/23851)).\r\n* Fix unicode in comments causing compilation errors ([24218](https://github.com/pytorch/pytorch/pull/24218)).\r\n* Correctly raise an error if an `nn.Module` has not been initialized but you try to script it ([24852](https://github.com/pytorch/pytorch/pull/24852)).\r\n* Fix annotated assignment to variables ([25094](https://github.com/pytorch/pytorch/pull/25094)).\r\n* dictPop: dereference dict.find() iterator before calling dict.erase() ([25056](https://github.com/pytorch/pytorch/pull/25056)).\r\n* fix closures which always throw. ([25278](https://github.com/pytorch/pytorch/pull/25278)).\r\n* Add source location to class instantiation error ([24990](https://github.com/pytorch/pytorch/pull/24990)).\r\n* Fix `AliasAnalysisKind::PURE` on MSVC ([25375](https://github.com/pytorch/pytorch/pull/25375)).\r\n* Emit script function calls during tracing. ([25089](https://github.com/pytorch/pytorch/pull/25089)).\r\n* Resolve `NamedTuple` types properly in Python ([26443](https://github.com/pytorch/pytorch/pull/26443)).\r\n* Fix schema matching of tuples to vartype lists ([25944](https://github.com/pytorch/pytorch/pull/25944)).\r\n* Correctly preserve ignored function return value type ([25262](https://github.com/pytorch/pytorch/pull/25262)).\r\n* Fix missing newline in compiled from source range highlight ([25802](https://github.com/pytorch/pytorch/pull/25802)).\r\n* Fix use-after-free bug in `optional` ([25965](https://github.com/pytorch/pytorch/pull/25965)).\r\n* Fix torch.arange traced as constant ([25363](https://github.com/pytorch/pytorch/pull/25363)).\r\n* Preserve module names in recursive script ([24505](https://github.com/pytorch/pytorch/pull/24505)).\r\n* Properly resolve ignored module method type annotations ([26683](https://github.com/pytorch/pytorch/pull/26683)).\r\n* Make `is_optional` check more robust ([26312](https://github.com/pytorch/pytorch/pull/26312)).\r\n* Fix builtin lookup for Python functions ([26688](https://github.com/pytorch/pytorch/pull/26688)).\r\n* Typevar matching fix + implicit conversions from Scalar to int/float ([26453](https://github.com/pytorch/pytorch/pull/26453)).\r\n* Fix range for non-int inputs and pow implementation ([26926](https://github.com/pytorch/pytorch/pull/26926)).\r\n\r\n### Other Bug Fixes\r\n\r\n* `torch.is_pinned` pin_memory should not copy on already pinned tensors ([23484](https://github.com/pytorch/pytorch/pull/23484)).\r\n* `torch.cdist` Fix incorrect gradients on CUDA non-batch tensors ([22915](https://github.com/pytorch/pytorch/pull/22915)).\r\n* `torch.from_numpy` Fix failure on windows for int32 ([25139](https://github.com/pytorch/pytorch/pull/25139)).\r\n* `torch.tensor` Fix memory leak creating a tensor from numpy ([24267](https://github.com/pytorch/pytorch/pull/24267)).\r\n* `torch.index` Don't save `self` in `index` backward ([25594](https://github.com/pytorch/pytorch/pull/25594)).\r\n* `torch.bincount` Fix int32 overflow on CUDA ([25748](https://github.com/pytorch/pytorch/pull/25748)).\r\n* `torch.bernoulli` Fix the distribution sampler ([26864](https://github.com/pytorch/pytorch/pull/26864)).\r\n* `torch.pow` Fix precision ([25476](https://github.com/pytorch/pytorch/pull/25476)).\r\n* `torch.cdist` Fix gradient computation when first arg is 1xn ([26254](https://github.com/pytorch/pytorch/pull/26254)).\r\n* `torch.scatter_add_` Fix scatter CPU kernel when (input size, src size) > index size ([25839](https://github.com/pytorch/pytorch/pull/25839)).\r\n* `nn.ConvTranspose2d` Fixed an error with float16 inputs and weights on CUDA.  ([23552](https://github.com/pytorch/pytorch/pull/23552)).\r\n* `nn.CTCLoss` Fix zero-length targets on CUDA ([23298](https://github.com/pytorch/pytorch/pull/23298)).\r\n* `nn.Conv2d` Correct an overflow in an error message ([25146](https://github.com/pytorch/pytorch/pull/25146)).\r\n* `optim.Adam` apply a small mathematical fix. ([23737](https://github.com/pytorch/pytorch/pull/23737)).\r\n* `dataloader` Fix IndexError on shutdown if not all workers are started ([23761](https://github.com/pytorch/pytorch/pull/23761)).\r\n* `Tensor.repeat` Fix crash on for 0 repeats ([23766](https://github.com/pytorch/pytorch/pull/23766)).\r\n* `torch.pin_memory` only use one thread ([25111](https://github.com/pytorch/pytorch/pull/25111)).\r\n* `distributions.Uniform,HalfCauchy,Gamma` Fix `log_prob` when value is a float ([23017](https://github.com/pytorch/pytorch/pull/23017)).\r\n* Fix typing error for Padding with asymmetric signatures ([24895](https://github.com/pytorch/pytorch/pull/24895)).\r\n* Avoid race condition in `intrusive_ptr.reset_()` ([24464](https://github.com/pytorch/pytorch/pull/24464)).\r\n* `torch.hub`: Fix SSL cert issue for hub in Python 2 ([25042](https://github.com/pytorch/pytorch/pull/25042)).\r\n* Fix int overflow issue in CUDA kernels. ([24818](https://github.com/pytorch/pytorch/pull/24818)).\r\n* `Module.cuda` Fix type hints ([25018](https://github.com/pytorch/pytorch/pull/25018)).\r\n* Fix bug in assertNotEqual for int tensors ([25412](https://github.com/pytorch/pytorch/pull/25412)).\r\n* Fix 'in' return true incorrectly ([24156](https://github.com/pytorch/pytorch/pull/24156)).\r\n* Fix bugs in bulk loader when `batch_size=None` or with namedtuple ([26065](https://github.com/pytorch/pytorch/pull/26065)).\r\n* Fix serialization issue in big endian arch ([26383](https://github.com/pytorch/pytorch/pull/26383)).\r\n* Fix `Vec256::abs()` for floating point when applied on -0.0 ([26422](https://github.com/pytorch/pytorch/pull/26422)).\r\n* Fix cyclic reference in _LRScheduler ([25776](https://github.com/pytorch/pytorch/pull/25776)).\r\n* Fix a build failure on s390x ([26233](https://github.com/pytorch/pytorch/pull/26233)).\r\n* [XLA] Fix tensor construction from array ([24283](https://github.com/pytorch/pytorch/pull/24283)).\r\n\r\n# Documentation Updates\r\n\r\n### Distributed\r\n\r\n* `torch.distributed` Error phrasing in torch.distributed helper functions ([25574](https://github.com/pytorch/pytorch/pull/25574))\r\n* `torch.distributions.negative_binomial` clarified ambiguous doc string in NegativeBinomial ([25923](https://github.com/pytorch/pytorch/pull/25923))\r\n\r\n### JIT\r\n\r\n* Add technical documentation for the serialization format ([23456](https://github.com/pytorch/pytorch/pull/23456)).\r\n* Fix trace docs ([24191](https://github.com/pytorch/pytorch/pull/24191)).\r\n* Add `trace_module` to docs ([24258](https://github.com/pytorch/pytorch/pull/24258)).\r\n* Cleanup distinction around `script` and `trace` ([24208](https://github.com/pytorch/pytorch/pull/24208)).\r\n* Fix `item()` call in docs ([25404](https://github.com/pytorch/pytorch/pull/25404)).\r\n* Misc doc updates / fixes ([24371](https://github.com/pytorch/pytorch/pull/24371), [24445](https://github.com/pytorch/pytorch/pull/24445)).\r\n\r\n### Other documentation improvements\r\n\r\n* `torch.record_stream` Add documentation ([24078](https://github.com/pytorch/pytorch/pull/24078)).\r\n* `torch.fold` Describe the relation between fold and unfold operations ([24840](https://github.com/pytorch/pytorch/pull/24840)).\r\n* `torch.argmax` Fix incorrect doc ([23775](https://github.com/pytorch/pytorch/pull/23775)).\r\n* `torch.random` add docs ([23553](https://github.com/pytorch/pytorch/pull/23553)).\r\n* `torch.empty_strided` Add docs ([23735](https://github.com/pytorch/pytorch/pull/23735)).\r\n* `torch.bitwise_not` Document for bool tensors ([23800](https://github.com/pytorch/pytorch/pull/23800)).\r\n* `torch.cdist` Add documentation ([25221](https://github.com/pytorch/pytorch/pull/25221)).\r\n* `torch.where` Update parameter names in doc ([25554](https://github.com/pytorch/pytorch/pull/25554)).\r\n* `torch.atan2` Clarify and correct the doc ([26180](https://github.com/pytorch/pytorch/pull/26180)).\r\n* `nn.functional.bilinear` Added documentation ([24951](https://github.com/pytorch/pytorch/pull/24951)).\r\n* `nn.functional.upsample` Fix align_corners doc ([23707](https://github.com/pytorch/pytorch/pull/23707)).\r\n* `nn.Transformer` Fixed an error in the example ([24837](https://github.com/pytorch/pytorch/pull/24837)).\r\n* `optim.lr_scheduler.CosineAnnealingWarmRestarts` Add documentation ([25421](https://github.com/pytorch/pytorch/pull/25421)).\r\n* `optim.SGD` Updated with subscripts ([23985](https://github.com/pytorch/pytorch/pull/23985)).\r\n* `optim.RMSprop` Highlighting in the doc that square root comes before adding epsilon ([26735](https://github.com/pytorch/pytorch/pull/26735)).\r\n* `autograd.detect_anomaly` Add a warning ([26615](https://github.com/pytorch/pytorch/pull/26615)).\r\n* Improve dataloader docs on when auto-batching is disabled ([23671](https://github.com/pytorch/pytorch/pull/23671)).\r\n* Updated docs and added deprecation warnings to acknowledge a bool tensor ([22261](https://github.com/pytorch/pytorch/pull/22261)).\r\n* Document benchmarking practice for CUDA ([23910](https://github.com/pytorch/pytorch/pull/23910)).\r\n* Add ASAN instructions to CONTRIBUTING.md ([24848](https://github.com/pytorch/pytorch/pull/24848)).\r\n\r\n\r\n\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.3.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.3.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.3.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/20597721", "release_id": 20597721, "date_created": "2019-10-10T16:23:22Z", "date_published": "2019-10-10T17:26:52Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/19168826", "tag": "v1.2.0", "name": "New TorchScript API with Improved Python Language Coverage, Expanded ONNX Export, NN.Transformer", "author": {"name": "gchanan", "type": "User"}, "description": "We have just released PyTorch v1.2.0.\r\n\r\nIt has over 1,900 commits and contains a significant amount of effort in areas spanning JIT, ONNX, Distributed, as well as Performance and Eager Frontend Improvements.\r\n\r\n## Highlights\r\n\r\n### [JIT] New TorchScript API\r\n\r\nVersion 1.2 includes a new, easier-to-use API for converting `nn.Module`s into `ScriptModule`s. A sample usage is:\r\n\r\n```\r\nclass MyModule(torch.nn.Module):\r\n    ...\r\n\r\n# Construct an nn.Module instance\r\nmodule = MyModule(args)\r\n\r\n# Pass it to `torch.jit.script` to compile it into a ScriptModule.\r\nmy_torchscript_module = torch.jit.script(module)\r\n```\r\n\r\n`torch.jit.script()` will attempt to recursively compile the given `nn.Module`, including any submodules or methods called from `forward()`. See the [migration guide](https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api) for more info on what's changed and how to migrate.\r\n\r\n### [JIT] Improved TorchScript Python language coverage\r\n\r\nIn 1.2, TorchScript has significantly improved its support for Python language constructs and Python's standard library. Highlights include:\r\n\r\n* Early returns, breaks and continues.\r\n* Iterator-based constructs, like `for..in` loops, `zip()`, and `enumerate()`.\r\n* `NamedTuples`.\r\n* `math` and `string` library support.\r\n* Support for most Python builtin functions.\r\n\r\nSee the detailed notes below for more information.\r\n\r\n### Expanded Onnx Export\r\n\r\n In PyTorch 1.2, working with Microsoft, we\u2019ve added full support to export ONNX Opset versions 7(v1.2), 8(v1.3), 9(v1.4) and 10 (v1.5). We\u2019ve have also enhanced the constant folding pass to support Opset 10, the latest available version of ONNX. Additionally, users now are able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export. Here is a summary of the all of the major improvements:\r\n\r\n* Support for multiple Opsets including the ability to export dropout, slice, flip and interpolate in Opset 10.\r\n* Improvements to ScriptModule including support for multiple outputs, tensor factories and tuples as inputs and outputs.\r\n* More than a dozen additional PyTorch operators supported including the ability to export a custom operator. \r\n\r\nUpdated docs can be found [here](https://pytorch.org/docs/stable/onnx.html) and also a refreshed tutorial using ONNXRuntime can be found [here](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html).\r\n\r\n### Tensorboard is no Longer Considered Experimental\r\n\r\nRead the [documentation](https://pytorch.org/docs/stable/tensorboard.html) or simply type **`from`**` torch.utils.tensorboard `**`import`**` SummaryWriter` to get started!\r\n\r\n### NN.Transformer\r\n\r\nWe include a standard [nn.Transformer](https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer) module, based on the paper \u201c[_Attention is All You Need_](https://arxiv.org/abs/1706.03762)\u201d.  The `nn.Transformer` module relies entirely on an [attention mechanism](https://pytorch.org/docs/stable/nn.html?highlight=nn%20multiheadattention#torch.nn.MultiheadAttention) to draw global dependencies between input and output.  The individual components of the `nn.Transformer` module are designed so they can be adopted independently.  For example, the [nn.TransformerEncoder](https://pytorch.org/docs/stable/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder) can be used by itself, without the larger `nn.Transformer`. New APIs include:\r\n\r\n* `nn.Transformer`\r\n* `nn.TransformerEncoder` and `nn.TransformerEncoderLayer`\r\n* `nn.TransformerDecoder` and `nn.TransformerDecoderLayer`\r\n\r\nSee the [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers) documentation for more info.\r\n\r\n## Breaking Changes\r\n\r\n### Comparison operations (`lt (<), le (<=), gt (>), ge (>=), eq (==), ne, (!=)` ) return dtype has changed from `torch.uint8` to `torch.bool` ([21113](https://github.com/pytorch/pytorch/pull/21113))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2])\r\ntensor([1, 0, 0], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2])\r\ntensor([True, False, False])\r\n```\r\n\r\n\r\nFor most programs, we don't expect that any changes will need to be made as a result of this change. There are a couple of possible exceptions listed below.\r\n\r\n**Mask Inversion**\r\n\r\nIn prior versions of PyTorch, the idiomatic way to invert a mask was to call `1 - mask`.  This behavior is no longer supported; use the `~` or `bitwise_not()` operator instead.\r\n\r\n*Version 1.1*:\r\n\r\n```\r\n>>> 1 - (torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2]))\r\ntensor([0, 1, 1], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> 1 - (torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2]))\r\nRuntimeError: Subtraction, the `-` operator, with a bool tensor is not supported.\r\nIf you are trying to invert a mask, use the `~` or `bitwise_not()` operator instead.\r\n\r\n>>> ~(torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2]))\r\ntensor([False,  True,  True])\r\n```\r\n\r\n**sum(Tensor) (python built-in) does not upcast `dtype` like `torch.sum`**\r\n\r\nPython's built-in `sum` returns results in the same `dtype` as the tensor itself, so it will not return the expected result if the value of the sum cannot be represented in the `dtype` of the tensor.\r\n\r\n*Version 1.1*:\r\n\r\n```\r\n# value can be represented in result dtype\r\n>>> sum(torch.tensor([1, 2, 3, 4, 5]) > 2)\r\ntensor(3, dtype=torch.uint8)\r\n\r\n# value can NOT be represented in result dtype\r\n>>> sum(torch.ones((300,)) > 0)\r\ntensor(44, dtype=torch.uint8)\r\n\r\n# torch.sum properly upcasts result dtype\r\n>>> torch.sum(torch.ones((300,)) > 0)\r\ntensor(300)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n# value cannot be represented in result dtype (now torch.bool)\r\n>>> sum(torch.tensor([1, 2, 3, 4, 5]) > 2)\r\ntensor(True)\r\n\r\n# value cannot be represented in result dtype\r\n>>> sum(torch.ones((300,)) > 0)\r\ntensor(True)\r\n\r\n# torch.sum properly upcasts result dtype\r\n>>> torch.sum(torch.ones((300,)) > 0)\r\ntensor(300)\r\n```\r\n\r\n**TLDR**: use `torch.sum` instead of the built-in `sum`.  Note that the built-in `sum()` behavior will more closely resemble `torch.sum` in the next release. \r\n\r\nNote also that masking via `torch.uint8` Tensors is now deprecated, see the **Deprecations** section for more information.\r\n\r\n\r\n### `__invert__` / `~`: now calls `torch.bitwise_not` instead of `1 - tensor` and is supported for all integral+Boolean dtypes instead of only `torch.uint8`.  ([22326](https://github.com/pytorch/pytorch/pull/22326))\r\n\r\n*Version 1.1*:\r\n\r\n```\r\n>>> ~torch.arange(8, dtype=torch.uint8)\r\ntensor([ 1, 0, 255, 254, 253, 252, 251, 250], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2*:\r\n\r\n```\r\n>>> ~torch.arange(8, dtype=torch.uint8)\r\ntensor([255, 254, 253, 252, 251, 250, 249, 248], dtype=torch.uint8)\r\n```\r\n\r\n\r\n\r\n### `torch.tensor(bool)` and `torch.as_tensor(bool)` now infer `torch.bool` dtype instead of `torch.uint8`.  ([19097](https://github.com/pytorch/pytorch/pull/19097))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> torch.tensor([True, False])\r\ntensor([1, 0], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> torch.tensor([True, False])\r\ntensor([ True, False])\r\n```\r\n\r\n\r\n\r\n### `nn.BatchNorm{1,2,3}D`: gamma (`weight`) is now initialized to all 1s rather than randomly initialized from *U(0, 1)*.  ([13774](https://github.com/pytorch/pytorch/pull/13774))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> torch.nn.BatchNorm2d(5).weight\r\nParameter containing:\r\ntensor([0.1635, 0.7512, 0.4130, 0.6875, 0.5496], \r\n       requires_grad=True)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> torch.nn.BatchNorm2d(5).weight\r\nParameter containing:\r\ntensor([1., 1., 1., 1., 1.], requires_grad=True)\r\n```\r\n\r\n\r\n\r\n### A number of deprecated Linear Algebra operators have been removed ([22841](https://github.com/pytorch/pytorch/pull/22841))\r\n\r\n| Removed        | Use Instead  | \r\n| ------------- | ------------- |\r\n| `btrifact`    | `lu` |\r\n| `btrifact_with_info`      | `lu` with `get_infos=True`      |\r\n| `btrisolve` | `lu_solve`     |\r\n| `btriunpack` | `lu_unpack`    |\r\n| `gesv` | `solve`     |\r\n| `pstrf` | `cholesky`     |\r\n| `potrf` | `cholesky`     |\r\n| `potri` | `cholesky_inverse`     |\r\n| `potrs` | `cholesky_solve`     |\r\n| `trtrs` | `triangular_solve`     |\r\n\r\n\r\n### Sparse Tensors: Changing the sparsity of a Tensor through `.data` is no longer supported.  ([17072](https://github.com/pytorch/pytorch/pull/17072))\r\n\r\n```\r\n>>> x = torch.randn(2,3)\r\n>>> x.data = torch.sparse_coo_tensor((2, 3))\r\nRuntimeError: Attempted to call `variable.set_data(tensor)`,\r\nbut `variable` and  `tensor` have incompatible tensor type.\r\n```\r\n\r\n\r\n\r\n### Sparse Tensors: in-place shape modifications of Dense Tensor Constructor Arguments will no longer modify the Sparse Tensor itself ([20614](https://github.com/pytorch/pytorch/pull/20614))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> i = torch.tensor([[0, 1]])\r\n>>> v = torch.ones(2)\r\n>>> s = torch.sparse_coo_tensor(i, v)\r\n>>> i.resize_(1, 1)\r\n>>> v.resize_(1)\r\n\r\n>>> s.coalesce().indices().shape\r\ntorch.Size([1, 1])\r\n\r\n>>> s.coalesce().values().shape\r\ntorch.Size([1])\r\n```\r\n\r\nNotice `indices()` and `values()` reflect the resized tensor shapes.\r\n\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> i = torch.tensor([[0, 1]])\r\n>>> v = torch.ones(2)\r\n>>> s = torch.sparse_coo_tensor(i, v)\r\n>>> i.resize_(1, 1)\r\n>>> v.resize_(1)\r\n\r\n>>> s.coalesce().indices().shape\r\ntorch.Size([1, 2])\r\n\r\n>>> s.coalesce().values().shape\r\ntorch.Size([2])\r\n```\r\n\r\nNotice `indices()` and `values()` reflect the original tensor shapes.\r\n\r\n### Sparse Tensors: Accumulating dense gradients into a sparse `.grad` will no longer retain Python object identity.  ([17072](https://github.com/pytorch/pytorch/pull/17072))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> m = torch.nn.Embedding(10, 3, sparse=True)\r\n>>> m(torch.tensor([[1,2,4,5],[4,3,2,9]])).sum().backward()\r\n>>> assert m.weight.grad.layout == torch.sparse_coo\r\n>>> m_weight_grad_saved = m.weight.grad\r\n\r\n# accumulate dense gradient into sparse .grad, change sparsity\r\n>>> m.weight.sum().backward()\r\n>>> assert m.weight.grad.layout == torch.strided\r\n# m_weight_grad_saved still refers to the .grad of m's weight\r\n# even though the sparsity has changed\r\n>>> assert id(m_weight_grad_saved) == id (m.weight.grad)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> m = torch.nn.Embedding(10, 3, sparse=True)\r\n>>> m(torch.tensor([[1,2,4,5],[4,3,2,9]])).sum().backward()\r\n>>> assert m.weight.grad.layout == torch.sparse_coo\r\n>>> m_weight_grad_saved = m.weight.grad\r\n\r\n# accumulate dense gradient into sparse .grad, change sparsity\r\n>>> m.weight.sum().backward()\r\n>>> assert m.weight.grad.layout == torch.strided\r\n# m_weight_grad_saved NO LONGER refers to the .grad of m's weight\r\n>>> assert id(m_weight_grad_saved) == id (m.weight.grad)\r\nAssertionError\r\n```\r\n\r\n\r\n\r\n### `nn.utils.convert_sync_batchnorm` has been replaced with `nn.SyncBatchNorm.convert_sync_batchnorm `([18787)](https://github.com/pytorch/pytorch/pull/18787)\r\n\r\nExample of new usage:\r\n\r\n```\r\n>>> # Network with nn.BatchNorm layer\r\n>>> module = torch.nn.Sequential(\r\n>>>     torch.nn.Linear(20, 100),\r\n>>>     torch.nn.BatchNorm1d(100)\r\n>>> ).cuda()\r\n>>> # creating process group (optional)\r\n>>> process_group = torch.distributed.new_group(process_ids)\r\n>>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\r\n```\r\n\r\n### Error Checking: `torch.addcmul` and `torch.lerp` operators enforce stronger shape requirements on the output tensor (`out=` keyword argument) and do not allow output tensor to be resized if it is also used as one of the inputs.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> x=torch.zeros(1)\r\n>>> torch.addcmul(x, x, torch.zeros(2,3), out=x)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> x=torch.zeros(1)\r\n>>> torch.addcmul(x, x, torch.zeros(2,3), out=x)\r\nRuntimeError: output with shape [1] doesn't match the broadcast shape [2, 3]\r\n```\r\n\r\nIf you run into this error, please ensure the `out` parameter is of the correct output shape (post-broadcasting).\r\n\r\n### Error Checking: Improved Variable version tracking ([20391](https://github.com/pytorch/pytorch/pull/20391), [22821](https://github.com/pytorch/pytorch/pull/22821), [21865](https://github.com/pytorch/pytorch/pull/21865))\r\n\r\nPyTorch\u2019s autograd system uses a version tracking mechanism to ensure that Tensors that are saved for backwards computations retain their correct values when the backward pass is computed (i.e. that they haven\u2019t been updated in-place since they were saved).  See [In Place Correctness Checks](https://pytorch.org/docs/stable/notes/autograd.html#in-place-correctness-checks) in the docs for more information.\r\n\r\nIn PyTorch 1.2 we have enhanced the version tracking in a number of cases, which may flag issues that were not caught previously.  There is now additional tracking through the `Variable()` constructor, the `nn.Parameter()` constructor, after setting `.data`, and via `nn.Module._apply` (internal API).\r\n\r\n*Track changes through Variable constructor:*\r\n\r\n```\r\n>>> x = torch.ones(1, requires_grad=True)+1\r\n>>> y = x*x\r\n\r\n# do an in-place update through Variable constructor\r\n>>> torch.autograd.Variable(x).add_(1)\r\n>>> y.backward()\r\nRuntimeError: one of the variables needed for gradient computation has been modified\r\nby an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 \r\ninstead.\r\n```\r\n\r\n*Track changes on an nn.Parameter:*\r\n\r\n```\r\n>>> x = torch.ones(1)\r\n>>> p = torch.nn.Parameter(x)\r\n>>> y = p * p\r\n\r\n# do an in-place update on a saved Parameter\r\n>>> x.add_(1)\r\n>>> y.sum().backward()\r\nRuntimeError: one of the variables needed for gradient computation has been modified\r\nby an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 \r\ninstead.\r\n```\r\n\r\n*Track changes after setting `.data`:*\r\n\r\n```\r\n>>> x = torch.zeros(1, requires_grad=True)+1\r\n>>> y = x * x\r\n>>> x.data = torch.zeros(1, requires_grad=True)+1\r\n\r\n>>> x.add_(1)\r\n>>> y.backward()\r\nRuntimeError: one of the variables needed for gradient computation has been modified\r\nby an inplace operation: [torch.FloatTensor [1]], which is output 0 of AddBackward0,\r\nis at version 1; expected version 0 instead.\r\n```\r\n\r\n### [JIT] Python called from scripted modules must be `@ignore`d\r\n\r\n`torch.jit.script` now recursively compiles everything it finds in the original function, so if you had Python functions called from in your scripted function or module, you must now explicitly `@ignore` it. See [the new API guide](https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api) for more details.\r\n\r\n*Version 1.1*\r\n\r\n```\r\ndef my_unscriptable_python_fn():\r\n    # weird stuff\r\n\r\n@torch.jit.script\r\ndef fn():\r\n    # This gets inserted as a Python call, and only errors on `save()`.\r\n    my_unscriptable_python_fn()\r\n```\r\n\r\n*Version 1.2*\r\n\r\n```\r\n@torch.jit.ignore  # this needs to be added ...\r\ndef my_unscriptable_python_fn():\r\n    ...\r\n\r\n@torch.jit.script\r\ndef fn():\r\n    # ... or else recursive compilation will attempt to compile this call\r\n    my_unscriptable_python_fn()\r\n```\r\n\r\nNOTE: This is also a change to behavior of the `@torch.jit.ignore` decorator. In version 1.1, `@ignore` tells the compiler to omit compiling a function entirely, to mark Python functions that you know will not be called after export. In version 1.2 `@ignore`, tells the compiler to insert a call back to the Python interpreter instead of trying to compile the function.\r\n\r\nTo get the old behavior, use `@torch.jit.ignore(drop_on_export=True)` (`@torch.jit.ignore` with no arguments is equivalent to `@torch.jit.ignore(drop_on_export=False`)).\r\n\r\n### [JIT] `optimize` for ScriptModules is now a context manager\r\n\r\nWhether optimization passes are run is now a thread-local flag. This better reflects how optimization actually happens in the JIT (i.e. it is decided at runtime, not compilation time). \r\n\r\n*Version 1.1*\r\n\r\n```\r\n@torch.jit.script(optimize=False)\r\ndef fn(inputs):\r\n    ...\r\n\r\nfn(inputs)\r\n```\r\n\r\n*Version 1.2*\r\n\r\n```\r\n@torch.jit.script\r\ndef fn(inputs):\r\n    ...\r\n\r\nwith @torch.jit.optimized_execution(False):\r\n    fn(inputs)\r\n```\r\n\r\n### [jit] `script::Module` is now a reference type\r\n\r\nTo better align with the [PyTorch C++ API philosophy](https://github.com/pytorch/pytorch/wiki/Writing-Python-in-cpp-(a-manifesto)), `script::Module` and `script::Method` are now reference types. Our APIs have been updated to use `script::Module` instead of `std::shared_ptr<script::Module>`.\r\n\r\n*Version 1.1*\r\n\r\n```\r\nusing torch::jit::script::Module;\r\n\r\nstd::shared_ptr<Module> m = torch::jit::load(\"my_model.py\");\r\nm->forward(...);\r\n```\r\n\r\n*Version 1.2*\r\n\r\n```\r\nusing torch::jit::script::Module;\r\n\r\nModule m = torch::jit::load(\"my_model.py\");\r\nm.forward(...);\r\n```\r\n\r\n### [C++ only] mean() / sum() / prod() APIs have changed slightly ([21088](https://github.com/pytorch/pytorch/pull/21088))\r\n\r\n*Version 1.1 API*:\r\n\r\n```\r\nTensor sum(IntArrayRef dim, bool keepdim=false) const;    \r\nTensor sum(IntArrayRef dim, ScalarType dtype) const;\r\n```\r\n\r\n*Version 1.2 API*:\r\n\r\n```\r\nTensor sum(IntArrayRef dim, bool keepdim=false,\r\n           c10::optional<ScalarType> dtype=c10::nullopt) const;\r\n```\r\n\r\nthat is, to override `dtype`, `keepdim` must now be provided.\r\n\r\n### Binary distribution and nightly changes\r\n\r\nWe have streamlined our conda and wheel binary distributions, so that it is easier than ever to install the version of PyTorch appropriate for your needs. The install instructions on https://pytorch.org/ have been updated, but if you have tooling to download and install PyTorch, here is a detailed description of the changes we made:\r\n\r\n**Wheels now have local version identifiers.** Wheels that are for non-default CUDA configurations (the default CUDA version for this release is 10.0) now have local version identifiers like +cpu and +cu92. This means that, when installing, it is no longer necessary to specify a full wheel URL\u2014just specify an appropriate version constraint like `torch==1.2.0+cu92`.\r\n\r\n*Version 1.1 (for Python 3.7 on Linux only):*\r\n\r\n```\r\npip install numpy\r\npip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-linux_x86_64.whl\r\n```\r\n\r\n*Version 1.2 (works for all versions of Python, and both Linux and Mac):*\r\n\r\n```\r\npip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\r\n```\r\n\r\n**CPU-only binaries on conda can be selected with the cpuonly feature.** We\u2019ve eliminated the pytorch-cpu conda package; instead, the cpu-only conda package can be enabled by installing the cpuonly metapackage. Similarly, there is no longer both a torchvision and torchvision-cpu package; the feature will ensure that the CPU version of torchvision is selected.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\nconda install -c pytorch pytorch-cpu\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\nconda install -c pytorch pytorch cpuonly\r\n```\r\n\r\n**Conda nightlies now live in the pytorch-nightly channel and no longer have \u201c-nightly\u201d in their name.** We have added a new dedicated channel for nightlies called pytorch-nightly; all nightlies (pytorch, torchvision, torchaudio, etc.) will now be uploaded to this channel, but with the same name as their corresponding stable versions (unlike before, when we had a separate pytorch-nightly, torchvision-nightly, etc. packages.) This makes it more difficult to accidentally install a copy of the nightly and stable at the same time.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\nconda install -c pytorch pytorch-nightly\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\nconda install -c pytorch-nightly pytorch\r\n```\r\n\r\n**Wheel nightlies no longer have -nightly in their name.** Similar to the changes we made in Conda, we no longer suffix wheel nightlies with \u201c-nightly\u201d, to make it harder to accidentally install a copy of nightly and stable at the same time.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\npip install --pre torch_nightly -f https://download.pytorch.org/whl/nightly/torch_nightly.html\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\npip install --pre torch -f https://download.pytorch.org/whl/nightly/torch_nightly.html\r\n```\r\n\r\n## New Features\r\n\r\n### Tensor Type Support\r\n\r\n* `torch.bool`: added support for many operators (masking, comparison, arithmetic operators) to achieve feature parity with `torch.uint8`.  See the **Breaking Changes** section for details about how this could affect existing programs. ([21032](https://github.com/pytorch/pytorch/pull/21032), etc.)\r\n* `torch.sparse.HalfTensor`: Added support for `torch.float16` sparse Tensors on both CPU and CUDA.  ([19695](https://github.com/pytorch/pytorch/pull/19695))\r\n* `torch.bfloat16`: Added basic creation and serialization support for [Brain Floating Point Tensors](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format). ([21522](https://github.com/pytorch/pytorch/pull/21522), [21523](https://github.com/pytorch/pytorch/pull/21523), [21860](https://github.com/pytorch/pytorch/pull/21860), [22852](https://github.com/pytorch/pytorch/pull/22852))\r\n\r\n### NN Package\r\n\r\n* `nn.Transformer`: added implementation of Transformer from [Attention is All You Need](https://arxiv.org/abs/1706.03762). ([20170](https://github.com/pytorch/pytorch/pull/20170), [22588](https://github.com/pytorch/pytorch/pull/22588))\r\n* `nn.Embedding`: support `float16` embeddings on CUDA.  ([19695](https://github.com/pytorch/pytorch/pull/19695))\r\n* `nn.Flatten`: added a Module that performs `torch.flatten`. ([22245](https://github.com/pytorch/pytorch/pull/22245/))\r\n* `nn.functional.gelu`: Added support for [Gaussian Error Linear Units](https://arxiv.org/abs/1606.08415). ([20665](https://github.com/pytorch/pytorch/pull/20665), [21237](https://github.com/pytorch/pytorch/pull/21237))\r\n* `nn.Module hooks`: add ability to replace input/output via `forward_pre_hook` and `forward_hook`. ([22285](https://github.com/pytorch/pytorch/pull/22285))\r\n* `nn.Module`: add `requires_grad_() `method for turning on/off `requires_grad` for Module parameters. ([22576](https://github.com/pytorch/pytorch/pull/22576))\r\n\r\n### Operators\r\n\r\n* `Tensor.to_sparse`: now supports autograd. ([20458](https://github.com/pytorch/pytorch/pull/20458))\r\n* `Tensor.fill_diagonal_`: operator to fill the main diagonal of a Tensor. ([21892](https://github.com/pytorch/pytorch/pull/21892))\r\n* `torch.qr`: supports autograd. ([21274](https://github.com/pytorch/pytorch/pull/21274))\r\n* `torch.bitwise_not`: add operator for boolean/integer types.  Also have python `~` operator use this. ([22283](https://github.com/pytorch/pytorch/pull/22283), [22320](https://github.com/pytorch/pytorch/pull/22320))\r\n* `torch.trapz`: integrate using the trapezoid rule; equivalent to [numpy.trapz](https://docs.scipy.org/doc/numpy/reference/generated/numpy.trapz.html). ([21610](https://github.com/pytorch/pytorch/pull/21610))\r\n* `torch.var_mean` / `torch.std_mean`: compute variance and mean at the same time.([18731](https://github.com/pytorch/pytorch/pull/18731))\r\n* `torch.utils.ThroughputBenchmark`: benchmark utility for measuring the throughput of PyTorch operators. ([20766](https://github.com/pytorch/pytorch/pull/20766)).\r\n* `Logging`: lightweight at-most-once logging to record operators that are used (`c10::Logging`). ([20745](https://github.com/pytorch/pytorch/pull/20745))\r\n\r\n### Optim Package\r\n\r\n* `optim.AdamW`: introduce AdamW optimizer from [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101). ([21250](https://github.com/pytorch/pytorch/pull/21250))\r\n* `optim.LBFGS`: added support for strong Wolfe line search. ([8824](https://github.com/pytorch/pytorch/pull/8824))\r\n\r\n### Distributed Package\r\n\r\n* `DistributedDataParallel`: support CPU modules.  ([20236](https://github.com/pytorch/pytorch/pull/20236))\r\n* `DistributedDataParallel`: support sparse tensors. ([19146](https://github.com/pytorch/pytorch/pull/19146))\r\n* `DistributedDataParallel`: support local gradient accumulation. ([21736](https://github.com/pytorch/pytorch/pull/21736))\r\n\r\n### IterableDataset\r\n\r\n* `IterableDataset`: introduces a new type of Dataset designed for data read from a stream. ([19228](https://github.com/pytorch/pytorch/pull/19228))\r\n\r\n### Tensorboard Package\r\n\r\n* TensorBoard support in PyTorch has improved and is no longer experimental!\r\n* `SummaryWriter.flush`: now supported. ([20607](https://github.com/pytorch/pytorch/pull/20607))\r\n* `SummaryWriter.add_mesh`: add support for 3D point clouds. ([20413](https://github.com/pytorch/pytorch/pull/20413))\r\n\r\n### JIT Features\r\n\r\n* Improved support for iterator infrastructure. TorchScript now supports looping through a `List`, `Tuple`, `Dict`, `Tensor`, `String` and you can also use `zip()`, `enumerate()`, and `for...in`. ([21801](https://github.com/pytorch/pytorch/pull/21801), [22006](https://github.com/pytorch/pytorch/pull/22006), [21990](https://github.com/pytorch/pytorch/pull/21990), [21985](https://github.com/pytorch/pytorch/pull/21985))\r\n* Support `in` membership checks. ([21527](https://github.com/pytorch/pytorch/pull/21527))\r\n* Improved support for strings and the string libraries. ([20826](https://github.com/pytorch/pytorch/pull/20826), [20188](https://github.com/pytorch/pytorch/pull/20188), [20761](https://github.com/pytorch/pytorch/pull/20761), [21656](https://github.com/pytorch/pytorch/pull/21656), [20617](https://github.com/pytorch/pytorch/pull/20617))\r\n* Improved `math` support. ([20979](https://github.com/pytorch/pytorch/pull/20979), [19707](https://github.com/pytorch/pytorch/pull/19707), [21151](https://github.com/pytorch/pytorch/pull/21151), [21131](https://github.com/pytorch/pytorch/pull/21131), [21129](https://github.com/pytorch/pytorch/pull/21129), [21130](https://github.com/pytorch/pytorch/pull/21130), [21512](https://github.com/pytorch/pytorch/pull/21512), [21126](https://github.com/pytorch/pytorch/pull/21126), [21127](https://github.com/pytorch/pytorch/pull/21127), [21128](https://github.com/pytorch/pytorch/pull/21128))\r\n* Support for various other Python builtin functions. ([21451](https://github.com/pytorch/pytorch/pull/21451))\r\n* Support for `NamedTuple`. ([21428](https://github.com/pytorch/pytorch/pull/21428))\r\n* All the rest of the `dict` methods. ([21979](https://github.com/pytorch/pytorch/pull/21979))\r\n* `sorted()` keyword for lists and dicts. ([23274](https://github.com/pytorch/pytorch/pull/23274))\r\n* Add support for breaks and continues. ([21692](https://github.com/pytorch/pytorch/pull/21692))\r\n* Improved custom operator API with several bugfixes and new features. It now allows more primitive types, supports `torch::List`, `torch::Dict` and `torch::Optional`, supports dispatch (i.e. registering a different function for CPU and CUDA for the same operator).\r\n* Support `nn.GRU` in script. ([23266](https://github.com/pytorch/pytorch/pull/23266))\r\n* Support `pack_padded_sequence` and `pad_packed_sequence`. ([23249](https://github.com/pytorch/pytorch/pull/23249))\r\n* Support `torch._C._get_tracing_state` in TorchScript. ([23248](https://github.com/pytorch/pytorch/pull/23248))\r\n* Support `torch.as_tensor` in TorchScript. ([23247](https://github.com/pytorch/pytorch/pull/23247))\r\n* add support for recursive compilation on `Modules`. ([20708](https://github.com/pytorch/pytorch/pull/20708))\r\n* add `all` builtin. ([20521](https://github.com/pytorch/pytorch/pull/20521))\r\n* Add `Final[T]` annotated members to `__constants__`. ([21603](https://github.com/pytorch/pytorch/pull/21603))\r\n* Add `save()` to scripted `Function`s. ([20386](https://github.com/pytorch/pytorch/pull/20386))\r\n* Support for serializing class attributes. ([22953](https://github.com/pytorch/pytorch/pull/22953))\r\n* Support for class annotations. ([21379](https://github.com/pytorch/pytorch/pull/21379))\r\n* support Python 3.8 `Constant` node. ([22007](https://github.com/pytorch/pytorch/pull/22007))\r\n* Support for type annotations instead of `torch.jit.annotate()`. ([21390](https://github.com/pytorch/pytorch/pull/21390))\r\n* Support operator overloading for user-defined classes. ([20033](https://github.com/pytorch/pytorch/pull/20033))\r\n* Support recursive `ModuleList` / `Sequential`. ([21306](https://github.com/pytorch/pytorch/pull/21306))\r\n* Trace multiple methods in a single `Module`. ([19905](https://github.com/pytorch/pytorch/pull/19905))\r\n\r\n## Improvements\r\n\r\n* `Tensor.pin_memory()`: only ask for context on current device. ([22229](https://github.com/pytorch/pytorch/pull/22229))\r\n* `Tensor.view()`: suggest using `reshape()` instead of `contiguous()` when the input is non-contiguous. ([20968](https://github.com/pytorch/pytorch/pull/20968))\r\n* `Tensor.numpy()`: throw `TypeError` instead of `ValueError` if the type isn\u2019t supported. ([21608](https://github.com/pytorch/pytorch/pull/21608))\r\n* `torch.norm`: add support for `p=\"nuc\"` with `dim` specified. ([21022](https://github.com/pytorch/pytorch/pull/21022))\r\n* `torch.qr`: support batching of input matrices. ([20689](https://github.com/pytorch/pytorch/pull/20689))\r\n* `torch.qr`: support `some` parameter akin to NumPy's `mode` option. ([20689](https://github.com/pytorch/pytorch/pull/20689))\r\n* `torch.det` / `torch.logdet` / `torch.slogdet`: added batching support. ([22909](https://github.com/pytorch/pytorch/pull/22909))\r\n* `torch.cdist`: support batching. ([20934](https://github.com/pytorch/pytorch/pull/20934))\r\n* `torch.symeig`: support batching. ([21858](https://github.com/pytorch/pytorch/pull/21858))\r\n* `torch._dirichlet_grad`: support CUDA. ([21191](https://github.com/pytorch/pytorch/pull/21191))\r\n* `torch.randperm`: support `torch.float16`. ([22102](https://github.com/pytorch/pytorch/pull/22102))\r\n* `torch.Size` is now pickle-able in Python2. ([20952](https://github.com/pytorch/pytorch/pull/20952))\r\n* `torch.tensor` / `torch.as_tensor`: infer device if input supports Numba\u2019s [`__cuda_array_interface__`](https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html). ([20584](https://github.com/pytorch/pytorch/pull/20584))\r\n* `torch.isinf` / `torch.isfinite`: throw `TypeError` instead of `ValueError` when a non-tensor is passed in. ([20817](https://github.com/pytorch/pytorch/pull/20817))\r\n* `nn.MultiheadedAttention`: add functional support. ([20415](https://github.com/pytorch/pytorch/pull/20415))\r\n* `nn.MultiheadedAttention`: added support for key/value to have different number of features. ([21288](https://github.com/pytorch/pytorch/pull/21288))\r\n* `nn.MultiheadAttention`: allow static key/values. ([21288](https://github.com/pytorch/pytorch/pull/21288))\r\n* `nn.Conv{1,2,3}D`: support `torch.int64` dtype in forward. ([20730](https://github.com/pytorch/pytorch/pull/20730), [22594](https://github.com/pytorch/pytorch/pull/22594))\r\n* `nn.AvgPool{1,2,3}D`: support `torch.int64` dtype in forward. ([22433](https://github.com/pytorch/pytorch/pull/22433))\r\n* `nn.Module`: make `_save_to_state_dict` overrideable. ([21933](https://github.com/pytorch/pytorch/pull/21933))\r\n* `autograd`: Checkpointing of modules inside large fanout networks no longer hits a recursion error. ([22397](https://github.com/pytorch/pytorch/pull/22397))\r\n* `autograd`: Track in-pace changes of Tensors through `Module._apply` (internal API). ([21865](https://github.com/pytorch/pytorch/pull/21865))\r\n* `autograd.profiler`: Add shape aggregation support.  [20035](https://github.com/pytorch/pytorch/pull/20035))\r\n* `autograd.profiler`: Profile custom c10 ops. ([20175](https://github.com/pytorch/pytorch/pull/20175))\r\n* `DataLoader`: support setting `batch_size=0` to disable automatic batching (collation) in `DataLoader` for easier bulk loading.  ([19228](https://github.com/pytorch/pytorch/pull/19228))\r\n* `DataLoader`: add `multiprocessing_context` parameter. ([22990](https://github.com/pytorch/pytorch/pull/22990))\r\n* `DataLoader`: added error detection for `worker_init_fn`. ([20150](https://github.com/pytorch/pytorch/pull/20150))\r\n* `DataLoader`: Retry on `EINTR`. ([21723](https://github.com/pytorch/pytorch/pull/21723))\r\n* `torch.cuda.set_rng_state` / `torch.cuda.get_rng_state`: accept string as `device` parameter. ([23448](https://github.com/pytorch/pytorch/pull/23448))\r\n* `CUDA`: add warning when using Turing GPUs and CUDA <= 9000. ([21468](https://github.com/pytorch/pytorch/pull/21468))\r\n* `CUDA`: warn on conditions that can trigger a cuBLAS 9.0 bug.  ([22034](https://github.com/pytorch/pytorch/pull/22034))\r\n* `CPU`: Improve CPUAllocator OOM message. ([20618](https://github.com/pytorch/pytorch/pull/20618))\r\n* `[memory_format]`: added support for `torch.empty`, `torch.empty_like`, `Tensor.contiguous()`, `Tensor.is_contiguous()` to specify / check the order in which dimensions are laid out in memory. ([20455](https://github.com/pytorch/pytorch/pull/20455), [20558](https://github.com/pytorch/pytorch/pull/20558))\r\n* `distributions.MultivariateNormal`: fix precision matrix instability. ([21366](https://github.com/pytorch/pytorch/pull/21366))\r\n* `distributions.transforms.SigmoidTransform`: fix numerical instability. ([19802](https://github.com/pytorch/pytorch/pull/19802))\r\n\r\n### Distributed Improvements\r\n\r\n* `DistributedDataParallel`: Support DDP forward/backward calls even if no module parameter is used. ([19821](https://github.com/pytorch/pytorch/pull/19821)) \r\n* `DistributedDataParallel`: Only call into reducer if grad is enabled. ([19897](https://github.com/pytorch/pytorch/pull/19897))\r\n* `DistributedDataParallel`:  Require finalize DDP backward only when there are indeed gradients computed, this allows application to completely discard DDP outputs and move on to the next iteration. ([19901](https://github.com/pytorch/pytorch/pull/19901))\r\n* `DistributedDataParallel`: Improve DDP backward reduction error messages. ([20586](https://github.com/pytorch/pytorch/pull/20586))\r\n* `DistributedDataParallel`:  make DDP failure recoverable. ([21591](https://github.com/pytorch/pytorch/pull/21591))\r\n* `DistributedDataParallel`:  Delay reduction of unused parameters until first autograd hook is called. ([22219](https://github.com/pytorch/pytorch/pull/22219))\r\n* `c10d:` support tensors shared across processes. ([21449](https://github.com/pytorch/pytorch/pull/21449))\r\n* `c10d:` `ProcessGroupMPI` Add device guard around MPI operations. ([22446](https://github.com/pytorch/pytorch/pull/22446))\r\n* `utils.data.distributed.DistributedSampler`: Make shuffling optional. ([22479](https://github.com/pytorch/pytorch/pull/22479))\r\n\r\n### Tensorboard Improvements\r\n\r\n* Usage of kwarg-only arguments has been removed. ([21786](https://github.com/pytorch/pytorch/pull/21786))  \r\n\r\n### Numpy Compatibility Improvements\r\n\r\n* `Tensor.T:` added numpy-like support for reversing dimensions. ([20598](https://github.com/pytorch/pytorch/pull/20598))\r\n* `Tensor.ndim`: NumPy equivalent property for the number of dimensions. ([20565](https://github.com/pytorch/pytorch/pull/20565))\r\n* `Tensor.nonzero`: added `as_tuple` argument (default `False`) that when `True`, will return a tuple of Tensors, which matches the behavior of [numpy.nonzero](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html). ([20293](https://github.com/pytorch/pytorch/pull/20293))\r\n* `torch.dtype`: support passing in NumPy dtypes as arguments. ([21215](https://github.com/pytorch/pytorch/pull/21215))\r\n* `torch.normal`: add `size` parameter when called with two floats. ([20545](https://github.com/pytorch/pytorch/pull/20545))\r\n* `torch.where`: add one-argument overload that is an alias for Numpy-like `nonzero`. ([21986](https://github.com/pytorch/pytorch/pull/21986))\r\n* support a number of argument name overrides, e.g. `axis` instead of `dim`. ([20451](https://github.com/pytorch/pytorch/pull/20451))\r\n\r\n### JIT Improvements\r\n\r\n* The original source code debug information is now saved with the model. If a model is saved and then loaded into another process, the loaded process can now print out error messages that point to the original source code. ([22177](https://github.com/pytorch/pytorch/pull/22177), [22178](https://github.com/pytorch/pytorch/pull/22178), [22179](https://github.com/pytorch/pytorch/pull/22179), [22180](https://github.com/pytorch/pytorch/pull/22180))\r\n* Error message source range highlighting now includes filename, line number, and column number. ([21157](https://github.com/pytorch/pytorch/pull/21157))\r\n* Better Constant Propagation through Tuples. ([22561](https://github.com/pytorch/pytorch/pull/22561))\r\n* Add `start` and `step` parameters for `range` in TorchScript. ([20795](https://github.com/pytorch/pytorch/pull/20795))\r\n* Support for threading options for TorchScript inference ([doc](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html))\r\n* Add `max_pool2d` to symbolic derivatives. ([19661](https://github.com/pytorch/pytorch/pull/19661))\r\n* Optimize `matmul` memory usage for certain cases. ([23433](https://github.com/pytorch/pytorch/pull/23433))\r\n* Avoid kernel launches for zero-sized tensor inputs. ([22790](https://github.com/pytorch/pytorch/pull/22790))\r\n* Add support for steps (strides) in tensor slices. ([20929](https://github.com/pytorch/pytorch/pull/20929))\r\n* Added error for classes that don't have an `__init__` function. ([21880](https://github.com/pytorch/pytorch/pull/21880))\r\n* Allow classes to be used in their own methods. ([20106](https://github.com/pytorch/pytorch/pull/20106))\r\n* Better error message when a variable is conditionally defined. ([20911](https://github.com/pytorch/pytorch/pull/20911))\r\n* Consider contained types in alias analysis. ([21431](https://github.com/pytorch/pytorch/pull/21431))\r\n* Convenience APIs for script objects. ([20226](https://github.com/pytorch/pytorch/pull/20226))\r\n* Don't print backtrace for interpreter errors. ([20925](https://github.com/pytorch/pytorch/pull/20925))\r\n* Improve error msg for missing attribute. ([20779](https://github.com/pytorch/pytorch/pull/20779))\r\n* Improve error msg on inferred type. ([21058](https://github.com/pytorch/pytorch/pull/21058))\r\n* Improve error msg on recursive class defs. ([21842](https://github.com/pytorch/pytorch/pull/21842))\r\n* Include module names in recursive error stacks. ([22921](https://github.com/pytorch/pytorch/pull/22921))\r\n* Improve recursive scripting error message. ([21841](https://github.com/pytorch/pytorch/pull/21841))\r\n* Index into a tuple with non constant integer. ([20081](https://github.com/pytorch/pytorch/pull/20081))\r\n* Let `ScriptModule` buffer attributes can also cast device/type. ([19700](https://github.com/pytorch/pytorch/pull/19700))\r\n* Lower batchmm to non-diff optimization. ([19987](https://github.com/pytorch/pytorch/pull/19987))\r\n* Make `ScriptModule.training` an attribute instead of a parameter. ([21078](https://github.com/pytorch/pytorch/pull/21078))\r\n* Make `strtod_c` compatible with different gcc abi. ([21293](https://github.com/pytorch/pytorch/pull/21293))\r\n* make magic methods work with casts too. ([20654](https://github.com/pytorch/pytorch/pull/20654))\r\n* Improve performance of alias analysis. ([20899](https://github.com/pytorch/pytorch/pull/20899))\r\n* Print a warning if a type annotation prefix is invalid according to mypy. ([20884](https://github.com/pytorch/pytorch/pull/20884))\r\n* schema_matching.cpp: improve error messages. ([21141](https://github.com/pytorch/pytorch/pull/21141))\r\n* Resolve with closed over variables instead of stack frame. ([22270](https://github.com/pytorch/pytorch/pull/22270))\r\n* Report errors through call stack. ([22280](https://github.com/pytorch/pytorch/pull/22280))\r\n* Reduce number of stack manipulation instructions in interpreter. ([21240](https://github.com/pytorch/pytorch/pull/21240))\r\n\r\n### C++ API Improvements\r\n\r\n* `nn::PoissonNLLLoss`: Added support. ([19316](https://github.com/pytorch/pytorch/pull/19316))\r\n* `nn::Module`: added `replace_module` API to overwrite submodules in C++ Frontend. ([22546](https://github.com/pytorch/pytorch/pull/22546))\r\n* `nn:Module::register_module` / `register_parameter` / `register_buffer`: make public ([23196](https://github.com/pytorch/pytorch/pull/23196))\r\n* `data::datasets::ChunkDataReader`: fix include headers and a vector issue. ([19485](https://github.com/pytorch/pytorch/pull/19485))\r\n* `data::datasets::ChunkDataset`: add new `get_batch` method. ([21797](https://github.com/pytorch/pytorch/pull/21797))\r\n* `data::datasets::ChunkDataset`: add checkpoint support. ([21889](https://github.com/pytorch/pytorch/pull/21889))\r\n* `data::datasets::ChunkDataset`: add support for cross-chunk shuffling. ([22347](https://github.com/pytorch/pytorch/pull/22347))\r\n* `data::datasets::ChunkDataset`: add sorting policy. ([23053](https://github.com/pytorch/pytorch/pull/23053))\r\n\r\n### MKLDNN Tensor Improvements\r\n\r\nAdd support for a number of operators on MKLDNN Tensors including:\r\n* `Tensor.is_mkldnn`: ([22386](https://github.com/pytorch/pytorch/pull/22386))\r\n* `Tensor.transpose()`: ([21943](https://github.com/pytorch/pytorch/pull/21943))\r\n* `Tensor.zero_()`: ([20573](https://github.com/pytorch/pytorch/pull/20573))\r\n* `torch.empty`: ([21184](https://github.com/pytorch/pytorch/pull/21184))\r\n* `torch.mul`: ([20575](https://github.com/pytorch/pytorch/pull/20575))\r\n* `nn.AdaptiveAvgPool{1,2,3}D`: ([19818](https://github.com/pytorch/pytorch/pull/19818))\r\n* `nn.Sigmoid`: ([20820](https://github.com/pytorch/pytorch/pull/20820))\r\n* `nn.Softmax`: ([21516](https://github.com/pytorch/pytorch/pull/21516))\r\n* `nn.Module`: support saving/loading MKLDNN modules. ([20799](https://github.com/pytorch/pytorch/pull/20799))\r\n* `nn.MaxPool{1,2,3}D`: support `ceil_mode`. ([21310](https://github.com/pytorch/pytorch/pull/21310))\r\n\r\n## Bug Fixes\r\n\r\n* Indexing: fix advanced indexing where there are more than (2^31)-1 bytes in the output. ([20919](https://github.com/pytorch/pytorch/pull/20919))\r\n* Indexing: fix indexing when there are more than 65535 elements in a non-indexing first dimension on CUDA. ([23123](https://github.com/pytorch/pytorch/pull/23123))\r\n* Indexing: fix issue with slicing empty tensors. ([20914](https://github.com/pytorch/pytorch/pull/20914))\r\n* `Tensor.index_copy_:` fix segfault by properly checking dimension is in range. ([21617](https://github.com/pytorch/pytorch/pull/21617))\r\n* `Tensor.copy_`: Fix a bug where non-blocking was not being respected. ([20305](https://github.com/pytorch/pytorch/pull/20305))\r\n* `Tensor.clone`: Fix an issue with MKLDNN tensors. ([20943](https://github.com/pytorch/pytorch/pull/20943))\r\n* Tensor subclassing: give a proper error instead of crashing. ([20283](https://github.com/pytorch/pytorch/pull/20283))\r\n* `torch.cat`:  Fix segfault with tensors that can't be indexed with 32-bit ints. ([21530](https://github.com/pytorch/pytorch/pull/21530))\r\n* `torch.range` / `torch.linspace` / `torch.logspace`: properly respect the current `Stream`. ([21619](https://github.com/pytorch/pytorch/pull/21619))\r\n* `torch.lu`: return the identity permutation instead of zeros when not using pivoting. ([22242](https://github.com/pytorch/pytorch/pull/22242))\r\n* `torch.einsum`: Fix an issue where the backward pass would potentially be skipped. ([22111](https://github.com/pytorch/pytorch/pull/22111))\r\n* `torch.cosh`: Fix an issue where `torch.cos` was instead calculated with `torch.double` dtype and vectorized instructions. ([20797](https://github.com/pytorch/pytorch/pull/20797))\r\n* `torch.triu` / `torch.tril`: handle strides correctly for in-place versions. ([22730](https://github.com/pytorch/pytorch/pull/22730)).\r\n* `torch.triu` / `torch.tril`: Fix handling of batches > 65535 on CUDA. ([21067](https://github.com/pytorch/pytorch/pull/21067))\r\n* `torch.inverse` / `torch.solve` / `torch.cholesky_solve` /  `torch.triangular_solve`: Fix batch sizes > 65535 on CUDA. ([21689](https://github.com/pytorch/pytorch/pull/21689))\r\n* `torch.histc`: return `dtype` is now the same as the input tensor on CUDA, matching CPU behavior. ([20369](https://github.com/pytorch/pytorch/pull/20369))\r\n* `torch.histc`: properly return 1-dim tensor on CPU with 0-dim input and 1 bin. ([21497](https://github.com/pytorch/pytorch/pull/21497))\r\n* `torch.randperm`: handle non-contiguous `out` parameter. ([23043](https://github.com/pytorch/pytorch/pull/23043))\r\n* `torch.unique`: Fix empty tensor handling when `dim` is passed as an argument. ([19000](https://github.com/pytorch/pytorch/pull/19000))\r\n* `torch.min` / `torch.max`: properly error on empty tensor inputs, as with CPU tensors. ([19612](https://github.com/pytorch/pytorch/pull/19612)).\r\n* `CUDA`: fix launch parameters for reductions. ([22827](https://github.com/pytorch/pytorch/pull/22827)).\r\n* `torch.hub`: fix an issue with `find_module`. ([20782](https://github.com/pytorch/pytorch/pull/20782))\r\n* `autograd`: Fix a number of custom autograd `Function` corner cases by inverting the relationship between PyFunction and THPFunction. ([22983](https://github.com/pytorch/pytorch/pull/22983))\r\n* `autograd`: give \u201cTrying to backward through the graph a second time\" error instead of internal assert when the buffers are a list of Tensors (with indexing). ([21533](https://github.com/pytorch/pytorch/pull/21533))\r\n* `optim.lr_scheduler.CosineAnnealingLR`: rename from CosineAnnealingLr. ([23242](https://github.com/pytorch/pytorch/pull/23242))\r\n* `distributions.Binomial`: Fix overflow of `log_prob` when `logits` is large. ([20679](https://github.com/pytorch/pytorch/pull/20679))\r\n* `distributions.SigmoidTransform`: Fix numerical issues that could result in `inf` / `-inf` return values. ([20288](https://github.com/pytorch/pytorch/pull/20288))\r\n* `distributions.Categorical.sample`: fix a view bug. ([23328](https://github.com/pytorch/pytorch/pull/23328))\r\n* `CUDA`: Give proper error message for bad cuda forks. ([23322](https://github.com/pytorch/pytorch/pull/23322))\r\n* `pickle`: Fix Unpickling error when loading multiple objects from a file. ([20270](https://github.com/pytorch/pytorch/pull/20270))\r\n* `NCCL`: Fix race condition. ([23040](https://github.com/pytorch/pytorch/pull/23040))\r\n\r\n### torch.nn Bug Fixes\r\n\r\n* `nn.Conv{1,2,3}D`: fix memory leak on MKLDNN code path. ([22392](https://github.com/pytorch/pytorch/pull/22392))\r\n* `nn.Conv{1,2,3}D`: properly unpickle older pickled versions. ([21687](https://github.com/pytorch/pytorch/pull/21687))\r\n* `nn.CTCLoss`: fix backward on CUDA when 2d target tensor is larger than `max_target_length`. ([20971](https://github.com/pytorch/pytorch/pull/20971))\r\n* `nn.CTCLoss`: fix some numerical stability issues. ([21392](https://github.com/pytorch/pytorch/pull/21392))\r\n* `nn.CTCLoss`: disable buggy non-deterministic CudNN algorithm. ([22977](https://github.com/pytorch/pytorch/pull/22977))\r\n* `nn.CTCLoss`: fixed empty target handling. ([21910](https://github.com/pytorch/pytorch/pull/21910), [23298](https://github.com/pytorch/pytorch/pull/23298))\r\n* `nn.SyncBatchNorm`: fix syncing of running statistics when count size differs between GPUs. ([22248](https://github.com/pytorch/pytorch/pull/22248))\r\n* `nn.SyncBatchNorm`: retain `requires_grad` value when converting from `nn.BatchNorm`. ([22569](https://github.com/pytorch/pytorch/pull/22569))\r\n* `nn.SyncBatchNorm`: correctly handle `process_group` in `convert_sync_batchnorm`. ([19240](https://github.com/pytorch/pytorch/pull/19240))\r\n* `nn.MultiheadedAttention`: fix for `torch.float16` dtype. ([21658](https://github.com/pytorch/pytorch/pull/21658)).\r\n* `nn.EmbeddingBag`: fix NaN output when input is empty. ([21400](https://github.com/pytorch/pytorch/pull/21400))\r\n* `nn.Dropout`: fix python crash (with SIGFPE) when called on an empty cuda tensor. ([20541](https://github.com/pytorch/pytorch/pull/20541))\r\n* `nn.MaxPool`: fix output size calculation in some corner cases. ([22304](https://github.com/pytorch/pytorch/pull/22304))\r\n* `nn.MaxPool`: return valid indices if all entries are `-inf`. ([23161](https://github.com/pytorch/pytorch/pull/23161))\r\n* `nn.Softmax`: respect the current Stream. ([22470](https://github.com/pytorch/pytorch/pull/22470))\r\n* `nn.LogSoftmax`: fix numerical stability issues. ([21672](https://github.com/pytorch/pytorch/pull/21672))\r\n* `nn.Module.load_state_dict`: break ref cycle. ([20397](https://github.com/pytorch/pytorch/pull/20397))\r\n* `nn.Module`: fix loading in 32-bit environments. ([20900](https://github.com/pytorch/pytorch/pull/20900))\r\n* `nn.utils.rnn.pack_padded_sequence`: Fix segfault on empty tensors. ([21461](https://github.com/pytorch/pytorch/pull/21461))\r\n* `nn.utils.spectral_norm`: fix loading `state_dict` when `strict=False`. ([22545](https://github.com/pytorch/pytorch/pull/22545))\r\n* `CudNN`: Fix uninitialized PoolWindow on Windows. ([22405](https://github.com/pytorch/pytorch/pull/22405))\r\n\r\n### Distributed Bug fixes\r\n\r\n* `nn.parallel.DataParallel`: fix error in `no_grad` mode. ([21262](https://github.com/pytorch/pytorch/pull/21262))\r\n* `torch.distributed.all_gather`: fix errors for views and aliases. ([21490](https://github.com/pytorch/pytorch/pull/21490))\r\n* `c10d`: fix collective communication errors on empty tensors. ([20658](https://github.com/pytorch/pytorch/pull/20658))\r\n\r\n### JIT Bug Fixes\r\n\r\n* Fix specialized list from dict keys. ([23267](https://github.com/pytorch/pytorch/pull/23267))\r\n* Switch keys to be sequential and stable in pickle serialization. ([23280](https://github.com/pytorch/pytorch/pull/23280))\r\n* `deepCopy` also copies type information of lists, ([23271](https://github.com/pytorch/pytorch/pull/23271))\r\n* `dictKeys` and `dictItems` ops on typed dicts return typed lists. ([23270](https://github.com/pytorch/pytorch/pull/23270))\r\n* Fix pickler bug where it would not load if no tensors were saved. ([23263](https://github.com/pytorch/pytorch/pull/23263))\r\n* Avoid multiple writes to files on export. ([21186](https://github.com/pytorch/pytorch/pull/21186))\r\n* Better error msg for mismatched `dict` key type. ([22231](https://github.com/pytorch/pytorch/pull/22231))\r\n* Better error msg for using Python `builtin_function_or_method`. ([22935](https://github.com/pytorch/pytorch/pull/22935))\r\n* Better error msg in `__get_state__` to let a user know that ScriptModules can't be deep-copied at the moment.([20885](https://github.com/pytorch/pytorch/pull/20885))\r\n* Better error msg when seeing a unsupported builtin function. ([21068](https://github.com/pytorch/pytorch/pull/21068))\r\n* `dropout` derivative should respect the `train` flag. ([20760](https://github.com/pytorch/pytorch/pull/20760))\r\n* Fix `__constants__` for some nn modules. ([21071](https://github.com/pytorch/pytorch/pull/21071))\r\n* Fix `ScriptModule.__dir__()`. ([22426](https://github.com/pytorch/pytorch/pull/22426))\r\n* Fix 3x DenseNet compile time regression by restoring earlier-out tests in AliasDB::writesToAlias. ([21425](https://github.com/pytorch/pytorch/pull/21425))\r\n* Fix a bug in loop unrolling. ([21239](https://github.com/pytorch/pytorch/pull/21239))\r\n* Fix alias annotations for dict ops. ([22900](https://github.com/pytorch/pytorch/pull/22900))\r\n* Fix inaccurate SourceRange reporting. ([21109](https://github.com/pytorch/pytorch/pull/21109))\r\n* Fix broken indexing when using None and ellipses indexing together. ([22905](https://github.com/pytorch/pytorch/pull/22905))\r\n* Fix bug in `CompilationUnit::define`. ([21886](https://github.com/pytorch/pytorch/pull/21886))\r\n* Fix compilation order for class methods. ([20094](https://github.com/pytorch/pytorch/pull/20094))\r\n* Fix dead code elimination over loops. ([22632](https://github.com/pytorch/pytorch/pull/22632))\r\n* Fix dead code elimination in onnx export. ([22476](https://github.com/pytorch/pytorch/pull/22476))\r\n* Fix incorrect default on `Graph::toString`. ([21370](https://github.com/pytorch/pytorch/pull/21370))\r\n* Fix optional type promotion for classes. ([21593](https://github.com/pytorch/pytorch/pull/21593))\r\n* Fix optional type unification. ([19813](https://github.com/pytorch/pytorch/pull/19813))\r\n* Fix `NameError` with `PYTORCH_JIT=0`. ([20120](https://github.com/pytorch/pytorch/pull/20120))\r\n* Fix overspecializing constants in compilation. ([22816](https://github.com/pytorch/pytorch/pull/22816))\r\n* Fix `pow()` bug on overloads. ([20824](https://github.com/pytorch/pytorch/pull/20824))\r\n* Fix recusive method compilation. ([21862](https://github.com/pytorch/pytorch/pull/21862))\r\n* Fix reflection on weak modules, copy attributes. ([20190](https://github.com/pytorch/pytorch/pull/20190))\r\n* Fix slow unpickling. ([21542](https://github.com/pytorch/pytorch/pull/21542))\r\n* Fix input/output type mismatch. ([20829](https://github.com/pytorch/pytorch/pull/20829))\r\n* Fix insert_guard for norm decomposation. ([19646](https://github.com/pytorch/pytorch/pull/19646))\r\n* Fix Trace inlining of graphs with optional inputs. ([22686](https://github.com/pytorch/pytorch/pull/22686))\r\n* Fix tracing bugs where using `1 - x` in C++ would cause the size of 1 to get hardcoded. ([20932](https://github.com/pytorch/pytorch/pull/20932))\r\n* Fix tuple indexing bug. ([21521](https://github.com/pytorch/pytorch/pull/21521))\r\n* Fix type hints for `None` constants. ([23029](https://github.com/pytorch/pytorch/pull/23029))\r\n* Fix weak module cuda() `_flat_weights bug`. ([21107](https://github.com/pytorch/pytorch/pull/21107))\r\n* Fix `WeakIValueEq`. ([21891](https://github.com/pytorch/pytorch/pull/21891))\r\n* Fixed gcd to use 64 bit integers. ([21041](https://github.com/pytorch/pytorch/pull/21041))\r\n* Fixed `list()` not making a copy. ([22093](https://github.com/pytorch/pytorch/pull/22093))\r\n* Fix race condition on `Module::forward` method. ([21398](https://github.com/pytorch/pytorch/pull/21398))\r\n* Made `a += b` for lists do an in place add. ([21896](https://github.com/pytorch/pytorch/pull/21896))\r\n* Made `floor/ceil` return ints. ([21124](https://github.com/pytorch/pytorch/pull/21124))\r\n* Out-of-memory on GPU due to the \"weak_script\" decorators. ([20588](https://github.com/pytorch/pytorch/issues/20588))\r\n* Override print when python is present. ([21625](https://github.com/pytorch/pytorch/pull/21625))\r\n* Set `__file__` for `torch.ops`. ([21888](https://github.com/pytorch/pytorch/pull/21888))\r\n* Set correct list type in pybind_utils. ([23188](https://github.com/pytorch/pytorch/pull/23188))\r\n\r\n### C++ Frontend bug fixes\r\n\r\n* `nn::RNN`: Fix assertions in bidirectional RNN. ([22850](https://github.com/pytorch/pytorch/pull/22850)).\r\n* `nn::MaxPool ` / ` nn::AvgPool`: expand incomplete kernel size, as in Python. ([22073](https://github.com/pytorch/pytorch/pull/22073), [22075](https://github.com/pytorch/pytorch/pull/22075))\r\n* `Optim`: Fix memory leak when `weight_decay` is applied to `Adam`, `Adagrad`,  `RMSProp`. ([23125](https://github.com/pytorch/pytorch/pull/23125))\r\n* `Optim::SGD`: fix memory leak with weight_decay. ([23007](https://github.com/pytorch/pytorch/pull/23007))\r\n* `torch::autograd::Scatter` `/ torch::autograd::Gather`: Fix nullptr bug. ([20286](https://github.com/pytorch/pytorch/pull/20286))\r\n* `torch::nn::parallel::data_parallel`: fix gradient computation error. ([20910](https://github.com/pytorch/pytorch/pull/20910))\r\n* [C++ Extensions] Fix an issue when building multiple extensions in the same directory. ([20221](https://github.com/pytorch/pytorch/pull/20221))\r\n\r\n## Deprecations\r\n\r\n### **Masking via `torch.uint8` Tensors is now deprecated in favor of masking via `torch.bool` Tensors.**\r\n\r\nSee the **Breaking Changes** section for more details about `torch.bool` Tensors and comparison operators.\r\n\r\n**`torch.masked_select`, `torch.masked_fill`, `torch.masked_scatter` now expect `torch.bool` masks rather than `torch.uint8`.**\r\n\r\n```\r\n>>> a = torch.tensor([1, 2, 3])\r\n>>> b = torch.tensor([3, 1, 2])\r\n\r\n>>> a.masked_select(tensor([0, 1, 1], dtype=torch.uint8))\r\nUserWarning: masked_select received a mask with dtype torch.uint8,\r\nthis behavior is now deprecated, please use a mask with dtype torch.bool instead.\r\n\r\ntensor([2, 3])\r\n\r\n# instead use torch.bool\r\n>>> a.masked_select(tensor([False,  True,  True]))\r\ntensor([2, 3])\r\n```\r\n\r\n\r\n**Comparison operators with `out=` parameters now expect `torch.bool` dtype rather than `torch.uint8`.**\r\n\r\n```\r\n>>> a = torch.tensor([1, 2, 3])\r\n>>> b = torch.tensor([3, 1, 2])\r\n>>> res = torch.empty_like(a, dtype=torch.uint8)\r\n>>> torch.gt(a, b, out=res)\r\nUserWarning: torch.gt received 'out' parameter with dtype torch.uint8, this behavior\r\nis now deprecated, please use 'out' parameter with dtype torch.bool instead.\r\n\r\ntensor([0, 1, 1], dtype=torch.uint8)\r\n\r\n# instead use torch.bool\r\n>>> res = torch.empty_like(a, dtype=torch.bool)\r\n>>> torch.gt(a, b, out=res)\r\ntensor([False, True, True])\r\n```\r\n\r\n\r\n\r\n### Legacy `autograd.Function` (Function without static forward method) is now deprecated\r\n\r\n```\r\n>>> class MyLegacyFunction(Function):\r\n>>>     def forward(self, x):\r\n>>>         return x\r\n>>>\r\n>>>     def backward(self, grad_output):\r\n>>>         return grad_output\r\n>>>\r\n>>> MyLegacyFunction()(torch.randn((3,), requires_grad=True)\r\nUserWarning: Legacy autograd function with non-static forward method is deprecated\r\nand will be removed in 1.3. Please use new-style autograd function\r\nwith static forward method.\r\n\r\n# instead use new-style Autograd Function\r\n>>> class MyFunction(Function):\r\n>>>     @staticmethod\r\n>>>     def forward(ctx, x):\r\n>>>         return x\r\n>>>\r\n>>>     @staticmethod\r\n>>>     def backward(ctx, grad_output):\r\n>>>         return grad_output\r\n>>>\r\n>>> MyFunction.apply(torch.randn((3,), requires_grad=True)\r\n```\r\n\r\nSee the [torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) documentation for more details.\r\n\r\n### `torch.gels`: has been renamed to `torch.lstsq`; `torch.gels` will work for this release but is now deprecated.  ([23460](https://github.com/pytorch/pytorch/pull/23460))\r\n\r\n## Performance\r\n\r\n* Advanced Indexing: significantly improve performance of advanced indexing backward. ([20557](https://github.com/pytorch/pytorch/pull/20557))\r\n* `Tensor.copy_`: increase broadcasting CUDA copy performance by 25%. ([20685](https://github.com/pytorch/pytorch/pull/20685))\r\n* `torch.matmul`: Optimize the case A.ndim <= 2 && B.ndim >= 3, shows up to 15x speed up. ([20448](https://github.com/pytorch/pytorch/pull/20448))\r\n* `torch.bmm`: Improve performance by up to 3x for small cases on CPU by applying TensorAccessor. ([20266](https://github.com/pytorch/pytorch/pull/20266))\r\n* `torch.inverse`: Move workspace query and allocation outside loop to improve performance by up to 5x. ([20904](https://github.com/pytorch/pytorch/pull/20904))\r\n* `torch.topk`: Optimize CPU perf using parallel and partial sort, up to 6x improvement. ([22865](https://github.com/pytorch/pytorch/pull/22865))\r\n* `torch.cdist`: Improve CPU perf by up to 10x for some cases. ([20605](https://github.com/pytorch/pytorch/pull/20605))\r\n* `torch.normal`: Move `normal`, `normal_means`, `normal_stddevs`, and `normal_means_stddevs` to ATen, increasing performance by up to 3x. ([21287](https://github.com/pytorch/pytorch/pull/21287))\r\n* `torch.bernoulli`: Speedup bernoulli_scalar_cuda_kernel with grid-stride loop, increasing performance by up to 2x. ([21300](https://github.com/pytorch/pytorch/pull/21300))\r\n* `torch.coalesce`: Use `_sparse_coo_tensor_unsafe` in `coalesce` for up to 10x speedup. ([21214](https://github.com/pytorch/pytorch/pull/21214))\r\n* `torch.sinh` / `torch.cosh`: Parallelize and vectorize on CPU. ([21115](https://github.com/pytorch/pytorch/pull/21115))\r\n* `torch.lerp`: Vectorize on CPU. ([22038](https://github.com/pytorch/pytorch/pull/22038))\r\n* `torch.eye`: Parallelize on CPU. ([21077](https://github.com/pytorch/pytorch/pull/21077))\r\n* `torch.randperm`: Parallelize initialization in randperm on CPU. ([21529](https://github.com/pytorch/pytorch/pull/21529))\r\n* Vectorization: Don't split 256-bit AVX2 load/store intrinsics. ([20609](https://github.com/pytorch/pytorch/pull/20609)).\r\n\r\n### Torch.NN Performance Improvements\r\n\r\n* `nn.Softmax`: Add persistent CUDA kernels that increase performance 2-10x on small inputs. ([20827](https://github.com/pytorch/pytorch/pull/20827))\r\n* `nn.Embedding` / `nn.EmbeddingBag`: Optimize CUDA kernel, increasing performance up to 2.7x. ([22016](https://github.com/pytorch/pytorch/pull/22016))\r\n* `nn.Linear`: optimize BERT model perf by using mkldnn inner product. ([21851](https://github.com/pytorch/pytorch/pull/21851))\r\n* `nn.Conv{1,2,3}D`: improve perf for depthwise convolutions in `torch.float16` on Volta and Turing GPUs. ([22302](https://github.com/pytorch/pytorch/pull/22302))\r\n* `nn.RNN`: optimize on CPU by fusing matmul ops. ([22512](https://github.com/pytorch/pytorch/pull/22512))\r\n* `nn.Upsample`: a number of significant perf improvements on CUDA. ([21879](https://github.com/pytorch/pytorch/pull/21879), [21694](https://github.com/pytorch/pytorch/pull/21694)).\r\n* `nn.functional.layer_norm`: optimize a fast path for layer_norm, increasing perf by up to 4x on CPU. ([20345](https://github.com/pytorch/pytorch/pull/20345), [20883](https://github.com/pytorch/pytorch/pull/20883))\r\n* Use `mkldnn` inner product for `nn.Linear()` to improve BERT perf. ([21851](https://github.com/pytorch/pytorch/pull/21851)).\r\n\r\n## Documentation\r\n\r\n* `torch.bool`: doc the Boolean tensor type. ([21601](https://github.com/pytorch/pytorch/pull/21601))\r\n* `torch.as_strided`: add docs. ([22842](https://github.com/pytorch/pytorch/pull/22842))\r\n* `torch.empty_strided`: add docs. ([23740](https://github.com/pytorch/pytorch/pull/23740))\r\n* `torch.lerp`: clarify broadcasting requirements. ([23268](https://github.com/pytorch/pytorch/pull/23268))\r\n* `torch.enable_grad` / `torch.no_grad` / `torch.set_grad_enable`: clarify interaction between these features. ([23310](https://github.com/pytorch/pytorch/pull/23310))\r\n* `torch.autograd.grad_mode`: Document that no_grad is thread local. ([21755](https://github.com/pytorch/pytorch/pull/21755))\r\n* `torch.multiprocessing`: Explain refcounting of CUDA tensors. ([19904](https://github.com/pytorch/pytorch/pull/19904))\r\n* `torch.Tensor`: Add a warning about memory usage. ([20801](https://github.com/pytorch/pytorch/pull/20801))\r\n* `torch.utils.data.Dataloader`: Document RNG state consumption. ([22540](https://github.com/pytorch/pytorch/pull/22540))\r\n* `torch.optim.lr_scheduler.CyclicLR`: Clarify `base_momentum` and `max_momentum`. ([20880](https://github.com/pytorch/pytorch/pull/20880)).\r\n* Document production environment features. ([23010](https://github.com/pytorch/pytorch/pull/23010))\r\n* Add note about contributing recently released research. ([23513](https://github.com/pytorch/pytorch/pull/23513))\r\n* Clarify performance implications of deterministic mode. ([21337](https://github.com/pytorch/pytorch/pull/21337))\r\n* Update cuda pinned memory note to include `tensor.to`. ([20977](https://github.com/pytorch/pytorch/pull/20977))\r\n\r\n### Torch.NN Documentation\r\n\r\n* `nn.functional / nn.init`: Break up NN in docs so they load faster. ([21291](https://github.com/pytorch/pytorch/pull/21291))\r\n* `nn.functional.conv{1,2,3}d`: Remove `padding_mode`.  ([20891](https://github.com/pytorch/pytorch/pull/20891))\r\n* `nn.functional.upsample` / `nn.functional.interpolate`: add note about overshooting with `mode=\u2018bicubic\u2019`. ([23321](https://github.com/pytorch/pytorch/pull/23321))\r\n* `nn.init.zeros_` / `nn.init.ones_`: add documentation. ([23145](https://github.com/pytorch/pytorch/pull/23145))\r\n* `nn.MultiheadAttention`: Add documentation for `add_bias_kv`, `add_zero_attn`, and `attn_mask`. ([20071](https://github.com/pytorch/pytorch/pull/20071))\r\n* `nn.MultiheadAttention`: Fix documentation for attention mask shape. ([20850](https://github.com/pytorch/pytorch/pull/20850))\r\n* `nn.Softmax`: Fixed to specify dimension to prevent warning in 1.1.0. ([20310](https://github.com/pytorch/pytorch/pull/20310)*)*\r\n\r\n### Contributor Documentation\r\n\r\n* Updated web links on contribution_guide and governance documentation. ([21243](https://github.com/pytorch/pytorch/pull/21243))\r\n* Improve documentation for publishing hub models. ([21307](https://github.com/pytorch/pytorch/pull/21307))\r\n* Suggest a faster linker in the contributing guide. ([21334](https://github.com/pytorch/pytorch/pull/21334))\r\n* Add CUDA C++11 and profiling notes to the contribution guide. ([21386](https://github.com/pytorch/pytorch/pull/21386))\r\n\r\n### Build Documentation\r\n\r\n* Add magma for CUDA 10.1 to Windows docs. ([19914](https://github.com/pytorch/pytorch/pull/19914))\r\n* Improve build-from-source instructions. ([20088](https://github.com/pytorch/pytorch/pull/20088))\r\n* Add `ninja` to build instructions. ([20079](https://github.com/pytorch/pytorch/pull/20079))\r\n* Update libtorch build docs. ([21150](https://github.com/pytorch/pytorch/pull/21150))\r\n\r\n### TensorBoard Documentation\r\n\r\n* Tensorboard Documentation has been greatly improved!  Browse the latest version [here](https://pytorch.org/docs/stable/tensorboard.html).\r\n\r\n### Torch HUB Documentation\r\n\r\n* Improve docs for publishing hub models. ([21307](https://github.com/pytorch/pytorch/pull/21307))\r\n* Update docs of entry point in hub. ([21568](https://github.com/pytorch/pytorch/pull/21568))\r\n\r\n## ONNX\r\n\r\n\r\nIn PyTorch 1.2, we have added the full support for ONNX Opset 7, 8, 9 and 10 in ONNX exporter, and we have also enhanced the constant folding pass to support Opset 10. The export of ScriptModule has better support. Additionally, users now are able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export.\r\n\r\n### Supporting More ONNX Opsets\r\n\r\n* Add basic supports for multiple ONNX Opsets and support for Opset 10. ([19294](https://github.com/pytorch/pytorch/pull/19294))\r\n* Support ONNX Opset 7 and 8 in PyTorch ONNX Exporter. ([22421](https://github.com/pytorch/pytorch/pull/22421), [20036](https://github.com/pytorch/pytorch/pull/20036))\r\n* Export `Dropout` for Opset 10. ([20710](https://github.com/pytorch/pytorch/pull/20710))\r\n* Export `Slice` and `Flip` for Opset 10. ([20533](https://github.com/pytorch/pytorch/pull/20533))\r\n* Export `Interpolate (Resize)` for Opset 10. ([21434](https://github.com/pytorch/pytorch/pull/21434))\r\n\r\n### Enhancing the Support for ScriptModule\r\n\r\n* Support multiple outputs in ScriptModule in ONNX Exporter. ([20256](https://github.com/pytorch/pytorch/pull/20256))\r\n* Support tensor factories in ScriptModule in ONNX Exporter. ([20255](https://github.com/pytorch/pytorch/pull/20255))\r\n* Support tuples as inputs and outputs in ScriptModule. ([20784](https://github.com/pytorch/pytorch/pull/20784))\r\n\r\n### Exporting More Torch Operators to ONNX\r\n\r\n* Export custom ops. ([21321](https://github.com/pytorch/pytorch/pull/21321))\r\n* Export `torch.arange `. ([22601](https://github.com/pytorch/pytorch/pull/22601))\r\n* Export `torch.masked_fill`. ([22521](https://github.com/pytorch/pytorch/pull/22521))\r\n* Export `torch.floor`, ` torch.ceil`, `torch.log2` and `prim::shape`. ([17895](https://github.com/pytorch/pytorch/pull/17895))\r\n* Export `torch._dim_arange`. ([20078](https://github.com/pytorch/pytorch/pull/20078))\r\n* Export `torch.randn_like`. ([20093](https://github.com/pytorch/pytorch/pull/20093))\r\n* Export `torch._standard_gamma`. ([20126](https://github.com/pytorch/pytorch/pull/20126))\r\n* Export `torch.topk`. ([21104](https://github.com/pytorch/pytorch/pull/21104))\r\n* Export `__ and__`, `__or__`. ([17894](https://github.com/pytorch/pytorch/pull/17894))\r\n* Export `torch.sign`. ([20470](https://github.com/pytorch/pytorch/pull/20470))\r\n* Export `torch.scatter`. ([18543](https://github.com/pytorch/pytorch/pull/18543))\r\n* Export `torch.rand`. ([20559](https://github.com/pytorch/pytorch/pull/20559))\r\n* Export `torch.gather`. ([21235](https://github.com/pytorch/pytorch/pull/21235))\r\n* Export `torch.cosine_similarity`. ([21884](https://github.com/pytorch/pytorch/pull/21884))\r\n* Export `torch.sum`. ([22240](https://github.com/pytorch/pytorch/pull/22240))\r\n* Export `torch.logsumexp`. ([22306](https://github.com/pytorch/pytorch/pull/22306))\r\n* Export `torch.layer_norm`. ([22265](https://github.com/pytorch/pytorch/pull/22265))\r\n\r\n### Extending Existing Exporting Logic\r\n\r\n* Support `torch.min` and `torch.max` with dim. ([19689](https://github.com/pytorch/pytorch/pull/19689))\r\n* Support `maxpool` with dilations. ([18721](https://github.com/pytorch/pytorch/pull/18721))\r\n* Support `RNN` with `batch_first=True`. ([19766](https://github.com/pytorch/pytorch/pull/19766))\r\n* Support `Upsample` with dynamic input. ([20116](https://github.com/pytorch/pytorch/pull/20116))\r\n* Improve support for Loop export. ([20445](https://github.com/pytorch/pytorch/pull/20445))\r\n* Enable `torch.full` with scalar parameters. ([21931](https://github.com/pytorch/pytorch/pull/21931))\r\n* Added support for exporting models with variable length input/output to ONNX. ([20034](https://github.com/pytorch/pytorch/pull/20034))\r\n\r\n### Optimizing Exported ONNX Graph\r\n\r\n* Support constant folding in Opset 10. ([22515](https://github.com/pytorch/pytorch/pull/22515))\r\n* Support negative indexing for `Slice` in constant folding optimization. ([21811](https://github.com/pytorch/pytorch/pull/21811))\r\n\r\n### Bugfixes/Improvements\r\n\r\n* Fix the shape of `PReLU` weight. ([21330](https://github.com/pytorch/pytorch/pull/21330))\r\n* Fix the export for `torch.pixel_shuffle`. ([21486](https://github.com/pytorch/pytorch/pull/21486))\r\n* Fix the export for `torch.full`. ([21669](https://github.com/pytorch/pytorch/pull/21669))\r\n* Update logic for folding `onnx::Constant` nodes. ([20109](https://github.com/pytorch/pytorch/pull/20109))\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.2.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.2.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.2.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/19168826", "release_id": 19168826, "date_created": "2019-08-08T12:54:09Z", "date_published": "2019-08-08T16:06:38Z"}, "confidence": 1, "technique": "GitHub_API"}, {"result": {"type": "Release", "value": "https://api.github.com/repos/pytorch/pytorch/releases/17080796", "tag": "v1.1.0", "name": "Official TensorBoard Support, Attributes, Dicts, Lists and User-defined types in JIT / TorchScript, Improved Distributed", "author": {"name": "soumith", "type": "User"}, "description": "Note: CUDA 8.0 is no longer supported\r\n\r\n## Highlights\r\n\r\n### TensorBoard (currently experimental)\r\n\r\nFirst-class and native support for visualization and model debugging with [TensorBoard](https://www.tensorflow.org/tensorboard), a web application suite for inspecting and understanding training runs, tensors, and graphs. PyTorch now supports TensorBoard logging with a simple `from torch.utils.tensorboard import SummaryWriter` command. Histograms, embeddings, scalars, images, text, graphs, and more can be visualized across training runs. TensorBoard support is currently experimental. You can browse the docs [here](https://pytorch.org/docs/stable/tensorboard.html).\r\n\r\n![](https://github.com/gchanan/pytorch/raw/tensorboard_screenshot/Screen%20Shot%202019-04-25%20at%204.53.42%20PM.png)\r\n\r\n### [JIT] Attributes in ScriptModules\r\nAttributes can be assigned on a `ScriptModule` by wrapping them with `torch.jit.Attribute` and specifying the type. Attributes are similar to parameters or buffers, but can be of any type. They will be serialized along with any paramters/buffers when you call `torch.jit.save()`, so they are a great way to store arbitrary state in your model. See [the docs](https://pytorch.org/docs/master/jit.html#module-attributes) for more info.\r\n\r\nExample:\r\n```\r\nclass Foo(torch.jit.ScriptModule):\r\n  def __init__(self, a_dict):\r\n    super(Foo, self).__init__(False)\r\n    self.words = torch.jit.Attribute([], List[str])\r\n    self.some_dict = torch.jit.Attribute(a_dict, Dict[str, int])\r\n\r\n  @torch.jit.script_method\r\n  def forward(self, input: str) -> int:\r\n    self.words.append(input)\r\n    return self.some_dict[input]\r\n```\r\n\r\n### [JIT] Dictionary and List Support in TorchScript\r\nTorchScript now has robust support for list and dictionary types. They behave much like Python lists and dictionaries, supporting most built-in methods, as well as simple comprehensions and `for\u2026in` constructs. \r\n\r\n### [JIT] User-defined classes in TorchScript (experimental)\r\nFor more complex stateful operations, TorchScript now supports annotating a class with `@torch.jit.script`. Classes used this way can be JIT-compiled and loaded in C++ like other TorchScript modules. See [the docs](https://pytorch.org/docs/master/jit.html#user-defined-types) for more info.\r\n```\r\n@torch.jit.script\r\nclass Pair:\r\n\tdef __init__(self, first, second)\r\n\t\tself.first = first\r\n\t\tself.second = second\r\n\r\n\tdef sum(self):\r\n\t\treturn self.first + self.second\r\n```\r\n\r\n\r\n### DistributedDataParallel new functionality and tutorials\r\n\r\n`nn.parallel.DistributedDataParallel`: can now wrap multi-GPU modules, which enables use cases such as model parallel ([tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)) on one server and data parallel ([tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)) across servers. \r\n ([19271](https://github.com/pytorch/pytorch/pull/19271)).\r\n\r\n## Breaking Changes\r\n* `Tensor.set_`: the `device` of a Tensor can no longer be changed via `Tensor.set_`.  This would most commonly happen when setting up a Tensor with the default CUDA device and later swapping in a `Storage` on a different CUDA device.  Instead, set up the Tensor on the correct device from the beginning.  ([18832](https://github.com/pytorch/pytorch/pull/18832)).\r\n* Pay attention to the order change of `lr_scheduler.step()`. ([7889](https://github.com/pytorch/pytorch/pull/7889)).\r\n* `torch.unique`: changed the default value of `sorted` to `True`.  ([15379](https://github.com/pytorch/pytorch/pull/15379)).\r\n* **[JIT]** Rename isTensor api -> isCompleteTensor. [#18437](https://github.com/pytorch/pytorch/pull/18437) \r\n* **[JIT]** Remove GraphExecutor's python bindings. [#19141](https://github.com/pytorch/pytorch/pull/19141) \r\n* **[C++]**: many methods on `Type` no longer exist; use the functional or Tensor method equivalent.  ([17991](https://github.com/pytorch/pytorch/pull/17991)).\r\n* **[C++]**: the `Backend` constructor of `TensorOptions` no longer exists.  ([18137](https://github.com/pytorch/pytorch/pull/18137)).\r\n* **[C++, Distributed]**: Remove c10d `ProcessGroup::getGroupRank` has been removed.  ([19147](https://github.com/pytorch/pytorch/pull/19147)).\r\n\r\n\r\n## New Features\r\n\r\n### Operators\r\n* `torch.tril_indices`, `torch.triu_indices`: added operator with same behavior as NumPy.  ([14904](https://github.com/pytorch/pytorch/pull/14904), [15203](https://github.com/pytorch/pytorch/pull/15203)).\r\n* `torch.combinations`, `torch.cartesian_prod`: added new `itertools`-like operators.  ([9393](https://github.com/pytorch/pytorch/pull/9393)).\r\n* `torch.repeat_interleave`: new operator similar to `numpy.repeat`.  ([18395](https://github.com/pytorch/pytorch/pull/18395)).\r\n* `torch.from_file`: new operator similar to `Storage.from_file`, but returning a tensor.  ([18688](https://github.com/pytorch/pytorch/pull/18688)).\r\n* `torch.unique_consecutive`: new operator with semantics similar to `std::unique` in C++.  ([19060](https://github.com/pytorch/pytorch/pull/19060)).\r\n* `torch.tril`, `torch.triu`, `torch.trtrs`: now support batching.  ([15257](https://github.com/pytorch/pytorch/pull/15257), [18025](https://github.com/pytorch/pytorch/pull/18025)).\r\n* `torch.gather`: add support for `sparse_grad` option.  ([17182](https://github.com/pytorch/pytorch/pull/17182)).\r\n* `torch.std`, `torch.max_values`, `torch.min_values`, `torch.logsumexp` can now operate over multiple dimensions at once.  ([14535](https://github.com/pytorch/pytorch/pull/14535), [15892](https://github.com/pytorch/pytorch/pull/15892), [16475](https://github.com/pytorch/pytorch/pull/16475)).\r\n* `torch.cdist`: added operator equivalent to `scipy.spatial.distance.cdist`.  ([16168](https://github.com/pytorch/pytorch/pull/16168), [17173](https://github.com/pytorch/pytorch/pull/17173)).\r\n* `torch.__config__.show()`: reports detailed version of all libraries.  ([18579](https://github.com/pytorch/pytorch/pull/18579)).\r\n\r\n### NN\r\n* `nn.MultiheadedAttention`: new module implementing MultiheadedAttention from `Attention Is All You Need`.  ([18334](https://github.com/pytorch/pytorch/pull/18334)).\r\n* `nn.functional.interpolate`: added support for `bicubic`.  ([9849](https://github.com/pytorch/pytorch/pull/9849)).\r\n* `nn.SyncBatchNorm`: support synchronous Batch Normalization.  ([14267](https://github.com/pytorch/pytorch/pull/14267)).\r\n* `nn.Conv`: added support for Circular Padding via `mode='circular'`.  ([17240](https://github.com/pytorch/pytorch/pull/17240)).\r\n* `nn.EmbeddingBag`: now supports trainable `per_sample_weights.  ([18799](https://github.com/pytorch/pytorch/pull/18799)).\r\n* `nn.EmbeddingBag`: add support for `from_pretrained` method, as in `nn.Embedding`.  ([15273](https://github.com/pytorch/pytorch/pull/15273)).\r\n* `RNNs`: automatically handle unsorted variable-length sequences via `enforce_sorted`.  ([15225](https://github.com/pytorch/pytorch/pull/15225)).\r\n* `nn.Identity`: new module for easier model surgery.  ([19249](https://github.com/pytorch/pytorch/pull/19249)).\r\n\r\n## Tensors / dtypes\r\n* `torch.bool`: added support for `torch.bool` dtype and Tensors with that dtype (1-byte storage).  NumPy conversion is supported, but operations are currently limited.  ([16810](https://github.com/pytorch/pytorch/pull/16810)).\r\n\r\n### Optim\r\n* `optim.lr_scheduler.CyclicLR`: Support for Cyclical Learning Rate and Momentum.  ([18001](https://github.com/pytorch/pytorch/pull/18001)).\r\n* `optim.lr_scheduler.CosineAnnealingWarmRestarts`: new scheduler: Stochastic Gradient Descent with Warm Restarts).  ([17226](https://github.com/pytorch/pytorch/pull/17226)).\r\n* Support multiple simultaneous LR schedulers.  ([14010](https://github.com/pytorch/pytorch/pull/14010))\r\n\r\n\r\n### Distributions\r\n* `torch.distributions`: now support multiple inheritance.  ([16772](https://github.com/pytorch/pytorch/pull/16772)).\r\n\r\n### Samplers\r\n* `quasirandom.SobolEngine`: new sampler.  ([10505](https://github.com/pytorch/pytorch/pull/10505)).\r\n\r\n### DistributedDataParallel\r\n* `nn.parallel.DistributedDataParallel`: now supports modules with unused parameters (e.g. control flow, like adaptive softmax, etc). ([18251](https://github.com/pytorch/pytorch/pull/18251), [18953](https://github.com/pytorch/pytorch/pull/18953)).\r\n\r\n### TorchScript and Tracer\r\n* Allow early returns from if-statements. ([#154463](https://github.com/pytorch/pytorch/pull/15463))\r\n* Add an `@ignore` annotation, which statically tells the TorchScript compiler to ignore the Python function. ([#16055](https://github.com/pytorch/pytorch/pull/16055))\r\n* Simple `for...in`  loops on lists. ([#16726](https://github.com/pytorch/pytorch/pull/16726))\r\n* Ellipses (`...`) in Tensor indexing. ([#17763](https://github.com/pytorch/pytorch/pull/17763))\r\n* `None` in Tensor indexing. ([#18615](https://github.com/pytorch/pytorch/pull/18615))\r\n* Support for basic list comprehensions. ([#17267](https://github.com/pytorch/pytorch/pull/17267))\r\n* Add implicit unwrapping of optionals on `if foo is not None`. ([#15587](https://github.com/pytorch/pytorch/pull/15587))\r\n* Tensors, ints, and floats will once again be implicitly cast to bool if used in a conditional. ([#18755](https://github.com/pytorch/pytorch/pull/18755)).\r\n* Implement `to()`, `cpu()`, and `cuda()` on ScriptModules. ([#15340](https://github.com/pytorch/pytorch/pull/15340) ,  [#15904](https://github.com/pytorch/pytorch/pull/15904))\r\n* Add support for various methods on lists: ([`clear()`](https://github.com/pytorch/pytorch/pull/17050), [`pop()`](https://github.com/pytorch/pytorch/pull/17015), [`reverse()`](https://github.com/pytorch/pytorch/pull/17001), [`copy()`](https://github.com/pytorch/pytorch/pull/17092) ,  [`extend()`](https://github.com/pytorch/pytorch/pull/17092),[`index()`](https://github.com/pytorch/pytorch/pull/17446), [`count()`](https://github.com/pytorch/pytorch/pull/17446), [`insert()`](https://github.com/pytorch/pytorch/pull/17200), [`remove()`](https://github.com/pytorch/pytorch/pull/17200) ).\r\n* Add support for `sort()` on lists of specialized type (`Tensors`, `int`, `float`, `bool`). ([#19572](https://github.com/pytorch/pytorch/pull/19572))\r\n* Add support for various methods on strings: ([`index()`](https://github.com/pytorch/pytorch/pull/18247), [`slice()`](https://github.com/pytorch/pytorch/pull/18247), [`len()`](https://github.com/pytorch/pytorch/pull/19320))\r\n* Support `Tensor.to()` in TorchScript. ( [#15976](https://github.com/pytorch/pytorch/pull/15976) )\r\n* Support for `Torch.tensor()` in TorchScript. ([#14913](https://github.com/pytorch/pytorch/pull/14913),  [#19445](https://github.com/pytorch/pytorch/pull/19445))\r\n* Support for `torch.manual_seed()` in TorchScript. ([#19510](https://github.com/pytorch/pytorch/pull/19510))\r\n* Support for `nn.LSTM` in TorchScript. ([#15744](https://github.com/pytorch/pytorch/pull/15744))\r\n* Support for `nn.init` in TorchScript. ([#19640](https://github.com/pytorch/pytorch/pull/19640))\r\n* Add `hash()` builtin. ([#18258](https://github.com/pytorch/pytorch/pull/18258))\r\n* Add `min()` and `max()` builtins for numerical types. ([#15680](https://github.com/pytorch/pytorch/pull/15680))\r\n* Add `isinstance()` builtin, which performs a static type check. ([#15076](https://github.com/pytorch/pytorch/pull/15076))\r\n* Add `train()` / `eval()` / `is_training()` to C++ ScriptModule API. ([#16044](https://github.com/pytorch/pytorch/pull/16044))\r\n* Allow List arguments to Python functions called from TorchScript. ([#15721](https://github.com/pytorch/pytorch/pull/19086))\r\n* Allow using `std::vector` and `std::unordered_map` as arguments to custom operators. ([#17587](https://github.com/pytorch/pytorch/pull/17587))\r\n* Tracer: now allows passing static dicts and lists as trace inputs. ([#18092](https://github.com/pytorch/pytorch/pull/18092), [#19580](https://github.com/pytorch/pytorch/pull/19580))\r\n* Allow generic containers as ScriptModule inputs. ([#16482](https://github.com/pytorch/pytorch/pull/16482))\r\n* Allow `nn.Sequential` in ModuleList. ([#16882](https://github.com/pytorch/pytorch/pull/16882))\r\n\r\n### Experimental Features\r\n* [Quantization] **(API unstable)**: added limited support for quantized datatypes via `torch.qint8` dtype, `torch.quantize_linear` conversion function.  ([18230](https://github.com/pytorch/pytorch/pull/18230)).\r\n* [MKLDNN tensor] **(API unstable)**: Added limited (opaque) support for `MKLDNN` tensors via `Tensor.to_mkldnn()`; operators are currently limited to ResNext101 operators.  ([17748](https://github.com/pytorch/pytorch/pull/17748)).\r\n\r\n## Improvements\r\n\r\n* `torch.min`, `torch.max`, `torch.median`, `torch.mode`, `torch.kthvalue`, `torch.symeig`, `torch.eig`, `torch.pstrf`, `torch.qr`, `torch.geqrf`, `torch.solve`, `torch.slogdet`, `torch.sort`, `torch.topk`, `torch.gels`, `torch.triangular_solve`, `torch.svd` now return namedtuples describing their outputs. ([16186](https://github.com/pytorch/pytorch/pull/16186), [16950](https://github.com/pytorch/pytorch/pull/16950), [17093](https://github.com/pytorch/pytorch/pull/17093), [17195](https://github.com/pytorch/pytorch/pull/17195), [15429](https://github.com/pytorch/pytorch/pull/15429)).\r\n* `torch.empty` (and other factory functions): now take a `pin_memory` kwarg; can now pin without going through `torch.Storage` interface..  ([18455](https://github.com/pytorch/pytorch/pull/18455)).\r\n* `torch.histc`: Now supported on CUDA.  ([15842](https://github.com/pytorch/pytorch/pull/15842))\r\n* `torch.unique`: Add `return_counts`.  ([18391](https://github.com/pytorch/pytorch/pull/18391), [18651](https://github.com/pytorch/pytorch/pull/18651)).\r\n* `torch.logspace`: add the ability to specify a `base`.  ([19542](https://github.com/pytorch/pytorch/pull/19542)).\r\n* `torch.set_printoptions`: added scientific notation support.  ([16876](https://github.com/pytorch/pytorch/pull/16876)).\r\n* `torch.btrifact` now handles tensors with greater than 3 dimensions.  ([14964](https://github.com/pytorch/pytorch/pull/14964)).\r\n* `torch.kthvalue`: now supported on CUDA.  ([17544](https://github.com/pytorch/pytorch/pull/17544)).\r\n* `torch.abs`: now supported on `uint8` and `int8` dtypes.  ([16893](https://github.com/pytorch/pytorch/pull/16893)).\r\n* `torch.stack`, `torch.cat`: now supported for CPU half tensors.  ([16389](https://github.com/pytorch/pytorch/pull/16389)).\r\n* `torch.cross`: added support for negative dimensions. ([17582](https://github.com/pytorch/pytorch/pull/17582)).\r\n* `torch.lerp`: add support for `weight` as a Tensor.  ([17348](https://github.com/pytorch/pytorch/pull/17348)).\r\n* `torch.transpose`: Made consistent with NumPy: 1-d and 0-d arrays are accepted and returned as-is.  ([17462](https://github.com/pytorch/pytorch/pull/17462), [17535](https://github.com/pytorch/pytorch/pull/17535)).\r\n* `torch.linspace`, `torch.logspace` can now be used with `steps=1` and `start != end`.  ([14748](https://github.com/pytorch/pytorch/pull/14748)).\r\n* `torch.cholesky`: changed the derivative from a triangular matrix to symmetric matrix.  ([19116](https://github.com/pytorch/pytorch/pull/19116)).\r\n* `torch.lerp`: Improved numerical stability.  ([18871](https://github.com/pytorch/pytorch/pull/18871)).\r\n* `torch.logdet`, `torch.slogdet`: improve numerical precision.  ([18449](https://github.com/pytorch/pytorch/pull/18449)).\r\n* `Tensor.__contains__` is now supported. ([17733](https://github.com/pytorch/pytorch/pull/17733)).\r\n* `Tensor.fill_` and `torch.zeros` now support half on CPU.  ([17536](https://github.com/pytorch/pytorch/pull/17536)).\r\n* `Tensor.resize_as_`, `Tensor.view`: now supported on half CPU tensors.  ([18821](https://github.com/pytorch/pytorch/pull/18821)).\r\n* `Tensor indexing`: allow indexing via NumPy booleans.  ([14932](https://github.com/pytorch/pytorch/pull/14932)).\r\n* `nn.EmbeddingBag`: enable half precision dense backward.  ([19293](https://github.com/pytorch/pytorch/pull/19293)).\r\n* `nn.Embedding`: fix dense Embedding to work with double backwards.  ([9078](https://github.com/pytorch/pytorch/pull/9078)).\r\n* `nn.MaxPool1d`: Allow list and tuples to be passed as `output_size`.  ([16489](https://github.com/pytorch/pytorch/pull/16489)).\r\n* `nn.CTCLoss`:  support zeroing infinite losses via `zero_infinity` argument.  ([16199](https://github.com/pytorch/pytorch/pull/16199)).\r\n* `nn.Dropout`: add support for enabling during eval.  ([17549](https://github.com/pytorch/pytorch/pull/17549)).\r\n* `nn.MSELoss`: add warning about unexpected broadcasting.  ([18349](https://github.com/pytorch/pytorch/pull/18349)).\r\n* `nn.Module.load_state_dict`: also return `missing_keys` and `unexpected_keys`.  ([18668](https://github.com/pytorch/pytorch/pull/18668)).\r\n* `nn.parallel.data_parallel`: Enforce devices match `device_ids`.  ([17129](https://github.com/pytorch/pytorch/pull/17129)).\r\n* `torch.device`: handle in more places that used to accept only device ordinals.  ([14929](https://github.com/pytorch/pytorch/pull/14929))\r\n* `dtype.int8` tensors can now be converted to NumPy arrays.  ([14710](https://github.com/pytorch/pytorch/pull/14710)).\r\n* `nn.functional.gumbel_softmax`: allow multidimensional input with `dim` argument.  ([13339](https://github.com/pytorch/pytorch/pull/13339)).\r\n* `nn.functional.cosine_similarity`: improved precision.  ([18250](https://github.com/pytorch/pytorch/pull/18250)).\r\n* `torch.autograd`: Don't keep unnecessary saved_inputs alive, increasing memory efficiency.  ([16583](https://github.com/pytorch/pytorch/pull/16583)).\r\n* `torch.autograd.profiler`: add Self (non-nested) CPU Time Total, CPU time total ([19378](https://github.com/pytorch/pytorch/pull/19378)).\r\n* `DataLoader`: support accepting a custom memory pinning function.  ([16743](https://github.com/pytorch/pytorch/pull/16743)).\r\n* `DataLoader`: retry libshm on EINTR.  ([15964](https://github.com/pytorch/pytorch/pull/15964)).\r\n* `DataLoader`: fixed an issue with `pin_memory` and `PackedSequence`.  ([18079](https://github.com/pytorch/pytorch/pull/18079))\r\n* `data.utils.collate`, `data.utils.pin_memory`: now preserve namedtuples.  ([16440](https://github.com/pytorch/pytorch/pull/16440))\r\n* Use `IndexError` instead of `RuntimeError` on many indexing error cases.  ([17049](https://github.com/pytorch/pytorch/pull/17049), [17114](https://github.com/pytorch/pytorch/pull/17114)).\r\n* Support indexing a `torch.float16` tensor on CPU.  ([17645](https://github.com/pytorch/pytorch/pull/17645)).\r\n* Add (limited) error checking in case of internal overlap on inplace operators.  ([19317](https://github.com/pytorch/pytorch/pull/19317), [17927](https://github.com/pytorch/pytorch/pull/17927)).\r\n* `utils.checkpoint.checkpoint`: support `None` as an argument to checkpoint function.  ([17969](https://github.com/pytorch/pytorch/pull/17969)).\r\n* `torch.autograd`: added more information for `one of the variables needed for gradient computation has been modified by an inplace operation` exception.  ([18523](https://github.com/pytorch/pytorch/pull/18523)).\r\n* `cuda.synchronize`: add a device argument.  ([19573](https://github.com/pytorch/pytorch/pull/19573)).\r\n* `cuda.reset_max_memory_*`: now supported.  ([15985](https://github.com/pytorch/pytorch/pull/15985)).\r\n* `distributions.Independent`:  can now calculate KL Divergence.  ([17681](https://github.com/pytorch/pytorch/pull/17681)).\r\n* `torch.distributed.new_group`: now supports overriding default backend. ([18595](https://github.com/pytorch/pytorch/pull/18595)).\r\n* `torch.distributed.init_process_group`: will now propagate timeout to underlying Store. ([16571](https://github.com/pytorch/pytorch/pull/16571)).\r\n* **[JIT]** Preserve module hierarchy on traced modules. ([#15101](https://github.com/pytorch/pytorch/pull/15101))\r\n* **[JIT]** Add metadata for TracedModules. ([#17311](https://github.com/pytorch/pytorch/pull/17311))\r\n* **[JIT]** Improve portability of int and float checks. ([#19532](https://github.com/pytorch/pytorch/pull/19532))\r\n* **[JIT]** Preserve method parameter names during serialization. ([#16750](https://github.com/pytorch/pytorch/pull/16750))\r\n* **[JIT]** Add a correctness check for C++ types to custom operators. ([#15247](https://github.com/pytorch/pytorch/pull/15247))\r\n* **[JIT]** Added a few extra python bindings to help with walking the IR graph from Python. [#17822](https://github.com/pytorch/pytorch/pull/17822) \r\n* **[JIT Error Messages]** Print out operator suggestions for \"unknown builtin op\" error. ([#15183](https://github.com/pytorch/pytorch/pull/15183))\r\n* **[JIT Error Messages]** Better error message when creating a module instance in TorchScript. ([#16416](https://github.com/pytorch/pytorch/pull/16416))\r\n* **[JIT Error Messages]** Print suggestion to add `nn.Module` attributes to `__constants__` when they are using in TorchScript. ([#18164](https://github.com/pytorch/pytorch/pull/18164))\r\n* **[JIT Error Messages]** `torch.save()`: Improve error message when you try to save a ScriptModule. ([#15321](https://github.com/pytorch/pytorch/pull/15321))\r\n* **[JIT Error Messages]** `torch.jit.save()`: Improve error message when trying to save a model with Python code. ([#16850](https://github.com/pytorch/pytorch/pull/16850))\r\n* **[JIT Error Messages]** Better errors when trying to close over a Tensor with grad enabled while tracing. ([#18298](https://github.com/pytorch/pytorch/pull/18298), [#19645](https://github.com/pytorch/pytorch/pull/19645))\r\n* **[JIT Error Messages]** Better error when trying to add a Tensor to `__constants__`. ([#16724](https://github.com/pytorch/pytorch/pull/16724))\r\n* **[JIT Error Messages]** Better error when a module list isn't added to `__constants__`. ([#17167](https://github.com/pytorch/pytorch/pull/17167)) \r\n* **[JIT Error Messages]** Add a warning when attempting to trace legacy constructors. ([#16770](https://github.com/pytorch/pytorch/pull/16770))\r\n* **[JIT Error Messages]** Improve hint when trying to trace non-deterministic nodes. ([#17957](https://github.com/pytorch/pytorch/pull/17957))\r\n* **[C++]** `nn::Module`: added Python interop.  ([13481](https://github.com/pytorch/pytorch/pull/13481)).\r\n* **[C++]** `autograd::profiler`: is now supported.  ([16580](https://github.com/pytorch/pytorch/pull/16580))\r\n* **[C++]** allow detection of C++ ABI flag for cpp extensions from available runtime information.  ([18994](https://github.com/pytorch/pytorch/pull/18994)).\r\n* **[C++]** `torch.argsort` is now supported in C++.  ([17099](https://github.com/pytorch/pytorch/pull/17099)).\r\n* **[C++]** `Tensor.isnan`: now supported in C++.  ([15722](https://github.com/pytorch/pytorch/pull/15722)).\r\n* **[C++]**: Added named submodule support to `nn::Sequential`.  ([17552](https://github.com/pytorch/pytorch/pull/17552)).\r\n* **[C++]**: Kaiming Initialization.  ([14718](https://github.com/pytorch/pytorch/pull/14718)).\r\n* **[C++]** `torch::data::transforms::Normalize`: now supported in C++.  ([15891](https://github.com/pytorch/pytorch/pull/15891)).\r\n* **[C++]**: Support call operator on module holder calling forward.  ([15831](https://github.com/pytorch/pytorch/pull/15831)).\r\n Random and Sequential distributed samplers.  ([16910](https://github.com/pytorch/pytorch/pull/16910)).\r\n* **[C++]**: pretty printing of C++ Modules.  ([15326](https://github.com/pytorch/pytorch/pull/15326)).\r\n* **[C++]** Support serializing `std::vector<torch::Tensor>`.  ([19677](https://github.com/pytorch/pytorch/pull/19677)).\r\n\r\n## Bug Fixes\r\n\r\n### Serious\r\n* `torch.prod`: correct erroneous calculation on large tensors.  ([15653](https://github.com/pytorch/pytorch/pull/15653)).\r\n* `torch.mean` (and other reductions): fix incorrect calculation on CUDA on large inputs.  ([16023](https://github.com/pytorch/pytorch/pull/16023)).\r\n* `nn.Conv`: correctly handle non-contiguous inputs on MKLDNN convolution codepath.  ([16300](https://github.com/pytorch/pytorch/pull/16300)).\r\n* `Tensor.eq_`:  Fix erroneous calculation.  ([15475](https://github.com/pytorch/pytorch/pull/15475)).\r\n* `torch.mean`: Fix fp16 output calculation.  ([14878](https://github.com/pytorch/pytorch/pull/14878)).\r\n* `nn.PoissonNLLLoss`:  Properly handle `reduction=None`.  ([17358](https://github.com/pytorch/pytorch/pull/17358)).\r\n* **[JIT]** Fix bug where custom ops could get optimized out if their outputs weren't used. ([#18711](https://github.com/pytorch/pytorch/pull/18711)).\r\n* **[JIT]** Fix bug where the model serializer would accidentally reorder statements. ([#17557](https://github.com/pytorch/pytorch/pull/17557)).\r\n\r\n### Other\r\n* `Tensor.round` is now consistently half to even.  ([17443](https://github.com/pytorch/pytorch/pull/17443)).\r\n* `Tensor.resize_`: Fix some 0-element cases.  ([14874](https://github.com/pytorch/pytorch/pull/14874)).\r\n* `Tensor.numpy`: Fix conversion of `torch.int8` dtype.  ([15194](https://github.com/pytorch/pytorch/pull/15194)).\r\n* `Tensor.grad`: correctly handle `del`.  ([16525](https://github.com/pytorch/pytorch/pull/16525)).\r\n* `Tensor.clamp`: correctly handle NaN on CUDA.  ([15479](https://github.com/pytorch/pytorch/pull/15479)).\r\n* `Tensor.topk`: properly set launch bounds on CUDA.  ([17296](https://github.com/pytorch/pytorch/pull/17296)).\r\n* `Tensor.kthvalue`: treat NaN as bigger than any number.  ([17824](https://github.com/pytorch/pytorch/pull/17824)).\r\n* `Tensor.copy_`: Properly synchronize on src and dst sreams.  ([16966](https://github.com/pytorch/pytorch/pull/16966)).\r\n* `Tensor indexing`: Fix incorrect dimension error message.  ([16495](https://github.com/pytorch/pytorch/pull/16495)).\r\n* `Tensor.coalesce`, `Tensor.clone`, `Tensor.to_dense`: fixed for sparse 0-dimensional tensors.  ([17379](https://github.com/pytorch/pytorch/pull/17379)).\r\n* `torch.isinf`: Don't error out on integral tensors.  ([15489](https://github.com/pytorch/pytorch/pull/15489)).\r\n* `torch.argsort`, `torch.sort`: Match NumPy by considering NaNs to be larger than any number.  ([15886](https://github.com/pytorch/pytorch/pull/15886)).\r\n* `torch.geqrf`, `torch.ormqr`: when an `out` parameter is specified, dispatch to the correct function.  ([16964](https://github.com/pytorch/pytorch/pull/16964)).\r\n* `torch.cuda.get_device_name` / `torch.cuda.get_device_capability`: Fix handling of optional.  ([17222](https://github.com/pytorch/pytorch/pull/17222)).\r\n* `Tensor.tril_` / `Tensor.triu_`: properly reuse input memory.  ([17031](https://github.com/pytorch/pytorch/pull/17031)).\r\n* `torch.arange`: fix shape inconsistency between CPU and CUDA.  ([18462](https://github.com/pytorch/pytorch/pull/18462)).\r\n* `torch.empty` (and other size-based factory functions): properly enforce non-negative sizes.  ([17077](https://github.com/pytorch/pytorch/pull/17077)).\r\n* `torch.load`: support serializing / deserializing `pathlib.Path` object.  ([18562](https://github.com/pytorch/pytorch/pull/18562)).\r\n* `nn.BatchNorm`: correctly handle very large batches.  ([17047](https://github.com/pytorch/pytorch/pull/17047)).\r\n* `nn.Softmax` / `nn.LogSoftmax`: fix double backward for `torch.half`.  ([17330](https://github.com/pytorch/pytorch/pull/17330)).\r\n* `nn.Softmax`: handle empty inputs in backward.  ([17259](https://github.com/pytorch/pytorch/pull/17259)).\r\n* `nn.NLLLoss`: Fix crash when `ignore_index` is out-of-bounds on CPU.  ([17328](https://github.com/pytorch/pytorch/pull/17328)).\r\n* `nn.Softmax`, `nn.LogSoftmax`: handle 0-element inputs.  ([17651](https://github.com/pytorch/pytorch/pull/17651)).\r\n* `nn.CTCLoss`: correct error checking.  ([16269](https://github.com/pytorch/pytorch/pull/16269)).\r\n* `nn.Conv`: better report convolution size mismatch.  ([17436](https://github.com/pytorch/pytorch/pull/17436)).\r\n* `torch.nn.functional.cosine_similarity`: fix output sometimes returning result > 1.0.  ([18168](https://github.com/pytorch/pytorch/pull/18168)).\r\n* `nn.parallel.data_parallel`: Fix handling of buffers that require_grad.  ([13352](https://github.com/pytorch/pytorch/pull/13352)).\r\n* `nn.parallel.data_parallel`: would previously sometimes frees tensors before all pending operations finish. ([18465](https://github.com/pytorch/pytorch/pull/18465)).\r\n* `torch.distributed.broadcast`: fixed repeated calls leading to OOM. ([19219](https://github.com/pytorch/pytorch/pull/19219)).\r\n* `torch.multiprocessing`: fix serialization of integer `nn.Parameters`.  ([18639](https://github.com/pytorch/pytorch/pull/18639)).\r\n* `torch.multiprocessing`: Fix handling of `distributions` on CUDA.  ([16854](https://github.com/pytorch/pytorch/pull/16854)).\r\n* `torch.nonzero`: Fix for 0-dimensional tensors on CUDA.  ([17406](https://github.com/pytorch/pytorch/pull/17406)).\r\n* `torch.slogdet`: Fix `sign` requiring grad when `input` required grad.  ([16337](https://github.com/pytorch/pytorch/pull/16337)).\r\n* `torch.cuda.Stream`: Properly restore stream on destination device when switching devices.  ([17439](https://github.com/pytorch/pytorch/pull/17439)).\r\n* `torch.cuda.Stream`: Fixed synchronization issue when used with non-current device.  ([15689](https://github.com/pytorch/pytorch/pull/15689)).\r\n* `torch.cuda.Stream`: properly change device in stream context manager.  ([16128](https://github.com/pytorch/pytorch/pull/16128)).\r\n* `DataLoader`: fixed a hang when no data was read and the buffer size is smaller than the chunk size.  ([17409](https://github.com/pytorch/pytorch/pull/17409)).\r\n* `DataLoader`: `_utils.collate.default_collate` now converts bool lists to byte Tensors, not integer tensors. \r\n ([14669](https://github.com/pytorch/pytorch/pull/14669)).\r\n* `DataLoader`: ensure dataset is indexed by integers.  ([17649](https://github.com/pytorch/pytorch/pull/17649)).\r\n* `torch.sparse.mm`:  Handle transposed dense tensors in backwards.  ([18737](https://github.com/pytorch/pytorch/pull/18737)).\r\n* `torch.sparse.sum`: Fix parsing of `dim`.  ([16517](https://github.com/pytorch/pytorch/pull/16517)).\r\n* `torch.sparse.mm` / `torch.sparse.addmm`: fix broadcasting and using uninitialized data.  ([16572](https://github.com/pytorch/pytorch/pull/16572)).\r\n* `Tensor.to_sparse`: Fix for 0-dimensional tensors.  ([17406](https://github.com/pytorch/pytorch/pull/17406)).\r\n* `SparseTensor`: fix add with non-contiguous `values` tensors.  ([18179](https://github.com/pytorch/pytorch/pull/18179)).\r\n* Fix `compare_exchange_weak` in `weak_intrusive_ptr`.  ([16302](https://github.com/pytorch/pytorch/pull/16302)).\r\n* `utils.model_zoo.load_url`: Fix race condition.  ([16578](https://github.com/pytorch/pytorch/pull/16578)).\r\n* `utils.data.RandomSampler`: have `len` properly take into account `num_samples`.  ([15991](https://github.com/pytorch/pytorch/pull/15991)).\r\n* `torch.distributions`:  Fix precision issue with expansion that prefers `probs` over `logits`.  ([18614](https://github.com/pytorch/pytorch/pull/18614)).\r\n* `distributions.dirichlet.Dirichlet`: fixed an underflow issue.  ([17488](https://github.com/pytorch/pytorch/pull/17488)).\r\n* `distributions.binomial.Binomial.log_prob`: fixed numerical stability issue.  ([15962](https://github.com/pytorch/pytorch/pull/15962)).\r\n* `Caching Allocator`: Free all blocks with outstanding events on OOM-retry.  ([19222](https://github.com/pytorch/pytorch/pull/19222)).\r\n* `torch.dtype`: fix pickling issue with Python 2.  ([18045](https://github.com/pytorch/pytorch/pull/18045)).\r\n* `utils.data.DataLoader`: Fix SIGCHLD checking.  ([19421](https://github.com/pytorch/pytorch/pull/19421)).\r\n* `optim.Optimizer`: Properly copy defaults.  ([19308](https://github.com/pytorch/pytorch/pull/19308)).\r\n* `optim.lr_scheduler.CosineAnnealingLR`: Fix division-by-zero error.  ([19180](https://github.com/pytorch/pytorch/pull/19180)).\r\n* `optim.lr_scheduler.ReduceLROnPlateau`: fix bug when the argument to `step` is reused outside the function. \r\n  ([16697](https://github.com/pytorch/pytorch/pull/16697)).\r\n* `cudNN`: fix race condition with multiple threads calling into the same device.  ([15080](https://github.com/pytorch/pytorch/pull/15080)).\r\n* `cudNN`: Properly specify accumulation types.  ([16825](https://github.com/pytorch/pytorch/pull/16825)).\r\n* `cuDNN`: Fix incorrectly selecting slower algorithms in certain cases.  ([15881](https://github.com/pytorch/pytorch/pull/15881)).\r\n* `cuFFT`:  Properly handle CUDA contexts.  ([19300](https://github.com/pytorch/pytorch/pull/19300))\r\n* Fix infinite loop in reduction functions when get_max_threads is nonzero but num_threads is 1.  ([15114](https://github.com/pytorch/pytorch/pull/15114)).\r\n* Fix tensor printing bug with Python 2.  ([12732](https://github.com/pytorch/pytorch/pull/12732)).\r\n* `MKLDNN`: fix thread safety.  ([17022](https://github.com/pytorch/pytorch/pull/17022)).\r\n* **[JIT]** `floordiv`: Fix integer division and divide-by-zero semantics. ([#15813](https://github.com/pytorch/pytorch/pull/15813)).\r\n* **[JIT]** Fix bug in alias analysis that disabled optimizations even in models without mutation. ([#18416](https://github.com/pytorch/pytorch/pull/18146)).\r\n* **[JIT]** `ord()`: Fix handling of utf8 chars. ([#19423](https://github.com/pytorch/pytorch/pull/19423)).\r\n* **[JIT]** Fix error when too many parameters are passed to a fused CUDA kernel. ([#18063](https://github.com/pytorch/pytorch/pull/18063)).\r\n* **[JIT]** Fix bug where common subexpression elimination accidentally introduced aliasing to function outputs. ([#19576](https://github.com/pytorch/pytorch/pull/19576)).\r\n* **[JIT]** Fix infinite loop in `requires_grad` analysis pass. ([#18361](https://github.com/pytorch/pytorch/pull/18361)).\r\n* **[JIT]** Fix ordering of parameters for in `rnn.py`. ([#18198](https://github.com/pytorch/pytorch/pull/18198)).\r\n* **[JIT]]** Fix contiguous autodiff and AutoGradZero inconsistency ([#18633](https://github.com/pytorch/pytorch/pull/18633)).\r\n* **[JIT]** Fix error reporting in NVRTC use of the fuser. ([#18327](https://github.com/pytorch/pytorch/pull/18327)).\r\n* **[JIT]** Ensure GIL is acquired before doing module lookup on import. ([#17135](https://github.com/pytorch/pytorch/pull/17135)).\r\n* **[JIT]** Fix bug where `_unique_state_dict` could contain duplicate Tensors. ([#18139](https://github.com/pytorch/pytorch/pull/18139)).\r\n* **[C++]**: Fix module serialization issue where one submodule doesn't have any parameters, but its submodules do.  ([15033](https://github.com/pytorch/pytorch/pull/15033)).\r\n* **[C++]**: Add `Stream` and `Event` APIs.  ([15937](https://github.com/pytorch/pytorch/pull/15937)).\r\n* **[C++]**: Fix Module serialization incompatibility between Python and C++ with weight-less layers.  ([19740](https://github.com/pytorch/pytorch/pull/19740)).\r\n* **[C++]**: Properly pass `extra_cuda_cflags` to C++ extensions on Windows.  ([18638](https://github.com/pytorch/pytorch/pull/18638)).\r\n* **[C++]** Make SGD semantics match python.  ([15840](https://github.com/pytorch/pytorch/pull/15840)).\r\n* **[C++]** `torch::nn::init::orthogonal_`: match Python API.  ([18915](https://github.com/pytorch/pytorch/pull/18915)).\r\n\r\n## Deprecations\r\n* `torch.btrifact`: the deprecated `info` argument has been removed.  ([14935](https://github.com/pytorch/pytorch/pull/14935)).\r\n* `torch.potrs` has been deprecated, use `torch.cholesky_solve` instead.  Note that `upper` defaults to `False`  for `torch.cholesky_solve`, and `True` for `torch.potrs`.  ([15334](https://github.com/pytorch/pytorch/pull/15334)).\r\n* `torch.pstrf` is deprecated; use `torch.cholesky` instead.  Note that `upper` defaults to `False`  for `torch.cholesky`, and `True` for `torch.pstrf`.  ([17866](https://github.com/pytorch/pytorch/pull/17866)).\r\n* `torch.potri` is deprecated; use `torch.cholesky_inverse` instead.  Note that `upper` defaults to `False`  for `torch.cholesky_inverse`, and `True` for `torch.potri`.  ([19498](https://github.com/pytorch/pytorch/pull/19498)).\r\n* `torch.btrifact_with_info` has been deprecated; use `torch.lu` with `get_infos=True` instead.([18435](https://github.com/pytorch/pytorch/pull/18435)).\r\n* `torch.btrifact` has been deprecated; use the new name `torch.lu` instead.  ([18435](https://github.com/pytorch/pytorch/pull/18435)).\r\n* `torch.gesv` is deprecated; use the new name `torch.solve instead.  ([18060](https://github.com/pytorch/pytorch/pull/18060)).\r\n* `torch.trtrs` has been deprecated; use the new name `torch.triangular_solve` instead.  ([18213](https://github.com/pytorch/pytorch/pull/18213)).\r\n* `torch. btriunpack` has been deprecated; use the new name `torch.lu_unpack ` instead.  ([18529](https://github.com/pytorch/pytorch/pull/18529)).\r\n* `torch.btrisolve` has been deprecated; use the new name `torch.lu_solve` instead.  ([18726](https://github.com/pytorch/pytorch/pull/18726)).\r\n* **[C++]** `IntList` has been deprecated, use `IntArrayRef` instead, as it better describes the type and ownership semantics in C++.  ([16751](https://github.com/pytorch/pytorch/pull/16751)).\r\n*  **[C++]** Dispatch macros with `Type` parameters, e.g. `AT_DISPATCH_ALL_TYPES(tensor.type(), ...`, are now deprecated; use `ScalarType` instead, e.g. `AT_DISPATCH_ALL_TYPES(tensor.scalar_type(), ...`.  ([17527](https://github.com/pytorch/pytorch/pull/17527), [17996](https://github.com/pytorch/pytorch/pull/17996)).\r\n* **[C++]** the deprecated `variable_tensor_functions` have been removed.  ([15003](https://github.com/pytorch/pytorch/pull/15003)).\r\n\r\n## Performance \r\n\r\n### Highlights\r\n* `nn.BatchNorm` CPU inference speed increased up to ~19x.([19152](https://github.com/pytorch/pytorch/pull/19152)).\r\n* `nn.AdaptiveAvgPool`: speed up common-case of size=1 output by ~30x.  ([17011](https://github.com/pytorch/pytorch/pull/17011)).\r\n* `nn.EmbeddingBag` CPU performance increased by ~4x.  ([19329](https://github.com/pytorch/pytorch/pull/19329)).\r\n* `Tensor.copy_`: sped up larger tensor copy ~2-3x, small regression in small tensor copy.  ([18618](https://github.com/pytorch/pytorch/pull/18618)).\r\n* `torch.nonzero`: is now ~2x faster than numpy on CPU.  ([15190](https://github.com/pytorch/pytorch/pull/15190))\r\n* Improve caching allocator for Pascal and newer GPUs; 10-20% better memory utilization on Mask-RCNN.  ([17120](https://github.com/pytorch/pytorch/pull/17120)).\r\n* `reduction functions`: Speed up some large Tensor cases by 50-80%.  ([17428](https://github.com/pytorch/pytorch/pull/17428)).\r\n* **[JIT]** Graph fuser: better fusion for backwards graphs in the presence of broadcasting. ([#14957](https://github.com/pytorch/pytorch/pull/14957))\r\n* **[JIT]** Graph fuser: `batch_norm` fusion for inference. ([#15146](https://github.com/pytorch/pytorch/pull/15146))\r\n* **[JIT]** Graph fuser: `layer_norm` fusion for inference. ([#18266](https://github.com/pytorch/pytorch/pull/18266))\r\n\r\n\r\n### Other\r\n\r\n* `torch.abs`, `torch.frac`, `torch.repiprocal`, `torch.neg` have been vectorized and parallelized ([19041](https://github.com/pytorch/pytorch/pull/19041)).\r\n* `torch.bmm`: CPU performance increased by 2x.  ([19338](https://github.com/pytorch/pytorch/pull/19338)).\r\n* `torch.sort`: CUDA performance increased by ~2x.  ([19379](https://github.com/pytorch/pytorch/pull/19379)).\r\n* `torch.cat` on CPU is now ~4x faster in the case where inputs are contiguous and `dim` != 0.  ([17032](https://github.com/pytorch/pytorch/pull/17032)).\r\n* `torch.multinomial` fixed a 2x performance regression.  ([17121](https://github.com/pytorch/pytorch/pull/17121)).\r\n* `torch.empty` (and another factory functions): reduce overhead by 20-40%.  ([17565](https://github.com/pytorch/pytorch/pull/17565)).\r\n* `torch.linspace` has been parallelized on CPU.  ([15320](https://github.com/pytorch/pytorch/pull/15320)).\r\n* `torch.logspace` has been parallelized on CPU.  ([15438](https://github.com/pytorch/pytorch/pull/15438)).\r\n* `torch.range` has been parallelized on CPU.  ([15484](https://github.com/pytorch/pytorch/pull/15484)).\r\n* `torch.arange` has been parallelized on CPU.  ([15667](https://github.com/pytorch/pytorch/pull/15667)).\r\n* `torch.load`: avoid unnecessary CPU-to-CUDA copy.  ([17297](https://github.com/pytorch/pytorch/pull/17297)).\r\n* `reduction functions`: improve efficiency on CUDA.  ([16224](https://github.com/pytorch/pytorch/pull/16224), [17040](https://github.com/pytorch/pytorch/pull/17040)).\r\n* Speed up some GEMM cases on CPU by up to 7x.([17730](https://github.com/pytorch/pytorch/pull/17730))\r\n* Tensor iterator loop unrolling.  ([17667](https://github.com/pytorch/pytorch/pull/17667)).\r\n* `sparse/dense matrix multiply`: improve speed by ~5x.  ([16905](https://github.com/pytorch/pytorch/pull/16905)).\r\n* `distributions.MultivariateNormal`: sped up.  ([17294](https://github.com/pytorch/pytorch/pull/17294)).\r\n* **[JIT]** Graph fuser: pow scalar exponent / base autodiff, fusion ([#19324](https://github.com/pytorch/pytorch/pull/19324))\r\n* **[JIT]** Graph fuser: allow fusion of function float arguments. ([#18087](https://github.com/pytorch/pytorch/pull/18087))\r\n* **[JIT]** Shape analysis: specialize optional Tensor inputs to graphs. ([#18360](https://github.com/pytorch/pytorch/pull/18360))\r\n* **[JIT]** Shape analysis: various correctness improvements. ([#18271](https://github.com/pytorch/pytorch/pull/18271))\r\n* **[JIT]** Shape analysis: `aten::_convolution` now participates in shape analysis. ([#16837](https://github.com/pytorch/pytorch/pull/16837)]\r\n* **[JIT]** Autodiff: coverage for ops used in maskrcnn & BERT. ([#16689](https\ufffc://github.com/pytorch/pytorch/pull/16689))\r\n* **[JIT]** Autodiff: support for scalar comparison ops and `randlike`. ([#14740](https://github.com/pytorch/pytorch/pull/14740))\r\n* **[JIT]** Autodiff: support for `adaptive_avg_pool2d`. ([#15459](https://github.com/pytorch/pytorch/pull/15459))\r\n* **[JIT]** Autodiff: support for `erf` and `erfc`. ([#15139](https://github.com/pytorch/pytorch/pull/15139))\r\n* **[JIT]** Autodiff: support for `layernorm`. ([#17702](https://github.com/pytorch/pytorch/pull/17702))\r\n* **[JIT]** Autodiff: support for `tanh`. ([#17816](https://github.com/pytorch/pytorch/pull/17816))\r\n* **[JIT]** Autodiff: support for `matmul`/`dropout`. ([#17523](https://github.com/pytorch/pytorch/pull/17523))\r\n* **[JIT]** Autodiff: specialized CUDA impl for dropout. ([#17756](https://github.com/pytorch/pytorch/pull/17756))\r\n* **[JIT]** Constant folding: improved inlining of control flow. ([#16244](https://github.com/pytorch/pytorch/pull/16244))\r\n\r\n## Documentation\r\n\r\n* `Tensor.scatter_`: add documentation about `value` parameter.  ([17467](https://github.com/pytorch/pytorch/pull/17467)).\r\n* `Tensor.unfold`: correctly document `dimension` parameter, not `dim`.  ([19020](https://github.com/pytorch/pytorch/pull/19020)).\r\n* `Tensor.is_floating_point()` is now documented.  ([15704](https://github.com/pytorch/pytorch/pull/15704)).\r\n* `torch.cholesky`: Fix broken `upper` example in documentation.  ([15215](https://github.com/pytorch/pytorch/pull/15215)).\r\n* `torch.gesv`: document `out` parameter.  ([15649](https://github.com/pytorch/pytorch/pull/15649)).\r\n* `torch.mul`: better explain elementwise multiplication.  ([15664](https://github.com/pytorch/pytorch/pull/15664)).\r\n* `torch.eig`, `torch.symeig`: better explain backwards limitations.  ([15929](https://github.com/pytorch/pytorch/pull/15929)).\r\n* `torch.ormqr`: fixed output specification.  ([15694](https://github.com/pytorch/pytorch/pull/15694)).\r\n* `torch.from_numpy`: replaced usage with `torch.as_tensor` in documentation.  ([16587](https://github.com/pytorch/pytorch/pull/16587)).\r\n* `torch.mvlgamma`: Fix the constant in the docs.  ([17045](https://github.com/pytorch/pytorch/pull/17045)).\r\n* `torch.mode`: more precisely describe what is returned.  ([17069](https://github.com/pytorch/pytorch/pull/17069)).\r\n* `torch.upsample`: documentation now matches `torch.interpolate`.  ([17134](https://github.com/pytorch/pytorch/pull/17134))\r\n* `torch.arange`: correct `dtype` documentation.  ([18604](https://github.com/pytorch/pytorch/pull/18604))\r\n* `torch.cumprod`: document `out` parameter.  ([19340](https://github.com/pytorch/pytorch/pull/19340)).\r\n* `torch.nonzero`: document indices being returned lexicographically.  ([19539](https://github.com/pytorch/pytorch/pull/19539)).\r\n* `torch.nn.functional.interpolate`: better explain `aligned_corners` parameter.  ([14806](https://github.com/pytorch/pytorch/pull/14806)).\r\n* `torch.nn.functional.pad`: documentation has been made consistent with other functional ops.  ([15984](https://github.com/pytorch/pytorch/pull/15984)).\r\n* `nn.functional.grid_sample`: clarify behavior of padding.  ([19754](https://github.com/pytorch/pytorch/pull/19754)).\r\n* `nn.TripletMarginLoss`: correct type of `swap` parameter.  ([18115](https://github.com/pytorch/pytorch/pull/18115)).\r\n* `nn.CrossEntropyLoss`: clarify `ignore_index` documentation.  ([18117](https://github.com/pytorch/pytorch/pull/18117)).\r\n* `nn.CrossEntropyLoss`: the input format is more clearly explained.  ([15990](https://github.com/pytorch/pytorch/pull/15990)).\r\n* `nn.CTCLoss`: Clarify a number of ambiguities.  ([18415](https://github.com/pytorch/pytorch/pull/18415)).\r\n* `nn.BCEWithLogitsLoss`: add better explanation.  ([19212](https://github.com/pytorch/pytorch/pull/19212)).\r\n* `nn.BCEWithLogitsLoss`: better explain positive samples.  ([17258](https://github.com/pytorch/pytorch/pull/17258)).\r\n* `nn.ModuleList` / `nn.ParameterList`: update documentation.  ([17731](https://github.com/pytorch/pytorch/pull/17731)).\r\n* `nn.Module.load_state_dict`: correct semantics of `strict`.  ([17618](https://github.com/pytorch/pytorch/pull/17618))\r\n* `nn.parallel.DataParallel`: more accurately specify how different argument types are handled.  ([15993](https://github.com/pytorch/pytorch/pull/15993)).\r\n* `nn.parallel.DistributedDataParallel`: Clarified batch size requirements.  ([16010](https://github.com/pytorch/pytorch/pull/16010)).\r\n* `torch.distributed`: Document mixed-precision training.  ([15440](https://github.com/pytorch/pytorch/pull/15440)).\r\n* `torch.multiprocessing`: Include example multiprocessing code.  ([16345](https://github.com/pytorch/pytorch/pull/16345)).\r\n* `torch.autograd`: Better explain computing Jacobian-vector product.  ([15197](https://github.com/pytorch/pytorch/pull/15197)).\r\n* `torch.cuda.get_rng_state`, `torch.cuda.set_rng_state`: document taking a `device` object.  ([14324](https://github.com/pytorch/pytorch/pull/14324)).\r\n* `torch.device`: Fix example of passing `device` to tensor factory.  ([16839](https://github.com/pytorch/pytorch/pull/16839)).\r\n* `DataLoader`: update documentation to describe how workers are managed.  ([18091](https://github.com/pytorch/pytorch/pull/18091)).\r\n* Unified shape formats throughout the documentation.  ([15741](https://github.com/pytorch/pytorch/pull/15741)).\r\n* Update documentation for `reduction` arguments to use non-deprecated format.  ([17300](https://github.com/pytorch/pytorch/pull/17300)).\r\n* `mark_non_differentiable`: document correct semantics.  ([17891](https://github.com/pytorch/pytorch/pull/17891)).\r\n* Warn about memory overlaps on inplace operations.  ([17576](https://github.com/pytorch/pytorch/pull/17576)).\r\n* Fix a number of small issues with conv and pooling docstrings.  ([17052](https://github.com/pytorch/pytorch/pull/17052)).\r\n* Fix a number of small issues with padding and activation docstrings.  ([17197](https://github.com/pytorch/pytorch/pull/17197)).\r\n* **[C++]**: mention packed accessors in Tensor basics.  ([19464](https://github.com/pytorch/pytorch/pull/19464)).\r\n\r\n## ONNX\r\n\r\n### Exporting More Torch Operators to ONNX\r\n\r\n* Export torch.isnan to ONNX ([17698](https://github.com/pytorch/pytorch/pull/17698)).\r\n* Export torch.flatten to ONNX ([16240](https://github.com/pytorch/pytorch/pull/16240)).\r\n* Export torch.where, torch.ceil, torch.floor to ONNX ([18571](https://github.com/pytorch/pytorch/pull/18571)).\r\n* Export torch.narrow to ONNX ([17550](https://github.com/pytorch/pytorch/pull/17550)).\r\n* Export torch.argmax and torch torch.argmin ([17382](https://github.com/pytorch/pytorch/pull/17382), [18264](https://github.com/pytorch/pytorch/pull/18264), [18261](https://github.com/pytorch/pytorch/pull/18261)).\r\n* Export adaptive_avg_pool1D, adaptive_avg_pool2D, adaptive_avg_pool3D, adaptive_max_pool1D, adaptive_max_pool2D, adaptive_max_pool3D to ONNX ([17412](https://github.com/pytorch/pytorch/pull/17412)).\r\n* Export torch.nonzero to ONNX ([17036](https://github.com/pytorch/pytorch/pull/17036), [18047](https://github.com/pytorch/pytorch/pull/18047)).\r\n* Export torch.erf to ONNX ([16106](https://github.com/pytorch/pytorch/pull/16106)).\r\n* Export torch.split ([15092](https://github.com/pytorch/pytorch/pull/15092)).\r\n* Export torch.lt, torch.gt, torch.le, torch.ge, torch.eq, torch.ne to ONNX ([15677](https://github.com/pytorch/pytorch/pull/15677)).\r\n* Export torch.expand and torch.ne to ONNX ([15050](https://github.com/pytorch/pytorch/pull/15050)).\r\n* Export torch.nn.LogSigmoid to ONNX ([14830](https://github.com/pytorch/pytorch/pull/14830)).\r\n* Export torch.nn.RReLU to ONNX ([14781](https://github.com/pytorch/pytorch/pull/14781)).\r\n* Export torch.reshape and torch.reshape_as to ONNX ([16632](https://github.com/pytorch/pytorch/pull/16632), [16971](https://github.com/pytorch/pytorch/pull/16971)).\r\n* Replace use of ConstantLike with with ConstantOfShape ([16095](https://github.com/pytorch/pytorch/pull/16095), [16214](https://github.com/pytorch/pytorch/pull/16214)).\r\n\r\n### Extending Existing Exporting Logic\r\n\r\n* Enable dim support in torch.nn.Softmax's export ([18482](https://github.com/pytorch/pytorch/pull/18482)).\r\n* Support exporting squeeze & unsqueeze with negative dim attribute ([19297](https://github.com/pytorch/pytorch/pull/19297)).\r\n* Support exporting max_pool1d, max_pool2d, max_pool3d with indices ([16455](https://github.com/pytorch/pytorch/pull/16455)).\r\n* Add dtype support in torch.logsoftmax and torch.softmax's export ([17672](https://github.com/pytorch/pytorch/pull/17672)).\r\n* Support ceil_mode in max_pool_1d, max_pool2d, max_pool3d, avg_pool1d, avg_pool2d, avg_pool3d's export ([16769](https://github.com/pytorch/pytorch/pull/16769)).\r\n\r\n### Optimizing Exported ONNX Graph\r\n\r\n* Add constant folding in ONNX exporter ([18698](https://github.com/pytorch/pytorch/pull/18698)).\r\n* Retain the parameter names in ONNX exporter ([17551](https://github.com/pytorch/pytorch/pull/17551)).\r\n* Omit slice op if it is a non-op ([19155](https://github.com/pytorch/pytorch/pull/19155)).\r\n* Add a flag to strip doc_string from exported ONNX models ([18882](https://github.com/pytorch/pytorch/pull/18882)).\r\n* Omit torch.dropout if the model is in eval mode ([16547](https://github.com/pytorch/pytorch/pull/16547)).\r\n\r\n### Adding Utility Functions and Refactoring\r\n\r\n* Remove unused arg f from _model_to_graph(). ([19647](https://github.com/pytorch/pytorch/pull/19647)).\r\n* Add the support for stable ONNX opsets in exporter ([16068](https://github.com/pytorch/pytorch/pull/16068), [17419](https://github.com/pytorch/pytorch/pull/17419)).\r\n* Set the default ONNX opset to the latest stable opset (i.e., 9) ([17736](https://github.com/pytorch/pytorch/pull/17736)).\r\n* Add an utility function to check whether it's in the middle of ONNX export or not ([19050](https://github.com/pytorch/pytorch/pull/19050)).\r\n* Refactoring serialization of ONNX initializers to be name-based ([17830](https://github.com/pytorch/pytorch/pull/17830)).\r\n* Expose dim() on type and use it in ONNX symbolics ([15933](https://github.com/pytorch/pytorch/pull/15933)).\r\n* Add scalar_type_to_pytorch_type dict in ONNX symbolic ([15965](https://github.com/pytorch/pytorch/pull/15965)).\r\n* Add an assertion to check the number of the parameters passed to ONNX exporter ([18145](https://github.com/pytorch/pytorch/pull/18145)).\r\n\r\n### Bugfixes\r\n\r\n* Fix different types in rsub caused bug ([15707](https://github.com/pytorch/pytorch/pull/15707)).\r\n* Fix list structure supports in ONNX exporter ([19102](https://github.com/pytorch/pytorch/pull/19102)).\r\n* Fix case for `activations` attribute in nn.RNN ONNX export. ([19368](https://github.com/pytorch/pytorch/pull/19368)).\r\n* Minor fix for onnx ConstantOfShape export ([18199](https://github.com/pytorch/pytorch/pull/18199)).\r\n* Fix the torch.(reduce)min and torch.(reduce)max's export ([15241](https://github.com/pytorch/pytorch/pull/15241)).\r\n* Fixing ONNX export of logical ops to have correct output datatype ([15185](https://github.com/pytorch/pytorch/pull/15185)).\r\n* Fix typo in docstring ([18216](https://github.com/pytorch/pytorch/pull/18216)).\r\n", "tarball_url": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.1.0", "zipball_url": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.1.0", "html_url": "https://github.com/pytorch/pytorch/releases/tag/v1.1.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/17080796", "release_id": 17080796, "date_created": "2019-04-30T23:22:19Z", "date_published": "2019-05-01T00:09:03Z"}, "confidence": 1, "technique": "GitHub_API"}], "contributing_guidelines": [{"result": {"value": "Thank you for your interest in contributing to PyTorch!\nIf you're a new contributor, please first take a read through our\n[Contributing Guide](https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions), specifically the [Submitting a Change](https://github.com/pytorch/pytorch/wiki/The-Ultimate-Guide-to-PyTorch-Contributions#submitting-a-change) section\nthat walks through the process of contributing a change to PyTorch.\n\nThe rest of this document (CONTRIBUTING.md) covers some of the more technical\naspects of contributing to PyTorch.\n\n# Table of Contents\n\n<!-- toc -->\n\n- [Developing PyTorch](#developing-pytorch)\n  - [Tips and Debugging](#tips-and-debugging)\n- [Nightly Checkout & Pull](#nightly-checkout--pull)\n- [Codebase structure](#codebase-structure)\n- [Unit testing](#unit-testing)\n  - [Python Unit Testing](#python-unit-testing)\n  - [Better local unit tests with `pytest`](#better-local-unit-tests-with-pytest)\n  - [Local linting](#local-linting)\n    - [Running `mypy`](#running-mypy)\n  - [C++ Unit Testing](#c-unit-testing)\n  - [Run Specific CI Jobs](#run-specific-ci-jobs)\n- [Merging your Change](#merging-your-change)\n- [Writing documentation](#writing-documentation)\n  - [Docstring type formatting](#docstring-type-formatting)\n  - [Building documentation](#building-documentation)\n    - [Tips](#tips)\n    - [Building C++ Documentation](#building-c-documentation)\n  - [Previewing changes locally](#previewing-changes-locally)\n  - [Previewing documentation on PRs](#previewing-documentation-on-prs)\n  - [Adding documentation tests](#adding-documentation-tests)\n- [Profiling with `py-spy`](#profiling-with-py-spy)\n- [Managing multiple build trees](#managing-multiple-build-trees)\n- [C++ development tips](#c-development-tips)\n  - [Build only what you need](#build-only-what-you-need)\n  - [Code completion and IDE support](#code-completion-and-ide-support)\n  - [Make no-op build fast](#make-no-op-build-fast)\n    - [Use Ninja](#use-ninja)\n    - [Use CCache](#use-ccache)\n    - [Use a faster linker](#use-a-faster-linker)\n    - [Use pre-compiled headers](#use-pre-compiled-headers)\n    - [Workaround for header dependency bug in nvcc](#workaround-for-header-dependency-bug-in-nvcc)\n  - [Rebuild few files with debug information](#rebuild-few-files-with-debug-information)\n  - [C++ frontend development tips](#c-frontend-development-tips)\n  - [GDB integration](#gdb-integration)\n  - [C++ stacktraces](#c-stacktraces)\n- [CUDA development tips](#cuda-development-tips)\n- [Windows development tips](#windows-development-tips)\n  - [Known MSVC (and MSVC with NVCC) bugs](#known-msvc-and-msvc-with-nvcc-bugs)\n  - [Building on legacy code and CUDA](#building-on-legacy-code-and-cuda)\n- [Running clang-tidy](#running-clang-tidy)\n- [Pre-commit tidy/linting hook](#pre-commit-tidylinting-hook)\n- [Building PyTorch with ASAN](#building-pytorch-with-asan)\n  - [Getting `ccache` to work](#getting-ccache-to-work)\n  - [Why this stuff with `LD_PRELOAD` and `LIBASAN_RT`?](#why-this-stuff-with-ld_preload-and-libasan_rt)\n  - [Why LD_PRELOAD in the build function?](#why-ld_preload-in-the-build-function)\n  - [Why no leak detection?](#why-no-leak-detection)\n- [Caffe2 notes](#caffe2-notes)\n- [CI failure tips](#ci-failure-tips)\n  - [Which commit is used in CI?](#which-commit-is-used-in-ci)\n- [Dev Infra Office Hours](#dev-infra-office-hours)\n\n<!-- tocstop -->\n\n## Developing PyTorch\nFollow the instructions for [installing PyTorch from source](https://github.com/pytorch/pytorch#from-source). If you get stuck when developing PyTorch on your machine, check out the [tips and debugging](#tips-and-debugging) section below for common solutions.\n\n### Tips and Debugging\n\n* If you want to have no-op incremental rebuilds (which are fast), see [Make no-op build fast](#make-no-op-build-fast) below.\n\n* When installing with `python setup.py develop` (in contrast to `python setup.py install`) Python runtime will use\n  the current local source-tree when importing `torch` package. (This is done by creating [`.egg-link`](https://wiki.python.org/moin/PythonPackagingTerminology#egg-link) file in `site-packages` folder)\n  This way you do not need to repeatedly install after modifying Python files (`.py`).\n  However, you would need to reinstall if you modify Python interface (`.pyi`, `.pyi.in`) or\n   non-Python files (`.cpp`, `.cc`, `.cu`, `.h`, ...).\n\n\n  One way to avoid running `python setup.py develop` every time one makes a change to C++/CUDA/ObjectiveC files on Linux/Mac,\n  is to create a symbolic link from `build` folder to `torch/lib`, for example, by issuing following:\n  ```bash\n   pushd torch/lib; sh -c \"ln -sf ../../build/lib/libtorch_cpu.* .\"; popd\n  ```\n   Afterwards rebuilding a library (for example to rebuild `libtorch_cpu.so` issue `ninja torch_cpu` from `build` folder),\n   would be sufficient to make change visible in `torch` package.\n\n\n  To reinstall, first uninstall all existing PyTorch installs. You may need to run `pip\n  uninstall torch` multiple times. You'll know `torch` is fully\n  uninstalled when you see `WARNING: Skipping torch as it is not\n  installed`. (You should only have to `pip uninstall` a few times, but\n  you can always `uninstall` with `timeout` or in a loop if you're feeling\n  lazy.)\n\n  ```bash\n  conda uninstall pytorch -y\n  yes | pip uninstall torch\n  ```\n\n  Next run `python setup.py clean`. After that, you can install in `develop` mode again.\n\n* If a commit is simple and doesn't affect any code (keep in mind that some docstrings contain code\n  that is used in tests), you can add `[skip ci]` (case sensitive) somewhere in your commit message to\n  [skip all build / test steps](https://github.blog/changelog/2021-02-08-github-actions-skip-pull-request-and-push-workflows-with-skip-ci/).\n  Note that changing the pull request body or title on GitHub itself has no effect.\n\n* If you run into errors when running `python setup.py develop`, here are some debugging steps:\n  1. Run `printf '#include <stdio.h>\\nint main() { printf(\"Hello World\");}'|clang -x c -; ./a.out` to make sure\n  your CMake works and can compile this simple Hello World program without errors.\n  2. Nuke your `build` directory. The `setup.py` script compiles binaries into the `build` folder and caches many\n  details along the way, which saves time the next time you build. If you're running into issues, you can always\n  `rm -rf build` from the toplevel `pytorch` directory and start over.\n  3. If you have made edits to the PyTorch repo, commit any change you'd like to keep and clean the repo with the\n  following commands (note that clean _really_ removes all untracked files and changes.):\n      ```bash\n      git submodule deinit -f .\n      git clean -xdf\n      python setup.py clean\n      git submodule update --init --recursive # very important to sync the submodules\n      python setup.py develop                 # then try running the command again\n      ```\n  4. The main step within `python setup.py develop` is running `make` from the `build` directory. If you want to\n    experiment with some environment variables, you can pass them into the command:\n      ```bash\n      ENV_KEY1=ENV_VAL1[, ENV_KEY2=ENV_VAL2]* python setup.py develop\n      ```\n\n* If you run into issue running `git submodule update --init --recursive`. Please try the following:\n  - If you encounter an error such as\n    ```\n    error: Submodule 'third_party/pybind11' could not be updated\n    ```\n    check whether your Git local or global config file contains any `submodule.*` settings. If yes, remove them and try again.\n    (please reference [this doc](https://git-scm.com/docs/git-config#Documentation/git-config.txt-submoduleltnamegturl) for more info).\n\n  - If you encounter an error such as\n    ```\n    fatal: unable to access 'https://github.com/pybind11/pybind11.git': could not load PEM client certificate ...\n    ```\n    this is likely that you are using HTTP proxying and the certificate expired. To check if the certificate is valid, run\n    `git config --global --list` and search for config like `http.proxysslcert=<cert_file>`. Then check certificate valid date by running\n    ```bash\n    openssl x509 -noout -in <cert_file> -dates\n    ```\n\n  - If you encounter an error that some third_party modules are not checked out correctly, such as\n    ```\n    Could not find .../pytorch/third_party/pybind11/CMakeLists.txt\n    ```\n    remove any `submodule.*` settings in your local git config (`.git/config` of your pytorch repo) and try again.\n* If you're a Windows contributor, please check out [Best Practices](https://github.com/pytorch/pytorch/wiki/Best-Practices-to-Edit-and-Compile-Pytorch-Source-Code-On-Windows).\n* For help with any part of the contributing process, please don\u2019t hesitate to utilize our Zoom office hours! See details [here](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)\n\n## Nightly Checkout & Pull\n\nThe `tools/nightly.py` script is provided to ease pure Python development of\nPyTorch. This uses `conda` and `git` to check out the nightly development\nversion of PyTorch and installs pre-built binaries into the current repository.\nThis is like a development or editable install, but without needing the ability\nto compile any C++ code.\n\nYou can use this script to check out a new nightly branch with the following:\n\n```bash\n./tools/nightly.py checkout -b my-nightly-branch\nconda activate pytorch-deps\n```\n\nOr if you would like to re-use an existing conda environment, you can pass in\nthe regular environment parameters (`--name` or `--prefix`):\n\n```bash\n./tools/nightly.py checkout -b my-nightly-branch -n my-env\nconda activate my-env\n```\n\nYou can also use this tool to pull the nightly commits into the current branch:\n\n```bash\n./tools/nightly.py pull -n my-env\nconda activate my-env\n```\n\nPulling will reinstall the PyTorch dependencies as well as the nightly binaries\ninto the repo directory.\n\n## Codebase structure\n\n* [c10](c10) - Core library files that work everywhere, both server\n  and mobile. We are slowly moving pieces from [ATen/core](aten/src/ATen/core)\n  here. This library is intended only to contain essential functionality,\n  and appropriate to use in settings where binary size matters. (But\n  you'll have a lot of missing functionality if you try to use it\n  directly.)\n* [aten](aten) - C++ tensor library for PyTorch (no autograd support)\n  * [src](aten/src) - [README](aten/src/README.md)\n    * [ATen](aten/src/ATen)\n      * [core](aten/src/ATen/core) - Core functionality of ATen. This\n        is migrating to top-level c10 folder.\n      * [native](aten/src/ATen/native) - Modern implementations of\n        operators. If you want to write a new operator, here is where\n        it should go. Most CPU operators go in the top level directory,\n        except for operators which need to be compiled specially; see\n        cpu below.\n        * [cpu](aten/src/ATen/native/cpu) - Not actually CPU\n          implementations of operators, but specifically implementations\n          which are compiled with processor-specific instructions, like\n          AVX. See the [README](aten/src/ATen/native/cpu/README.md) for more\n          details.\n        * [cuda](aten/src/ATen/native/cuda) - CUDA implementations of\n          operators.\n        * [sparse](aten/src/ATen/native/sparse) - CPU and CUDA\n          implementations of COO sparse tensor operations\n        * [mkl](aten/src/ATen/native/mkl) [mkldnn](aten/src/ATen/native/mkldnn)\n          [miopen](aten/src/ATen/native/miopen) [cudnn](aten/src/ATen/native/cudnn)\n          - implementations of operators which simply bind to some\n            backend library.\n        * [quantized](aten/src/ATen/native/quantized/) - Quantized tensor (i.e. QTensor) operation implementations. [README](aten/src/ATen/native/quantized/README.md) contains details including how to implement native quantized operations.\n* [torch](torch) - The actual PyTorch library. Everything that is not\n  in [csrc](torch/csrc) is a Python module, following the PyTorch Python\n  frontend module structure.\n  * [csrc](torch/csrc) - C++ files composing the PyTorch library. Files\n    in this directory tree are a mix of Python binding code, and C++\n    heavy lifting. Consult `setup.py` for the canonical list of Python\n    binding files; conventionally, they are often prefixed with\n    `python_`. [README](torch/csrc/README.md)\n    * [jit](torch/csrc/jit) - Compiler and frontend for TorchScript JIT\n      frontend. [README](torch/csrc/jit/README.md)\n    * [autograd](torch/csrc/autograd) - Implementation of reverse-mode automatic differentiation. [README](torch/csrc/autograd/README.md)\n    * [api](torch/csrc/api) - The PyTorch C++ frontend.\n    * [distributed](torch/csrc/distributed) - Distributed training\n      support for PyTorch.\n* [tools](tools) - Code generation scripts for the PyTorch library.\n  See [README](tools/README.md) of this directory for more details.\n* [test](test) - Python unit tests for PyTorch Python frontend.\n  * [test_torch.py](test/test_torch.py) - Basic tests for PyTorch\n    functionality.\n  * [test_autograd.py](test/test_autograd.py) - Tests for non-NN\n    automatic differentiation support.\n  * [test_nn.py](test/test_nn.py) - Tests for NN operators and\n    their automatic differentiation.\n  * [test_jit.py](test/test_jit.py) - Tests for the JIT compiler\n    and TorchScript.\n  * ...\n  * [cpp](test/cpp) - C++ unit tests for PyTorch C++ frontend.\n    * [api](test/cpp/api) - [README](test/cpp/api/README.md)\n    * [jit](test/cpp/jit) - [README](test/cpp/jit/README.md)\n    * [tensorexpr](test/cpp/tensorexpr) - [README](test/cpp/tensorexpr/README.md)\n  * [expect](test/expect) - Automatically generated \"expect\" files\n    which are used to compare against expected output.\n  * [onnx](test/onnx) - Tests for ONNX export functionality,\n    using both PyTorch and Caffe2.\n* [caffe2](caffe2) - The Caffe2 library.\n  * [core](caffe2/core) - Core files of Caffe2, e.g., tensor, workspace,\n    blobs, etc.\n  * [operators](caffe2/operators) - Operators of Caffe2.\n  * [python](caffe2/python) - Python bindings to Caffe2.\n  * ...\n* [.circleci](.circleci) - CircleCI configuration management. [README](.circleci/README.md)\n\n## Unit testing\n\n### Python Unit Testing\n\n**Prerequisites**:\nThe following packages should be installed with either `conda` or `pip`:\n- `expecttest` and `hypothesis` - required to run tests\n- `mypy` - recommended for linting\n- `pytest` - recommended to run tests more selectively\n\nAll PyTorch test suites are located in the `test` folder and start with\n`test_`. Run the entire test\nsuite with\n\n```bash\npython test/run_test.py\n```\n\nor run individual test suites using the command `python test/FILENAME.py`,\nwhere `FILENAME` represents the file containing the test suite you wish\nto run.\n\nFor example, to run all the TorchScript JIT tests (located at\n`test/test_jit.py`), you would run:\n\n```bash\npython test/test_jit.py\n```\n\nYou can narrow down what you're testing even further by specifying the\nname of an individual test with `TESTCLASSNAME.TESTNAME`. Here,\n`TESTNAME` is the name of the test you want to run, and `TESTCLASSNAME`\nis the name of the class in which it is defined.\n\nGoing off the above example, let's say you want to run\n`test_Sequential`, which is defined as part of the `TestJit` class\nin `test/test_jit.py`. Your command would be:\n\n```bash\npython test/test_jit.py TestJit.test_Sequential\n```\n\n**Weird note:** In our CI (Continuous Integration) jobs, we actually run the tests from the `test` folder and **not** the root of the repo, since there are various dependencies we set up for CI that expects the tests to be run from the test folder. As such, there may be some inconsistencies between local testing and CI testing--if you observe an inconsistency, please [file an issue](https://github.com/pytorch/pytorch/issues/new/choose).\n\n### Better local unit tests with `pytest`\n\nWe don't officially support `pytest`, but it works well with our\n`unittest` tests and offers a number of useful features for local\ndeveloping. Install it via `pip install pytest`.\n\nIf you want to just run tests that contain a specific substring, you can\nuse the `-k` flag:\n\n```bash\npytest test/test_nn.py -k Loss -v\n```\n\nThe above is an example of testing a change to all Loss functions: this\ncommand runs tests such as `TestNN.test_BCELoss` and\n`TestNN.test_MSELoss` and can be useful to save keystrokes.\n\n### Local linting\n\nInstall all prerequisites by running\n\n```bash\nmake setup_lint\n```\n\nYou can now run the same linting steps that are used in CI locally via `make`:\n\n```bash\nmake lint\n```\n\nLearn more about the linter on the [lintrunner wiki page](https://github.com/pytorch/pytorch/wiki/lintrunner)\n\n#### Running `mypy`\n\n`mypy` is an optional static type checker for Python. We have multiple `mypy`\nconfigs for the PyTorch codebase that are automatically validated against whenever the linter is run.\n\nSee [Guide for adding type annotations to\nPyTorch](https://github.com/pytorch/pytorch/wiki/Guide-for-adding-type-annotations-to-PyTorch)\nfor more information on how to set up `mypy` and tackle type annotation\ntasks.\n\n### C++ Unit Testing\n\nPyTorch offers a series of tests located in the `test/cpp` folder.\nThese tests are written in C++ and use the Google Test testing framework.\nAfter compiling PyTorch from source, the test runner binaries will be\nwritten to the `build/bin` folder. The command to run one of these tests\nis `./build/bin/FILENAME --gtest_filter=TESTSUITE.TESTNAME`, where\n`TESTNAME` is the name of the test you'd like to run and `TESTSUITE` is\nthe suite that test is defined in.\n\nFor example, if you wanted to run the test `MayContainAlias`, which\nis part of the test suite `ContainerAliasingTest` in the file\n`test/cpp/jit/test_alias_analysis.cpp`, the command would be:\n\n```bash\n./build/bin/test_jit --gtest_filter=ContainerAliasingTest.MayContainAlias\n```\n\n\n### Run Specific CI Jobs\n\nYou can generate a commit that limits the CI to only run a specific job by using\n`tools/testing/explicit_ci_jobs.py` like so:\n\n```bash\n# --job: specify one or more times to filter to a specific job + its dependencies\n# --filter-gha: specify github actions workflows to keep\n# --make-commit: commit CI changes to git with a message explaining the change\npython tools/testing/explicit_ci_jobs.py --job binary_linux_manywheel_3_6m_cpu_devtoolset7_nightly_test --filter-gha '*generated*gcc5.4*' --make-commit\n\n# Make your changes\n\nghstack submit\n```\n\n**NB**: It is not recommended to use this workflow unless you are also using\n[`ghstack`](https://github.com/ezyang/ghstack). It creates a large commit that is\nof very low signal to reviewers.\n\n## Merging your Change\nIf you know the right people or team that should approve your PR (and you have the required permissions to do so), add them to the Reviewers list.\n\nIf not, leave the Reviewers section empty. Our triage squad will review your PR, add a module label, and assign it to the appropriate reviewer in a couple business days.  The reviewer will then look at your PR and respond.\n\nOccasionally, things might fall through the cracks (sorry!). In case your PR either doesn't get assigned to a reviewer or doesn't get any response from the reviewer for 4 business days, please leave comment on the PR (mentioning the reviewer if one has been assigned). That'll get it nudged back onto people's radar.\n\nIf that still doesn't help, come see us during [our office hours](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours)\n\nOnce your PR is approved, you can merge it in by entering a comment with the content `@pytorchmergebot merge` ([what's this bot?](https://github.com/pytorch/pytorch/wiki/Bot-commands))\n\n## Writing documentation\n\nSo you want to write some documentation and don't know where to start?\nPyTorch has two main types of documentation:\n- **User facing documentation**:\nThese are the docs that you see over at [our docs website](https://pytorch.org/docs).\n- **Developer facing documentation**:\nDeveloper facing documentation is spread around our READMEs in our codebase and in\nthe [PyTorch Developer Wiki](https://pytorch.org/wiki).\nIf you're interested in adding new developer docs, please read this [page on the wiki](https://github.com/pytorch/pytorch/wiki/Where-or-how-should-I-add-documentation) on our best practices for where to put it.\n\nThe rest of this section is about user-facing documentation.\n\nPyTorch uses [Google style](https://www.sphinx-doc.org/en/master/usage/extensions/example_google.html)\nfor formatting docstrings. Each line inside a docstrings block must be limited to 80 characters so that it fits into Jupyter documentation popups.\n\n\n### Docstring type formatting\n\nIn addition to the standard Google Style docstring formatting rules, the following guidelines should be followed for docstring types (docstring types are the type information contained in the round brackets after the variable name):\n\n* The \"`Callable`\", \"`Any`\", \"`Iterable`\", \"`Iterator`\", \"`Generator`\" types should have their first letter capitalized.\n\n* The \"`list`\" and \"`tuple`\" types should be completely lowercase.\n\n* Types should not be made plural. For example: `tuple of int` should be used instead of `tuple of ints`.\n\n* The only acceptable delimiter words for types are `or` and `of`. No other non-type words should be used other than `optional`.\n\n* The word `optional` should only be used after the types, and it is only used if the user does not have to specify a value for the variable. Default values are listed after the variable description. Example:\n\n    ```\n    my_var (int, optional): Variable description. Default: 1\n    ```\n\n* Basic Python types should match their type name so that the [Intersphinx](https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html) extension can correctly identify them. For example:\n    * Use `str` instead of `string`.\n    * Use `bool` instead of `boolean`.\n    * Use `dict` instead of `dictionary`.\n\n* Square brackets should be used for the dictionary type. For example:\n\n    ```\n    my_var (dict[str, int]): Variable description.\n    ```\n\n* If a variable has two different possible types, then the word `or` should be used without a comma. Otherwise variables with 3 or more types should use commas to separate the types. Example:\n\n    ```\n    x (type1 or type2): Variable description.\n    y (type1, type2, or type3): Variable description.\n    ```\n\n\n### Building documentation\n\nTo build the documentation:\n\n1. Build and install PyTorch\n\n2. Install the prerequisites\n\n```bash\ncd docs\npip install -r requirements.txt\n# `katex` must also be available in your PATH.\n# You can either install katex globally if you have properly configured npm:\n# npm install -g katex\n# Or if you prefer an uncontaminated global executable environment or do not want to go through the node configuration:\n# npm install katex && export PATH=\"$PATH:$(pwd)/node_modules/.bin\"\n```\n> Note: if you installed `nodejs` with a different package manager (e.g.,\n`conda`) then `npm` will probably install a version of `katex` that is not\ncompatible with your version of `nodejs` and doc builds will fail.\nA combination of versions that is known to work is `node@6.13.1` and\n`katex@0.13.18`. To install the latter with `npm` you can run\n```npm install -g katex@0.13.18```\n\n\n> Note that if you are a Facebook employee using a devserver, yarn may be more convenient to install katex:\n\n```bash\nyarn global add katex\n```\n> If a specific version is required you can use for example `yarn global add katex@0.13.18`.\n\n3. Generate the documentation HTML files. The generated files will be in `docs/build/html`.\n\n```bash\nmake html\n```\n\n#### Tips\n\nThe `.rst` source files live in [docs/source](docs/source). Some of the `.rst`\nfiles pull in docstrings from PyTorch Python code (for example, via\nthe `autofunction` or `autoclass` directives). To vastly shorten doc build times,\nit is helpful to remove the files you are not working on, only keeping the base\n`index.rst` file and the files you are editing. The Sphinx build will produce\nmissing file warnings but will still complete. For example, to work on `jit.rst`:\n\n```bash\ncd docs/source\nfind . -type f | grep rst | grep -v index | grep -v jit | xargs rm\n\n# Make your changes, build the docs, etc.\n\n# Don't commit the deletions!\ngit add index.rst jit.rst\n...\n```\n\n#### Building C++ Documentation\n\nFor C++ documentation (https://pytorch.org/cppdocs), we use\n[Doxygen](http://www.doxygen.nl/) and then convert it to\n[Sphinx](http://www.sphinx-doc.org/) via\n[Breathe](https://github.com/michaeljones/breathe) and\n[Exhale](https://github.com/svenevs/exhale). Check the [Doxygen\nreference](https://www.doxygen.nl/manual/) for more\ninformation on the documentation syntax.\n\nWe run Doxygen in CI (Travis) to verify that you do not use invalid Doxygen\ncommands. To run this check locally, run `./check-doxygen.sh` from inside\n`docs/cpp/source`.\n\nTo build the documentation, follow the same steps as above, but run them from\n`docs/cpp` instead of `docs`.\n\n### Previewing changes locally\n\nTo view HTML files locally, you can open the files in your web browser. For example,\nnavigate to `file:///your_pytorch_folder/docs/build/html/index.html` in a web\nbrowser.\n\nIf you are developing on a remote machine, you can set up an SSH tunnel so that\nyou can access the HTTP server on the remote machine from your local machine. To map\nremote port 8000 to local port 8000, use either of the following commands.\n\n```bash\n# For SSH\nssh my_machine -L 8000:my_machine:8000\n\n# For Eternal Terminal\net my_machine -t=\"8000:8000\"\n```\n\nThen navigate to `localhost:8000` in your web browser.\n\n**Tip:**\nYou can start a lightweight HTTP server on the remote machine with:\n\n```bash\npython -m http.server 8000 <path_to_html_output>\n```\n\nAlternatively, you can run `rsync` on your local machine to copy the files from\nyour remote machine:\n\n```bash\nmkdir -p build cpp/build\nrsync -az me@my_machine:/path/to/pytorch/docs/build/html build\nrsync -az me@my_machine:/path/to/pytorch/docs/cpp/build/html cpp/build\n```\n\n### Previewing documentation on PRs\n\nPyTorch will host documentation previews at `https://docs-preview.pytorch.org/pytorch/pytorch/<pr number>/index.html` once the\n`pytorch_python_doc_build` GitHub Actions job has completed on your PR. You can visit that page directly\nor find its link in the automated Dr. CI comment on your PR.\n\n### Adding documentation tests\n\nIt is easy for code snippets in docstrings and `.rst` files to get out of date. The docs\nbuild includes the [Sphinx Doctest Extension](https://www.sphinx-doc.org/en/master/usage/extensions/doctest.html),\nwhich can run code in documentation as a unit test. To use the extension, use\nthe `.. testcode::` directive in your `.rst` and docstrings.\n\nTo manually run these tests, follow steps 1 and 2 above, then run:\n\n```bash\ncd docs\nmake doctest\n```\n\n## Profiling with `py-spy`\n\nEvaluating the performance impact of code changes in PyTorch can be complicated,\nparticularly if code changes happen in compiled code. One simple way to profile\nboth Python and C++ code in PyTorch is to use\n[`py-spy`](https://github.com/benfred/py-spy), a sampling profiler for Python\nthat has the ability to profile native code and Python code in the same session.\n\n`py-spy` can be installed via `pip`:\n\n```bash\npip install py-spy\n```\n\nTo use `py-spy`, first write a Python test script that exercises the\nfunctionality you would like to profile. For example, this script profiles\n`torch.add`:\n\n```python\nimport torch\n\nt1 = torch.tensor([[1, 1], [1, 1.]])\nt2 = torch.tensor([[0, 0], [0, 0.]])\n\nfor _ in range(1000000):\n    torch.add(t1, t2)\n```\n\nSince the `torch.add` operation happens in microseconds, we repeat it a large\nnumber of times to get good statistics. The most straightforward way to use\n`py-spy` with such a script is to generate a [flame\ngraph](http://www.brendangregg.com/flamegraphs.html):\n\n```bash\npy-spy record -o profile.svg --native -- python test_tensor_tensor_add.py\n```\n\nThis will output a file named `profile.svg` containing a flame graph you can\nview in a web browser or SVG viewer. Individual stack frame entries in the graph\ncan be selected interactively with your mouse to zoom in on a particular part of\nthe program execution timeline. The `--native` command-line option tells\n`py-spy` to record stack frame entries for PyTorch C++ code. To get line numbers\nfor C++ code it may be necessary to compile PyTorch in debug mode by prepending\nyour `setup.py develop` call to compile PyTorch with `DEBUG=1`. Depending on\nyour operating system it may also be necessary to run `py-spy` with root\nprivileges.\n\n`py-spy` can also work in an `htop`-like \"live profiling\" mode and can be\ntweaked to adjust the stack sampling rate, see the `py-spy` readme for more\ndetails.\n\n## Managing multiple build trees\n\nOne downside to using `python setup.py develop` is that your development\nversion of PyTorch will be installed globally on your account (e.g., if\nyou run `import torch` anywhere else, the development version will be\nused.\n\nIf you want to manage multiple builds of PyTorch, you can make use of\n[conda environments](https://conda.io/docs/using/envs.html) to maintain\nseparate Python package environments, each of which can be tied to a\nspecific build of PyTorch. To set one up:\n\n```bash\nconda create -n pytorch-myfeature\nsource activate pytorch-myfeature\n# if you run python now, torch will NOT be installed\npython setup.py develop\n```\n\n## C++ development tips\n\nIf you are working on the C++ code, there are a few important things that you\nwill want to keep in mind:\n\n1. How to rebuild only the code you are working on.\n2. How to make rebuilds in the absence of changes go faster.\n\n### Build only what you need\n\n`python setup.py build` will build everything by default, but sometimes you are\nonly interested in a specific component.\n\n- Working on a test binary? Run `(cd build && ninja bin/test_binary_name)` to\n  rebuild only that test binary (without rerunning cmake). (Replace `ninja` with\n  `make` if you don't have ninja installed).\n- Don't need Caffe2?  Pass `BUILD_CAFFE2=0` to disable Caffe2 build.\n\nOn the initial build, you can also speed things up with the environment\nvariables `DEBUG`, `USE_DISTRIBUTED`, `USE_MKLDNN`, `USE_CUDA`, `USE_FLASH_ATTENTION`, `USE_MEM_EFF_ATTENTION`, `BUILD_TEST`, `USE_FBGEMM`, `USE_NNPACK` and `USE_QNNPACK`.\n\n- `DEBUG=1` will enable debug builds (-g -O0)\n- `REL_WITH_DEB_INFO=1` will enable debug symbols with optimizations (-g -O3)\n- `USE_DISTRIBUTED=0` will disable distributed (c10d, gloo, mpi, etc.) build.\n- `USE_MKLDNN=0` will disable using MKL-DNN.\n- `USE_CUDA=0` will disable compiling CUDA (in case you are developing on something not CUDA related), to save compile time.\n- `BUILD_TEST=0` will disable building C++ test binaries.\n- `USE_FBGEMM=0` will disable using FBGEMM (quantized 8-bit server operators).\n- `USE_NNPACK=0` will disable compiling with NNPACK.\n- `USE_QNNPACK=0` will disable QNNPACK build (quantized 8-bit operators).\n- `USE_XNNPACK=0` will disable compiling with XNNPACK.\n- `USE_FLASH_ATTENTION=0` and `USE_MEM_EFF_ATTENTION=0` will disable compiling flash attention and memory efficient kernels respectively\n\nFor example:\n\n```bash\nDEBUG=1 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```\n\nFor subsequent builds (i.e., when `build/CMakeCache.txt` exists), the build\noptions passed for the first time will persist; please run `ccmake build/`, run\n`cmake-gui build/`, or directly edit `build/CMakeCache.txt` to adapt build\noptions.\n\n### Code completion and IDE support\n\nWhen using `python setup.py develop`, PyTorch will generate\na `compile_commands.json` file that can be used by many editors\nto provide command completion and error highlighting for PyTorch's\nC++ code. You need to `pip install ninja` to generate accurate\ninformation for the code in `torch/csrc`. More information at:\n- https://sarcasm.github.io/notes/dev/compilation-database.html\n\n### Make no-op build fast\n\n#### Use Ninja\n\nBy default, cmake will use its Makefile generator to generate your build\nsystem.  You can get faster builds if you install the ninja build system\nwith `pip install ninja`.  If PyTorch was already built, you will need\nto run `python setup.py clean` once after installing ninja for builds to\nsucceed.\n\nNote: Make sure to use a machine with a larger number of CPU cores, this will significantly reduce your build times.\n\n#### Use CCache\n\nEven when dependencies are tracked with file modification, there are many\nsituations where files get rebuilt when a previous compilation was exactly the\nsame. Using ccache in a situation like this is a real time-saver.\n\nBefore building pytorch, install ccache from your package manager of choice:\n\n```bash\nconda install ccache -c conda-forge\nsudo apt install ccache\nsudo yum install ccache\nbrew install ccache\n```\n\nYou may also find the default cache size in ccache is too small to be useful.\nThe cache sizes can be increased from the command line:\n\n```bash\n# config: cache dir is ~/.ccache, conf file ~/.ccache/ccache.conf\n# max size of cache\nccache -M 25Gi  # -M 0 for unlimited\n# unlimited number of files\nccache -F 0\n```\n\nTo check this is working, do two clean builds of pytorch in a row. The second\nbuild should be substantially and noticeably faster than the first build. If\nthis doesn't seem to be the case, check the `CMAKE_<LANG>_COMPILER_LAUNCHER`\nrules in `build/CMakeCache.txt`, where `<LANG>` is `C`, `CXX` and `CUDA`.\nEach of these 3 variables should contain ccache, e.g.\n\n```\n//CXX compiler launcher\nCMAKE_CXX_COMPILER_LAUNCHER:STRING=/usr/bin/ccache\n```\n\nIf not, you can define these variables on the command line before invoking `setup.py`.\n\n```bash\nexport CMAKE_C_COMPILER_LAUNCHER=ccache\nexport CMAKE_CXX_COMPILER_LAUNCHER=ccache\nexport CMAKE_CUDA_COMPILER_LAUNCHER=ccache\npython setup.py develop\n```\n\n#### Use a faster linker\n\nIf you are editing a single file and rebuilding in a tight loop, the time spent\nlinking will dominate. The system linker available in most Linux distributions\n(GNU `ld`) is quite slow. Use a faster linker, like [lld](https://lld.llvm.org/).\n\nPeople on Mac, follow [this guide](https://stackoverflow.com/questions/42730345/how-to-install-llvm-for-mac) instead.\n\nThe easiest way to use `lld` this is download the\n[latest LLVM binaries](http://releases.llvm.org/download.html#8.0.0) and run:\n\n```bash\nln -s /path/to/downloaded/ld.lld /usr/local/bin/ld\n```\n\n#### Use pre-compiled headers\n\nSometimes there's no way of getting around rebuilding lots of files, for example\nediting `native_functions.yaml` usually means 1000+ files being rebuilt. If\nyou're using CMake newer than 3.16, you can enable pre-compiled headers by\nsetting `USE_PRECOMPILED_HEADERS=1` either on first setup, or in the\n`CMakeCache.txt` file.\n\n```sh\nUSE_PRECOMPILED_HEADERS=1 python setup.py develop\n```\n\nThis adds a build step where the compiler takes `<ATen/ATen.h>` and essentially\ndumps it's internal AST to a file so the compiler can avoid repeating itself for\nevery `.cpp` file.\n\nOne caveat is that when enabled, this header gets included in every file by default.\nWhich may change what code is legal, for example:\n- internal functions can never alias existing names in `<ATen/ATen.h>`\n- names in `<ATen/ATen.h>` will work even if you don't explicitly include it.\n\n#### Workaround for header dependency bug in nvcc\nIf re-building without modifying any files results in several CUDA files being\nre-compiled, you may be running into an `nvcc` bug where header dependencies are\nnot converted to absolute paths before reporting it to the build system. This\nmakes `ninja` think one of the header files has been deleted, so it runs the\nbuild again.\n\nA compiler-wrapper to fix this is provided in `tools/nvcc_fix_deps.py`. You can use\nthis as a compiler launcher, similar to `ccache`\n```bash\nexport CMAKE_CUDA_COMPILER_LAUNCHER=\"python;`pwd`/tools/nvcc_fix_deps.py;ccache\"\npython setup.py develop\n```\n\n### Rebuild few files with debug information\n\nWhile debugging a problem one often had to maintain a debug build in a separate folder.\nBut often only a few files needs to be rebuild with debug info to get a symbolicated backtrace or enable source debugging\nOne can easily solve this with the help of `tools/build_with_debinfo.py`\n\nFor example, suppose one wants to debug what is going on while tensor index is selected, which can be achieved by setting a breakpoint at `applySelect` function:\n```\n% lldb -o \"b applySelect\" -o \"process launch\" -- python3 -c \"import torch;print(torch.rand(5)[3])\"\n(lldb) target create \"python\"\nCurrent executable set to '/usr/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"-c\" \"import torch;print(torch.rand(5)[3])\"\n(lldb) b applySelect\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\n(lldb) process launch\n2 locations added to breakpoint 1\nProcess 87729 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001023d55a8 libtorch_python.dylib`at::indexing::impl::applySelect(at::Tensor const&, long long, c10::SymInt, long long, c10::Device const&, std::__1::optional<c10::ArrayRef<c10::SymInt>> const&)\nlibtorch_python.dylib`at::indexing::impl::applySelect:\n->  0x1023d55a8 <+0>:  sub    sp, sp, #0xd0\n    0x1023d55ac <+4>:  stp    x24, x23, [sp, #0x90]\n    0x1023d55b0 <+8>:  stp    x22, x21, [sp, #0xa0]\n    0x1023d55b4 <+12>: stp    x20, x19, [sp, #0xb0]\nTarget 0: (python) stopped.\nProcess 87729 launched: '/usr/bin/python' (arm64)\n```\nWhich is not very informative, but can be easily remedied by rebuilding `python_variable_indexing.cpp` with debug information\n```\n% ./tools/build_with_debinfo.py torch/csrc/autograd/python_variable_indexing.cpp\n[1 / 2] Building caffe2/torch/CMakeFiles/torch_python.dir/csrc/autograd/python_variable_indexing.cpp.o\n[2 / 2] Building lib/libtorch_python.dylib\n```\nAnd afterwards:\n```\n% lldb -o \"b applySelect\" -o \"process launch\" -- python3 -c \"import torch;print(torch.rand(5)[3])\"\n(lldb) target create \"python\"\nCurrent executable set to '/usr/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"-c\" \"import torch;print(torch.rand(5)[3])\"\n(lldb) b applySelect\nBreakpoint 1: no locations (pending).\nWARNING:  Unable to resolve breakpoint to any actual locations.\n(lldb) process launch\n2 locations added to breakpoint 1\nProcess 87741 stopped\n* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1\n    frame #0: 0x00000001024e2628 libtorch_python.dylib`at::indexing::impl::applySelect(self=0x00000001004ee8a8, dim=0, index=(data_ = 3), real_dim=0, (null)=0x000000016fdfe535, self_sizes= Has Value=true ) at TensorIndexing.h:239:7\n   236         const at::Device& /*self_device*/,\n   237         const c10::optional<SymIntArrayRef>& self_sizes) {\n   238       // See NOTE [nested tensor size for indexing]\n-> 239       if (self_sizes.has_value()) {\n   240         auto maybe_index = index.maybe_as_int();\n   241         if (maybe_index.has_value()) {\n   242           TORCH_CHECK_INDEX(\nTarget 0: (python) stopped.\nProcess 87741 launched: '/usr/bin/python3' (arm64)\n```\nWhich is much more useful, isn't it?\n\n### C++ frontend development tips\n\nWe have very extensive tests in the [test/cpp/api](test/cpp/api) folder. The\ntests are a great way to see how certain components are intended to be used.\nWhen compiling PyTorch from source, the test runner binary will be written to\n`build/bin/test_api`. The tests use the [GoogleTest](https://github.com/google/googletest/blob/master/googletest)\nframework, which you can read up about to learn how to configure the test runner. When\nsubmitting a new feature, we care very much that you write appropriate tests.\nPlease follow the lead of the other tests to see how to write a new test case.\n\n### GDB integration\n\nIf you are debugging pytorch inside GDB, you might be interested in\n[pytorch-gdb](tools/gdb/pytorch-gdb.py). This script introduces some\npytorch-specific commands which you can use from the GDB prompt. In\nparticular, `torch-tensor-repr` prints a human-readable repr of an at::Tensor\nobject. Example of usage:\n\n```\n$ gdb python\nGNU gdb (GDB) 9.2\n[...]\n(gdb) # insert a breakpoint when we call .neg()\n(gdb) break at::Tensor::neg\nFunction \"at::Tensor::neg\" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (at::Tensor::neg) pending.\n\n(gdb) run\n[...]\n>>> import torch\n>>> t = torch.tensor([1, 2, 3, 4], dtype=torch.float64)\n>>> t\ntensor([1., 2., 3., 4.], dtype=torch.float64)\n>>> t.neg()\n\nThread 1 \"python\" hit Breakpoint 1, at::Tensor::neg (this=0x7ffb118a9c88) at aten/src/ATen/core/TensorBody.h:3295\n3295    inline at::Tensor Tensor::neg() const {\n(gdb) # the default repr of 'this' is not very useful\n(gdb) p this\n$1 = (const at::Tensor * const) 0x7ffb118a9c88\n(gdb) p *this\n$2 = {impl_ = {target_ = 0x55629b5cd330}}\n(gdb) torch-tensor-repr *this\nPython-level repr of *this:\ntensor([1., 2., 3., 4.], dtype=torch.float64)\n```\n\nGDB tries to automatically load `pytorch-gdb` thanks to the\n[.gdbinit](.gdbinit) at the root of the pytorch repo. However, auto-loadings is disabled by default, because of security reasons:\n\n```bash\n$ gdb\nwarning: File \"/path/to/pytorch/.gdbinit\" auto-loading has been declined by your `auto-load safe-path' set to \"$debugdir:$datadir/auto-load\".\nTo enable execution of this file add\n        add-auto-load-safe-path /path/to/pytorch/.gdbinit\nline to your configuration file \"/home/YOUR-USERNAME/.gdbinit\".\nTo completely disable this security protection add\n        set auto-load safe-path /\nline to your configuration file \"/home/YOUR-USERNAME/.gdbinit\".\nFor more information about this security protection see the\n\"Auto-loading safe path\" section in the GDB manual.  E.g., run from the shell:\n        info \"(gdb)Auto-loading safe path\"\n(gdb)\n```\n\nAs gdb itself suggests, the best way to enable auto-loading of `pytorch-gdb`\nis to add the following line to your `~/.gdbinit` (i.e., the `.gdbinit` file\nwhich is in your home directory, **not** `/path/to/pytorch/.gdbinit`):\n\n```bash\nadd-auto-load-safe-path /path/to/pytorch/.gdbinit\n```\n\n### C++ stacktraces\nSet `TORCH_SHOW_CPP_STACKTRACES=1` to get the C++ stacktrace when an error occurs in Python.\n\n## CUDA development tips\n\nIf you are working on the CUDA code, here are some useful CUDA debugging tips:\n\n1. `CUDA_DEVICE_DEBUG=1` will enable CUDA device function debug symbols (`-g -G`).\n    This will be particularly helpful in debugging device code. However, it will\n    slow down the build process for about 50% (compared to only `DEBUG=1`), so use wisely.\n2. `cuda-gdb` and `cuda-memcheck` are your best CUDA debugging friends. Unlike`gdb`,\n   `cuda-gdb` can display actual values in a CUDA tensor (rather than all zeros).\n3. CUDA supports a lot of C++11/14 features such as, `std::numeric_limits`, `std::nextafter`,\n   `std::tuple` etc. in device code. Many of such features are possible because of the\n   [--expt-relaxed-constexpr](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#constexpr-functions)\n   nvcc flag. There is a known [issue](https://github.com/ROCm-Developer-Tools/HIP/issues/374)\n   that ROCm errors out on device code, which uses such stl functions.\n4. A good performance metric for a CUDA kernel is the\n   [Effective Memory Bandwidth](https://devblogs.nvidia.com/how-implement-performance-metrics-cuda-cc/).\n   It is useful for you to measure this metric whenever you are writing/optimizing a CUDA\n   kernel. Following script shows how we can measure the effective bandwidth of CUDA `uniform_`\n   kernel.\n   ```python\n   import torch\n   from torch.utils.benchmark import Timer\n   size = 128*512\n   nrep = 100\n   nbytes_read_write = 4 # this is number of bytes read + written by a kernel. Change this to fit your kernel.\n\n   for i in range(10):\n       a=torch.empty(size).cuda().uniform_()\n       torch.cuda.synchronize()\n       out = a.uniform_()\n       torch.cuda.synchronize()\n       t = Timer(stmt=\"a.uniform_()\", globals=globals())\n       res = t.blocked_autorange()\n       timec = res.median\n       print(\"uniform, size, elements\", size, \"forward\", timec, \"bandwidth (GB/s)\", size*(nbytes_read_write)*1e-9/timec)\n       size *=2\n   ```\n\n  See more cuda development tips [here](https://github.com/pytorch/pytorch/wiki/CUDA-basics)\n\n## Windows development tips\n\nFor building from source on Windows, consult\n[our documentation](https://pytorch.org/docs/stable/notes/windows.html) on it.\n\nOccasionally, you will write a patch which works on Linux, but fails CI on Windows.\nThere are a few aspects in which MSVC (the Windows compiler toolchain we use) is stricter\nthan Linux, which are worth keeping in mind when fixing these problems.\n\n1. Symbols are NOT exported by default on Windows; instead, you have to explicitly\n   mark a symbol as exported/imported in a header file with `__declspec(dllexport)` /\n   `__declspec(dllimport)`. We have codified this pattern into a set of macros\n   which follow the convention `*_API`, e.g., `TORCH_API` inside Caffe2, Aten and Torch.\n   (Every separate shared library needs a unique macro name, because symbol visibility\n   is on a per shared library basis. See c10/macros/Macros.h for more details.)\n\n   The upshot is if you see an \"unresolved external\" error in your Windows build, this\n   is probably because you forgot to mark a function with `*_API`. However, there is\n   one important counterexample to this principle: if you want a *templated* function\n   to be instantiated at the call site, do NOT mark it with `*_API` (if you do mark it,\n   you'll have to explicitly instantiate all of the specializations used by the call\n   sites.)\n\n2. If you link against a library, this does not make its dependencies transitively\n   visible. You must explicitly specify a link dependency against every library whose\n   symbols you use. (This is different from Linux where in most environments,\n   transitive dependencies can be used to fulfill unresolved symbols.)\n\n3. If you have a Windows box (we have a few on EC2 which you can request access to) and\n   you want to run the build, the easiest way is to just run `.ci/pytorch/win-build.sh`.\n   If you need to rebuild, run `REBUILD=1 .ci/pytorch/win-build.sh` (this will avoid\n   blowing away your Conda environment.)\n\nEven if you don't know anything about MSVC, you can use cmake to build simple programs on\nWindows; this can be helpful if you want to learn more about some peculiar linking behavior\nby reproducing it on a small example. Here's a simple example cmake file that defines\ntwo dynamic libraries, one linking with the other:\n\n```CMake\nproject(myproject CXX)\nset(CMAKE_CXX_STANDARD 14)\nadd_library(foo SHARED foo.cpp)\nadd_library(bar SHARED bar.cpp)\n# NB: don't forget to __declspec(dllexport) at least one symbol from foo,\n# otherwise foo.lib will not be created.\ntarget_link_libraries(bar PUBLIC foo)\n```\n\nYou can build it with:\n\n```bash\nmkdir build\ncd build\ncmake ..\ncmake --build .\n```\n\n### Known MSVC (and MSVC with NVCC) bugs\n\nThe PyTorch codebase sometimes likes to use exciting C++ features, and\nthese exciting features lead to exciting bugs in Windows compilers.\nTo add insult to injury, the error messages will often not tell you\nwhich line of code actually induced the erroring template instantiation.\n\nWe've found the most effective way to debug these problems is to\ncarefully read over diffs, keeping in mind known bugs in MSVC/NVCC.\nHere are a few well known pitfalls and workarounds:\n\n* This is not actually a bug per se, but in general, code generated by MSVC\n  is more sensitive to memory errors; you may have written some code\n  that does a use-after-free or stack overflows; on Linux the code\n  might work, but on Windows your program will crash. ASAN may not\n  catch all of these problems: stay vigilant to the possibility that\n  your crash is due to a real memory problem.\n\n* (NVCC) `c10::optional` does not work when used from device code. Don't use\n  it from kernels. Upstream issue: https://github.com/akrzemi1/Optional/issues/58\n  and our local issue #10329.\n\n* `constexpr` generally works less well on MSVC.\n\n  * The idiom `static_assert(f() == f())` to test if `f` is constexpr\n    does not work; you'll get \"error C2131: expression did not evaluate\n    to a constant\". Don't use these asserts on Windows.\n    (Example: `c10/util/intrusive_ptr.h`)\n\n* (NVCC) Code you access inside a `static_assert` will eagerly be\n  evaluated as if it were device code, and so you might get an error\n  that the code is \"not accessible\".\n\n```cpp\nclass A {\n  static A singleton_;\n  static constexpr inline A* singleton() {\n    return &singleton_;\n  }\n};\nstatic_assert(std::is_same(A*, decltype(A::singleton()))::value, \"hmm\");\n```\n\n* The compiler will run out of heap space if you attempt to compile files that\n  are too large. Splitting such files into separate files helps.\n  (Example: `THTensorMath`, `THTensorMoreMath`, `THTensorEvenMoreMath`.)\n\n* MSVC's preprocessor (but not the standard compiler) has a bug\n  where it incorrectly tokenizes raw string literals, ending when it sees a `\"`.\n  This causes preprocessor tokens inside the literal like an`#endif`  to be incorrectly\n  treated as preprocessor directives. See https://godbolt.org/z/eVTIJq as an example.\n\n* Either MSVC or the Windows headers have a PURE macro defined and will replace\n  any occurrences of the PURE token in code with an empty string. This is why\n  we have AliasAnalysisKind::PURE_FUNCTION and not AliasAnalysisKind::PURE.\n  The same is likely true for other identifiers that we just didn't try to use yet.\n\n### Building on legacy code and CUDA\n\nCUDA, MSVC, and PyTorch versions are interdependent; please install matching versions from this table:\n| CUDA version | Newest supported VS version                             | PyTorch version |\n| ------------ | ------------------------------------------------------- | --------------- |\n| 10.1         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |  1.3.0 ~ 1.7.0  |\n| 10.2         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |  1.5.0 ~ 1.7.0  |\n| 11.0         | Visual Studio 2019 (16.X) (`_MSC_VER` < 1930)           |      1.7.0      |\n\nNote: There's a [compilation issue](https://github.com/oneapi-src/oneDNN/issues/812) in several Visual Studio 2019 versions since 16.7.1, so please make sure your Visual Studio 2019 version is not in 16.7.1 ~ 16.7.5\n\n## Running clang-tidy\n\n[Clang-Tidy](https://clang.llvm.org/extra/clang-tidy/index.html) is a C++\nlinter and static analysis tool based on the clang compiler. We run clang-tidy\nin our CI to make sure that new C++ code is safe, sane and efficient. See the\n[`clang-tidy` job in our GitHub Workflow's\nlint.yml file](https://github.com/pytorch/pytorch/blob/main/.github/workflows/lint.yml)\nfor the simple commands we use for this.\n\nTo run clang-tidy locally, follow these steps:\n\n1. Install clang-tidy.\nWe provide custom built binaries which have additional checks enabled. You can install it by running:\n```bash\npython3 -m tools.linter.clang_tidy.generate_build_files\n```\nWe currently only support Linux and MacOS (x86).\n\n2. Install clang-tidy driver script dependencies\n```bash\npip3 install -r tools/linter/clang_tidy/requirements.txt\n```\n\n3. Run clang-tidy\n```bash\n# Run clang-tidy on the entire codebase\nmake clang-tidy\n# Run clang-tidy only on your changes\nmake clang-tidy CHANGED_ONLY=--changed-only\n```\nThis internally invokes our driver script and closely mimics how clang-tidy is run on CI.\n\n## Pre-commit tidy/linting hook\n\nWe use clang-tidy to perform additional\nformatting and semantic checking of code. We provide a pre-commit git hook for\nperforming these checks, before a commit is created:\n\n  ```bash\n  ln -s ../../tools/git-pre-commit .git/hooks/pre-commit\n  ```\n\nIf you have already committed files and\nCI reports `flake8` errors, you can run the check locally in your PR branch with:\n\n  ```bash\n  flake8 $(git diff --name-only $(git merge-base --fork-point main))\n  ```\n\nYou'll need to install an appropriately configured flake8; see\n[Lint as you type](https://github.com/pytorch/pytorch/wiki/Lint-as-you-type)\nfor documentation on how to do this.\n\nFix the code so that no errors are reported when you re-run the above check again,\nand then commit the fix.\n\n## Building PyTorch with ASAN\n\n[ASAN](https://github.com/google/sanitizers/wiki/AddressSanitizer) is very\nuseful for debugging memory errors in C++. We run it in CI, but here's how to\nget the same thing to run on your local machine.\n\nFirst, install LLVM 8. The easiest way is to get [prebuilt\nbinaries](http://releases.llvm.org/download.html#8.0.0) and extract them to\nfolder (later called `$LLVM_ROOT`).\n\nThen set up the appropriate scripts. You can put this in your `.bashrc`:\n\n```bash\nLLVM_ROOT=<wherever your llvm install is>\nPYTORCH_ROOT=<wherever your pytorch checkout is>\n\nLIBASAN_RT=\"$LLVM_ROOT/lib/clang/8.0.0/lib/linux/libclang_rt.asan-x86_64.so\"\nbuild_with_asan()\n{\n  LD_PRELOAD=${LIBASAN_RT} \\\n  CC=\"$LLVM_ROOT/bin/clang\" \\\n  CXX=\"$LLVM_ROOT/bin/clang++\" \\\n  LDSHARED=\"clang --shared\" \\\n  LDFLAGS=\"-stdlib=libstdc++\" \\\n  CFLAGS=\"-fsanitize=address -fno-sanitize-recover=all -shared-libasan -pthread\" \\\n  CXX_FLAGS=\"-pthread\" \\\n  USE_CUDA=0 USE_OPENMP=0 BUILD_CAFFE2_OPS=0 USE_DISTRIBUTED=0 DEBUG=1 \\\n  python setup.py develop\n}\n\nrun_with_asan()\n{\n  LD_PRELOAD=${LIBASAN_RT} $@\n}\n\n# you can look at build-asan.sh to find the latest options the CI uses\nexport ASAN_OPTIONS=detect_leaks=0:symbolize=1:strict_init_order=true\nexport UBSAN_OPTIONS=print_stacktrace=1:suppressions=$PYTORCH_ROOT/ubsan.supp\nexport ASAN_SYMBOLIZER_PATH=$LLVM_ROOT/bin/llvm-symbolizer\n```\n\nThen you can use the scripts like:\n\n```\nsuo-devfair ~/pytorch \u276f build_with_asan\nsuo-devfair ~/pytorch \u276f run_with_asan python test/test_jit.py\n```\n\n### Getting `ccache` to work\n\nThe scripts above specify the `clang` and `clang++` binaries directly, which\nbypasses `ccache`. Here's how to get `ccache` to work:\n\n1. Make sure the ccache symlinks for `clang` and `clang++` are set up (see\n   CONTRIBUTING.md)\n2. Make sure `$LLVM_ROOT/bin` is available on your `$PATH`.\n3. Change the `CC` and `CXX` variables in `build_with_asan()` to point\n   directly to `clang` and `clang++`.\n\n### Why this stuff with `LD_PRELOAD` and `LIBASAN_RT`?\n\nThe \u201cstandard\u201d workflow for ASAN assumes you have a standalone binary:\n\n1. Recompile your binary with `-fsanitize=address`.\n2. Run the binary, and ASAN will report whatever errors it find.\n\nUnfortunately, PyTorch is a distributed as a shared library that is loaded by\na third-party executable (Python). It\u2019s too much of a hassle to recompile all\nof Python every time we want to use ASAN. Luckily, the ASAN folks have a\nworkaround for cases like this:\n\n1. Recompile your library with `-fsanitize=address -shared-libasan`. The\n   extra `-shared-libasan` tells the compiler to ask for the shared ASAN\n   runtime library.\n2. Use `LD_PRELOAD` to tell the dynamic linker to load the ASAN runtime\n   library before anything else.\n\nMore information can be found\n[here](https://github.com/google/sanitizers/wiki/AddressSanitizerAsDso).\n\n### Why LD_PRELOAD in the build function?\n\nWe need `LD_PRELOAD` because there is a cmake check that ensures that a\nsimple program builds and runs. If we are building with ASAN as a shared\nlibrary, we need to `LD_PRELOAD` the runtime library, otherwise there will\ndynamic linker errors and the check will fail.\n\nWe don\u2019t actually need either of these if we fix the cmake checks.\n\n### Why no leak detection?\n\nPython leaks a lot of memory. Possibly we could configure a suppression file,\nbut we haven\u2019t gotten around to it.\n\n## Caffe2 notes\n\nIn 2018, we merged Caffe2 into the PyTorch source repository. While the\nsteady state aspiration is that Caffe2 and PyTorch share code freely,\nin the meantime there will be some separation.\n\nThere are a few \"unusual\" directories which, for historical reasons,\nare Caffe2/PyTorch specific. Here they are:\n\n- `CMakeLists.txt`, `Makefile`, `binaries`, `cmake`, `conda`, `modules`,\n  `scripts` are Caffe2-specific. Don't put PyTorch code in them without\n  extra coordination.\n\n- `mypy*`, `requirements.txt`, `setup.py`, `test`, `tools` are\n  PyTorch-specific. Don't put Caffe2 code in them without extra\n  coordination.\n\n## CI failure tips\n\nOnce you submit a PR or push a new commit to a branch that is in\nan active PR, CI jobs will be run automatically. Some of these may\nfail and you will need to find out why, by looking at the logs.\n\nFairly often, a CI failure might be unrelated to your changes. You can\nconfirm by going to our [HUD](https://hud.pytorch.org) and seeing if the CI job\nis failing upstream already. In this case, you\ncan usually ignore the failure. See [the following\nsubsection](#which-commit-is-used-in-ci) for more details.\n\nSome failures might be related to specific hardware or environment\nconfigurations. In this case, if you're a Meta employee, you can ssh into\nthe job's session to perform manual debugging following the instructions in\nour [CI wiki](https://github.com/pytorch/pytorch/wiki/Debugging-using-with-ssh-for-Github-Actions).\n\n\n### Which commit is used in CI?\n\nFor CI run on `main`, this repository is checked out for a given `main`\ncommit, and CI is run on that commit (there isn't really any other choice).\n\nFor PRs, however, it's a bit more complicated. Consider this commit graph, where\n`main` is at commit `A`, and the branch for PR #42 (just a placeholder) is at\ncommit `B`:\n\n```\n       o---o---B (refs/pull/42/head)\n      /         \\\n     /           C (refs/pull/42/merge)\n    /           /\n---o---o---o---A (merge-destination) - usually main\n```\n\nThere are two possible choices for which commit to use:\n\n1. Checkout commit `B`, the head of the PR (manually committed by the PR\n   author).\n2. Checkout commit `C`, the hypothetical result of what would happen if the PR\n   were merged into it's destination (usually `main`).\n\nFor all practical purposes, most people can think of the commit being used as\ncommit `B` (choice **1**).\n\nHowever, if workflow files (which govern CI behavior) were modified (either by your PR or since dev branch were created ) there's\na nuance to know about:\nThe workflow files themselves get taken from checkpoint `C`, the merger of your\nPR and the `main` branch. But only the workflow files get taken from that merged\ncheckpoint. Everything else (tests, code, etc) all get taken directly from your\nPR's commit (commit `B`). Please note, this scenario would never affect PRs authored by `ghstack` as they would not automatically ingest the updates from default branch.\n\n\n## Dev Infra Office Hours\n[Dev Infra Office Hours](https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours) are hosted every Friday to answer any questions regarding developer experience, Green HUD, and CI.\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/CONTRIBUTING.md"}, {"result": {"value": "Typically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n", "type": "Text_excerpt", "original_header": "Releases and Contributing"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "code_of_conduct": [{"result": {"value": "# Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to make participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, sex characteristics, gender identity and expression,\nlevel of experience, education, socio-economic status, nationality, personal\nappearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\nadvances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\naddress, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\nprofessional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies within all project spaces, and it also applies when\nan individual is representing the project or its community in public spaces.\nExamples of representing a project or community include using an official\nproject e-mail address, posting via an official social media account, or acting\nas an appointed representative at an online or offline event. Representation of\na project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at <conduct@pytorch.org>. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see\nhttps://www.contributor-covenant.org/faq\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/CODE_OF_CONDUCT.md"}], "citation": [{"result": {"value": "cff-version: 1.2.0\nmessage: If you use this software, please cite it as below.\ntitle: PyTorch\nauthors:\n  - family-names: PyTorch Team\nurl: https://pytorch.org\npreferred-citation:\n  type: conference-paper\n  title: \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\"\n  authors:\n    - family-names: Paszke\n      given-names: Adam\n    - family-names: Gross\n      given-names: Sam\n    - family-names: Massa\n      given-names: Francisco\n    - family-names: Lerer\n      given-names: Adam\n    - family-names: Bradbury\n      given-names: James\n    - family-names: Chanan\n      given-names: Gregory\n    - family-names: Killeen\n      given-names: Trevor\n    - family-names: Lin\n      given-names: Zeming\n    - family-names: Gimelshein\n      given-names: Natalia\n    - family-names: Antiga\n      given-names: Luca\n    - family-names: Desmaison\n      given-names: Alban\n    - family-names: Kopf\n      given-names: Andreas\n    - family-names: Yang\n      given-names: Edward\n    - family-names: DeVito\n      given-names: Zachary\n    - family-names: Raison\n      given-names: Martin\n    - family-names: Tejani\n      given-names: Alykhan\n    - family-names: Chilamkurthy\n      given-names: Sasank\n    - family-names: Steiner\n      given-names: Benoit\n    - family-names: Fang\n      given-names: Lu\n    - family-names: Bai\n      given-names: Junjie\n    - family-names: Chintala\n      given-names: Soumith\n  collection-title: Advances in Neural Information Processing Systems 32\n  collection-type: proceedings\n  editors:\n    - family-names: Wallach\n      given-names: H.\n    - family-names: Larochelle\n      given-names: H.\n    - family-names: Beygelzimer\n      given-names: A.\n    - family-names: \"d'Alch\u00e9-Buc\"\n      given-names: F.\n    - family-names: Fox\n      given-names: E.\n    - family-names: Garnett\n      given-names: R.\n  start: 8024\n  end: 8035\n  year: 2019\n  publisher:\n    name: Curran Associates, Inc.\n  url: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\n", "type": "File_dump", "format": "cff"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/CITATION.cff"}], "has_build_file": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.devcontainer/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.devcontainer/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/linter/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/linter/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/centos-rocm/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/centos-rocm/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu-rocm/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu-rocm/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu-xpu/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu-xpu/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/linter-cuda/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/linter-cuda/Dockerfile"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu-cuda/Dockerfile", "type": "Url", "format": "dockerfile"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/ubuntu-cuda/Dockerfile"}], "readme_url": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "documentation": [{"result": {"value": "https://github.com/pytorch/pytorch/tree/main/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/pytorch/tree/main/functorch/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/pytorch/tree/main/caffe2/contrib/aten/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://github.com/pytorch/pytorch/tree/main/torch/csrc/jit/docs", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "To build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and the\nreadthedocs theme.\n\n```bash\ncd docs/\npip install -r requirements.txt\n```\nYou can then build the documentation by running `make <format>` from the\n`docs/` folder. Run `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> Note: if you installed `nodejs` with a different package manager (e.g.,\n`conda`) then `npm` will probably install a version of `katex` that is not\ncompatible with your version of `nodejs` and doc builds will fail.\nA combination of versions that is known to work is `node@6.13.1` and\n`katex@0.13.18`. To install the latter with `npm` you can run\n```npm install -g katex@0.13.18```\n", "type": "Text_excerpt", "original_header": "Building the Documentation", "parent_header": ["Installation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Url", "value": "https://github.com/pytorch/pytorch/wiki", "format": "wiki"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "has_script_file": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/docs/cpp/source/check-doxygen.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/android/common.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/android/build_test_app.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/android/build_test_app_custom.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/android/run_tests.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/gen_flatbuffers.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/git_add_generated_dirs.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/git_reset_generated_dirs.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/onnx/gen_diagnostics.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/onnx/sarif/gen_sarif.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/bazel_tools/shellwrap.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/tools/iwyu/run.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/add_apache_header.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_pytorch_android.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/remove_apache_header.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/install_triton_wheel.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_tegra_x1.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/read_conda_versions.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_mobile.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/temp.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_ios.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/buck_setup.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_raspbian.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_local.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_host_protoc.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_tizen.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_android_gradle.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/build_android.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/onnx/test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/onnx/install-develop.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/onnx/install.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/apply-release-changes.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/restore-backup.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/cut-release-branch.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/anaconda-prune/run.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/anaconda-prune/prune.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/promote/s3_to_s3.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/promote/common_utils.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/promote/wheel_to_pypi.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/promote/prep_binary_for_pypi.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release/promote/conda_to_conda.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/fbcode-dev-setup/ccache_setup.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/analysis/run_test_csv.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/regenerate.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/scripts/lintrunner.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/scripts/pr-sanity-check.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/scripts/report_git_status.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/scripts/stop_runner_service.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/codegen_validation/overwrite_with_normalized.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/codegen_validation/compare_normalized_yaml.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/setup_linux_system_environment.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_upload.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_linux_build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/functorch_doc_push_script.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/setup_ci_environment.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_macos_test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_ios_test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_populate_env.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_run_in_docker.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_install_miniconda.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_linux_test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_ios_upload.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_macos_build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_ios_build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_windows_test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_checkout.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/binary_windows_build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.circleci/scripts/publish_android_snapshot.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.devcontainer/scripts/update_alternatives_clang.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.devcontainer/scripts/install-dev-tools.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/onnx/common.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/onnx/test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_cudnn.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_openssl.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_rocm_magma.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_xpu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/cache_vision_models.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_db.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_executorch.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_docs_reqs.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_base.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_devtoolset.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_linter.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_inductor_benchmark_deps.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_protobuf.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_clang.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/common_utils.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_jni.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_conda.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_glibc.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_lcov.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_android.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_cache.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_ninja.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_vision.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_rocm.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_onnx.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_triton.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_swiftshader.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_user.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_cusparselt.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_cmake.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_openmpi.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_ucc.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_gcc.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/docker/common/install_vulkan_sdk.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/docker-build-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/win-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/common.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/docs-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/cpp_doc_push_script.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/macos-common.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/macos-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/python_doc_push_script.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/functorch_doc_push_script.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/common_utils.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/run_glootls_test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/win-build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/multigpu-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/short-perf-test-cpu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/short-perf-test-gpu.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/macos-build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/macos-build-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/common-build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/build-mobile.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/pytorch/codegen-test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/caffe2/common.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/.ci/caffe2/test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/caffe2/proto/gen_proto_typestubs.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/tools/test_install.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/tools/run_tests.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/conda/build.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/test-android-armv7.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-armv7s.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-arm64.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-android-x86.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-x86_64.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-android-armv7.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-local.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/test-android-x86.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-arm64e.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-android-arm64.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/test-android-arm64.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-i386.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-armv7.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/benchmarks/compare.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/benchmarks/dynamo/run_delta.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/benchmarks/dynamo/run_all.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/benchmarks/sparse/test_csr.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/benchmarks/sparse/dlmc/test.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/benchmarks/inference/runner.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/benchmark/examples/prepare_e2e.sh", "type": "Url"}, "confidence": 1, "technique": "file_exploration"}], "installation": [{"result": {"value": "## Building Caffe2\n\nThis guide builds from source. For alternatives, refer to https://caffe2.ai/docs/getting-started.html\n\nGet latest source from GitHub.\n\n    git clone --recursive https://github.com/caffe2/caffe2.git\n    cd caffe2\n\nNote that you might need to uninstall existing Eigen and pybind11 packages due to compile-time dependencies when building from source. For this reason, Caffe2 uses git submodules to reference external packages in the third_party folder. These are downloaded with the --recursive option.\n\n#### MacOS X\n\n    brew install openblas glog gtest automake protobuf leveled lmdb\n    mkdir build && cd build\n    cmake .. -DBLAS=OpenBLAS -DUSE_OPENCV=off\n    make\n\n#### Ubuntu\n\n###### Ubuntu 14.04 LTS\n    sudo apt-get install libprotobuf-dev protobuf-compiler libatlas-base-dev libgoogle-glog-dev libgtest-dev liblmdb-dev libleveldb-dev libsnappy-dev python-dev python-pip libiomp-dev libopencv-dev libpthread-stubs0-dev cmake\n    sudo pip install numpy\n    wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1404/x86_64/cuda-repo-ubuntu1404_8.0.44-1_amd64.deb\n    sudo dpkg -i cuda-repo-ubuntu1404_8.0.44-1_amd64.deb\n    sudo apt-get update\n    sudo apt-get install cuda\n    sudo apt-get install git\n\n    CUDNN_URL=\"http://developer.download.nvidia.com/compute/redist/cudnn/v5.1/cudnn-8.0-linux-x64-v5.1.tgz\" &&\n    curl -fsSL ${CUDNN_URL} -O &&\n    sudo tar -xzf cudnn-8.0-linux-x64-v5.1.tgz -C /usr/local &&\n    rm cudnn-8.0-linux-x64-v5.1.tgz &&\n    sudo ldconfig\n\n    mkdir build && cd build\n    cmake ..\n    make\n\n###### Ubuntu 16.04 LTS\n    sudo apt-get install libprotobuf-dev protobuf-compiler libatlas-base-dev libgoogle-glog-dev libgtest-dev liblmdb-dev libleveldb-dev libsnappy-dev python-dev python-pip libiomp-dev libopencv-dev libpthread-stubs0-dev cmake\n    sudo pip install numpy\n    wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n    sudo dpkg -i cuda-repo-ubuntu1604_8.0.61-1_amd64.deb\n    sudo apt-get update\n    sudo apt-get install cuda\n    sudo apt-get install git\n\n    CUDNN_URL=\"http://developer.download.nvidia.com/compute/redist/cudnn/v5.1/cudnn-8.0-linux-x64-v5.1.tgz\" &&\n    curl -fsSL ${CUDNN_URL} -O &&\n    sudo tar -xzf cudnn-8.0-linux-x64-v5.1.tgz -C /usr/local &&\n    rm cudnn-8.0-linux-x64-v5.1.tgz &&\n    sudo ldconfig\n\n    mkdir build && cd build\n    cmake ..\n    make\n\n## Python support\n\nTo run the tutorials, download additional source from GitHub.\n\n    git clone --recursive https://github.com/caffe2/tutorials.git caffe2_tutorials\n    cd caffe2_tutorials\n\nYou'll also need jupyter (formerly ipython) notebooks and matplotlib, which can be installed on MacOS X with\n\n    brew install matplotlib --with-python3\n    pip install jupyter\n", "type": "File_dump"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/docs/caffe2/installation.md"}, {"result": {"value": "Commands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n", "type": "Text_excerpt", "original_header": "Binaries", "parent_header": ["Installation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "Python wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\n", "type": "Text_excerpt", "original_header": "NVIDIA Jetson Platforms", "parent_header": ["Installation", "Binaries"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "**Common**\n\n```bash\nconda install cmake ninja\n# Run this command from the PyTorch directory after cloning the source code using the \u201cGet the PyTorch Source\u201c section below\npip install -r requirements.txt\n```\n\n**On Linux**\n\n```bash\nconda install intel::mkl-static intel::mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\nconda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\nconda install intel::mkl-static intel::mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\nconda install intel::mkl-static intel::mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.39\n```\n", "type": "Text_excerpt", "original_header": "Install Dependencies", "parent_header": ["Installation", "From Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "**On Linux**\n\nIf you would like to compile PyTorch with [new C++ ABI](https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html) enabled, then first run this command:\n```bash\nexport _GLIBCXX_USE_CXX11_ABI=1\n```\n\nIf you're compiling for AMD ROCm then first run this command:\n```bash\n# Only run this if you're compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\nInstall PyTorch\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py develop\n```\n\n> _Aside:_ If you are using [Anaconda](https://www.anaconda.com/distribution/#download-section), you may experience an error caused by the linker:\n>\n> ```plaintext\n> build/temp.linux-x86_64-3.7/torch/csrc/stub.o: file not recognized: file format not recognized\n> collect2: error: ld returned 1 exit status\n> error: command 'g++' failed with exit status 1\n> ```\n>\n> This is caused by `ld` from the Conda environment shadowing the system `ld`. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.8.1+.\n\n**On macOS**\n\n```bash\npython3 setup.py develop\n```\n\n**On Windows**\n\nChoose Correct Visual Studio Version.\n\nPyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*\ncome with Visual Studio Code by default.\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU\n\n```cmd\nconda activate\npython setup.py develop\n```\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython setup.py develop\n\n```\n", "type": "Text_excerpt", "original_header": "Install PyTorch", "parent_header": ["Installation", "From Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "You can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\npython setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n```bash\nexport CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\nMACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only\nccmake build  # or cmake-gui build\n```\n", "type": "Text_excerpt", "original_header": "Adjust Build Options (Optional)", "parent_header": ["Installation", "From Source", "Install PyTorch"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n", "type": "Text_excerpt", "original_header": "Using pre-built images", "parent_header": ["Installation", "Docker Image"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\nYou can also pass the `CMAKE_VARS=\"...\"` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\n```bash\nCMAKE_VARS=\"BUILD_CAFFE2=ON BUILD_CAFFE2_OPS=ON\" make -f docker.Makefile\n```\n", "type": "Text_excerpt", "original_header": "Building the image yourself", "parent_header": ["Installation", "Docker Image"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "Installation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/previous-versions).\n\n", "type": "Text_excerpt", "original_header": "Previous Versions", "parent_header": ["Installation"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "If you use NumPy, then you have used Tensors (a.k.a. ndarray). \n", "original_header": "A GPU-Ready Tensor Library"}, "confidence": 0.9614771994663341, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n \n", "original_header": "Communication"}, "confidence": 1.0, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Text_excerpt", "value": "You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed. \n"}, "confidence": 0.9999383949181567, "technique": "supervised_classification", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "executable_example": [{"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/per_sample_grads.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/per_sample_grads.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/aot_autograd_optimizations.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/aot_autograd_optimizations.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/jacobians_hessians.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/jacobians_hessians.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/whirlwind_tour.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/whirlwind_tour.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/minifier.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/minifier.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/neural_tangent_kernels.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/neural_tangent_kernels.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/ensembling.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/functorch/notebooks/ensembling.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release_notes/explore.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/scripts/release_notes/explore.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/data/standard_pipes.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/data/standard_pipes.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/data/dataframes_pipes.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/data/dataframes_pipes.ipynb"}, {"result": {"value": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/data/typing.ipynb", "type": "Url", "format": "jupyter_notebook"}, "confidence": 1, "technique": "file_exploration", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/torch/utils/data/typing.ipynb"}], "type": [{"result": {"value": "commandline-application", "type": "String"}, "confidence": 0.82, "technique": "software_type_heuristics"}], "requirements": [{"result": {"value": "If you are installing from source, you will need:\n- Python 3.8 or later (for Linux, Python 3.8.1+ is needed)\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required)\n\nWe highly recommend installing an [Anaconda](https://www.anaconda.com/download) environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.\n\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/pdf/cuDNN-Support-Matrix.pdf) for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems.\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n", "type": "Text_excerpt", "original_header": "Prerequisites", "parent_header": ["Installation", "From Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "**Common**\n\n```bash\nconda install cmake ninja\n# Run this command from the PyTorch directory after cloning the source code using the \u201cGet the PyTorch Source\u201c section below\npip install -r requirements.txt\n```\n\n**On Linux**\n\n```bash\nconda install intel::mkl-static intel::mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\nconda install -c pytorch magma-cuda110  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\nconda install intel::mkl-static intel::mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\nconda install intel::mkl-static intel::mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.39\n```\n", "type": "Text_excerpt", "original_header": "Install Dependencies", "parent_header": ["Installation", "From Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "usage": [{"result": {"value": "```bash\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n", "type": "Text_excerpt", "original_header": "Get the PyTorch Source", "parent_header": ["Installation", "From Source"]}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"value": "Three-pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)\n", "type": "Text_excerpt", "original_header": "Getting Started"}, "confidence": 1, "technique": "header_analysis", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "application_domain": [{"result": {"type": "String", "value": "Computer Vision"}, "confidence": 16.18, "technique": "supervised_classification"}], "full_title": [{"result": {"type": "String", "value": ""}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "logo": [{"result": {"type": "Url", "value": "https://raw.githubusercontent.com/pytorch/pytorch/main/docs/source/_static/img/pytorch-logo-dark.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}], "images": [{"result": {"type": "Url", "value": "https://raw.githubusercontent.com/pytorch/pytorch/main/./docs/source/_static/img/tensor_illustration.png"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}, {"result": {"type": "Url", "value": "https://raw.githubusercontent.com/pytorch/pytorch/main/docs/source/_static/img/dynamic_graph.gif"}, "confidence": 1, "technique": "regular_expression", "source": "https://raw.githubusercontent.com/pytorch/pytorch/main/README.md"}]}