{"description": [{"excerpt": "PyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system \n", "confidence": [[0.9668267626196996]], "technique": "Supervised classification"}, {"excerpt": "- [Releases and Contributing](#releases-and-contributing) \n- [The Team](#the-team) \n", "confidence": [[0.908925214220865, 0.8670209717789463]], "technique": "Supervised classification", "originalHeader": "Communication"}, {"excerpt": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "confidence": [1.0], "technique": "GitHub API"}], "citation": [{"excerpt": "@incollection{NEURIPS2019_9015,\ntitle = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},\nauthor = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},\nbooktitle = {Advances in Neural Information Processing Systems 32},\neditor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch\\'{e}-Buc and E. Fox and R. Garnett},\npages = {8024--8035},\nyear = {2019},\npublisher = {Curran Associates, Inc.},\nurl = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}\n}", "confidence": [1.0], "technique": "File Exploration"}, {"excerpt": "  - [Fast and Lean](#fast-and-lean) \n  - [Extensions Without Pain](#extensions-without-pain) \n", "confidence": [[0.9594253736471909, 0.8283216015784888]], "technique": "Supervised classification", "originalHeader": "Imperative Experiences", "parentHeader": "More About PyTorch"}, {"excerpt": "- [Releases and Contributing](#releases-and-contributing) \n", "confidence": [[0.855313318686353]], "technique": "Supervised classification", "originalHeader": "Resources"}], "installation": [{"originalHeader": "Install PyTorch", "excerpt": "  - [Docker Image](#docker-image)", "parentHeader": "Installation --- From Source", "confidence": [1], "technique": "Header extraction"}, {"originalHeader": "Install Dependencies", "excerpt": "    - [Install PyTorch](#install-pytorch)", "parentHeader": "Installation --- From Source", "confidence": [1], "technique": "Header extraction"}, {"excerpt": "You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed. \n", "confidence": [[0.9666476737066539]], "technique": "Supervised classification"}, {"excerpt": "- [Installation](#installation) \n  - [Binaries](#binaries) \n  - [From Source](#from-source) \n    - [Install Dependencies](#install-dependencies) \n    - [Get the PyTorch Source](#get-the-pytorch-source) \n", "confidence": [[0.9769420979004304, 0.8237203173575234, 0.913345215760235, 0.9954583257582014, 0.9878123587809987]], "technique": "Supervised classification", "originalHeader": "From Source", "parentHeader": "Installation"}], "invocation": [{"excerpt": "  - [Python First](#python-first) \n", "confidence": [[0.8653528179410523]], "technique": "Supervised classification", "originalHeader": "A GPU-Ready Tensor Library", "parentHeader": "More About PyTorch"}], "documentation": [{"originalHeader": "Building the Documentation", "excerpt": "- [Getting Started](#getting-started)", "parentHeader": "Installation", "confidence": [1], "technique": "Header extraction"}], "license": {"excerpt": {"name": "Other", "url": "https://raw.githubusercontent.com/pytorch/pytorch/master/LICENSE"}, "confidence": [1.0], "technique": "GitHub API"}, "requirement": [{"originalHeader": "Install Dependencies", "excerpt": "    - [Install PyTorch](#install-pytorch)", "parentHeader": "Installation --- From Source", "confidence": [1], "technique": "Header extraction"}], "usage": [{"originalHeader": "Get the PyTorch Source", "excerpt": "      - [Adjust Build Options (Optional)](#adjust-build-options-optional)", "parentHeader": "Installation --- From Source", "confidence": [1], "technique": "Header extraction"}, {"originalHeader": "Getting Started", "excerpt": "- [Communication](#communication)", "confidence": [1], "technique": "Header extraction"}], "codeRepository": {"excerpt": "https://github.com/pytorch/pytorch", "confidence": [1.0], "technique": "GitHub API"}, "owner": {"excerpt": "pytorch", "confidence": [1.0], "technique": "GitHub API"}, "ownerType": {"excerpt": "Organization", "confidence": [1.0], "technique": "GitHub API"}, "dateCreated": {"excerpt": "2016-08-13T05:26:41Z", "confidence": [1.0], "technique": "GitHub API"}, "dateModified": {"excerpt": "2022-03-14T17:58:58Z", "confidence": [1.0], "technique": "GitHub API"}, "name": {"excerpt": "pytorch", "confidence": [1.0], "technique": "GitHub API"}, "fullName": {"excerpt": "pytorch/pytorch", "confidence": [1.0], "technique": "GitHub API"}, "issueTracker": {"excerpt": "https://api.github.com/repos/pytorch/pytorch/issues{/number}", "confidence": [1.0], "technique": "GitHub API"}, "forksUrl": {"excerpt": "https://api.github.com/repos/pytorch/pytorch/forks", "confidence": [1.0], "technique": "GitHub API"}, "downloadUrl": {"excerpt": "https://github.com/pytorch/pytorch/releases", "confidence": [1.0], "technique": "GitHub API"}, "topics": {"excerpt": ["neural-network", "autograd", "gpu", "numpy", "deep-learning", "tensor", "python", "machine-learning"], "confidence": [1.0], "technique": "GitHub API"}, "stargazersCount": {"excerpt": {"count": 54592, "date": "Mon, 14 Mar 2022 18:56:05 GMT"}, "confidence": [1.0], "technique": "GitHub API"}, "forksCount": {"excerpt": {"count": 15095, "date": "Mon, 14 Mar 2022 18:56:05 GMT"}, "confidence": [1.0], "technique": "GitHub API"}, "languages": {"excerpt": ["C++", "Python", "Cuda", "C", "CMake", "Objective-C++", "Shell", "Assembly", "Starlark", "Java", "PureBasic", "Jinja", "GLSL", "Jupyter Notebook", "Metal", "JavaScript", "Batchfile", "Dockerfile", "Objective-C", "Ruby", "Makefile", "PowerShell", "HTML", "Yacc", "CSS", "LLVM", "GDB", "Vim script"], "confidence": [1.0], "technique": "GitHub API"}, "readmeUrl": {"excerpt": "https://github.com/pytorch/pytorch/blob/master/README.md", "confidence": [1.0], "technique": "GitHub API"}, "codeOfConduct": {"excerpt": "https://raw.githubusercontent.com/pytorch/pytorch/master/CODE_OF_CONDUCT.md", "confidence": [1.0], "technique": "File Exploration"}, "licenseText": {"excerpt": "b'Copyright 2019-2020 Kakao Brain\\n\\nRedistribution and use in source and binary forms, with or without\\nmodification, are permitted provided that the following conditions are met:\\n\\n1. Redistributions of source code must retain the above copyright\\n   notice, this list of conditions and the following disclaimer.\\n\\n2. Redistributions in binary form must reproduce the above copyright\\n   notice, this list of conditions and the following disclaimer in the\\n   documentation and/or other materials provided with the distribution.\\n\\n3. Neither the name of the copyright holder nor the names of its\\n   contributors may be used to endorse or promote products derived from this\\n   software without specific prior written permission.\\n\\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\\nPOSSIBILITY OF SUCH DAMAGE.\\n'", "confidence": [1.0], "technique": "File Exploration"}, "contributingGuidelines": {"excerpt": "Contributing to QNNPACK\nWe want to make contributing to this project as easy and transparent as\npossible.\nCode of Conduct\nThe code of conduct is described in CODE_OF_CONDUCT.md.\nPull Requests\nWe actively welcome your pull requests.\n\nFork the repo and create your branch from master.\nIf you've added code that should be tested, add tests.\nIf you've added new micro-kernels, update or add micro-benchmarks.\nEnsure the test suite passes.\nMake sure your code lints.\nIf you haven't already, complete the Contributor License Agreement (\"CLA\").\n\nContributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\nComplete your CLA here: https://code.facebook.com/cla\nIssues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\nFacebook has a bounty program for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\nLicense\nBy contributing to QNNPACK, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.", "confidence": [1.0], "technique": "File Exploration"}, "hasExecutableNotebook": {"excerpt": ["https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/data/typing.ipynb", "https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/data/dataframes_pipes.ipynb", "https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/data/standard_pipes.ipynb"], "confidence": [1.0], "technique": "File Exploration"}, "hasBuildFile": {"excerpt": ["https://raw.githubusercontent.com/pytorch/pytorch/master/Dockerfile", "https://raw.githubusercontent.com/pytorch/pytorch/master/caffe2/contrib/docker-ubuntu-14.04/Dockerfile", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/ubuntu-cuda/Dockerfile", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/centos-rocm/Dockerfile", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/ubuntu-rocm/Dockerfile", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/ubuntu/Dockerfile"], "confidence": [1.0], "technique": "File Exploration"}, "hasDocumentation": {"excerpt": ["https://github.com/pytorch/pytorch/tree/master/docs", "https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/docs", "https://github.com/pytorch/pytorch/tree/master/torch/csrc/jit/codegen/cuda/docs", "https://github.com/pytorch/pytorch/tree/master/caffe2/python/docs", "https://github.com/pytorch/pytorch/tree/master/caffe2/contrib/aten/docs"], "confidence": [1.0], "technique": "File Exploration"}, "hasScriptFile": {"excerpt": ["https://raw.githubusercontent.com/pytorch/pytorch/master/torch/csrc/deploy/interpreter/configure_cpython.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/benchmark/examples/prepare_e2e.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/android/common.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/android/build_test_app_custom.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/android/run_tests.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/android/build_test_app.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_local.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/remove_apache_header.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/temp.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/read_conda_versions.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_mobile.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_android.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_ios.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/add_apache_header.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_host_protoc.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_raspbian.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_pytorch_android.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_tegra_x1.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/build_tizen.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/onnx/test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/onnx/install-develop.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/onnx/install.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/restore-backup.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/promote/s3_to_s3.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/promote/prep_binary_for_pypi.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/promote/wheel_to_pypi.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/promote/conda_to_conda.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/promote/common_utils.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/anaconda-prune/run.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/release/anaconda-prune/prune.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/fbcode-dev-setup/onnx_c2_setup.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/fbcode-dev-setup/onnx_c2_sanity_check.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/scripts/fbcode-dev-setup/ccache_setup.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/caffe2/proto/gen_proto_typestubs.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/regenerate.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/setup_ci_environment.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_run_in_docker.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/cpp_doc_push_script.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_linux_test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_macos_build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_upload.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/windows_cuda_install.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_macos_test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/build_android_gradle.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_windows_build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_ios_build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/setup_linux_system_environment.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_windows_test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_populate_env.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_install_miniconda.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_ios_upload.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_ios_test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_checkout.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/windows_cudnn_install.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/python_doc_push_script.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/binary_linux_build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/scripts/publish_android_snapshot.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/build_docker.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_openssl.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_glibc.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_thrift.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_vision.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_cmake.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_swiftshader.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_katex.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_jni.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_clang.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_lcov.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_db.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_devtoolset.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_android.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_gcc.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_protobuf.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_openmpi.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_user.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_cache.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_conda.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_ninja.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_vulkan_sdk.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_rocm.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/docker/common/install_base.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/codegen_validation/compare_normalized_yaml.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.circleci/codegen_validation/overwrite_with_normalized.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/tools/git_reset_generated_dirs.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/tools/git_add_generated_dirs.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/tools/gen_flatbuffers.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/tools/iwyu/run.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/tools/linter/run_shellcheck.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/tools/linter/clang_format_ci.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/conda/build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/tools/test_install.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/tools/run_tests.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-x86_64.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-armv7.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/test-android-x86.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-android-x86.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/test-android-armv7.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-local.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-armv7s.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-android-armv7.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/test-android-arm64.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-arm64.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-i386.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-android-arm64.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/aten/src/ATen/native/quantized/cpu/qnnpack/scripts/build-ios-arm64e.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.github/regenerate.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.github/scripts/wait_for_ssh_to_drain.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.github/scripts/build_publish_nightly_docker.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.github/scripts/report_git_status.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.github/scripts/install_nvidia_utils_linux.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/docs/cpp/source/check-doxygen.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/multigpu-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/codegen-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/win-build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/macos-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/docker-build-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/common.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/run_glootls_test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/short-perf-test-cpu.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/dirty.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/docs-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/short-perf-test-gpu.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/build-asan.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/macos-build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/build-mobile.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/common_utils.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/macos-build-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/win-test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/macos-common.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_cpu_speed_mnist.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_gpu_speed_cudnn_lstm.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_cpu_speed_torch.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_gpu_speed_word_language_model.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/common.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_gpu_speed_mnist.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_gpu_speed_mlstm.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_cpu_speed_mini_sequence_labeler.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_gpu_speed_lstm.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/pytorch/perf_test/test_cpu_speed_torch_tensor.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/caffe2/test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/caffe2/common.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/caffe2/bench.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/caffe2/dirty.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/.jenkins/caffe2/build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/benchmarks/compare.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/benchmarks/sparse/test_csr.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/benchmarks/sparse/dlmc/test.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/test/mobile/custom_build/build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/test/mobile/nnc/test_aot_compile.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/test/mobile/lightweight_dispatch/build.sh", "https://raw.githubusercontent.com/pytorch/pytorch/master/test/distributed/launcher/bin/test_script.sh"], "confidence": [1.0], "technique": "File Exploration"}, "releases": {"excerpt": [{"tagName": "v1.11.0", "name": " PyTorch 1.11, TorchData, and functorch are now available", "authorName": "bdhirsh", "authorType": "User", "body": "# PyTorch 1.11 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.11. This release is composed of over 3,300 commits since 1.10, made by 434 contributors. Along with 1.11, we are releasing beta versions of TorchData and functorch. We want to sincerely thank our community for continuously improving PyTorch.\r\n\r\n\r\n* TorchData is a new library for common modular data loading primitives for easily constructing flexible and performant data pipelines. [_View it on GitHub_](https://github.com/pytorch/data). \r\n* functorch, a library that adds composable function transforms to PyTorch, is now available in beta. [_View it on GitHub_](https://github.com/pytorch/functorch).\r\n* Distributed Data Parallel (DDP) static graph optimizations available in stable.\r\n\r\nYou can check the blogpost that shows the new features [here](https://pytorch.org/blog/pytorch-1.11-released/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### Fixed python `deepcopy` to correctly copy all attributes on `Tensor` objects ([#65584](https://github.com/pytorch/pytorch/pull/65584))\r\n\r\nThis change ensures that the `deepcopy` operation on Tensor properly copies all the attributes (and not just the plain Tensor properties).\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\na = torch.rand(2)\r\na.foo = 3\r\ntorch.save(a, \"bar\")\r\nb = torch.load(\"bar\")\r\nprint(b.foo)\r\n# Raise AttributeError: \"Tensor\" object has no attribute \"foo\"\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\na = torch.rand(2)\r\na.foo = 3\r\ntorch.save(a, \"bar\")\r\nb = torch.load(\"bar\")\r\nprint(b.foo)\r\n# 3\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### **`steps` argument is no longer optional in `torch.linspace` and `torch.logspace`**\r\n\r\nThis argument used to default to 100 in PyTorch 1.10.2, but was deprecated (previously you would see a deprecation warning if you didn\u2019t explicitly pass in `steps`). In PyTorch 1.11, it is not longer optional.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# Works, but raises a deprecation warning\r\n# Steps defaults to 100\r\na = torch.linspace(1, 10)\r\n# UserWarning: Not providing a value for linspace's steps is deprecated\r\n# and will throw a runtime error in a future release.\r\n# This warning will appear only once per process.\r\n# (Triggered internally at  ../aten/src/ATen/native/RangeFactories.cpp:19\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# In 1.11, you must specify steps\r\na = torch.linspace(1, 10, steps=100)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Remove `torch.hub.import_module` function that was mistakenly public ([#67990](https://github.com/pytorch/pytorch/pull/67990))\r\n\r\nThis function is not intended for public use.\r\nIf you have existing code that relies on it, you can find an equivalent function at `torch.hub._import_module`.\r\n\r\n## C++ API\r\n\r\n### **We\u2019ve cleaned up many of the headers in the C++ frontend to only include the subset of `aten` operators that they actually used ([#68247](https://github.com/pytorch/pytorch/pull/68247), [#68687](https://github.com/pytorch/pytorch/pull/68687), [#68688](https://github.com/pytorch/pytorch/pull/68688), [#68714](https://github.com/pytorch/pytorch/pull/68714), [#68689](https://github.com/pytorch/pytorch/pull/68689), [#68690](https://github.com/pytorch/pytorch/pull/68690), [#68697](https://github.com/pytorch/pytorch/pull/68697), [#68691](https://github.com/pytorch/pytorch/pull/68691), [#68692](https://github.com/pytorch/pytorch/pull/68692), [#68693](https://github.com/pytorch/pytorch/pull/68693), [#69840](https://github.com/pytorch/pytorch/pull/69840))**\r\n\r\nWhen you `#include` a header from the C++ frontend, you can no longer assume that every `aten` operators are transitively included. You can work around this by directly adding `#include <ATen/ATen.h>` in your file, which will maintain the old behavior of including every `aten` operators.\r\n\r\n###  **Custom implementation for `c10::List` and `c10::Dict` move constructors have been removed (**[**#69370**](https://github.com/pytorch/pytorch/pull/69370)**)**\r\n\r\nThe semantics have changed from \"make the moved-from List/Dict empty\" to \"keep the moved-from List/Dict unchanged\"\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"cpp\">\r\nc10::List<string> list1({\"3\", \"4\"});\r\nc10::List<string> list2(std::move(list1));\r\nstd::cout << list1.size() // 0\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"cpp\">\r\nc10::List<string> list1({\"3\", \"4\"});\r\nc10::List<string> list2(std::move(list1)); // calls copy ctr\r\nstd::cout << list1.size() // 2\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## CUDA\r\n\r\n### **Removed `THCeilDiv` function and corresponding `THC/THCDeviceUtils.cuh` header ([#65472](https://github.com/pytorch/pytorch/pull/65472))**\r\n\r\nAs part of cleaning up `TH` from the codebase, the `THCeilDiv` function has been removed. Instead, please use `at::ceil_div`, and include the corresponding `ATen/ceil_div.h` header\r\n\r\n### **Removed `THCudaCheck` (**[**#66391**](https://github.com/pytorch/pytorch/pull/66391)**)**\r\n\r\nYou can replace it with `C10_CUDA_CHECK`, which has been available since at least PyTorch 1.4, so just replacing is enough even if you support older versions\r\n\r\n### **Removed `THCudaMalloc()`, `THCudaFree()`,  `THCThrustAllocator.cuh` (**[**#65492**](https://github.com/pytorch/pytorch/pull/65492)**)**\r\n\r\nIf your extension is using `THCThrustAllocator.cuh`, please replace it with `ATen/cuda/ThrustAllocator.h` and corresponding APIs (see examples in this PR).\r\n\r\nThis PR also removes `THCudaMalloc/THCudaFree` calls. Please use `c10::cuda::CUDACachingAllocator::raw_alloc(size)/raw_delete(ptr)`, or, preferably, switch to `c10:cuda::CUDaCachingAllocator::allocate` which manages deallocation. Caching allocator APIs are available since PyTorch 1.2, so just replacing it is enough even if you support older versions of PyTorch.\r\n\r\n## Build\r\n\r\n### Stopped building shared library for AOT Compiler, `libaot_compiler.so` ([#66227](https://github.com/pytorch/pytorch/pull/66227))\r\n\r\nBuilding `aot_compiler.cpp` as a separate library is not necessary, as it\u2019s already included in `libtorch.so`.\r\nYou can update your build system to only dynamically link `libtorch.so`.\r\n\r\n## Mobile\r\n\r\n### Make `typing.Union` type unsupported for mobile builds ([#65556](https://github.com/pytorch/pytorch/pull/65556))\r\n\r\n`typing.Union` support was added for TorchScript in 1.10. It was removed specifically for mobile due to its lack of use and increase in binary size of PyTorch for Mobile builds.\r\n\r\n## Distributed\r\n\r\n### `torch.distributed.rpc`: Final Removal of ProcessGroup RPC backend ([#67363](https://github.com/pytorch/pytorch/pull/67363))\r\n\r\nProcessGroup RPC backend is deprecated. In 1.10, it threw an error to help users update their code, and, in 1.11, it is removed completely.\r\n\r\nThe backend type \u201cPROCESS_GROUP\u201d is now deprecated, e.g.\r\n`torch.distributed.rpc.init_rpc(\"worker0\", backend=\"PROCESS_GROUP\", rank=0, world_size=1)`\r\nand should be replaced with:\r\n`torch.distributed.rpc.init_rpc(\"worker0\", backend=\"TENSORPIPE\", rank=0, world_size=1)`\r\n\r\n## Quantization\r\n\r\n### Disabled the support for `getitem` in FX Graph Mode Quantization ([#66647](https://github.com/pytorch/pytorch/pull/66647))\r\n\r\n`getitem` used to be quantized in `FX Graph Mode Quantization`, and it is no longer quantized. This won\u2019t break any models but could result in a slight difference in numerics.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = torch.nn.Linear(5, 5)\r\n    def forward(self, x):\r\n        x = self.linear(x)\r\n        y = torch.stack([x], 0)\r\n        return y[0]\r\nm = M().eval()\r\nm = prepare_fx(m, {\"\": torch.ao.quantization.default_qconfig})\r\nm = convert_fx(m)\r\nprint(m)\r\n# prints\r\n# GraphModule(\r\n#   (linear): QuantizedLinear(in_features=5, out_features=5,\r\n#      scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\r\n# )\r\n# def forward(self, x):\r\n#     linear_input_scale_0 = self.linear_input_scale_0\r\n#     linear_input_zero_point_0 = self.linear_input_zero_point_0\r\n#     quantize_per_tensor = torch.quantize_per_tensor(x,\r\n#         linear_input_scale_0, linear_input_zero_point_0, torch.quint8)\r\n#     x = linear_input_scale_0 = linear_input_zero_point_0 = None\r\n#     linear = self.linear(quantize_per_tensor)\r\n#     quantize_per_tensor = None\r\n#     stack = torch.stack([linear], 0);  linear = None\r\n#     getitem = stack[0]; stack = None\r\n#     dequantize_2 = getitem.dequantize();  getitem = None\r\n#     return getitem\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nfrom torch.ao.quantization.quantize_fx import convert_fx, prepare_fx\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = torch.nn.Linear(5, 5)\r\n    def forward(self, x):\r\n        x = self.linear(x)\r\n        y = torch.stack([x], 0)\r\n        return y[0]\r\nm = M().eval()\r\nm = prepare_fx(m, {\"\": torch.ao.quantization.default_qconfig})\r\nm = convert_fx(m)\r\nprint(m)\r\n# prints\r\n# GraphModule(\r\n#   (linear): QuantizedLinear(in_features=5, out_features=5, scale=1.0,\r\n                    zero_point=0, qscheme=torch.per_tensor_affine)\r\n# )\r\n# def forward(self, x):\r\n#     linear_input_scale_0 = self.linear_input_scale_0\r\n#     linear_input_zero_point_0 = self.linear_input_zero_point_0\r\n#     quantize_per_tensor = torch.quantize_per_tensor(x, linear_input_scale_0,\r\n                     linear_input_zero_point_0, torch.quint8)\r\n#     x = linear_input_scale_0 = linear_input_zero_point_0 = None\r\n#     linear = self.linear(quantize_per_tensor);  quantize_per_tensor = None\r\n#     stack = torch.stack([linear], 0);  linear = None\r\n#     dequantize_2 = stack.dequantize();  stack = None\r\n#     getitem = dequantize_2[0];  dequantize_2 = None\r\n#     return getitem\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### **Users should now use `fuse_modules` for PTQ fusion and `fuse_modules_qat` for QAT fusion ([#69878](https://github.com/pytorch/pytorch/pull/69878), [#71956](https://github.com/pytorch/pytorch/pull/71956))**\r\n\r\nThere are two types of fusion supported by fuse_modules api: PTQ and QAT fusion. Previously we relied on `module.training` to decide which mode user wanted, but this was a misuse of the `training` attribute since that is not the intended purpose. This PR removes the dependency on `module.training` and uses separate APIs to make the fusion requested by the user explicit.\r\n\r\nPreviously, `fuse_module` used to support both cases and distinguished PTQ/QAT fusion based on `module.training`, but now `fuse_module` only supports the PTQ fusion. So, in the case when user wants to do QAT fusion, they need to change the call to `fuse_modules_qat`, instead of using `fuse_modules`, otherwise, they would silently get unwanted fusion results (PTQ fusion), or if the model is in training mode, it might result in error.\r\n\r\n**Note:** Currently it is still enforced that if the model is in eval mode, only PTQ fusion can be used; if the model is in training mode, then only QAT fusion can be used. In the future this constraint will be relaxed.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nfrom torch.ao.quantization import fuse_modules\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = torch.nn.Conv2d(3, 3, 3)\r\n        self.bn = torch.nn.BatchNorm2d(3)\r\n    def forward(self, x):\r\n        return self.bn(self.conv(x))\r\nm = M().train()\r\nm = fuse_modules(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\nm = M().eval()\r\nm = fuse_modules(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\n&lt;class 'torch.nn.intrinsic.modules.fused.ConvBn2d'&gt;\r\n&lt;class 'torch.nn.modules.conv.Conv2d'&gt;\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nfrom torch.ao.quantization import fuse_modules\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.conv = torch.nn.Conv2d(3, 3, 3)\r\n        self.bn = torch.nn.BatchNorm2d(3)\r\n    def forward(self, x):\r\n        return self.bn(self.conv(x))\r\nm = M().train()\r\n# For Quantization Aware Training, use fuse_modules_qat()\r\nm = fuse_modules_qat(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\nm = M().eval()\r\nm = fuse_modules(m, [\"conv\", \"bn\"])\r\nprint(type(m.conv))\r\n# Result (doesn't change):\r\n&lt;class 'torch.nn.intrinsic.modules.fused.ConvBn2d'&gt;\r\n&lt;class 'torch.nn.modules.conv.Conv2d'&gt;\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n## ONNX\r\n\r\n### Removed `f` arg from `onnx.export_to_pretty_string` ([#69546](https://github.com/pytorch/pytorch/pull/69546))\r\n\r\nThe arg has always been ignored. Simply remove it from your code.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export_to_pretty_string(model, inputs, \"file_name\")\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export_to_pretty_string(model, inputs)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `use_external_data_format` arg from `onnx.export` ([#67809](https://github.com/pytorch/pytorch/pull/67809))\r\n\r\nThe arg has been deprecated and ignored since 1.10. The external data format is now used automatically if and only if the exported file would exceed protocol buffer\u2019s file size limit. Simply remove it from your code.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name, use_external_data_format=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `example_outputs` arg from `torch.onnx.export` ([#67809](https://github.com/pytorch/pytorch/pull/67809))\r\n\r\nThe arg has been deprecated and ignored since 1.10. The provided model is instead executed once to produce example outputs. Simply remove it from your code.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name, exaple_outputs=(foo,))\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `enable_onnx_checker` arg from `onnx.export` ([#67276](https://github.com/pytorch/pytorch/pull/67276))\r\n\r\nThe arg has been deprecated and ignored since 1.10. The ONNX checker is always enabled. If it fails, `onnx.CheckerError` will be raised. Users can catch and ignore that exception.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name, enable_onnx_checker=False)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntry:\r\n    torch.onnx.export(model, inputs, f_name)\r\nexcept torch.onnx.CheckerError:\r\n    pass # ignore error\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Moved and renamed `onnx.utils.ONNXCheckerError` to `onnx.CheckerError` ([#66644](https://github.com/pytorch/pytorch/pull/66644))\r\n\r\nPreviously the documentation was incorrect and stated `ONNXCheckerError` was in the `onnx` module, so this moves the class to the originally intended module and brings the code in line with the documentation. The new name is shorter and less redundant with the module name.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nexcept torch.onnx.utils.ONNXCheckerError:\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nexcept torch.onnx.CheckerError:\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Removed `_retain_param_name` arg from `onnx.export` ([#67276](https://github.com/pytorch/pytorch/pull/67276))\r\n\r\nThe arg has been deprecated and ignored since 1.10. Param names are now always retained. Simply remove it from your code. If you want to remove param names, you can do so by editing the exported ONNX model.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# NOTE: No way to get same behavior as _retain_param_name=False.\r\ntorch.onnx.export(model, inputs, f_name, _retain_param_name=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(model, inputs, f_name)\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### Deprecated `x.T` on tensors of dimension other than 0 or 2 ([#64180](https://github.com/pytorch/pytorch/pull/64180))\r\n\r\n`x.T` only accepts tensors with 0 or 2 dimensions. Calling `x.T` on tensors with a different number of dimensions has been deprecated.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\na = torch.ones(2, 3, 4)\r\na.T.size()\r\n# torch.Size([4, 3, 2])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\na = torch.ones(2, 3, 4)\r\na.T.size()\r\n# UserWarning: The use of `x.T` on tensors of dimension other than 2\r\n# to reverse their shape is deprecated and it will throw an error in a future release.\r\n# Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))`\r\n# to reverse the dimensions of a tensor. (Triggered internally at \r\n# aten/src/ATen/native/TensorShape.cpp:2386.)\r\n# torch.Size([4, 3, 2])\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Quantization\r\n\r\n### `torch.ao.quantization.QConfigDynamic` is deprecated and going to be removed in next the release, please use `torch.ao.quantization.QConfig` instead ([#69875](https://github.com/pytorch/pytorch/pull/69875), [#69864](https://github.com/pytorch/pytorch/pull/69864))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.10.2</th><th>1.11.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nqconfig = torch.ao.quantization.QConfigDynamic(...)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nqconfig = torch.ao.quantization.QConfig(...)\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# New features\r\n\r\n## Python API\r\n\r\n* Added `set_deterministic_debug_mode` and `get_deterministic_debug_mode` ([#67778](https://github.com/pytorch/pytorch/pull/67778), [#66233](https://github.com/pytorch/pytorch/pull/66233))\r\n* Added n-dimensional Hermitian FFT: `torch.fft.ifftn` and `torch.fft.hfftn` ([#63890](https://github.com/pytorch/pytorch/pull/63890))\r\n* Added `Wishart` distribution to `torch.distributions` ([#70377](https://github.com/pytorch/pytorch/pull/70377))\r\n* Preliminary support for the [Python Array API](https://data-apis.org/array-api/latest/) standard has been added to the `torch` and `torch.linalg` modules. PyTorch implements over 90% of the operators defined by the Python Array API, including the `torch.from_dlpack` operation for improved DLPack support ([#60627](https://github.com/pytorch/pytorch/pull/60627))\r\n* Moved `torch.testing` from prototype to beta ([#69668](https://github.com/pytorch/pytorch/pull/69668))\r\n\r\n## Autograd\r\n\r\n* Added new `torch.utils.checkpoint` implementation that does not use reentrant autograd (can be toggled with the new `use_reentrant` flag) ([#69508](https://github.com/pytorch/pytorch/pull/69508))\r\n* Added `batched_grad` parameter to `autograd.grad` to allow batched gradient computation ([#65564](https://github.com/pytorch/pytorch/pull/65564))\r\n* Forward mode AD:\r\n    * Added support for most ops (and many of their backwards as well) ([#71026](https://github.com/pytorch/pytorch/pull/71026), [#69956](https://github.com/pytorch/pytorch/pull/69956), [#70355](https://github.com/pytorch/pytorch/pull/70355), [#71901](https://github.com/pytorch/pytorch/pull/71901), [#69908](https://github.com/pytorch/pytorch/pull/69908), [#69884](https://github.com/pytorch/pytorch/pull/69884), [#67837](https://github.com/pytorch/pytorch/pull/67837), [#68566](https://github.com/pytorch/pytorch/pull/68566), [#69661](https://github.com/pytorch/pytorch/pull/69661), [#69384](https://github.com/pytorch/pytorch/pull/69384), [#68631](https://github.com/pytorch/pytorch/pull/68631), [#70468](https://github.com/pytorch/pytorch/pull/70468), [#70460](https://github.com/pytorch/pytorch/pull/70460), [#67820](https://github.com/pytorch/pytorch/pull/67820), [#70460](https://github.com/pytorch/pytorch/pull/70460), [#65546](https://github.com/pytorch/pytorch/pull/65546), [#67043](https://github.com/pytorch/pytorch/pull/67043),  [#67268](https://github.com/pytorch/pytorch/pull/67268), [#67837](https://github.com/pytorch/pytorch/pull/67837), [#69727](https://github.com/pytorch/pytorch/pull/69727))\r\n        * *Check the following issue ([#71117](https://github.com/pytorch/pytorch/issues/71117)) to see the list of ops that do not yet support forward AD. Please comment there if you run into any ops that don\u2019t support forward AD that you want prioritized or are missing from that list.*\r\n    * Added `ctx.save_for_forward` function to `autograd.Function` ([#71569](https://github.com/pytorch/pytorch/pull/71569))\r\n    * `autograd.forward_ad.unpack_dual` returns a named tuple instead of plain tuple ([#68062](https://github.com/pytorch/pytorch/pull/68062), [#68628](https://github.com/pytorch/pytorch/pull/68628))\r\n* Linear algebra operation support:\r\n    * Added forward AD support for `torch.linalg.{eig, inverse, householder_product, qr}` and `torch.*_solve` ([#65546](https://github.com/pytorch/pytorch/pull/65546), [#67043](https://github.com/pytorch/pytorch/pull/67043), [#67268](https://github.com/pytorch/pytorch/pull/67268), [#67837](https://github.com/pytorch/pytorch/pull/67837))\r\n    * Added forward and backward AD support for `torch.linalg.lstsq` ([#65054](https://github.com/pytorch/pytorch/pull/65054)) \r\n    * Added support for a wider range of inputs for `linalg.pinv` ([#66092](https://github.com/pytorch/pytorch/pull/66092))\r\n\r\n## Build\r\n\r\n* Added FlexiBLAS build support ([#64815](https://github.com/pytorch/pytorch/pull/64815))\r\n* Added `IS_LINUX` and `IS_MACOS` global vars for cpp extensions building ([#69093](https://github.com/pytorch/pytorch/pull/69093))\r\n* Added ARC for iOS CMake builds ([#67884](https://github.com/pytorch/pytorch/pull/67884))\r\n* Added support for IBM z14/15 SIMD ([#66407](https://github.com/pytorch/pytorch/pull/66407))\r\n\r\n## Complex Numbers\r\n\r\n* Added complex number support to `Adagrad` and `Adadelta` optimizers ([#66671](https://github.com/pytorch/pytorch/pull/66671), [#66587](https://github.com/pytorch/pytorch/pull/66587))\r\n\r\n## Dataloader\r\n\r\n* TorchData library is going to provide modular data loading primitives for easily constructing flexible and performant data pipelines. Beta release will be provided after the release of PyTorch Core (https://github.com/pytorch/data)\r\n\r\n## LinAlg\r\n\r\n* Added an **experimental** flag that allows specifying a preferred linear algebra library (see the docs [here](https://pytorch.org/docs/master/backends.html?highlight=preferred_linalg_library#torch.backends.cuda.preferred_linalg_library)) ([#67980](https://github.com/pytorch/pytorch/pull/67980))\r\n* Added the `linalg.matrix_exp` operation (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.matrix_exp.html?highlight=matrix_exp#torch.linalg.matrix_exp)) ([#62715](https://github.com/pytorch/pytorch/pull/62715))\r\n* Added the `linalg.cross` operation (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.cross.html?highlight=linalg%20cross#torch.linalg.cross)) ([#63285](https://github.com/pytorch/pytorch/pull/63285))\r\n* Added the `linalg.diagonal` operation, an alias for torch.diagonal (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.diagonal.html?highlight=linalg%20diagonal#torch.linalg.diagonal)) ([#70599](https://github.com/pytorch/pytorch/pull/70599))\r\n* Added the `linalg.lu_factor` operation (see the docs [here](https://pytorch.org/docs/master/generated/torch.linalg.lu_factor.html?highlight=lu_factor#torch.linalg.lu_factor)) ([#66933](https://github.com/pytorch/pytorch/pull/66933))\r\n\r\n## torch.nn\r\n\r\n* Added `torch.nn.utils.rnn.{unpack_sequence,unpad_sequence}` functions ([#66550](https://github.com/pytorch/pytorch/pull/66550))\r\n\r\n## Sparse\r\n\r\n* Added `torch.sparse.sampled_addmm` for CSR Tensors on GPU ([#68007](https://github.com/pytorch/pytorch/pull/68007))\r\n\r\n## CUDA\r\n\r\n* The Jiterator  - enables compiling rarely used CUDA kernels at runtime ([#69439](https://github.com/pytorch/pytorch/pull/69439))\r\n    * Low precision supported for jiterator ([#70157](https://github.com/pytorch/pytorch/pull/70157)) - enables runtime-compilation of ops on low precision tensors (half and bfloat16)\r\n    * Enable cpu scalar arguments for jiterator ([#69861](https://github.com/pytorch/pytorch/pull/69861)) - enables passing cpu scalars as an argument to the jit-compiled kernels at runtime\r\n    * The Cacherator ([#71350](https://github.com/pytorch/pytorch/pull/71350)) - caches the jit-compiled kernels on disk, so that they can be reused between different processes\r\n    * Added complex support for Jiterator, port sinc to Jiterator ([#71577](https://github.com/pytorch/pytorch/pull/71577))\r\n    * Jiterates `lcm`, `i0e`, `i1e`, `ndtri`, `efcx`, `digamma`, `trigamma`, `lgamma` ([#70663](https://github.com/pytorch/pytorch/pull/70663))\r\n    * Jiterates `exp2`, `erfc`, `erfinv` and `entr` ([#71295](https://github.com/pytorch/pytorch/pull/71295))\r\n    * Fixes jiterator cache macro include + updates CUDA note with cache variables ([#71452](https://github.com/pytorch/pytorch/pull/71452))\r\n    * Jiterates `polygamma` ([#71162](https://github.com/pytorch/pytorch/pull/71162))\r\n* Added cuSPARSE descriptors and updated CSR addmm ([#60838](https://github.com/pytorch/pytorch/pull/60838))\r\n* Sparse CSR CUDA: added `addmv_out` ([#61407](https://github.com/pytorch/pytorch/pull/61407))\r\n* Added nvidia-smi memory and utilization as native Python API ([#69104](https://github.com/pytorch/pytorch/pull/69104))\r\n\r\n## Vulkan\r\n\r\n* Added Vulkan support for several torch operators:\r\n    * `torch.cat` (support for concatenation along the height and channel dimensions for 4-D tensors)  ([#66669](https://github.com/pytorch/pytorch/pull/66669), [#66103](https://github.com/pytorch/pytorch/pull/66103), [#67207](https://github.com/pytorch/pytorch/pull/67207))\r\n    * `torch.nn``.ConvTranspose2d` ([#67104](https://github.com/pytorch/pytorch/pull/67104), [#67358](https://github.com/pytorch/pytorch/pull/67358))\r\n    * `torch.permute` ([#68274](https://github.com/pytorch/pytorch/pull/68274))\r\n    * Tensor indexing (`at::slice`) ([#69382](https://github.com/pytorch/pytorch/pull/69382))\r\n    * `torch.clone` ([#69551](https://github.com/pytorch/pytorch/pull/69551))\r\n* Added the `vulkan_perf_test` benchmark binary to benchmark Vulkan ops under various input conditions. ([#67230](https://github.com/pytorch/pytorch/pull/67230))\r\n\r\n## Mobile\r\n\r\n* Tracing Based Selective Build (PyTorch Mobile Build Size Reduction) is a new feature that reduces a mobile model\u2019s binary size by only including the operators that the model uses.\r\n    * Build tracer for tracing based workflow ([#66267](https://github.com/pytorch/pytorch/pull/66267))\r\n    * Used operator.yaml to build LibTorch library ([#66237](https://github.com/pytorch/pytorch/pull/66237))\r\n    * Unified tracer between internal and external ([#64152](https://github.com/pytorch/pytorch/pull/64152))\r\n    * Reorganized model tracer dependency ([#63421](https://github.com/pytorch/pytorch/pull/63421))\r\n    * Added support for the `bool` and `int` dtypes in the copy kernel by default when using Tracing Based Selective Build ([#69106](https://github.com/pytorch/pytorch/pull/69106), [#69297](https://github.com/pytorch/pytorch/pull/69297))\r\n    * Generic build features for selective build ([#67817](https://github.com/pytorch/pytorch/pull/67817))\r\n    * Made more classes selective ([#67397](https://github.com/pytorch/pytorch/pull/67397))\r\n    * Added custom classes to selective build and compatibility APIs ([#67004](https://github.com/pytorch/pytorch/pull/67004), [#66972](https://github.com/pytorch/pytorch/pull/66972), [#67340](https://github.com/pytorch/pytorch/pull/67340))\r\n\r\n## Distributed\r\n\r\n* `FullyShardedDataParallel`\r\n    * FSDP is a type of data-parallel training but unlike traditional data-parallel it shards model\u2019s parameters, gradients and optimizer states across data parallel workers and can optionally offload the sharded model parameters to the CPUs. This new API can help users to scale their large model training with minimal code change when switching from DDP to FSDP.  ([#63881](https://github.com/pytorch/pytorch/pull/63881), [#64964](https://github.com/pytorch/pytorch/pull/64964), [#66578](https://github.com/pytorch/pytorch/pull/66578), [#66904](https://github.com/pytorch/pytorch/pull/66904), [#66956](https://github.com/pytorch/pytorch/pull/66956), [#66957](https://github.com/pytorch/pytorch/pull/66957), [#67117](https://github.com/pytorch/pytorch/pull/67117), [#67292](https://github.com/pytorch/pytorch/pull/67292), [#67249](https://github.com/pytorch/pytorch/pull/67249), [#67135](https://github.com/pytorch/pytorch/pull/67135), [#67813](https://github.com/pytorch/pytorch/pull/67813), [#68308](https://github.com/pytorch/pytorch/pull/68308), [#68155](https://github.com/pytorch/pytorch/pull/68155), [#68417](https://github.com/pytorch/pytorch/pull/68417), [#68776](https://github.com/pytorch/pytorch/pull/68776), [#69356](https://github.com/pytorch/pytorch/pull/69356), [#69357](https://github.com/pytorch/pytorch/pull/69357), [#69358](https://github.com/pytorch/pytorch/pull/69358), [#70340](https://github.com/pytorch/pytorch/pull/70340), [#71803](https://github.com/pytorch/pytorch/pull/71803), [#71804](https://github.com/pytorch/pytorch/pull/71804), [#70341](https://github.com/pytorch/pytorch/pull/70341), [#70235](https://github.com/pytorch/pytorch/pull/70235), [#72084](https://github.com/pytorch/pytorch/pull/72084))\r\n* `DistributedDataParallel`\r\n    * Made static graph to be stable ([#71459](https://github.com/pytorch/pytorch/pull/71459), [#68413](https://github.com/pytorch/pytorch/pull/68413))\r\n    * Made LocalSGD beta, cleaned up some docs ([#71621](https://github.com/pytorch/pytorch/pull/71621))\r\n    * Support custom buffer reduction in DDP via hooks ([#64513](https://github.com/pytorch/pytorch/pull/64513))\r\n\r\n## TorchScript\r\n\r\n* Enabled running `torch.jit.freeze()` and `torch.jit.optimize_for_inference` on functions that are not forward ([#68668](https://github.com/pytorch/pytorch/pull/68668), [#69367](https://github.com/pytorch/pytorch/pull/69367))\r\n* Enabled `torch.jit.freeze` to work on for sparse COO tensors ([#69614](https://github.com/pytorch/pytorch/pull/69614))\r\n* Enabled `torch.jit.script()`, `torch.jit.freeze()` and serialization for tensors in Compressed Sparse Row (CSR) format ([#69555](https://github.com/pytorch/pytorch/pull/69555))\r\n* Allowed users to set the fusion strategy for `torch.jit.fuser` through the now public  `torch.jit.set_fusion_strategy` . ([#72937](https://github.com/pytorch/pytorch/pull/72937)) \r\n* Enabled Dynamic Shape Fusion For GPU & CPU, configurable via `torch.jit.set_fusion_strategy` ([#72036](https://github.com/pytorch/pytorch/pull/72036))\r\n\r\n## Quantization\r\n\r\n* Added bilinear quantized implementation of `torch.nn.functional.grid_sample` 2d operator ([#66879](https://github.com/pytorch/pytorch/pull/66879))\r\n* Added the `torch.quantize_per_tensor_dynamic` operator ([#68004](https://github.com/pytorch/pytorch/pull/68004))\r\n* Added Quantization Aware Training support for `torch.nn.Embedding` and `torch.nn.EmbeddingBag`\r\n    * Added basic EmbeddingBag QAT fakeQuant workflow ([#65443](https://github.com/pytorch/pytorch/pull/65443))\r\n    * Added support for quantization of Embedding{Bag} in dynamic quant APIs ([#65674](https://github.com/pytorch/pytorch/pull/65674))\r\n    * Eager mode QAT for Embeddings ([#66429](https://github.com/pytorch/pytorch/pull/66429))\r\n    * Add benchmarks for QAT Embedding+EmbeddingBag ([#66560](https://github.com/pytorch/pytorch/pull/66560))\r\n    * Supported Embedding QAT via FX API ([#69333](https://github.com/pytorch/pytorch/pull/69333))\r\n    * Add FX support for QAT EmbeddingBag ([#69334](https://github.com/pytorch/pytorch/pull/69334))\r\n* Added support for depthwise quantized `torch.nn.Conv3d` in qnnpack, for use in quantization\r\n    * Depthwise Conv3d Indirection Buffer Setup ([#69311](https://github.com/pytorch/pytorch/pull/69311))\r\n    * Depthwise Conv3d Weight Packing ([#69312](https://github.com/pytorch/pytorch/pull/69312))\r\n    * Depthwise Conv3d mp8x27 (per channel) Neon Kernel ([#69313](https://github.com/pytorch/pytorch/pull/69313))\r\n    * Depthwise Conv3d mp8x27 (per-channel) Sse2 Kernel ([#69314](https://github.com/pytorch/pytorch/pull/69314))\r\n    * Tightened Step Height for Indirection Buffers ([#70530](https://github.com/pytorch/pytorch/pull/70530))\r\n    * Enabled Depthwise Specific Conv3d Kernel for Kernel Size 3x3x3 ([#69315](https://github.com/pytorch/pytorch/pull/69315))\r\n    * Implemented 3d convolution in qnnpack ([#66350](https://github.com/pytorch/pytorch/pull/66350))\r\n\r\n## ONNX\r\n\r\n* Supports opset version 15 ([#67805](https://github.com/pytorch/pytorch/pull/67805))\r\n* Supports exporting `nn.Module` calls as ONNX local functions ([#66140](https://github.com/pytorch/pytorch/pull/66140), [#67803](https://github.com/pytorch/pytorch/pull/67803))\r\n* Supports for exporting new ops:\r\n    * `tanhshrink`, `hardshrink`, `softshrink` ([#68492](https://github.com/pytorch/pytorch/pull/68492))\r\n    * `__xor__` ([#64581](https://github.com/pytorch/pytorch/pull/64581))\r\n    * `isfinite` ([#64754](https://github.com/pytorch/pytorch/issues/64754))\r\n    * `log10` ([#64374](https://github.com/pytorch/pytorch/pull/64374))\r\n    * `diagonal` ([#66144](https://github.com/pytorch/pytorch/pull/66144))\r\n* Added BFloat16 type support ([#66788](https://github.com/pytorch/pytorch/pull/66788))\r\n* Supports exporting with Apex O2 ([#66700](https://github.com/pytorch/pytorch/pull/66700))\r\n\r\n## Infra (Releng)\r\n\r\n* Added support for ROCm 4.3.1 ([#65624](https://github.com/pytorch/pytorch/pull/65624))\r\n* Added support for ROCm 4.5.2 ([#71064](https://github.com/pytorch/pytorch/pull/71064))\r\n* Added support for CUDA 11.5 ([#69262](https://github.com/pytorch/pytorch/pull/69262))\r\n* Added support for CUDA enabled Bazel builds ([#66241](https://github.com/pytorch/pytorch/pull/66241))\r\n* Added support for Python 3.10 ([#71132](https://github.com/pytorch/pytorch/pull/71132), [#71419](https://github.com/pytorch/pytorch/pull/71419))\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n* NumPy compatibility:\r\n    * Improved `torch.searchsorted` to be more consistent with NumPy ([#66818](https://github.com/pytorch/pytorch/pull/66818))\r\n    * Added `torch.argwhere` to match NumPy ([#64257](https://github.com/pytorch/pytorch/pull/64257))\r\n    * Added an alias for `torch.special.softmax` ([#62251](https://github.com/pytorch/pytorch/pull/62251))\r\n* Improved `torch.Tensor.view(dtype)`: enable all dtype combinations ([#66493](https://github.com/pytorch/pytorch/pull/66493))\r\n* Improved `torch.diff` by adding support for n greater than 1 ([#67260](https://github.com/pytorch/pytorch/pull/67260))\r\n* Improved `torch.movedim` to handle scalar as no-op ([#69537](https://github.com/pytorch/pytorch/pull/69537))\r\n* Improved `cartesian_prod`: fixed a warning in the docs example ([#68753](https://github.com/pytorch/pytorch/pull/68753))\r\n* Improved error messages for `max_unpool{}d` operators ([#67328](https://github.com/pytorch/pytorch/pull/67328))\r\n* `torch.distributions`\r\n    * Implemented positive-semidefinite constraint in `torch.distributions` ([#71375](https://github.com/pytorch/pytorch/pull/71375))\r\n    * Implemented Entropy methods for Binomial and Multinomial distributions ([#67609](https://github.com/pytorch/pytorch/pull/67609))\r\n    * Implemented support for `non-negative` constraint in exponential distribution (allowing it to include zero). ([#67184](https://github.com/pytorch/pytorch/pull/67184))\r\n    * Implemented `kl divergence` between `normal` and `laplace` distribution. ([#68807](https://github.com/pytorch/pytorch/pull/68807))\r\n* Improved meta tensor support for operators:\r\n    * `max` ([#61449](https://github.com/pytorch/pytorch/pull/61449)) `min` ([#61450](https://github.com/pytorch/pytorch/pull/61450))  `tril`, `triu` ([#67055](https://github.com/pytorch/pytorch/pull/67055)) `mv` ([#67373](https://github.com/pytorch/pytorch/pull/67373)) `range`, `arange`, `linspace`, `logspace` ([#67032](https://github.com/pytorch/pytorch/pull/67032)) `lerp` ([#68924](https://github.com/pytorch/pytorch/pull/68924)) `smooth_l1_loss` ([#67404](https://github.com/pytorch/pytorch/pull/67404)) `fractional_max_pool2d_backward` ([#68245](https://github.com/pytorch/pytorch/pull/68245)) `linalg.lu_factor` ([#66934](https://github.com/pytorch/pytorch/pull/66934)) `fractional_maxpool3d`: port to structured kernel ([#70414](https://github.com/pytorch/pytorch/pull/70414))\r\n* Added support for `torch.Tensor.real` for real-valued tensors ([#71718](https://github.com/pytorch/pytorch/pull/71718))\r\n*  `torch.logaddexp, torch.logaddexp2, torch.remainder`: added BFloat16 support on CPU ([#63621](https://github.com/pytorch/pytorch/pull/63621))\r\n*  `torch.bucketize` and `searchsorted`: added Half precision support ([#67077](https://github.com/pytorch/pytorch/pull/67077))\r\n* Added new `torch.slice_scatter`,`torch.select_scatter`, `torch.diagonal_scatter` ops ([#64430](https://github.com/pytorch/pytorch/pull/64430))\r\n* Made `torch.scatter_reduce` a public API ([#68580](https://github.com/pytorch/pytorch/pull/68580), [#73125](https://github.com/pytorch/pytorch/pull/73125/files))\r\n\r\n## C++ API\r\n\r\n* Added C++ API and docs for `hfftn` ([#66127](https://github.com/pytorch/pytorch/pull/66127))\r\n* Added support for `MaybeOwned<IValue>` ([#68157](https://github.com/pytorch/pytorch/pull/68157))\r\n* Added `set_to_none` option for `zero_grad()` to C++ API ([#68801](https://github.com/pytorch/pytorch/pull/68801))\r\n* Added an environment variable, `TORCH_CPP_LOG_LEVEL`, that you can use to toggle the log level in the c10 library ([#71746](https://github.com/pytorch/pytorch/pull/71746))\r\n\r\n## Autograd\r\n\r\n* Added nesting support for `torch.autograd.graph.saved_tensor_hooks` ([#70932](https://github.com/pytorch/pytorch/pull/70932))\r\n* Delayed all warnings encountered during the backward pass until the end of backward execution ([#66235](https://github.com/pytorch/pytorch/pull/66235))\r\n* Added complex autograd support to `torch.{col2im,im2col}` ([#68199](https://github.com/pytorch/pytorch/pull/68199))\r\n* Added new reduce options and autograd support for `torch.scatter_reduce` ([#71788](https://github.com/pytorch/pytorch/pull/71788))\r\n* Added derivatives wrt the second argument for `torch.{remainder,fmod}` ([#69908](https://github.com/pytorch/pytorch/pull/69908))\r\n* Added new `strategy` flag to `autograd.functional.{Jacobian, Hessian}` to enable vectorized computation ([#67041](https://github.com/pytorch/pytorch/pull/67041), [#66292](https://github.com/pytorch/pytorch/pull/66292)) \r\n* Added `check_backward_ad` flag to `torch.autograd.gradcheck` to be able to skip backward mode AD checks ([#65040](https://github.com/pytorch/pytorch/pull/65040))\r\n* Relaxed forward AD layout check to allow primal and tangent stride to differ when their size is 1 ([#66294](https://github.com/pytorch/pytorch/pull/66294))\r\n\r\n## Build\r\n\r\n* Improved incremental build times of PyTorch core by removing a dependency on `native_functions.yaml` in many core files ([#64499](https://github.com/pytorch/pytorch/pull/64499), [#66914](https://github.com/pytorch/pytorch/pull/66914), [#64172](https://github.com/pytorch/pytorch/pull/64172), [#64171](https://github.com/pytorch/pytorch/pull/64171), [#66620](https://github.com/pytorch/pytorch/pull/66620), [#66793](https://github.com/pytorch/pytorch/pull/66793), [#66913](https://github.com/pytorch/pytorch/pull/66913), [#66794](https://github.com/pytorch/pytorch/pull/66794), [#64169](https://github.com/pytorch/pytorch/pull/64169), [#64173](https://github.com/pytorch/pytorch/pull/64173), [#64170](https://github.com/pytorch/pytorch/pull/64170), [#67735](https://github.com/pytorch/pytorch/pull/67735))\r\n* Enabled bazel build without glog and gflags ([#70850](https://github.com/pytorch/pytorch/pull/70850))\r\n* Added support for C++ frontend wrapper on Linux ([#69094](https://github.com/pytorch/pytorch/pull/69094))\r\n* Added support for dynamic codegen outputs in CMake ([#68246](https://github.com/pytorch/pytorch/pull/68246))\r\n* Max CMake version is now used by default with setup.py ([#69355](https://github.com/pytorch/pytorch/pull/69355))\r\n* Upgraded oneDNN to v2.3.3 and package oneDNN Graph API together ([#63748](https://github.com/pytorch/pytorch/pull/63748))\r\n* Code base should now be `-Wno-unused-variable` compliant ([#66041](https://github.com/pytorch/pytorch/pull/66041))\r\n* Added lazy import for `packaging` in `torch_version` ([#71345](https://github.com/pytorch/pytorch/pull/71345))\r\n\r\n## Dataloader\r\n\r\n* Support custom `Sequence` and `Mapping` for `utils.data.default_collate` ([#68779](https://github.com/pytorch/pytorch/pull/68779))\r\n* Allowed specifying `num_samples` to `RandomSampler` when `replacement` is `False` ([#71568](https://github.com/pytorch/pytorch/pull/71568))\r\n* Fixed the warning of shape inconsistency `utils.data.default_collate` ([#71065](https://github.com/pytorch/pytorch/pull/71065))\r\n\r\n## ForEach\r\n\r\n* Implemented `ForEach` L1 & L2 norm ([#62646](https://github.com/pytorch/pytorch/pull/62646))\r\n\r\n## LinAlg\r\n\r\n* The `linalg.matrix_rank` ([docs](https://pytorch.org/docs/master/generated/torch.linalg.matrix_rank.html?highlight=matrix_rank#torch.linalg.matrix_rank)) and `linalg.pinv` ([docs](https://pytorch.org/docs/master/generated/torch.linalg.pinv.html?highlight=pinv#torch.linalg.pinv)) operations now support specifying absolute and relative tolerances for better handling of singular values ([#63102](https://github.com/pytorch/pytorch/pull/63102))\r\n\r\n## torch.nn\r\n\r\n* Added `channels_last` support for `ChannelShuffle` ([#50247](https://github.com/pytorch/pytorch/pull/50247))\r\n* Added no-batch-dim support for `nn.{AdaptiveLogSoftmaxWithLoss, Bilinear, Conv*d, ConvTranspose*d, CrossEntropyLoss, CTCLoss, Fold, FractionalMaxPool3d, GaussianNLLLoss, GRU, GRUCell, InstanceNorm*d, LSTM, LSTMCell, MarginRankingLoss, MultiheadAttention, MultiLabelSoftMarginLoss, RNN, RNNCell, Transformer, TransformerDecoderLayer, TransformerEncoderLayer}` ([#69054](https://github.com/pytorch/pytorch/pull/69054), [#69539](https://github.com/pytorch/pytorch/pull/69539), [#70506](https://github.com/pytorch/pytorch/pull/70506), [#71055](https://github.com/pytorch/pytorch/pull/71055), [#70092](https://github.com/pytorch/pytorch/pull/70092), [#64909](https://github.com/pytorch/pytorch/pull/64909), [#69732](https://github.com/pytorch/pytorch/pull/69732), [#69783](https://github.com/pytorch/pytorch/pull/69783), [#70236](https://github.com/pytorch/pytorch/pull/70236), [#65323](https://github.com/pytorch/pytorch/pull/65323), [#71056](https://github.com/pytorch/pytorch/pull/71056), [#64975](https://github.com/pytorch/pytorch/pull/64975), [#67176](https://github.com/pytorch/pytorch/pull/67176), [#70590](https://github.com/pytorch/pytorch/pull/70590), [#65690](https://github.com/pytorch/pytorch/pull/65690), [#70977](https://github.com/pytorch/pytorch/pull/70977), [#70597](https://github.com/pytorch/pytorch/pull/70597), [#70322](https://github.com/pytorch/pytorch/pull/70322), [#69291](https://github.com/pytorch/pytorch/pull/69291))\r\n* Added `BFloat16` support on CPU to `nn.{AdaptiveAvgPool2d, AdaptiveMaxPool2d, AvgPool2d, MaxPool2d}` ([#56902](https://github.com/pytorch/pytorch/pull/56902), [#66929](https://github.com/pytorch/pytorch/pull/66929), [#66927](https://github.com/pytorch/pytorch/pull/66927), [#56903](https://github.com/pytorch/pytorch/pull/56903))\r\n* Added `maximize` support to `optim.{Adam, AdamW, SGD}` ([#68164](https://github.com/pytorch/pytorch/pull/68164), [#70146](https://github.com/pytorch/pytorch/pull/70146), [#67847](https://github.com/pytorch/pytorch/pull/67847), [#68733](https://github.com/pytorch/pytorch/pull/68733), [#71023](https://github.com/pytorch/pytorch/pull/71023))\r\n* `F.interpolate`: Add `nearest-exact` mode to fix off-by-one error in `nearest` mode ([#64501](https://github.com/pytorch/pytorch/pull/64501))\r\n* `F.interpolate`: Added support for anti-aliasing to bilinear and bicubic algorithms ([#70930](https://github.com/pytorch/pytorch/pull/70930), [#68819](https://github.com/pytorch/pytorch/pull/68819), [#65142](https://github.com/pytorch/pytorch/pull/65142), [#69318](https://github.com/pytorch/pytorch/pull/69318))\r\n* `F.interpolate`: Improved error message for invalid shapes ([#66417](https://github.com/pytorch/pytorch/pull/66417))\r\n* `nn.Conv*d`: Accepts 0-sized channel inputs ([#66256](https://github.com/pytorch/pytorch/pull/66256))\r\n* `nn.LogSigmoid`: Used `log1p` for improved precision ([#66441](https://github.com/pytorch/pytorch/pull/66441))\r\n* `nn.Module`: Added flag for removing duplicates from parameters ([#71542](https://github.com/pytorch/pytorch/pull/71542))\r\n* `nn.Module`: Added `register_module` alias for registering a sub-module ([#65174](https://github.com/pytorch/pytorch/pull/65174))\r\n* `nn.ModuleList`: Supported concatenation ([#70887](https://github.com/pytorch/pytorch/pull/70887))\r\n* `nn.MultiheadAttention`: Added flag to optionally average output attention weights across heads ([#70055](https://github.com/pytorch/pytorch/pull/70055))\r\n* `nn.ParameterDict`: Supported full set of `dict` methods ([#69403](https://github.com/pytorch/pytorch/pull/69403))\r\n* `nn.{RNN, GRU}`: Allowed `hidden_size` to be 0 ([#70556](https://github.com/pytorch/pytorch/pull/70556))\r\n* `nn.Sequential`: Added `append` method ([#71326](https://github.com/pytorch/pytorch/pull/71326))\r\n* `nn.Upsample`: Exposed `recompute_scale_factor` ([#66419](https://github.com/pytorch/pytorch/pull/66419))\r\n* `nn.ZeroPad2d`: Added `extra_repr` for printing purposes ([#69206](https://github.com/pytorch/pytorch/pull/69206))\r\n* `optim.{ChainedScheduler, SequentialLR}`: Added `optimizer` attribute ([#67406](https://github.com/pytorch/pytorch/pull/67406), [#69817](https://github.com/pytorch/pytorch/pull/69817))\r\n* `optim.swa_utils.AveragedModel`: Added `use_buffers` flag for averaging buffers in addition to parameters ([#65921](https://github.com/pytorch/pytorch/pull/65921), [#71763](https://github.com/pytorch/pytorch/pull/71763))\r\n\r\n## torch.fx\r\n\r\n* Improved the customizability of `fx.Graph`\u2019s code generation function, including support for setting a breakpoint in the generated code ([#67139](https://github.com/pytorch/pytorch/pull/67139))\r\n* Supported printing inplace operators in FX ([#71887](https://github.com/pytorch/pytorch/pull/71887))\r\n\r\n## Sparse\r\n\r\n* Add CSR support for several operators:\r\n    * `torch.triangular_solve`, `torch.addmv`, `torch.addmm`, `torch.add`  for all arguments on CPU [(#62180](https://github.com/pytorch/pytorch/pull/62180), [#61536](https://github.com/pytorch/pytorch/pull/61536), [#65606](https://github.com/pytorch/pytorch/pull/65606), [#64391](https://github.com/pytorch/pytorch/pull/64391))\r\n    * `torch.triangular_solve`, `torch.addmv`, `torch.addmm`, `torch.add`  for all arguments on GPU ([#61407](https://github.com/pytorch/pytorch/pull/61407), [#61858](https://github.com/pytorch/pytorch/pull/61858), [#63511](https://github.com/pytorch/pytorch/pull/63511), [#63948](https://github.com/pytorch/pytorch/pull/63948))\r\n    * zero-preserving unary functions ([#68123](https://github.com/pytorch/pytorch/pull/68123), [#69292](https://github.com/pytorch/pytorch/pull/69292))\r\n    * `torch.empty`, `torch.resize_`, `torch.copy_`, `torch.randn_like`, `torch.clone` ([#63509](https://github.com/pytorch/pytorch/pull/63509), [#63510](https://github.com/pytorch/pytorch/pull/63510), [#68083](https://github.com/pytorch/pytorch/pull/68083), [#70581](https://github.com/pytorch/pytorch/pull/70581))\r\n    * `transpose` ([#70582](https://github.com/pytorch/pytorch/pull/70582))\r\n* Added torch.sparse_coo Layout support to `zeros_like` ([#68108](https://github.com/pytorch/pytorch/pull/68108))\r\n* Added Half, BFloat16, and Complex dtype support for matrix multiplication of two COO Tensors on GPU ([#59980](https://github.com/pytorch/pytorch/pull/59980))\r\n* Added support for conversion of CSR to COO Tensor to `to_sparse` ([#66774](https://github.com/pytorch/pytorch/pull/66774))\r\n* Added support for empty COO Tensors to sparse.sum ([#71091](https://github.com/pytorch/pytorch/pull/71091))\r\n\r\n## AMD\r\n\r\n* Added sparse mappings for CUDA->HIP translation ([#67323](https://github.com/pytorch/pytorch/pull/67323))\r\n* Enabled frexp support for ROCm builds ([#67226](https://github.com/pytorch/pytorch/pull/67226))\r\n* Used hipCUB/rocPRIM scan algorithms for large index support ([#68487](https://github.com/pytorch/pytorch/pull/68487))\r\n\r\n## CUDA\r\n\r\n* Allows external CUDA streams to be set as current ([#66324](https://github.com/pytorch/pytorch/pull/66324))\r\n* Added an option to disable reduced precision reductions for FP16 GEMM ([#67946](https://github.com/pytorch/pytorch/pull/67946))\r\n* Improved CUDA memory usage of `nanmedian` result ([#68591](https://github.com/pytorch/pytorch/pull/68591))\r\n* Reduced number of `igamma` kernel instantiations ([#70666](https://github.com/pytorch/pytorch/pull/70666))\r\n* Reduced number of `compare` kernels by unifying them ([#69111](https://github.com/pytorch/pytorch/pull/69111))\r\n* Reduced number of `bernoulli` tensor tensor kernel instantiations ([#70169](https://github.com/pytorch/pytorch/pull/70169))\r\n* Used `cub::FutureValue` to simplify 64bit indexing split of cub scan ([#66711](https://github.com/pytorch/pytorch/pull/66711))\r\n* Added `hascuSOLVER` flag to Context ([#69825](https://github.com/pytorch/pytorch/pull/69825))\r\n* Improved error message from `CUDACachingAllocator` ([#69174](https://github.com/pytorch/pytorch/pull/69174))\r\n* Fixed `masked_softmax` perf for element_size is not 8 ([#70271](https://github.com/pytorch/pytorch/pull/70271))\r\n* Reduced binary size of `TensorCompare.cu` ([#68835](https://github.com/pytorch/pytorch/pull/68835))\r\n* Improved error message for `interpolation` ([#72066](https://github.com/pytorch/pytorch/pull/72066))\r\n* Doesn't compile `pow` kernels for non-existent case ([#70017](https://github.com/pytorch/pytorch/pull/70017))\r\n\r\n## Profiler\r\n\r\n* Added flop count formulas for `bmm` and `baddbmm` ([#66636](https://github.com/pytorch/pytorch/pull/66636))\r\n\r\n## Vulkan\r\n\r\n* Allowed Vulkan models to return multiple outputs by improving Vulkan tensor lifecycle management to release GPU resources when the tensor is destroyed, instead of being released at the end of every inference ([#66477](https://github.com/pytorch/pytorch/pull/66477), [#66478](https://github.com/pytorch/pytorch/pull/66478))\r\n* Enabled multiple Vulkan models to execute concurrently in parallel threads, by moving components of the Vulkan global context into thread local objects ([#67733](https://github.com/pytorch/pytorch/pull/67733), [#69576](https://github.com/pytorch/pytorch/pull/69576))\r\n\r\n## Mobile\r\n\r\n* Introduced multiple improvements for `NNAPI` \r\n    * Added converters for torchscript ops `quantized::mul` and `quantized::convtranspose2d` to converter (`torch.backends._nnapi.prepare.convert_model_to_nnapi`) ([#63913](https://github.com/pytorch/pytorch/pull/63913), [#63914](https://github.com/pytorch/pytorch/pull/63914))\r\n    * Supported `int32` and `qint16` type in Torchscript expressions ([#70197](https://github.com/pytorch/pytorch/pull/70197), [#70621](https://github.com/pytorch/pytorch/pull/70621))\r\n    * Supported runtime flexible shapes and return shapes ([#70334](https://github.com/pytorch/pytorch/pull/70334))\r\n* Improved Model Tracer Coverage and Selective Metal Ops ([#68134, #69492, #69328](https://github.com/pytorch/pytorch/pull/68134))\r\n* Introduced multiple improvements for `CoreML`\r\n    * Fixed error messages ([#67410](https://github.com/pytorch/pytorch/pull/67410))\r\n    * Assigned `computationUnit` to executor ([#67411](https://github.com/pytorch/pytorch/pull/67411))\r\n    * Cleaned up shape information from `TensorSpec` ([#67412](https://github.com/pytorch/pytorch/pull/67412))\r\n* Type Support in Mobile Lite Interpreter\r\n    * Extended `type_parser` to handle `NamedTuple` type ([#63130](https://github.com/pytorch/pytorch/pull/63130), [#62612](https://github.com/pytorch/pytorch/pull/62612))\r\n\r\n## Distributed\r\n\r\n* `torch.distributed`\r\n    * Improvements to error handling in `TCPStore\u2019`s socket implementation (#68225)\r\n    * Enabled `ncclAvg` for reductions ([#62835](https://github.com/pytorch/pytorch/pull/62835))\r\n    * Init dummy `NCCL` comms in constructor ([#65173](https://github.com/pytorch/pytorch/pull/65173), [#66393](https://github.com/pytorch/pytorch/pull/66393))\r\n    * Added pybind trampoline for `ProcessGroup` and `Work` ([#66338](https://github.com/pytorch/pytorch/pull/66338))\r\n    * Setup `c10d` extension Backend class attr the same way as builtin ones ([#66991](https://github.com/pytorch/pytorch/pull/66991))\r\n    * Added barrier to `ProcessGroup` trampoline ([#67236](https://github.com/pytorch/pytorch/pull/67236))\r\n    * Raised warning when calling collectives on non-member group objects ([#67639](https://github.com/pytorch/pytorch/pull/67639))\r\n    * Patched `bfloat16` support for NCCL ([#67843](https://github.com/pytorch/pytorch/pull/67843))\r\n    * Fixed `c10d` TCP store race condition with mutex ([#68499](https://github.com/pytorch/pytorch/pull/68499))\r\n    * Surfaced `ncclUniqueId` store broadcast error ([#68597](https://github.com/pytorch/pytorch/pull/68597))\r\n    * Checks for file existence before invoking cleanup logic in `FileStore` destructor ([#68603](https://github.com/pytorch/pytorch/pull/68603))\r\n    * Implemented gather primitive for `ProcessGroupNCCL` ([#66745](https://github.com/pytorch/pytorch/pull/66745))\r\n    * Implemented scatter primitive for `ProcessGroupNCCL` ([#70029](https://github.com/pytorch/pytorch/pull/70029))\r\n    * Enabled `gather_object` on `NCCL` ([#71623](https://github.com/pytorch/pytorch/pull/71623))\r\n    * Implemented `allreduce_coalesced` for `ProcessGroupNCCL` ([#62140](https://github.com/pytorch/pytorch/pull/62140))\r\n    * Set non-default backend names to lower case ([#69400](https://github.com/pytorch/pytorch/pull/69400))\r\n    * Added support for `deleteKey` for `FileStore` ([#69953](https://github.com/pytorch/pytorch/pull/69953))\r\n    * Fixed `TSAN` issue in `TCPStore` ([#69590](https://github.com/pytorch/pytorch/pull/69590))\r\n\r\n* `DistributedDataParallel`\r\n    * Refactored and removed `sync_params` ([#64514](https://github.com/pytorch/pytorch/pull/64514))\r\n    * Used `named_params` and `named_buffers` explicitly ([#65181](https://github.com/pytorch/pytorch/pull/65181))\r\n    * Allow await of custom buffer reduction in backward ([#64515](https://github.com/pytorch/pytorch/pull/64515))\r\n    * Profiling range for bucket copy ([#65769](https://github.com/pytorch/pytorch/pull/65769))\r\n    * Logs iteration in debug mode ([#65770](https://github.com/pytorch/pytorch/pull/65770))\r\n\r\n* `torch.distributed.rpc`\r\n    * Added a timeout argument to RPC shutdown() ([#65425](https://github.com/pytorch/pytorch/pull/65425))\r\n    * Released GIL during RPC shutdown. ([#69586](https://github.com/pytorch/pytorch/pull/69586))\r\n    * Updated RPC `shutdown()` logic to remove process group usage. ([#65946](https://github.com/pytorch/pytorch/pull/65946))\r\n    * Removal of Process Group dependency for TensorPipe Agent. ([#68128](https://github.com/pytorch/pytorch/pull/68128))\r\n\r\n* `torch.distributed.autograd`\r\n    * Made Kineto + distributed a warning rather than an error ([#71120](https://github.com/pytorch/pytorch/pull/71120))\r\n\r\n* `torch.distributed.elastic`\r\n    * Added ability to override sys.executable for `torch.distributed.run` ([#66179](https://github.com/pytorch/pytorch/pull/66179))\r\n\r\n## TorchScript\r\n\r\n* Several improvements to NVFuser, which is an optimization that speeds up all JIT graphs with a CUDA Tensors on Nvidia GPUs. This includes extending fusing support to normalization and reduction kernels, enabling multiple kernel launch for single `CudaFusionGroup`, and addition of a graph segmentation cache to the hierarchical caching system. ([#63745](https://github.com/pytorch/pytorch/pull/63745), [#65137](https://github.com/pytorch/pytorch/pull/65137), [#63745](https://github.com/pytorch/pytorch/pull/63745), [#65137](https://github.com/pytorch/pytorch/pull/65137))\r\n* Enabled `profile_ivalue` to convert dynamic scalar into compile time constants in NVFuser. (e.g. reduction axes). ([#63745,](https://github.com/pytorch/pytorch/pull/63745) [#65137](https://github.com/pytorch/pytorch/pull/65137))\r\n* Added support in `torch.jit.trace` for tracing already JITted subgraphs([#59949](https://github.com/pytorch/pytorch/pull/59949))\r\n* We now provide full types on graph inputs when tracing graphs that are already JITted([#67424](https://github.com/pytorch/pytorch/pull/67424))\r\n* `torch.jit.freeze` now can preserve attributes of submodules - previously, it was only possible to prevent inlining of attributes of the top level module.([#66102](https://github.com/pytorch/pytorch/pull/66102))\r\n* The peephole optimizer, which is used in `torch.jit.freeze` now coalesces consecutive calls to `torch.concat` into a single call ([#67000](https://github.com/pytorch/pytorch/pull/67000))\r\n* Added ability for Torch.JIT C dispatch to convert python `None` into an undefined Tensor([#67793](https://github.com/pytorch/pytorch/pull/67793))\r\n* `torch.jit.script` now recognizes union of scalars as a JIT NumberType ([#66591](https://github.com/pytorch/pytorch/pull/66591))\r\n* No longer adds a tensor in a returned list to the wildcard alias set in AliasDB, allowing for additional optimizations in JIT optimization passes. ([#71170](https://github.com/pytorch/pytorch/pull/71170))\r\n* In `torch.jit.optimize_for_inference`, there is a new graph pass to precompute transposes for linear layers. ([#65631](https://github.com/pytorch/pytorch/pull/65631), [68024](https://github.com/pytorch/pytorch/pull/68024))\r\n* In `torch.jit.freeze`, there is a new pass where we concat together multiple linear layers with same input Tensor (different weight/bias) ([#63198](https://github.com/pytorch/pytorch/pull/63198), #[68024](https://github.com/pytorch/pytorch/pull/68024))\r\n* Added support for normalizing `torch.Tensor.__rsub__` in `normalize_ops` JIT pass([#65014](https://github.com/pytorch/pytorch/pull/65014))\r\n\r\n## Quantization\r\n\r\n* Quantized op improvements\r\n    * `torch.ao.FakeQuantize` now supports `fp32/fp16` `zero_point`. ([#65836](https://github.com/pytorch/pytorch/pull/65836))\r\n    * `torch.ops.quantized.add` now supports broadcasting ([#66049](https://github.com/pytorch/pytorch/pull/66049))\r\n    * `torch.Tensor.dequantize` now supports fp16 + cuda ([#67234](https://github.com/pytorch/pytorch/pull/67234))\r\n    * Added quantized CPU support for `torch.nn.GELU` ([#69968](https://github.com/pytorch/pytorch/pull/69968))\r\n    * `torch.nn.quantized.functional.hardsigmoid` supports an `inplace` flag ([#65740](https://github.com/pytorch/pytorch/pull/65740))\r\n* Workflow improvements\r\n    * FX graph mode quantization: enable `torch.nn.Linear + torch.nn.BatchNorm1d` fusion for PTQ ([#66484](https://github.com/pytorch/pytorch/pull/66484))\r\n    * Added an option in `torch.ao.quantization.quantize_fx.convert_fx` to accept `qconfig_dict` to skip quantization ([#66878](https://github.com/pytorch/pytorch/pull/66878))\r\n    * Added `torch.nn.qat.dynamic.modules.Linear` module ([#67325](https://github.com/pytorch/pytorch/pull/67325))\r\n    * Added `torch.nn.ConvTranspose{n}d + torch.nn.BatchNorm{n}d` fusion support ([#70022](https://github.com/pytorch/pytorch/pull/70022))\r\n    * Extended `torch.ao.quantization.prepare_qat` with `allow_list` argument, to allow custom mapping and custom QAT module ([#65119](https://github.com/pytorch/pytorch/pull/65119))\r\n    * Added `torch.ao.quantization.default_replay_qconfig` which allows observer reuse for `torch.reshape` in FX graph mode quantization ([#69249](https://github.com/pytorch/pytorch/pull/69249))\r\n\r\n## ONNX\r\n\r\n* Set `ir_version` of the exported model based on `opset_version`. This increases the odds that the exported ONNX model will be usable. Before this change, we were setting the IR version to a hard-coded value which may be higher than what the model consumer supports. ([#67803](https://github.com/pytorch/pytorch/pull/67803))\r\n* Preserved op input names when op just passes through the input to the output ([#67275](https://github.com/pytorch/pytorch/pull/67275))\r\n* Shape inference improvements:\r\n    * Updated slice process shape to support rank only inference ([#66149](https://github.com/pytorch/pytorch/pull/66149))\r\n    * Represent symbolic shape as value ([#69545](https://github.com/pytorch/pytorch/pull/69545))\r\n* Included op type in exported models\u2019 input and output names ([#68976](https://github.com/pytorch/pytorch/pull/68976))\r\n* Supports Conv-BatchNorm fusion inside blocks ([#67272](https://github.com/pytorch/pytorch/pull/67272))\r\n* Exported `torch.reciprocal` to ONNX Reciprocal operator instead of `Div(1, x)` ([#67271](https://github.com/pytorch/pytorch/pull/67271))\r\n* Supports `beta!=1` in softplus ([#66146](https://github.com/pytorch/pytorch/pull/66146))\r\n* Added warning for inplace updates on `tensor.shape` in tracing mode ([#66142](https://github.com/pytorch/pytorch/pull/66142))\r\n* Supports `instance_norm` in training mode ([#64375](https://github.com/pytorch/pytorch/pull/64375))\r\n* Allow registration of custom symbolics for ops specifying aten namespace (i.e. `aten::foo` is allowed as well as \u201cfoo\u201d). ([#67810](https://github.com/pytorch/pytorch/pull/67810))\r\n* Allow registration of custom symbolics for `prim` namespace ([#66139](https://github.com/pytorch/pytorch/pull/66139))\r\n* Supports dynamic inputs for `OneHot`, bool for `Einsum` ([#66147](https://github.com/pytorch/pytorch/pull/66147))\r\n\r\n## Infra (Releng)\r\n\r\n* Build with BUILD_SPLIT_CUDA for all 11.X Windows builds ([#70899](https://github.com/pytorch/pytorch/pull/70899))\r\n\r\n## torch.package\r\n\r\n* Add ability to retrieve the dependency graph via `all_path` function([#65602](https://github.com/pytorch/pytorch/pull/65602))\r\n* Add support for pickle v4 ([#70642](https://github.com/pytorch/pytorch/pull/70642))\r\n* Add better testing support for Package Exporter ([#70641](https://github.com/pytorch/pytorch/pull/70641))\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n* Fixed scalar inputs for aliased binary ops {`multiply`, `subtract`, `divide`} ([#65937](https://github.com/pytorch/pytorch/pull/65937))\r\n* Fixed `torch.save` when saving storages that view same data with different type ([#66949](https://github.com/pytorch/pytorch/pull/66949))\r\n* Fixed `torch.save` error if storages are unallocated ([#68787](https://github.com/pytorch/pytorch/pull/68787))\r\n* Fixed `k` out-of-bounds in `torch.kthvalue` (cpu kernel) ([#68863](https://github.com/pytorch/pytorch/pull/68863))\r\n* Fixed `inference_mode` decorator: `with inference_mode(mode=False)` used to ignore the `mode` argument and always set inference mode. ([#68617](https://github.com/pytorch/pytorch/pull/68617))\r\n* Fixed `cdist_backward` in the case when `cdist` inputs are not contiguous ([#70016](https://github.com/pytorch/pytorch/pull/70016))\r\n* Fixed `cdist` error message typo ([#70178](https://github.com/pytorch/pytorch/pull/70178))\r\n* Fixed `scatter` for empty indexes ([#70662](https://github.com/pytorch/pytorch/pull/70662))\r\n* Fixed `torch.{unique, unique_consecutive}` out of bound ([#71540](https://github.com/pytorch/pytorch/pull/71540))\r\n* Fixed `torch.isin` in the case when inputs are non-contiguous on CPU ([#70659](https://github.com/pytorch/pytorch/pull/70659))\r\n* Fixed `hsplit vsplit dsplit` crash when section is 0 ([#69342](https://github.com/pytorch/pytorch/pull/69342))\r\n* Fixed: `torch.gradient` ignores dim argument when checking edge_order ([#67926](https://github.com/pytorch/pytorch/pull/67926))\r\n* Fixed: `TransformedDistribution.icdf` should perform validation *after* applying the inverse transformation rather than before. ([#71393](https://github.com/pytorch/pytorch/pull/71393))\r\n* Fixed `torch.all and torch.any` internal assert error with requires_grad=True ([#65714](https://github.com/pytorch/pytorch/pull/65714))\r\n* Fixed `torch.logsumexp` type promotion: promote integral inputs to floating for([#63393](https://github.com/pytorch/pytorch/pull/63393))\r\n\r\n## C++ API\r\n\r\n* Fixed libtorch `at::Tensor::print()` linking error ([#69615](https://github.com/pytorch/pytorch/pull/69615))\r\n* Avoided UB when indexing into size-0 tensors ([#65878](https://github.com/pytorch/pytorch/pull/65878))\r\n* Fixed an ICE when compiling PyTorch from source on MacOS with clang-1300 ([#65655](https://github.com/pytorch/pytorch/pull/65655))\r\n\r\n## Autograd\r\n\r\n* Fixed autocast state propagation in the `torch.utils.checkpoint` API ([#71169](https://github.com/pytorch/pytorch/pull/71169))\r\n* Fixed `torch.nn.functional.conv_transpose3d` backward when grad_out is non-contiguous ([#67829](https://github.com/pytorch/pytorch/pull/67829))\r\n* Forward mode AD:\r\n    * Fixed a case where forward AD in-place-over-view silently copies the view ([#67816](https://github.com/pytorch/pytorch/pull/67816))\r\n    * Fixed deadlock in forward AD for functions that return multiple outputs ([#67995](https://github.com/pytorch/pytorch/pull/67995))\r\n    * Fixed forward AD codegen for functions that have multiple formulas ([#68535](https://github.com/pytorch/pytorch/pull/68535))\r\n    * Fixed deadlock when forward and backward AD are used at the same time ([#67360](https://github.com/pytorch/pytorch/pull/67360))\r\n    * Fixed  `Tensor.copy_` forward AD to handle broadcasting ([#69592](https://github.com/pytorch/pytorch/pull/69592))\r\n    * Do not generate not_implemented error for forward AD when input with tangent passed to non-differentiable function ([#66926](https://github.com/pytorch/pytorch/pull/66926))\r\n* Fixed `autograd.Function` when non-Tensor argument precedes tensor argument ([#71530](https://github.com/pytorch/pytorch/pull/71530))\r\n* Fixed `autograd.Function` forward AD when forward is a no-op to no longer raise an internal error ([#71531](https://github.com/pytorch/pytorch/pull/71531))\r\n\r\n## Build\r\n\r\n* Stopped reporting CPU Capability as AVX512 on machines with AVX512 support but without AVX512 kernels ([#66703](https://github.com/pytorch/pytorch/pull/66703))\r\n* Disabled SVE when cross-compiling for M1 ([#67114](https://github.com/pytorch/pytorch/pull/67114))\r\n* Added failure if `pocketfft` is not found and `at_mkl` is not enabled ([#67909](https://github.com/pytorch/pytorch/pull/67909))\r\n* Fixed clang issues when compiling with `_GLIBCXX_USE_CXX11_ABI` ([#72081](https://github.com/pytorch/pytorch/pull/72081))\r\n\r\n## Complex Numbers\r\n\r\n* Fixed `torch.autograd.gradcheck` to generate valid inputs for forward AD computation for complex functions ([#68001](https://github.com/pytorch/pytorch/pull/68001))\r\n* Fixed `torch.Tensor.copy_` transpose path for tensors with conjugate or negative bit set ([#69026](https://github.com/pytorch/pytorch/pull/69026))\r\n* Fixed `torch.Tensor.copy_` behavior for the case when two conjugated or negated tensors of the same dtype (one or both of which are non-contiguous) are copied into each other ([#68963](https://github.com/pytorch/pytorch/pull/68963))\r\n\r\n## Dataloader\r\n\r\n* Made `ProcessException` picklable ([#70118](https://github.com/pytorch/pytorch/pull/70118))\r\n* Fixed persistent worker exiting before `pin_memory_thread` ([#71579](https://github.com/pytorch/pytorch/pull/71579))\r\n\r\n## torch.nn\r\n\r\n* `nn.AdaptiveAvgPool*d`: Throws an error for negative `output_size` ([#70488](https://github.com/pytorch/pytorch/pull/70488))\r\n* `nn.Conv1d`: Fixed for 1D convolution on MKL-DNN backend ([#68166](https://github.com/pytorch/pytorch/pull/68166))\r\n* `nn.CrossEntropyLoss`: Fixed for usage of `weight`, `ignore_index`, and `label_smoothing` together ([#69511](https://github.com/pytorch/pytorch/pull/69511))\r\n* `nn.Fold`: Checked that block height and width are positive ([#69048](https://github.com/pytorch/pytorch/pull/69048))\r\n* `nn.LayerNorm`: Fixed incorrect result on CUDA when `gamma` or `bias` are missing ([#69210](https://github.com/pytorch/pytorch/pull/69210))\r\n* `nn.LayerNorm`: Avoided overflow by doing computation in `float` for `half` ([#66920](https://github.com/pytorch/pytorch/pull/66920))\r\n* `nn.Module`: Throws a proper error message from `load_state_dict` for non-tensor values ([#70596](https://github.com/pytorch/pytorch/pull/70596))\r\n* `nn.ModuleList`: Fixed incorrect return type in `__getitem__` ([#69083](https://github.com/pytorch/pytorch/pull/69083))\r\n* `nn.MultiheadAttention`: Used query dtype for mask type ([#68077](https://github.com/pytorch/pytorch/pull/68077))\r\n* `nn.NLLLoss`: Fixed backward computation with negative weights ([#64572](https://github.com/pytorch/pytorch/pull/64572))\r\n* `nn.{RNN, GRU}`: Fixed RNN modules with input shapes containing-0 in CUDA ([#71696](https://github.com/pytorch/pytorch/pull/71696))\r\n* `nn.utils.rnn.pad_sequence`: Fix regression to support tuples for padding ([#72436](https://github.com/pytorch/pytorch/pull/72436))\r\n* `optim._LrScheduler`: Fixed print formatting ([#68338](https://github.com/pytorch/pytorch/pull/68338))\r\n* `optim.ChainedScheduler`: Fixed `get_last_lr()` ([#69112](https://github.com/pytorch/pytorch/pull/69112))\r\n* `optim.CosineAnnealingWarmRestarts`: Fixed ordering bug when `last_epoch > 0` ([#64758](https://github.com/pytorch/pytorch/pull/64758))\r\n* `optim.SequentialLR`: Updated `_last_lr` on step ([#70558](https://github.com/pytorch/pytorch/pull/70558))\r\n\r\n## torch.fx\r\n\r\n* Supported `torch.layout` as arg ([#66048](https://github.com/pytorch/pytorch/pull/66048))\r\n* Specified a default value when possible for placeholders created from `concrete_args` ([#59569](https://github.com/pytorch/pytorch/pull/59569))\r\n* Fixed issue where `GraphModule.delete_all_unused_submodules` deletes submodules from called leaf modules ([#66430](https://github.com/pytorch/pytorch/pull/66430))\r\n* Fixed `torch.fx.subgraph_rewriter.replace_pattern` mechanism so that multiple one-liner instances of the pattern are captured correctly ([#66442](https://github.com/pytorch/pytorch/pull/66442))\r\n* Fixed bug in graph matcher that caused certain nodes to be matched twice ([#69238](https://github.com/pytorch/pytorch/pull/69238))\r\n* Ensured node stack trace survives copying ([#69368](https://github.com/pytorch/pytorch/pull/69368))\r\n* Fixed `to_folder` not saving dtype ([#69983](https://github.com/pytorch/pytorch/pull/69983))\r\n* Added a `default_value` arg to `fx.Graph.placeholder` and fix `split_module` ([#71016](https://github.com/pytorch/pytorch/pull/71016))\r\n\r\n## Sparse\r\n\r\n* Fixed CSR storage access to throw when used ([#70072](https://github.com/pytorch/pytorch/pull/70072))\r\n* Fixed multiplication of 0-D sparse tensors ([#70749](https://github.com/pytorch/pytorch/pull/70749))\r\n* Fixed result dtype for neg if given sparse Tensor ([#68885](https://github.com/pytorch/pytorch/pull/68885))\r\n\r\n## CUDA\r\n\r\n* Fixed CUDA vs CPU consistency for index_put_ when accumulating ([#66790](https://github.com/pytorch/pytorch/pull/66790))\r\n* Fixed CUDA vs CPU consistency for index_put_ when accumulating (part 2) ([#67189](https://github.com/pytorch/pytorch/pull/67189))\r\n* Fixed error in warning about unsupported GPU ([#67900](https://github.com/pytorch/pytorch/pull/67900))\r\n* Disabled TF32 in `pinv_jvp` and `pinv_backward` ([#67948](https://github.com/pytorch/pytorch/pull/67948))\r\n* Fixed DLPack CUDA stream convention ([#67618](https://github.com/pytorch/pytorch/pull/67618))\r\n* Sets device guard in `_cudnn_impl` functions ([#70406](https://github.com/pytorch/pytorch/pull/70406))\r\n* Fixed `mem_get_info` when querying on a device other than the current device ([#69640](https://github.com/pytorch/pytorch/pull/69640))\r\n\r\n## Benchmark\r\n\r\n* Fixed divide-by-zero errors in `torch.utils.benchmark.Timer` ([#70050](https://github.com/pytorch/pytorch/pull/70050))\r\n\r\n## Dispatcher\r\n\r\n* Added explicit `OperatorHandle` destructor, so that the symbol shows up in windows builds ([#70033](https://github.com/pytorch/pytorch/pull/70033))\r\n\r\n## Profiler\r\n\r\n* Fixed race condition in profiler ([#65812](https://github.com/pytorch/pytorch/pull/65812))\r\n* Fixed TensorBoard memory profiling ([#71417](https://github.com/pytorch/pytorch/pull/71417))\r\n\r\n## Visualization\r\n\r\n* Fixed `torch.utils.tensorboard` parsing JIT graph incorrectly ([#65692](https://github.com/pytorch/pytorch/pull/65692))\r\n\r\n## Vulkan\r\n\r\n* Greatly reduced memory usage of the Vulkan backend by updating the configuration of the Vulkan Memory Allocator ([#69088](https://github.com/pytorch/pytorch/pull/69088))\r\n* Addressed several warnings raised by the Vulkan Validation layers:\r\n    * Updated all texture resources to have the same dimensionality ([#67647](https://github.com/pytorch/pytorch/pull/67647))\r\n    * Added image format qualifier to shader files ([#69330](https://github.com/pytorch/pytorch/pull/69330))\r\n    * Disabled SPIR-V compiler size optimization ([#69331](https://github.com/pytorch/pytorch/pull/69331))\r\n\r\n## Mobile\r\n\r\n* Fixed quantized logistic converter for `NNAPI` ([#70847](https://github.com/pytorch/pytorch/pull/70847))\r\n* Fixed potential crash if `MTLCreateSystemDefaultDevice` returns nil ([#66859](https://github.com/pytorch/pytorch/pull/66859))\r\n* Used full name to look for the promoted prim operator table ([#66081](https://github.com/pytorch/pytorch/pull/66081))\r\n* Fixed function name bug in mobile export ([#66915](https://github.com/pytorch/pytorch/pull/66915))\r\n* Fixed issues with `irange` not having a header included in `Metal` ([#66877](https://github.com/pytorch/pytorch/pull/66877))\r\n* Fixed backward compatibility issue for UnionType on mobile in `type_parser`. ([#71341](https://github.com/pytorch/pytorch/pull/71341))\r\n* Fixed forward flatbuffer type handling with dynamic type in `flatbuffer_loader`. ([#71500](https://github.com/pytorch/pytorch/pull/71500))\r\n* Fixed type equalities issue in `pytorch_jni_common` ([#71508](https://github.com/pytorch/pytorch/pull/71508))\r\n* Fixed missing properties to the executor in `CoreML` ([#67737](https://github.com/pytorch/pytorch/pull/67737/files))\r\n* Fixed memory computation when both constants and data tensors are present in model_dump ([#66006](https://github.com/pytorch/pytorch/pull/66006))\r\n* Ensured that function participating in bundled inputs have their \u201c__name__\" attribute set ([#65856](https://github.com/pytorch/pytorch/pull/65856))\r\n\r\n## Distributed\r\n\r\n* `torch.distributed`\r\n    * Fixed bug on empty `GLOO_SOCKET_IFNAME_ENV` ([#68933](https://github.com/pytorch/pytorch/pull/68933))\r\n* `DistributedDataParallel`\r\n    * Fixed \u201cCannot modify in-place due to DDPSink\u201d ([#66015](https://github.com/pytorch/pytorch/pull/66015))\r\n* `torch.distributed.elastic`\r\n    * Fixed scale down bug caused by calling `rdzv_handler.shutdown()` on premature agent failures ([#67749](https://github.com/pytorch/pytorch/pull/67749))\r\n\r\n## TorchScript\r\n\r\n* Fixed a race condition in the JIT interpreter when unpickling source ranges ([5525e9a591](https://github.com/pytorch/pytorch/commit/5525e9a5910a01b880f5f34827c43c29a1473775))\r\n* Fixed a ref counting loop for `CompilationUnit`, resulting in memory leaks when class objects were in JIT graphs. ([#65442](https://github.com/pytorch/pytorch/pull/65442))\r\n* Fixed bug where output type was discarded after calling SubgraphRewriter in C++ ([#65453](https://github.com/pytorch/pytorch/pull/65453))\r\n* Fixed bug where `torch.jit.optimize_for_inference` did not `torch.jit.freeze` a module when passed a a non-frozen module ([#71436](https://github.com/pytorch/pytorch/pull/71436))\r\n* Fixed bug where running module.forward() on a `torch.jit.freeze` ed  module ran the wrong graph ([#68316](https://github.com/pytorch/pytorch/pull/68316))\r\n* Fixed bug where alias analysis in the JIT optimizer was incorrect for the int[] version of `torch.split` , resulting in invalid optimizations in various JIT optimization passes ([#69745](https://github.com/pytorch/pytorch/pull/69745))\r\n* Fixed places where using `torch.autocast`  together with autodiff (module.backwards())  in a JIT graph had the wrong number of arguments and would error out. ([#67648](https://github.com/pytorch/pytorch/pull/67648))\r\n* Forbid propagating gradients through views in JIT graphs as currently it is broken ([#67732](https://github.com/pytorch/pytorch/pull/67732))\r\n* Fixed bug where graph input types were incorrect after running `torch.jit.trace` ([#68242](https://github.com/pytorch/pytorch/pull/68242))\r\n* Fixed case where BroadcastMKLDNN breaks the stack invariant by pushing more than 2 tensors to the stack for when `torch.jit.freeze` ops are converted to MKLDNN([#66628](https://github.com/pytorch/pytorch/pull/66628))\r\n* Raised error instead of segfaulting when passing None into torch.jit.Graph.create ([#68253](https://github.com/pytorch/pytorch/pull/68253))\r\n* Raised error instead of crashing when a JIT ScriptFunction is pickled with an incompatible Python `pickle` version.([#69807](https://github.com/pytorch/pytorch/pull/69807))\r\n* Fixed bug where `torch.jit.script` fails when comments in function has less indent than surrounding code ([#70227](https://github.com/pytorch/pytorch/pull/70227))\r\n* Fixed incorrect device type when torch.device is called inside scripted (`torch.jit.script`) code ([#69645](https://github.com/pytorch/pytorch/pull/69645))\r\n* Fixed warning: overloaded virtual function `torch::jit::Function::call` is only partially overridden in class `torch::jit::GraphFunction` ([4bf1be898d](https://github.com/pytorch/pytorch/commit/4bf1be898d))\r\n\r\n## Quantization\r\n\r\n* Fixed applying non-zero offset 1 to null pointer in `torch.nn.functional.interpolate` for quantized tensors ([#65570](https://github.com/pytorch/pytorch/pull/65570))\r\n* Doesn't assume bias is a keyword argument to `torch.nn.Conv{n}d` ([#61647](https://github.com/pytorch/pytorch/pull/61647), [#71426](https://github.com/pytorch/pytorch/pull/71426))\r\n* Made error message when trying to use `torch.quantize_per_tensor` on non floats more specific ([#66050](https://github.com/pytorch/pytorch/pull/66050))\r\n* Quantized `torch.nn.Embedding` conversion with unsupported dtype: make error message clearer ([#66051](https://github.com/pytorch/pytorch/pull/66051))\r\n* Fixed `torch.nn.qat.EmbeddingBag` from_float error message ([#66989](https://github.com/pytorch/pytorch/pull/66989))\r\n* Fixed bug enforcing quant_min <= zero_point <= quant_max for float zeropoint in `torch.nn.Embedding` QAT ([#68852](https://github.com/pytorch/pytorch/pull/68852))\r\n* Fixed scale+zp serialization of `torch.nn.quantized.BatchNorm{2|3}d` ([#70432](https://github.com/pytorch/pytorch/pull/70432))\r\n* Fixed `torch.nn.Dropout` in FX graph mode quantization ([#71043](https://github.com/pytorch/pytorch/pull/71043), [#71438](https://github.com/pytorch/pytorch/pull/71438))\r\n* Fixed `qconfig` setting for fused modules in FX graph mode quantization ([#71254](https://github.com/pytorch/pytorch/pull/71254))\r\n* Removed assumption number of rows is in 32 bit in fbgemm ([#69066](https://github.com/pytorch/pytorch/pull/69066))\r\n* Fixed `reduce_range` warning when using default observers ([#71027](https://github.com/pytorch/pytorch/pull/71027))\r\n\r\n## ONNX\r\n\r\n* Doesn\u2019t create invalid `index_select` op when constant folding through ONNX Gather with indices rank > 1. Fixes export of some uses of Embedding. ([#68493](https://github.com/pytorch/pytorch/pull/68493))\r\n* Shape inference:\r\n    * ConstantMap setters to update existing value instead of emplace, and fix default value of `keepdims` for Reduce ([#67812](https://github.com/pytorch/pytorch/pull/67812))\r\n    * Fixed memory leak ([#68210](https://github.com/pytorch/pytorch/pull/68210))\r\n    * Fixed reshape shape inference regression affecting LSTM ([#72532](https://github.com/pytorch/pytorch/pull/72532))\r\n* Fixed inplace `fill_` dtype export mismatch ([#64580](https://github.com/pytorch/pytorch/pull/64580))\r\n* Fixed `remainder` ([#64578](https://github.com/pytorch/pytorch/pull/64578))\r\n* Fixed `reciprocal` when input is not floating point ([#67808](https://github.com/pytorch/pytorch/pull/67808))\r\n* Fixed `new_full` and `full_like` for Python 3.9 ([#67806](https://github.com/pytorch/pytorch/pull/67806))\r\n* Fixed reduce ops on `binary_cross_entropy_with_logits` ([#67805](https://github.com/pytorch/pytorch/pull/67805))\r\n* Propagated node metadata across passes ([#45256](https://github.com/pytorch/pytorch/pull/45256))\r\n* Ensured outputs don\u2019t have the same name ([#66137](https://github.com/pytorch/pytorch/pull/66137))\r\n* Fixed `pad` with sequence inputs ([#64377](https://github.com/pytorch/pytorch/pull/64377))\r\n* Fixed `instance_norm` with `track_running_stats=True` ([#64375](https://github.com/pytorch/pytorch/pull/64375))\r\n* Fixed `all` and `any` with `dim` arg ([#67270](https://github.com/pytorch/pytorch/pull/67270))\r\n* Allows autograd functions (`prim::PythonOp`) to be exported with `OperatorExportTypes.ONNX_FALLTHROUGH` ([#67273](https://github.com/pytorch/pytorch/pull/67273))\r\n\r\n## torch.package\r\n\r\n* Prevent import race condition that leaves `torch.package.PackagePickler` with unwanted dispatch table entries. ([#71025](https://github.com/pytorch/pytorch/pull/71025))\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n* Speed up pickling for `torch.dtype` ([#65182](https://github.com/pytorch/pytorch/pull/65182))\r\n* Speed up `histogram`: avoid index_put_ overhead in histogram kernel's inner loop ([#67815](https://github.com/pytorch/pytorch/pull/67815))\r\n* Speed up `torch.topk` with sort for some cases ([#68632](https://github.com/pytorch/pytorch/pull/68632))\r\n* Speed up `torch.stack`: don't unsqueeze every stack arg if possible ([#70288](https://github.com/pytorch/pytorch/pull/70288))\r\n* Speed up `LayerNorm` 4-5% ([#71423](https://github.com/pytorch/pytorch/pull/71423))\r\n* Speed up structured kernels: fix some unnecessary refcount bumps ([#71140](https://github.com/pytorch/pytorch/pull/71140))\r\n* Speed up `indexing` functions: release GIL in a few places ([#71728](https://github.com/pytorch/pytorch/pull/71728))\r\n* Speed up `torch.empty` a bit: define check_sizes_nonnegative as inline ([#71640](https://github.com/pytorch/pytorch/pull/71640))\r\n* Speed up `XLA` tensor printing by reducing compilations ([#71147](https://github.com/pytorch/pytorch/pull/71147))\r\n\r\n## C++ API\r\n\r\n* Updated `c10::SmallVector` from LLVM ([#69110](https://github.com/pytorch/pytorch/pull/69110))\r\n* Reduced some framework overhead in `at::copy_()` ([#68950](https://github.com/pytorch/pytorch/pull/68950))\r\n* Reduced some overhead in `StorageImpl::set_data_ptr` ([#65432](https://github.com/pytorch/pytorch/pull/65432))\r\n* Improved `IValue` performance for tuples by inlining tuple storage ([#64066](https://github.com/pytorch/pytorch/pull/64066))\r\n\r\n## Autograd\r\n\r\n* Stopped materializing Tensors full of 0s in forward AD when possible ([#64837](https://github.com/pytorch/pytorch/pull/64837))\r\n* Rewrote the backward of `linalg.lu` and `linalg.lu_solve` to use `linalg_solve_triangular` ([#63569](https://github.com/pytorch/pytorch/pull/63569)) \r\n* Updated `nn.functional.grid_sample` backward to compute input gradient only if required ([#66069](https://github.com/pytorch/pytorch/pull/66069), [#66070](https://github.com/pytorch/pytorch/pull/66070))\r\n* Stopped erroneously saving the output of `torch.softplus` for backward ([#70296](https://github.com/pytorch/pytorch/pull/70296))\r\n\r\n## Complex Numbers\r\n\r\n* Release GIL when assigning to real or imaginary components of a complex tensor ([#71747](https://github.com/pytorch/pytorch/pull/71747))\r\n* Restored conjugate and negative bits of a tensor when calling `repeat_interleave`  ([#68523](https://github.com/pytorch/pytorch/pull/68523))\r\n\r\n## CUDA\r\n\r\n* Used a better hash table in `CUDACachingAllocator` ([#71667](https://github.com/pytorch/pytorch/pull/71667))\r\n*  `TopK` CUDA Optimization: used multiple block per slice ([#71081](https://github.com/pytorch/pytorch/pull/71081))\r\n* Removed sync in `Embedding` caused by `unique` ([#66091](https://github.com/pytorch/pytorch/pull/66091))\r\n* `EmbeddingBackward` exclusive_scan thrust->cub ([#66566](https://github.com/pytorch/pytorch/pull/66566))\r\n* `sort_out_cuda`: Used custom kernels to fill index tensors ([#66668](https://github.com/pytorch/pytorch/pull/66668))\r\n* `masked_scatter`: fuse mask count check into one kernel ([#66871](https://github.com/pytorch/pytorch/pull/66871))\r\n* Enabled better depthwise conv perf on cudnn 8.2+ ([#58749](https://github.com/pytorch/pytorch/pull/58749))\r\n* Improved native `layer_norm` forward perf ([#67977](https://github.com/pytorch/pytorch/pull/67977))\r\n* Improved native `layer_norm` backward perf ([#68238](https://github.com/pytorch/pytorch/pull/68238))\r\n* Fast path for size 0 GPU host malloc ([#68532](https://github.com/pytorch/pytorch/pull/68532))\r\n* Alternative implementation of CUDA pinned memory allocator focusing on multi-threaded scalability ([#69299](https://github.com/pytorch/pytorch/pull/69299))\r\n* Used legacy unrolled kernel for non-trivial offset calc cases ([#71710](https://github.com/pytorch/pytorch/pull/71710))\r\n* Removed `call_once` from `CUDACachingAllocator` ([#71668](https://github.com/pytorch/pytorch/pull/71668))\r\n* Reworked stat collection in `CUDACachingAllocator` ([#71669](https://github.com/pytorch/pytorch/pull/71669))\r\n* Fixed CUDA `LpNormFunctor` ([#70601](https://github.com/pytorch/pytorch/pull/70601))\r\n\r\n## Dispatcher\r\n\r\n* Made `c10::KernelFunction` struct smaller, which should reduce some memory usage by the dispatcher ([#65618](https://github.com/pytorch/pytorch/pull/65618))\r\n\r\n## torch.fx\r\n\r\n* Made `torch.fx.symbolic_trace` reuse buffers if they're the same ([#66211](https://github.com/pytorch/pytorch/pull/66211))\r\n\r\n## Profiler\r\n\r\n* Optimized profiler internals ([#68397](https://github.com/pytorch/pytorch/pull/68397), [#68411](https://github.com/pytorch/pytorch/pull/68411), [#69737](https://github.com/pytorch/pytorch/pull/69737), [#68412](https://github.com/pytorch/pytorch/pull/68412), [#70001](https://github.com/pytorch/pytorch/pull/70001), [#70002](https://github.com/pytorch/pytorch/pull/70002), [#70133](https://github.com/pytorch/pytorch/pull/70133))\r\n\r\n## Mobile\r\n\r\n* Reduced PyTorch Library startup time by 40% for mobile and edge deployments([#65735](https://github.com/pytorch/pytorch/pull/65735), [#65732](https://github.com/pytorch/pytorch/pull/65732), [#65939](https://github.com/pytorch/pytorch/pull/65939), [#66112](https://github.com/pytorch/pytorch/pull/66112), [#66064](https://github.com/pytorch/pytorch/pull/66064), [#66131](https://github.com/pytorch/pytorch/pull/66131))\r\n* Reduced PyTorch Library heap memory utilization by 40% for mobile and edge deployments([#65732](https://github.com/pytorch/pytorch/pull/65732), [#66112](https://github.com/pytorch/pytorch/pull/66112), [#66064](https://github.com/pytorch/pytorch/pull/66064), [#66131](https://github.com/pytorch/pytorch/pull/66131))\r\n* Improve efficiency of IValue and reduce overhead in code paths that use IValue and perform Type Parsing ([#65710](https://github.com/pytorch/pytorch/pull/65710), [#64278](https://github.com/pytorch/pytorch/pull/64278), [#66717](https://github.com/pytorch/pytorch/pull/66717), [#65381](https://github.com/pytorch/pytorch/pull/65381), [#66134](https://github.com/pytorch/pytorch/pull/66134), [#65951](https://github.com/pytorch/pytorch/pull/65951), [#70477](https://github.com/pytorch/pytorch/pull/70477))\r\n\r\n## TorchScript\r\n\r\n* Improved performance of autodiff on small JIT graphs ([#71666](https://github.com/pytorch/pytorch/pull/71666))\r\n* Enabled autocasting of tensors between fp16, bfloat 16 and fp32 in torchscript models ([#63939](https://github.com/pytorch/pytorch/pull/63939), [#67707](https://github.com/pytorch/pytorch/pull/67707))\r\n* Enables optimizations in more gradSumToSize cases in the JIT Autograd support([#63941](https://github.com/pytorch/pytorch/pull/63941))\r\n* In Unpickling a JIT graph, avoid reading file from a stream for 0 byte tensor storage([#67787](https://github.com/pytorch/pytorch/pull/67787))\r\n\r\n## Quantization\r\n\r\n* Sped up quantized `torch.nn.functional.interpolate` for channels last ([#66525](https://github.com/pytorch/pytorch/pull/66525))\r\n* Sped up `torch.nn.functional.upsample` for channels last ([#70903](https://github.com/pytorch/pytorch/pull/70903))\r\n* Parallelized computation in `torch.quantize_per_tensor_affine` and `torch.dequantize` ([#65845](https://github.com/pytorch/pytorch/pull/65845))\r\n\r\n# Documentation\r\n\r\n## Python API\r\n\r\n* Added docs for `torch.adjoint`. ([#68869](https://github.com/pytorch/pytorch/pull/68869))\r\n* Clarified difference in behavior of `empty_strided` and `as_strided` ([#64568](https://github.com/pytorch/pytorch/pull/64568))\r\n* Added some missing generated doc entries (`torch.select`, `torch.slice_scatter`, `torch.diagonal_scatter`, `torch.select_scatter`) ([#69030](https://github.com/pytorch/pytorch/pull/69030)),  `histogramdd`  ([#68273](https://github.com/pytorch/pytorch/pull/68273))\r\n* Typo and formatting fixes. `LinearLR`  ([#67840](https://github.com/pytorch/pytorch/pull/67840)), `torch.any` ([#65310](https://github.com/pytorch/pytorch/pull/65310), [#70187](https://github.com/pytorch/pytorch/pull/70187)), `torch.futures`  ([#70630](https://github.com/pytorch/pytorch/pull/70630)), jit docs ([#68557](https://github.com/pytorch/pytorch/pull/68557)), `Tensor.type`  ([#67019](https://github.com/pytorch/pytorch/pull/67019)), `torch.lobpcg` ([#71464](https://github.com/pytorch/pytorch/pull/71464)), `Tensor.triu()`, `Tensor.tril()`, `Tensor.ravel()`. ([#71057](https://github.com/pytorch/pytorch/pull/71057)), `torch.acosh` ([#66814](https://github.com/pytorch/pytorch/pull/66814)), ([#70439](https://github.com/pytorch/pytorch/pull/70439))\r\n* General Doc improvements for individual ops.  `torch.finfo` (mention `torch.bfloat16`) ([#68496](https://github.com/pytorch/pytorch/pull/68496)), `torch.quantile` interpolation kwarg ([#70637](https://github.com/pytorch/pytorch/pull/70637)), `from_dlpack` and `to_dlpack` ([#70437](https://github.com/pytorch/pytorch/pull/70437)), `set_printoptions` added examples ([#68324](https://github.com/pytorch/pytorch/pull/68324)), `index_add` ([#65806](https://github.com/pytorch/pytorch/pull/65806)), topk doc ([#65938](https://github.com/pytorch/pytorch/pull/65938)), `unique` ([#66132](https://github.com/pytorch/pytorch/pull/66132)), `chi2`  ([#67379](https://github.com/pytorch/pytorch/pull/67379)), `torch.histc` ([#64191](https://github.com/pytorch/pytorch/pull/64191)),  `empty` and `empty_like`  ([#68874](https://github.com/pytorch/pytorch/pull/68874)), `torch.cholesky_inverse` ([#69069](https://github.com/pytorch/pytorch/pull/69069)), `torch.dsplit`  ([#70557](https://github.com/pytorch/pytorch/pull/70557))\r\n* Changed README getting started link to explicit instructions ([#66828](https://github.com/pytorch/pytorch/pull/66828))\r\n* Modernized and clarified docs for `torch.tensor` and `torch.as_tensor` ([#63308](https://github.com/pytorch/pytorch/pull/63308))\r\n* Improved `torchhub` docs ([#69970](https://github.com/pytorch/pytorch/pull/69970))\r\n* Updated docs for `torch.Tensor.real` to indicate that it's supported for real tensors ([#71962](https://github.com/pytorch/pytorch/pull/71962))\r\n\r\n## C++ API\r\n\r\n* Fixed typos in ATen README ([#69170](https://github.com/pytorch/pytorch/pull/69170))\r\n* Mentioned `TORCH_SHOW_CPP_STACKTRACES` in `Contributing.md` docs ([#64052](https://github.com/pytorch/pytorch/pull/64052))\r\n* Updated link to C++ frontend examples ([#66095](https://github.com/pytorch/pytorch/pull/66095))\r\n* Added docs for Visual Studio extension ([#63944](https://github.com/pytorch/pytorch/pull/63944))\r\n* Added docs about an issue with compiling C++ extensions with CUDA 11.5 and Windows ([#73013](https://github.com/pytorch/pytorch/pull/73013))\r\n\r\n## Autograd\r\n\r\n* Updated docs for forward AD and make them public ([#71643](https://github.com/pytorch/pytorch/pull/71643), [#71159](https://github.com/pytorch/pytorch/pull/71159))\r\n* Updated \u201cExtending PyTorch\u201d doc to cover forward AD ([#66962](https://github.com/pytorch/pytorch/pull/66962))\r\n* Fixed broken code syntax in autograd.rst ([#69362](https://github.com/pytorch/pytorch/pull/69362))\r\n* Fixed incorrect variable in autograd docs ([#70884](https://github.com/pytorch/pytorch/pull/70884))\r\n* Fixed typo in `torch.autograd.Function` docs that prevented it from compiling ([#66754](https://github.com/pytorch/pytorch/pull/66754))\r\n\r\n## Dataloader\r\n\r\n* Added docstring for `default_collate` and `default_convert` ([#69862](https://github.com/pytorch/pytorch/pull/69862))\r\n* Updated the documentation for AMP with DataParallel ([#69218](https://github.com/pytorch/pytorch/pull/69218))\r\n\r\n## torch.nn\r\n\r\n* `F.binary_cross_entropy`: Updated examples to avoid deprecated calls ([#69816](https://github.com/pytorch/pytorch/pull/69816))\r\n* `F.linear`: Fixed shape docs to indicate no-batch-dim support ([#66884](https://github.com/pytorch/pytorch/pull/66884))\r\n* `F.max_pool*d`: Added functional docs ([#63264](https://github.com/pytorch/pytorch/pull/63264))\r\n* `F.multilabel_soft_margin_loss`: Added reduction args to signature ([#70420](https://github.com/pytorch/pytorch/pull/70420))\r\n* `nn.AdaptiveLogSoftmaxWithLoss`: Fixed typo in `log_prob` name ([#68926](https://github.com/pytorch/pytorch/pull/68926))\r\n* `nn.{BatchNorm1d, InstanceNorm1d}`: Fixed input shape notation inconsistencies ([#71371](https://github.com/pytorch/pytorch/pull/71371))\r\n* `nn.CrossEntropyLoss`: Corrected typo in formula for class probability targets ([#70220](https://github.com/pytorch/pytorch/pull/70220))\r\n* `nn.{ELU, Hardshrink, Hardsigmoid, MultiHeadAttention, Softplus, Tanh}`: Made first line of docstring readable for overview docs ([#70574](https://github.com/pytorch/pytorch/pull/70574), [#71012](https://github.com/pytorch/pytorch/pull/71012), [#70987](https://github.com/pytorch/pytorch/pull/70987), [#71100](https://github.com/pytorch/pytorch/pull/71100), [#70576](https://github.com/pytorch/pytorch/pull/70576), [#70577](https://github.com/pytorch/pytorch/pull/70577))\r\n* `nn.Flatten`: Simplified example code ([#67472](https://github.com/pytorch/pytorch/pull/67472))\r\n* `nn.{Hardsigmoid, Hardswish, Mish, RReLU, SiLU}`: Added activation function images ([#65415](https://github.com/pytorch/pytorch/pull/65415))\r\n* `nn.KLDivLoss`: Fixed rendering of `reduction` arg ([#66583](https://github.com/pytorch/pytorch/pull/66583))\r\n* `nn.KLDivLoss`: Rewrote docs to clarify math ([#67443](https://github.com/pytorch/pytorch/pull/67443))\r\n* `nn.MaxUnpool2d`: Changed misleading example to better demonstrate `output_size` usage ([#68936](https://github.com/pytorch/pytorch/pull/68936))\r\n* `nn.Module`: Added note describing required `super().__init__()` call ([#66909](https://github.com/pytorch/pytorch/pull/66909))\r\n* `nn.Module`: Changed `super()` usage to Python 3 syntax in example ([#65748](https://github.com/pytorch/pytorch/pull/65748))\r\n* `nn.Module`: Fixed formatting for `named_modules()` ([#70491](https://github.com/pytorch/pytorch/pull/70491))\r\n* `nn.NLLLoss`: Corrected default value for `reduce` ([#68426](https://github.com/pytorch/pytorch/pull/68426))\r\n* `nn.SmoothL1Loss`: Clarified equivalence with `nn.L1Loss` when `beta == 0` ([#70673](https://github.com/pytorch/pytorch/pull/70673))\r\n* `nn.{TransformerDecoderLayer, TransformerEncoderLayer}`: Clarified default `batch_first=False` dimension format ([#66574](https://github.com/pytorch/pytorch/pull/66574))\r\n* `nn.Upsample`: Indicated that `align_corners` takes effect in `bicubic` mode ([#66756](https://github.com/pytorch/pytorch/pull/66756))\r\n* `nn.utils.clip_grad_norm_`: Fixed rendering of `parameters` in `error_if_nonfinite` arg docs ([#69958](https://github.com/pytorch/pytorch/pull/69958))\r\n* `optim.Adam`: Fixed formatting ([#70387](https://github.com/pytorch/pytorch/pull/70387))\r\n* `optim.AdamW`: Fixed formula ([#68587](https://github.com/pytorch/pytorch/pull/68587))\r\n* `optim.RAdam`: Corrected default value of `lr` arg ([#69186](https://github.com/pytorch/pytorch/pull/69186))\r\n* Removed orphan from cuDNN persistent note ([#65160](https://github.com/pytorch/pytorch/pull/65160))\r\n* Updated link to tutorial on defining NN modules ([#65534](https://github.com/pytorch/pytorch/pull/65534))\r\n* `nn.{AvgPool1d, AdaptiveAvgPool3d, MultiMarginLoss, PairwiseDistance, TripletMarginLoss}, ``F.{conv3d, conv_transpose3d, fold, linear}`: Fix doc formatting regressions from no-batch-dim support ([#73014](https://github.com/pytorch/pytorch/pull/73014))\r\n\r\n## torch.fx\r\n\r\n* Fixed for retracing documentation which would break for n-ary operators ([#71599](https://github.com/pytorch/pytorch/pull/71599))\r\n* Updated `torch.fx.passes.split_module` docstring ([#65542](https://github.com/pytorch/pytorch/pull/65542))\r\n* Updated `fx.rst` example outputs ([#68043](https://github.com/pytorch/pytorch/pull/68043))\r\n* Added document gotcha about training flag ([#68915](https://github.com/pytorch/pytorch/pull/68915))\r\n* Defined `get_dot_``graph` to match documentation ([#70541](https://github.com/pytorch/pytorch/pull/70541))\r\n\r\n## Sparse\r\n\r\n* Updated sparse.rst to warn about _values() ([#71088](https://github.com/pytorch/pytorch/pull/71088))\r\n\r\n## CUDA\r\n\r\n* Updated Stream `wait` documentation to reference underlying `cudaStreamWaitEvent` call ([#67973](https://github.com/pytorch/pytorch/pull/67973))\r\n* Documented `torch.cuda.ExternalStream`, `torch.cuda.caching_allocator_alloc` and `torch.cuda.caching_allocator_delete` ([#70126](https://github.com/pytorch/pytorch/pull/70126))\r\n* Updated `CUDA Graphs` docs: Fixed `make_graphed_callables` example typos ([#69379](https://github.com/pytorch/pytorch/pull/69379))\r\n\r\n## Mobile\r\n\r\n* Added user facing documentation for tracing-based selective build mobile interpreter in Android and iOS ([#1709](https://github.com/pytorch/tutorials/pull/1709))\r\n* Added recipe for bundled inputs in TorchScript models ([#1524](https://github.com/pytorch/tutorials/pull/1524/files))\r\n\r\n## Distributed\r\n\r\n* `DistributedDataParallel`\r\n    * DDP doc fix ([#71363](https://github.com/pytorch/pytorch/pull/71363))\r\n    * Clarified how to check memory saving if using gradient_as_bucket_view ([#71483](https://github.com/pytorch/pytorch/pull/71483))\r\n\r\n* `torch.distributed`\r\n    * Updated distributed.rst to show that CUDA send/recv on GPU is supported ([#65601](https://github.com/pytorch/pytorch/pull/65601))\r\n    * Clarified checkpoint support ([#68827](https://github.com/pytorch/pytorch/pull/68827))\r\n    * Updated distributed.rst for ProcessGroup Extensions ([#71482](https://github.com/pytorch/pytorch/pull/71482))\r\n* `torch.distributed.elastic`\r\n    * Made --max_restarts explicit in the quickstart and runner docs ([#65838](https://github.com/pytorch/pytorch/pull/65838))\r\n* `torch.distributed.optim`\r\n    * Rendered `torch.distributed.optim` members ([#67885](https://github.com/pytorch/pytorch/pull/67885))\r\n* `torch.distributed.rpc`\r\n    * Deleted distributed optimizer section from RPC and add reference to namespace docs page ([#68068](https://github.com/pytorch/pytorch/pull/68068))\r\n\r\n## TorchScript\r\n\r\n* Added `typing.Union` to supported types in documentation ([#68435](https://github.com/pytorch/pytorch/pull/68435))\r\n* Added documentation to `torch.jit.is_tracing()` ([#67326](https://github.com/pytorch/pytorch/pull/67326))\r\n* Fixed typos in `jit_language_reference.rst` ([#68706](https://github.com/pytorch/pytorch/pull/68706))\r\n\r\n## Quantization\r\n\r\n* Added documentation for quantized model save/load instructions ([#69789](https://github.com/pytorch/pytorch/pull/69789))\r\n* Updated link to qnnpack in quantization doc. ([#66226](https://github.com/pytorch/pytorch/pull/66226))\r\n* Improved quantization API docs ([#66379](https://github.com/pytorch/pytorch/pull/66379))\r\n* Quantization docs: add pages for Numeric Suite (Eager and FX) ([#66380](https://github.com/pytorch/pytorch/pull/66380))\r\n* Documented the quantization custom module APIs ([#67449](https://github.com/pytorch/pytorch/pull/67449))\r\n* Improved quantization documentation ([#68907](https://github.com/pytorch/pytorch/pull/68907))\r\n\r\n## ONNX\r\n\r\n* Improved documentation of `operator_export_type` and `opset_version` args ([#69549](https://github.com/pytorch/pytorch/pull/69549))\r\n* Fixed documentation for `do_constant_folding` arg default ([#71348](https://github.com/pytorch/pytorch/pull/71348))\r\n* Documented `ExportTypes`, `CheckerError`, and `unregister_custom_op_symbolic` ([#68489](https://github.com/pytorch/pytorch/pull/68489))\r\n* Fixed link to ONNX Runtime custom op documentation ([#67944](https://github.com/pytorch/pytorch/pull/67944))\r\n* Added section \u201cDiscovering all unconvertible ATen ops at once\u201d ([#66143](https://github.com/pytorch/pytorch/pull/66143))\r\n* Fixed typos ([#66090](https://github.com/pytorch/pytorch/pull/66090))\r\n* Documented work-arounds for indexing export limitations, and improve error messages ([#64579](https://github.com/pytorch/pytorch/pull/64579))\r\n\r\n## torch.package\r\n\r\n* Add some docs describing how to debug `torch.package` dependencies ([#65704](https://github.com/pytorch/pytorch/pull/65704))\r\n\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.11.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.11.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.11.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/61381447", "dateCreated": "2022-03-08T15:46:44Z", "datePublished": "2022-03-10T16:59:55Z"}, {"tagName": "v1.10.2", "name": "PyTorch 1.10.2 Release, small bug fix release", "authorName": "atalman", "authorType": "User", "body": "This release is meant to deploy additional fixes not included in 1.10.1 release:\r\n\r\n* fix pybind issue for get_autocast_cpu_dtype and get_autocast_gpu_dtype #66396\r\n* Remove fgrad_input from slow_conv2d #64280\r\n* fix formatting CIRCLE_TAG when building docs #67026\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.10.2", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.10.2", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.10.2", "url": "https://api.github.com/repos/pytorch/pytorch/releases/58138618", "dateCreated": "2021-12-14T17:24:18Z", "datePublished": "2022-01-27T21:51:23Z"}, {"tagName": "v1.10.1", "name": "PyTorch 1.10.1 Release, small bug fix release", "authorName": "seemethere", "authorType": "User", "body": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n* torch.nn.cross_entropy silently incorrect in PyTorch 1.10 on CUDA on non-contiguous inputs #67167\r\n* channels_last significantly degrades accuracy #67239\r\n* Potential strict aliasing rule violation in bitwise_binary_op (on ARM/NEON) #66119\r\n* torch.get_autocast_cpu_dtype() returns a new dtype #65786\r\n* Conv2d grad bias gets wrong value for bfloat16 case  #68048\r\n\r\nThe [release tracker](https://github.com/pytorch/pytorch/issues/69100) should contain all relevant pull requests related to this release as well as links to related issues", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.10.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.10.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.10.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/55404241", "dateCreated": "2021-12-09T16:59:45Z", "datePublished": "2021-12-15T22:27:43Z"}, {"tagName": "v1.10.0", "name": "PyTorch 1.10 Release, including CUDA Graphs APIs, Frontend and compiler improvements", "authorName": "albanD", "authorType": "User", "body": "# 1.10.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.10. This release is composed of over 3,400 commits since 1.9, made by 426 contributors. We want to sincerely thank our community for continuously improving PyTorch. \r\n\r\nPyTorch 1.10 updates are focused on improving training and performance of PyTorch, and developer usability. Highlights include:\r\n* CUDA Graphs APIs are integrated to reduce CPU overheads for CUDA workloads.\r\n* Several frontend APIs such as FX, `torch.special`, and `nn.Module` Parametrization, have moved from beta to stable.  \r\n* Support for automatic fusion in JIT Compiler expands to CPUs in addition to GPUs.\r\n* Android NNAPI support is now available in beta.\r\n\r\nYou can check the blogpost that shows the new features [here](https://pytorch.org/blog/pytorch-1.10-released/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### `torch.any`/`torch.all` behavior changed slightly to be more consistent for zero-dimension, `uint8` tensors. ([#64642](https://github.com/pytorch/pytorch/pull/64642))\r\n\r\nThese two functions match the behavior of NumPy, returning an output dtype of bool for all support dtypes, except for `uint8` (in which case they return a 1 or a 0, but with `uint8` dtype). In some cases with 0-dim tensor inputs, the returned `uint8` value could mistakenly take on a value > 1. This has now been fixed.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8))\r\ntensor(1, dtype=torch.uint8)\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8), dim=0)\r\ntensor(42, dtype=torch.uint8) # wrong, old behavior\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8))\r\ntensor(1, dtype=torch.uint8)\r\n>>> torch.all(torch.tensor(42, dtype=torch.uint8), dim=0)\r\ntensor(1, dtype=torch.uint8) # new, corrected and consistent behavior\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Remove deprecated `torch.{is,set}_deterministic` ([#62158](https://github.com/pytorch/pytorch/pull/62158))\r\n\r\nThis is the end of the deprecation cycle for both of these functions. You should be using `torch.use_deterministic_algorithms` and`torch.are_deterministic_algorithms_enabled` instead.\r\n\r\n## Complex Numbers\r\n\r\n### **Conjugate View: [`tensor.conj()`](https://pytorch.org/docs/1.10./generated/torch.conj.html) now returns a view tensor that aliases the same memory and has conjugate bit set ([#54987](https://github.com/pytorch/pytorch/pull/54987), [#60522](https://github.com/pytorch/pytorch/pull/60522), [#66082](https://github.com/pytorch/pytorch/pull/66082), [#63602](https://github.com/pytorch/pytorch/pull/63602)).** \r\n\r\nThis means that `.conj()` is now an O(1) operation and returns a tensor that views the same memory as `tensor` and has conjugate bit set. This notion of conjugate bit enables fusion of operations with conjugation which gives a lot of performance benefit for operations like matrix multiplication. All out-of-place operations will have the same behavior as before, but an in-place operation on a conjugated tensor will additionally modify the input tensor. \r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> y.add_(2)\r\n>>> print(x)\r\ntensor([1.+2.j])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> y.add_(2)\r\n>>> print(x)\r\ntensor([3.+2.j])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nNote: You can verify if the conj bit is set by calling `tensor.is_conj()`. The conjugation can be resolved, i.e., you can obtain a new tensor that doesn\u2019t share storage with the input tensor at any time by calling `conjugated_tensor.clone()` or `conjugated_tensor.resolve_conj()` .\r\n\r\nNote that these conjugated tensors behave differently from the corresponding numpy arrays obtained from `np.conj()` when an in-place operation is performed on them (similar to the example shown above).\r\n\r\n\r\n### **Negative View: `tensor.conj().neg()` returns a view tensor that aliases the same memory as both tensor and `tensor.conj()` and has a negative bit set ([#56058](https://github.com/pytorch/pytorch/pull/56058)).**\r\n\r\n`conjugated_tensor.neg()` continues to be an O(1) operation, but the returned tensor shares memory with both `tensor` and `conjugated_tensor`.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> z = y.imag\r\n>>> z.add_(2)\r\n>>> print(x)\r\ntensor([1.+2.j])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj()\r\n>>> z = y.imag\r\n>>> print(z.is_neg())\r\nTrue\r\n>>> z.add_(2)\r\n>>> print(x)\r\ntensor([1.-0.j])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `tensor.numpy()` now throws `RuntimeError` when called on a tensor with conjugate or negative bit set ([#61925](https://github.com/pytorch/pytorch/pull/61925)).\r\n\r\nBecause the notion of conjugate bit and negative bit doesn\u2019t exist outside of PyTorch, calling operations that return a Python object viewing the same memory as input like `.numpy()` would no longer work for tensors with conjugate or negative bit set.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj().imag\r\n>>> print(y.numpy())\r\n[2.]\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.tensor([1+2j])\r\n>>> y = x.conj().imag\r\n>>> print(y.numpy())\r\nRuntimeError: Can't call numpy() on Tensor that has negative\r\nbit set. Use tensor.resolve_neg().numpy() instead.\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Autograd\r\n\r\n### Raise `TypeError` instead of `RuntimeError` when assigning to a Tensor\u2019s grad field with wrong type ([#64876](https://github.com/pytorch/pytorch/pull/64876))\r\n\r\nSetting the `.grad` field with a non-None and non-Tensor object used to return a `RuntimeError` but it now properly returns a `TypeError`. If your code was catching this error, you should simply update it to catch a `TypeError` instead of a `RuntimeError`.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntry:\r\n    # Assigning an int to a Tensor's grad field\r\n    a.grad = 0\r\nexcept RuntimeError as e:\r\n    pass\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntry:\r\n   a.grad = 0\r\nexcept TypeError as e:\r\n    pass\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Raise error when inputs to `autograd.grad` are empty ([#52016](https://github.com/pytorch/pytorch/pull/52016))\r\n\r\nCalling `autograd.grad` with an empty list of inputs used to do the same as backward. To reduce confusion, it now raises the expected error. If you were relying on this, you can simply update your code as follows:\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ngrad = autograd.grad(out, tuple())\r\nassert grad == tuple()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nout.backward()\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Optional arguments to `autograd.gradcheck` and `autograd.gradgradcheck` are now kwarg-only ([#65290](https://github.com/pytorch/pytorch/pull/65290))\r\n\r\nThese two functions now have a significant number of optional arguments controlling what they do (i.e., `eps`, `atol`, `rtol`, `raise_exception`, etc.). To improve readability, we made these arguments kwarg-only. If you are passing these arguments to `autograd.gradcheck` or `autograd.gradgradcheck` as positional arguments, you can update your code as follows:\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.autograd.gradcheck(fn, x, 1e-6)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.autograd.gradcheck(fn, x, eps=1e-6)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### In-place detach (`detach_`) now errors for views that return multiple outputs ([#58285](https://github.com/pytorch/pytorch/pull/58285))\r\n\r\nThis change is finishing the deprecation cycle for the inplace-over-view logic. In particular, a few things that were warning are updated:\r\n\r\n    * `detach_` will now raise an error when invoked on any view created by `split`, `split_with_sizes`, or `chunk`. You should use the non-inplace `detach` instead.\r\n    * The error message for when an in-place operation (that is not detach) is performed on a view created by `split`, `split_with_size`, and `chunk` has been changed from \"This view is an output of a function...\" to \"This view is the output of a function...\".\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nb = a.split(1)[0]\r\nb.detach_()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nb = a.split(1)[0]\r\nc = b.detach()\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Fix saved variable unpacking version counter ([#60195](https://github.com/pytorch/pytorch/pull/60195))\r\n\r\nIn-place on the unpacked SavedVariables used to be ignored. They are now properly detected which can lead to errors saying that a variable needed for backward was modified in-place.\r\nThis is a valid error and the user should fix this by cloning the unpacked saved variable before using it.\r\n\r\nNo internal formula will trigger this, but it might be triggered by user custom `autograd.Function` if the backward modifies a saved Tensor inplace and you do multiple backwards. This used to silently return the wrong result and will now raise the expected error.\r\n\r\n## torch.nn\r\n\r\n### Added optional tensor arguments to `__torch_function__` handling checks ([#63967](https://github.com/pytorch/pytorch/pull/63967))\r\n\r\nThis fixes the `has_torch_function*()` checks throughout `torch.nn.functional` to correctly pass in optional tensor arguments; prior to this fix, `handle_torch_function()` was not called for these optional tensor arguments. Previously, passing a tensor-like object into a function that accepts an optional tensor might not trigger that object's `__torch_function__`. Now, the object's `__torch_function__` will be triggered as expected.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.nn.functional as F\r\nclass TestTensor(object):\r\n    def __init__(self, weight):\r\n        self.weight = weight\r\n    def __torch_function__(self, func, _, args=(), kwargs=None):\r\n        print(func)\r\n        print(func == F.group_norm)\r\n# Call F.group_norm with a custom Tensor as the non-optional arg 'features'\r\nfeatures = TestTensor(torch.randn(3,3))\r\nF.group_norm(features, 3)\r\n# ...prints \"group_norm\" and True\r\n# Call F.group_norm with a custom Tensor as the optional arg 'weight'\r\nfeatures = torch.randn(3,3)\r\nweight = TestTensor(torch.randn(3))\r\nF.group_norm(features, 3, weight=weight)\r\n# ...prints \"group_norm\" and False because weight's __torch_function__ is\r\n# called with func as torch.group_norm instead of F.group_norm\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.nn.functional as F\r\nclass TestTensor(object):\r\n    def __init__(self, weight):\r\n        self.weight = weight\r\n    def __torch_function__(self, func, _, args=(), kwargs=None):\r\n        print(func)\r\n        print(func == F.group_norm)\r\n# Call F.group_norm with a custom Tensor as the non-optional arg 'features'\r\nfeatures = TestTensor(torch.randn(3,3))\r\nF.group_norm(features, 3)\r\n# ...prints \"group_norm\" and True\r\n# Call F.group_norm with a custom Tensor as the optional arg 'weight'\r\nfeatures = torch.randn(3,3)\r\nweight = TestTensor(torch.randn(3))\r\nF.group_norm(features, 3, weight=weight)\r\n# ...prints \"group_norm\" and True\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## CUDA\r\n\r\n### Removed post-backward syncs on default stream ([#60421](https://github.com/pytorch/pytorch/pull/60421))\r\n\r\nCalls to backward() or grad() synced only the calling thread's default stream with autograd leaf streams at the end of backward. This made the following weird pattern safe:\r\n\r\n```python\r\nwith torch.cuda.stream(s):\r\n    # imagine forward used many streams, so backward leaf nodes may run on many streams\r\n    loss.backward()# no sync\r\nuse grads\r\n```\r\n\r\nbut a more benign-looking pattern was unsafe:\r\n\r\n```python\r\nwith torch.cuda.stream(s):\r\n    # imagine forward used a lot of streams, so backward leaf nodes may run on many streams\r\n    loss.backward()\r\n    # backward() syncs the default stream with all the leaf streams, but does not sync s with anything,\r\n    # so counterintuitively (even though we're in the same stream context as backward()!)\r\n    # it is NOT SAFE to use grads here, and there's no easy way to make it safe,\r\n    # unless you manually sync on all the streams you used in forward,\r\n    # or move \"use grads\" back to default stream outside the context.\r\n    use grads\r\n```\r\n\r\nNote: this change makes it so that backward() has [same user-facing stream semantics as any cuda op](https://pytorch.org/docs/master/notes/cuda.html#stream-semantics-of-backward-passes).** In other words, the weird pattern is unsafe, and the benign-looking pattern is safe. Implementation-wise, this meant backward() should sync its calling thread's current stream, not default stream, with the leaf streams. This PR  deletes syncs on the default stream. \r\n\r\n## torch.package\r\n\r\n* Removed verbose mode from PackageExporter ([#61145](https://github.com/pytorch/pytorch/pull/61145))\r\n    * PackageExporter is losing \u201cverbose\u201d mode argument as we have found it is not useful and sometimes confusing. See following examples on how to modify your code to accommodate this change.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nwith PackageExporter(buffer, verbose=False) as e:\r\n    e.intern(\"**\")\r\n    e.save_pickle(\"res\", \"mod1.pkl\", mod1)\r\n    e.save_pickle(\"res\", \"mod2.pkl\", mod2)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nwith PackageExporter(buffer) as e:\r\n    e.intern(\"**\")\r\n    e.save_pickle(\"res\", \"mod1.pkl\", mod1)\r\n    e.save_pickle(\"res\", \"mod2.pkl\", mod2)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n## Quantization\r\n\r\n### Added extra observer/fake_quant (the same observer/fake_quant instance as the input) for some operators in prepare_fx, e.g. maxpool, add_scalar and mul_scalar ([#61687](https://github.com/pytorch/pytorch/pull/61687), [#61859](https://github.com/pytorch/pytorch/pull/61859))\r\n\r\nPreviously the way we insert observers/fake_quants are specific to fbgemm/qnnpack backend, as we work on making FX Graph Mode Quantization extensible to custom backends, we are changing some behaviors for the fbgemm/qnnpack path as well. The above changes are adding extra observer/fake_quant to the output of some operators to make sure we model the quantized operator more accurately in quantization aware training, the comprehensive list of operators where the behavior changes are the following:\r\n\r\n* modules: torch.nn.MaxPool1d, torch.nn.MaxPool2d, torch.nn.MaxPool3d, torch.nn.Identity\r\n* torch functions: torch.nn.functional.max_pool1d, torch.nn.functional.max_pool2d, torch.nn.functional.max_pool3d, torch.chunk, torch.flatten, torch.transpose, torch.repeat_interleave, torch.sort, torch.squeeze, torch.stack, torch.unsqueeze, operator.getitem, \r\n* Tensor methods: chunk, contiguous, detach, detach_, numel, permute, repeat, repeat_interleave, reshape, resize_, shape, size, squeeze, squeeze_, transpose, unsqueeze, unsqueeze_, view\r\n* Tensor operations: add scalar and mul scalar (add/mul with a Tensor and a Scalar input)\r\n\r\n\r\nWe will show an example with torch.nn.MaxPool2d:\r\n\r\n```python\r\nclass M(torch.nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.maxpool2d = torch.nn.MaxPool2d(kernel_size=3)\r\n\r\n    def forward(self, x):\r\n        x = self.maxpool2d(x)\r\n        return x\r\nm = M().eval()        \r\nm = prepare_fx(m, {\"\": torch.quantization.default_qconfig})\r\nprint(m.code)\r\n```\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ndef forward(self, x):\r\n    x_activation_post_process_0 = self.x_activation_post_process_0(x); x = None\r\n    maxpool2d = self.maxpool2d(x_activation_post_process_0); x_activation_post_process_0 = None\r\n    return maxpool2d\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ndef forward(self, x):\r\n    x_activation_post_process_0 = self.x_activation_post_process_0(x); x = None\r\n    maxpool2d = self.maxpool2d(x_activation_post_process_0); x_activation_post_process_0 = None\r\n    maxpool2d_activation_post_process_0 = self.maxpool2d_activation_post_process_0(maxpool2d); maxpool2d = None\r\n    return maxpool2d_activation_post_process_0\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nNote that `self.maxpool2d_activation_post_process_0` and `self.x_activation_post_process_0` will refer to the same observer/fake_quant instance, this is to simulate the numerics for the quantized maxpool implementation, where the output would reuse the quantization parameter of the input. Simple illustration with graph:\r\n\r\nBefore:\r\n\r\n```\r\nobserver_0 - maxpool - ...\r\n```\r\n\r\nAfter:\r\n\r\n```\r\nobserver_0 - maxpool - observer_0 (same observer instance as input observer) - ...\r\n```\r\n\r\n## ONNX\r\n\r\n### Removed `aten` arg from `torch.onnx.export()`. ([#62759](https://github.com/pytorch/pytorch/pull/62759))\r\n\r\nThe new `OperatorExportTypes.ONNX` removes the need for an explicit `aten` argument. If Pytorch was built with `-DPYTORCH_ONNX_CAFFE2_BUNDLE` the a `None` value means `OperatorExportTypes.ONNX_ATEN_FALLBACK`\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.9.1</th><th>1.10.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(..., aten=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ntorch.onnx.export(..., operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### Deprecate **`__torch_function__`** as a plain methods ([#64843](https://github.com/pytorch/pytorch/pull/64843))\r\n\r\nThe `__torch_function__` function used to create Tensor like objects did not have any constraint whether it should be a method, class method or static method.\r\n\r\nTo make it compatible with newer features on Tensor-like objects, we are deprecating setting it as a plain method. You can define it as a class method to get the current class and scan the argument list if you need an object that is an instance of this class.\r\n\r\n## Mobile\r\n\r\n### Removed API torch.utils.bundled_inputs.run_on_bundled_input ([#58344](https://github.com/pytorch/pytorch/pull/58344))\r\n\r\nThis API caused many issues and is not really necessary. The functionality (run model with bundled input) can be achieved by using `get_all_bundled_inputs`. For example:\r\n\r\n1.9.1:\r\n\r\n```python\r\nmodel.run_on_bundled_input(0)\r\n```\r\n\r\n1.10.0:\r\n\r\n```python\r\nmodel(*model.get_all_bundled_inputs()[0])\r\n```\r\n\r\n## Distributed\r\n\r\n### `torch.distributed.rpc`: Removed ProcessGroup RPC backend ([#62411](https://github.com/pytorch/pytorch/pull/62411) , [#62985](https://github.com/pytorch/pytorch/pull/62985))\r\n\r\nProcessGroup RPC backend has been deprecated and 1.9 was the last release which carried it. The default RPC backend is TensorPipe which is the recommended backend for RPC. Users who use `torch.distributed.rpc.BackendType.PROCESS_GROUP` will be given an error message to switch to `torch.distributed.rpc.BackendType.TENSORPIPE`.\r\n\r\n## ONNX\r\n\r\n### Removed following arguments in torch.onnx.export(): enable_onnx_checker, strip_doc_string, _retain_param_name  ([#64369](https://github.com/pytorch/pytorch/pull/64369), [#64371](https://github.com/pytorch/pytorch/pull/64371), [#64370](https://github.com/pytorch/pytorch/pull/64370))\r\n\r\n`enable_onnx_checker` argument is removed. ONNX checker will now always run by default. Users can catch exceptions to ignore raised failures. `strip_doc_string` has been rolled into the `verbose` arg in `torch.onnx.export()`. `_retain_param_name` argument has been removed in  `torch.onnx.export()` will default to `True` . There is no way to get the old behavior of `_retain_param_name=False`. Users should stop setting this arg.\r\n\r\n1.9.1:\r\n\r\n```\r\ntorch.onnx.export(..., enable_onnx_checker=False, strip_doc_string=False)\r\n```\r\n\r\n1.10.0:\r\n\r\n```\r\ntry:\r\n    torch.onnx.export(verbose=True)\r\nexcept torch.onnx.utils.ONNXCheckerError:\r\n   pass\r\n```\r\n\r\n## Infra (Releng)\r\n\r\n### Disable ParallelTBB ([#65092](https://github.com/pytorch/pytorch/pull/65092))\r\n\r\n`ParallelTBB` config/codepath is no longer actively tested by PyTorch CI and as result is subject to code/functionality degradation\r\n\r\n\r\n# New features\r\n\r\n## Python API\r\n\r\n* Added new functions:\r\n    *  `torch.isin()` ([#53125](https://github.com/pytorch/pytorch/pull/53125)), `torch.bitwise_{left/right}_shift`, `__rlshift__`, `__rrshift__` ([#59544](https://github.com/pytorch/pytorch/pull/59544)), `torch.Tensor.{__rand__, __ror__,__rxor__}` ([#59240](https://github.com/pytorch/pytorch/pull/59240)),  `torch.aminmax` ([#62401](https://github.com/pytorch/pytorch/pull/62401)),  `torch.new_ones` ([#58405](https://github.com/pytorch/pytorch/pull/58405))\r\n    * For numpy compatibility `torch.cov` ([#58311](https://github.com/pytorch/pytorch/pull/58311)), `torch.frombuffer` ([#59077](https://github.com/pytorch/pytorch/pull/59077)), `torch.corrcoef` ([#60420](https://github.com/pytorch/pytorch/pull/60420)), `torch.nanmean` ([#62671](https://github.com/pytorch/pytorch/pull/62671)), `torch.cumulative_trapezoid` ([#61615](https://github.com/pytorch/pytorch/pull/61615))\r\n* The [torch.special module](https://pytorch.org/docs/1.10.0/special.html?highlight=special) is now stable! This module, consistent with SciPy\u2019s special module, has 30 operations including the Hurwitz zeta function and various gamma functions.  ([#59623](https://github.com/pytorch/pytorch/pull/59623), [#56352](https://github.com/pytorch/pytorch/pull/56352), [#58126](https://github.com/pytorch/pytorch/pull/58126), [#59141](https://github.com/pytorch/pytorch/pull/59141), [#59143](https://github.com/pytorch/pytorch/pull/59143), [#58650](https://github.com/pytorch/pytorch/pull/58650), [#55878](https://github.com/pytorch/pytorch/pull/55878), [#58838](https://github.com/pytorch/pytorch/pull/58838), [#60512](https://github.com/pytorch/pytorch/pull/60512), [#60641](https://github.com/pytorch/pytorch/pull/60641), [#61633](https://github.com/pytorch/pytorch/pull/61633), [#60519](https://github.com/pytorch/pytorch/pull/60519), [#59691](https://github.com/pytorch/pytorch/pull/59691), [#58194](https://github.com/pytorch/pytorch/pull/58194))\r\n* Added support for slots and subclass magic getstate/setstate method for Tensor serialization ([#62745](https://github.com/pytorch/pytorch/pull/62745))\r\n* `torch.optim`:\r\n    * Added Nesterov Adam as `NAdam` ([#59009](https://github.com/pytorch/pytorch/pull/59009))\r\n    * Added `lr_scheduler.ChainedScheduler` ([#63491](https://github.com/pytorch/pytorch/pull/63491), [#63457](https://github.com/pytorch/pytorch/pull/63457), [#65034](https://github.com/pytorch/pytorch/pull/65034)))\r\n    * Added `lr_scheduler.SequentialLR` ([#64037](https://github.com/pytorch/pytorch/pull/64037), [#65035](https://github.com/pytorch/pytorch/pull/65035))\r\n    * Added `lr_scheduler.{ConstantLR,LinearLR}` ([#64395](https://github.com/pytorch/pytorch/pull/64395))\r\n* `torch.cpu.amp.autocast`: enable new API for CPU autocast ([#57386](https://github.com/pytorch/pytorch/pull/57386), [#63534](https://github.com/pytorch/pytorch/pull/63534))\r\n* Added `BFloat16` support for `torch.{cross, tril, triu, tril_indices, triu_indices, cumsum, cummax, cummin, median, kthvalue, nansum, nextafter, range, sinh, cosh, frexp, nan_to_num, sigmoid, sigmoid_backward, tanh_backward, addcmul, addcdiv, bucketize, bernoulli, dropout, fold, unfold, MaxPool2D, AdaptiveAvgPool2D, topk}` on CPU ([#62454](https://github.com/pytorch/pytorch/pull/62454), [#63307](https://github.com/pytorch/pytorch/pull/63307), [#55210](https://github.com/pytorch/pytorch/pull/55210), [#60074](https://github.com/pytorch/pytorch/pull/60074), [#61083](https://github.com/pytorch/pytorch/pull/61083), [#61829](https://github.com/pytorch/pytorch/pull/61829), [#55221](https://github.com/pytorch/pytorch/pull/55221),  [#61826](https://github.com/pytorch/pytorch/pull/61826), [#55588](https://github.com/pytorch/pytorch/pull/55588), [#56372](https://github.com/pytorch/pytorch/pull/56372), [#62880](https://github.com/pytorch/pytorch/pull/62880), [#55202](https://github.com/pytorch/pytorch/pull/55202), [#59547](https://github.com/pytorch/pytorch/pull/59547))\r\n* Added `BFloat16` support for  `torch.{ceil, floor, frac, round, trunc, sort, topk, aminmax, cumsum, logcumsumexp, cumprod, cummin, cummax}` on CUDA ([#57910](https://github.com/pytorch/pytorch/pull/57910), [#58196](https://github.com/pytorch/pytorch/pull/58196), [#59977](https://github.com/pytorch/pytorch/pull/59977), [#62767](https://github.com/pytorch/pytorch/pull/62767), [#57904](https://github.com/pytorch/pytorch/pull/57904)).\r\n* Added  `torch.cuda.is_bf16_supported` ([#63798](https://github.com/pytorch/pytorch/pull/63798))\r\n* Added zero rate to Poisson distribution ([#61511](https://github.com/pytorch/pytorch/pull/61511))\r\n* Added `torch.segment_reduce` ([#59951](https://github.com/pytorch/pytorch/pull/59951), [#60018](https://github.com/pytorch/pytorch/pull/60018), [#61141](https://github.com/pytorch/pytorch/pull/61141), [#61266](https://github.com/pytorch/pytorch/pull/61266), [#59521](https://github.com/pytorch/pytorch/pull/59521), [#60379](https://github.com/pytorch/pytorch/pull/60379), [#60379](https://github.com/pytorch/pytorch/pull/60379))\r\n* Added boolean support to `torch.isclose` ([#61271](https://github.com/pytorch/pytorch/pull/61271))\r\n* Added `torch.trapezoid` ([#61475](https://github.com/pytorch/pytorch/pull/61475)).\r\n* Added `torch.gradient` support for second order central differences (edge_order=2) ([#58165](https://github.com/pytorch/pytorch/pull/58165))\r\n* `torch.sigmoid`: CUDA support and complex autograd support ([#48647](https://github.com/pytorch/pytorch/pull/48647))\r\n* Added channels-last support for `torch.bilinear` and `torch.nn,MaxUnpool2d` ([#56322](https://github.com/pytorch/pytorch/pull/56322), [#49984](https://github.com/pytorch/pytorch/pull/49984))\r\n\r\n## Autograd\r\n\r\n* [Experimental] Forward mode AD:\r\n    * *NOTE: In addition to operators listed below, many simple ops are already supported. If you encounter an operator that does not have a forward-mode AD formula implemented, please file an issue. As a workaround, you can use custom `autograd.Function` to implement your own forward-mode-AD-supported operator.*\r\n    * Added forward-mode AD support for custom `autograd.Function` ([#64061](https://github.com/pytorch/pytorch/pull/64061), [#63434](https://github.com/pytorch/pytorch/pull/63434))\r\n    * Added forward-mode AD support for `torch.{acos, add, addbmm, addcdiv, addcmul, addmm, addmv, addr, angle, acosh, asinh, atanh, asin, atan, conj, baddbmm, bmm, cat, ceil, clamp, clamp_min, clamp_max, complex, copy_sign, cos, cosh, cross, cumprod, cumsum, cummax, cummin, deg2rad, div, dot, vdot, exp, exp2, expm1, expand, floor, frac, frexp, gather, hardswish, hstack, hypot, index_add_, index_copy_, index_put_, index_select, kthvalue, lerp, lgamma, digamma, polygamma, log, log10, log1p, log2, logaddexp, logaddexp2, xlogy, masked_fill_, masked_fill_, masked_scatter_, masked_select, max, maximum, fmax, mean, min, mininum, fmin, mm, mode, mul, lu, lu_solve, vstack}` ([#57768](https://github.com/pytorch/pytorch/pull/57768), [#57863](https://github.com/pytorch/pytorch/pull/57863) [#59711](https://github.com/pytorch/pytorch/pull/59711), [#64742](https://github.com/pytorch/pytorch/pull/64742))\r\n    * Added Forward AD support for the following element-wise and linear operators `torch.{mvlgamma, nan_to_num, permute, pow,  reciprocal, remainder, repeat, round, rsqrt, sigmoid, logit, sign, sgn, sin, sinc, sinh, sqrt, squeeze, sub, sum, t, flip, roll, rot90, take, tan, tanh, trace, transpose, tril, triu, trunc, unfold, unsqueeze, view, zero_, hardshrink} `([#59993](https://github.com/pytorch/pytorch/pull/59993))\r\n    * Added Forward AD support for `torch.special.`{`xlog1py, entr}` ([#59711](https://github.com/pytorch/pytorch/pull/59711), [#59993](https://github.com/pytorch/pytorch/pull/59993))\r\n    * Added forward AD support for `torch.linalg.{cholesky, cholesky_ex, eigh, inv, inv_ex, solve}`  ([#62160](https://github.com/pytorch/pytorch/pull/62160), [#64646](https://github.com/pytorch/pytorch/pull/64646), [#62163](https://github.com/pytorch/pytorch/pull/62163), [#62159](https://github.com/pytorch/pytorch/pull/62159))\r\n    * Added forward AD support for `torch.functional.leak_relu` ([#59993](https://github.com/pytorch/pytorch/pull/59993)) \r\n* Added saved tensor hooks to customize packing/unpacking behavior of tensors saved for backward ([#60685](https://github.com/pytorch/pytorch/pull/60685), [#60663](https://github.com/pytorch/pytorch/pull/60663), [#62564](https://github.com/pytorch/pytorch/pull/62564), [#60975](https://github.com/pytorch/pytorch/pull/60975), [#62909](https://github.com/pytorch/pytorch/pull/62909), [#62717](https://github.com/pytorch/pytorch/pull/62717))\r\n* Exposed raw saved tensors for custom `autograd.Function` to use with the saved tensor hooks ([#60551](https://github.com/pytorch/pytorch/pull/60551))\r\n* Added default saved tensor hooks ([#61834](https://github.com/pytorch/pytorch/pull/61834), [#62563](https://github.com/pytorch/pytorch/pull/62563), [#62361](https://github.com/pytorch/pytorch/pull/62361))\r\n* Added context manager using default saved tensor hooks to automatically move saved tensors on CPU and back ([#61928](https://github.com/pytorch/pytorch/pull/61928), [#62410](https://github.com/pytorch/pytorch/pull/62410))\r\n* Added C++ and python bindings for `.is_inference()` method ([#58729](https://github.com/pytorch/pytorch/pull/58729)) \r\n* `torch.lu_solve`: Implement support for backward AD ([#61681](https://github.com/pytorch/pytorch/pull/61681)).\r\n\r\n## torch.nn\r\n\r\n* Added new modules: `nn.{ReflectionPad3d, LazyInstanceNorm*d}` ([#59791](https://github.com/pytorch/pytorch/pull/59791), [#60837](https://github.com/pytorch/pytorch/pull/60837), [#61308](https://github.com/pytorch/pytorch/pull/61308), [#60982](https://github.com/pytorch/pytorch/pull/60982))\r\n* `nn.CrossEntropyLoss`: Added support for class probability targets ([#61044](https://github.com/pytorch/pytorch/pull/61044))\r\n* `nn.CrossEntropyLoss`: Added support for label smoothing ([#63122](https://github.com/pytorch/pytorch/pull/63122))\r\n* `nn.Module`: Added support for arbitrary objects in state_dicts via `get_extra_state()` / `set_extra_state()` ([#62976](https://github.com/pytorch/pytorch/pull/62976))\r\n* `nn.utils.skip_init()`: Added function to skip module parameter / buffer initialization ([#57555](https://github.com/pytorch/pytorch/pull/57555))\r\n\r\n## Profiler\r\n\r\n* Added profiler support for mobile ([#62419](https://github.com/pytorch/pytorch/pull/62419), [#62418](https://github.com/pytorch/pytorch/pull/62418), [#62417](https://github.com/pytorch/pytorch/pull/62417),[#62228](https://github.com/pytorch/pytorch/pull/62228),[#62191,](https://github.com/pytorch/pytorch/pull/62191)[#61792](https://github.com/pytorch/pytorch/pull/61792))\r\n* Ported Nvtx support to new profiler ([#61634](https://github.com/pytorch/pytorch/pull/61634))\r\n* Added Tensor core usage stats and recommendations in Tensorboard ([`#364`](https://github.com/pytorch/kineto/pull/364)[,](https://github.com/pytorch/kineto/pull/402/commits/e435a8f55fdbf2a2331931782404b9020eefa4ba)[`#368`](https://github.com/pytorch/kineto/pull/368)[,](https://github.com/pytorch/kineto/pull/402/commits/d3132ebc51faed586e6699e895fecc6b4d255334)[`#383`](https://github.com/pytorch/kineto/pull/383), [`#422`](https://github.com/pytorch/kineto/pull/422))\r\n\r\n## CUDA\r\n\r\n* Allow enabling warnings on CUDA synchronization ([#62092](https://github.com/pytorch/pytorch/pull/62092))\r\n* Added CUDA graph Prototype API and documentation ([#63269](https://github.com/pytorch/pytorch/pull/63269))\r\n* Make stream semantics of backward calls consistent with other cuda ops ([#57833](https://github.com/pytorch/pytorch/pull/57833), [#60230](https://github.com/pytorch/pytorch/pull/60230), [#60127](https://github.com/pytorch/pytorch/pull/60127))\r\n* Enabled autocast support for user-specified device and dtype ([#61002](https://github.com/pytorch/pytorch/pull/61002), [#63416](https://github.com/pytorch/pytorch/pull/63416))\r\n\r\n## C++ API\r\n\r\n* Added C++ API for meta functions. They are available in the `at::meta::` namespace ([#58570](https://github.com/pytorch/pytorch/pull/58570))\r\n* Exposed interface to set grain size on `cpu_kernel`, `cpu_kernel_vec` and `cpu_kernel_multiple_outputs` ([#58949](https://github.com/pytorch/pytorch/pull/58949))\r\n* Added `at::native::resize_bytes_cpu` to resize `Storage` in ATen ([#60324](https://github.com/pytorch/pytorch/pull/60324))\r\n* Added `transpose` to PackedTensorAccessor ([#61114](https://github.com/pytorch/pytorch/pull/61114))\r\n* Added `torch::linalg::qr` as the C++ API ([#60529](https://github.com/pytorch/pytorch/pull/60529))\r\n* Exposed `amin` and `amax` to aten symbols ([#61550](https://github.com/pytorch/pytorch/pull/61550))\r\n* Added support to invoke callable activation function for Transformer modules ([#62342](https://github.com/pytorch/pytorch/pull/62342))\r\n* Added support for `c10::optional` to compare with different but comparable types ([#62890](https://github.com/pytorch/pytorch/pull/62890))\r\n* Added a unified API `c10::util::check_env` to check environment variable ([#59052](https://github.com/pytorch/pytorch/pull/59052))\r\n\r\n## TorchScript\r\n\r\n* Added reference semantics to TorchScript classes ([#44324](https://github.com/pytorch/pytorch/pull/44324)) \r\n* Conservatively moved all suitable prim ops from full-jit to mobile, and make them selective. ([#58353](https://github.com/pytorch/pytorch/pull/58353)) \r\n* Added change to predicate uses of RPC APIs on `torch.distributed.rpc.is_available()` ([#58887](https://github.com/pytorch/pytorch/pull/58887)) \r\n* Added a phase to perform inplace<->functional conversion for activation operators ([#57477](https://github.com/pytorch/pytorch/pull/57477)) \r\n* Enabled Profile-Directed Typing in `torch.jit.script` ([#62420](https://github.com/pytorch/pytorch/pull/62420)) \r\n* Introduced enhancement for smart serialization for operator schemas with out arg ([#63096](https://github.com/pytorch/pytorch/pull/63096))\r\n* Added a pass to transform better handle concatenation ops ([#59881](https://github.com/pytorch/pytorch/pull/59881)) \r\n* Added a new operator for concat that takes in variadic parameters ([#59880](https://github.com/pytorch/pytorch/pull/59880)) \r\n* Added support for union in TorchScript ([#64234](https://github.com/pytorch/pytorch/pull/64234)) \r\n\r\n## torch.package\r\n\r\n* Added basic tooling to enable users to see what is inside of a PackageExporter ([#61147](https://github.com/pytorch/pytorch/pull/61147))\r\n* Added hasattr to `torch::deploy` C++ API ([#62669](https://github.com/pytorch/pytorch/pull/62669))\r\n* Added support to re-save a PackageImporter module ([#65101](https://github.com/pytorch/pytorch/pull/65101))\r\n* Added support to make frozen symbol name customizable in `torch::deploy`. ([#63817](https://github.com/pytorch/pytorch/pull/63817))\r\n\r\n## Mobile\r\n\r\n* Enabled kineto profiler on mobile via EdgeKinetoProfiler ([#62419](https://github.com/pytorch/pytorch/pull/62419))\r\n* Added support of loading lite interpreter module from assets in Android ([#61609](https://github.com/pytorch/pytorch/pull/61609))\r\n* Enabled tracing based selective build ([#63421,](https://github.com/pytorch/pytorch/pull/63421) [#64087](https://github.com/pytorch/pytorch/pull/64087), [#66237,](https://github.com/pytorch/pytorch/pull/66237) [#66395](https://github.com/pytorch/pytorch/pull/66395))\r\n    * built tracer in OSS  ([#64087](https://github.com/pytorch/pytorch/pull/64087))\r\n    * used operator.yaml to build libtorch library ([#66237)](https://github.com/pytorch/pytorch/pull/66237)\r\n    * Built tracer and enabled tracing-based build with tracer output  ([#66395](https://github.com/pytorch/pytorch/pull/66395))\r\n* NNAPI\r\n    * Android NNAPI delegate implementation of runtime initialization (compilation) and execution ([#62272](https://github.com/pytorch/pytorch/pull/62272))\r\n    * Added `aten::{avgpool2d,softmax,to,div,flatten,detach,slice,log_softmax,conv2d_transpose}` to NNAPI converter ([#58538](https://github.com/pytorch/pytorch/pull/58538), [#58539](https://github.com/pytorch/pytorch/pull/58539), [#58540](https://github.com/pytorch/pytorch/pull/58540), [#58541](https://github.com/pytorch/pytorch/pull/58541), [#60885](https://github.com/pytorch/pytorch/pull/60885), [#58543](https://github.com/pytorch/pytorch/pull/58543), [#59364](https://github.com/pytorch/pytorch/pull/59364), [#61378](https://github.com/pytorch/pytorch/pull/61378), [#59529](https://github.com/pytorch/pytorch/pull/59529)\r\n    * Added Int32 support for NNAPI ([#59365](https://github.com/pytorch/pytorch/pull/59365))\r\n    * Made nnapi `aten::{conv2d,linear,cat,flatten}` converter accept flexible batch ([#61021](https://github.com/pytorch/pytorch/pull/61021), [#61022](https://github.com/pytorch/pytorch/pull/61022), [76c0f223d3](https://github.com/pytorch/pytorch/commit/76c0f223d3), [#61024](https://github.com/pytorch/pytorch/pull/61024))\r\n    * Added option to specify custom NNAPI serializer ([#61025](https://github.com/pytorch/pytorch/pull/61025))\r\n    * Made Android NNAPI preprocess to accept both single Tensor inputs and Tensor List inputs ([#61752](https://github.com/pytorch/pytorch/pull/61752))\r\n    * Added a few improvements in NNAPI delegation ([#63489](https://github.com/pytorch/pytorch/pull/63489))\r\n    * Added support const values in binary ops ([2d58f3f56d](https://github.com/pytorch/pytorch/commit/2d58f3f56d))\r\n* Added unary/binary ops necessary and more shape functions for mobilenet ([#56828](https://github.com/pytorch/pytorch/pull/56828), [#58932](https://github.com/pytorch/pytorch/pull/58932))\r\n* Added `aten::{hardswish,tanh,clamp}` for iOS Metal ([#64588](https://github.com/pytorch/pytorch/pull/64588), [#61383](https://github.com/pytorch/pytorch/pull/61383))\r\n* Added CoreML support ([#64521](https://github.com/pytorch/pytorch/pull/64521), [#64522](https://github.com/pytorch/pytorch/pull/64522), [#64523](https://github.com/pytorch/pytorch/pull/64523))\r\n* Added compatibility API ([#61477](https://github.com/pytorch/pytorch/pull/61477), [#57501](https://github.com/pytorch/pytorch/pull/57501))\r\n* Added support operators with default argument in front of out argument ([#63651](https://github.com/pytorch/pytorch/pull/63651), [#63540](https://github.com/pytorch/pytorch/pull/63540))\r\n\r\n## Distributed\r\n\r\n`DistributedDataParallel`\r\n\r\n* Local SGD and variants for DDP communication optimization ([#60303](https://github.com/pytorch/pytorch/pull/60303), [#60320](https://github.com/pytorch/pytorch/pull/60320), [#60632](https://github.com/pytorch/pytorch/pull/60632), [#60891](https://github.com/pytorch/pytorch/pull/60891), [#61206](https://github.com/pytorch/pytorch/pull/61206), [#61207](https://github.com/pytorch/pytorch/pull/61207), [#62105](https://github.com/pytorch/pytorch/pull/62105), [#62111](https://github.com/pytorch/pytorch/pull/62111), [#62131](https://github.com/pytorch/pytorch/pull/62131), [#62132](https://github.com/pytorch/pytorch/pull/62132), [#62392](https://github.com/pytorch/pytorch/pull/62392), [#63277](https://github.com/pytorch/pytorch/pull/63277), [#63340](https://github.com/pytorch/pytorch/pull/63340), [#64885](https://github.com/pytorch/pytorch/pull/64885), [#65197](https://github.com/pytorch/pytorch/pull/65197))\r\n* Provided a noop hook for performance debugging ([#64344](https://github.com/pytorch/pytorch/pull/64344), [#64352](https://github.com/pytorch/pytorch/pull/64352))\r\n* Implemented BF16 allreduce gradient communication hook ([#63260](https://github.com/pytorch/pytorch/pull/63260))\r\n* Allowed retrieval of model parameters in communication hook ([#61637](https://github.com/pytorch/pytorch/pull/61637))\r\n\r\n`torch.distributed`\r\n\r\n* Added a function to create new subgroups of a given size ([#59111](https://github.com/pytorch/pytorch/pull/59111))\r\n* Introduced a new torchrun entry point for elastic ([#64049](https://github.com/pytorch/pytorch/pull/64049))\r\n\r\n## torch.fx\r\n\r\n* Added APIs to mutate specific args/kwargs ([#58571](https://github.com/pytorch/pytorch/pull/58571))\r\n* Introduced EngineHolder for serializing and running TRT Engines with PyTorch ([06399d441d](https://github.com/pytorch/pytorch/commit/06399d441d))\r\n* Introduced `__fx_create_arg__` dunder method for controlling custom classes are handled as node args ([#61780](https://github.com/pytorch/pytorch/pull/61780))\r\n* Added `autowrap_functions` kwarg to Tracer ([#62106](https://github.com/pytorch/pytorch/pull/62106))\r\n* Gradual typing\r\n    * Added type annotation field to nodes ([#60621](https://github.com/pytorch/pytorch/pull/60621))\r\n    * Added experimental gradual typechecker ([#60805](https://github.com/pytorch/pytorch/pull/60805))\r\n    * Extended all experimental type-checking operations to support `conv2d`, `BatchNorm2D`,  `ReLU`, `maxpool2D`, `AdaptiveAvgPooling2D`, `flatten` ([#61093](https://github.com/pytorch/pytorch/pull/61093), [#61012](https://github.com/pytorch/pytorch/pull/61012), [#61150](https://github.com/pytorch/pytorch/pull/61150), [#61188](https://github.com/pytorch/pytorch/pull/61188), [#61239](https://github.com/pytorch/pytorch/pull/61239), [#61265](https://github.com/pytorch/pytorch/pull/61265))\r\n    * Added experimental refinement types and unification for symbolic shape inference ([#61776](https://github.com/pytorch/pytorch/pull/61776))\r\n    * Changed output node handling for typechecker to deal with tuples ([#62582](https://github.com/pytorch/pytorch/pull/62582))\r\n    * Added handle of `get_attr` operations in typechecker ([#62682](https://github.com/pytorch/pytorch/pull/62682))\r\n    * Added equality constraints for some acc operations for symbolic inference ([#63689](https://github.com/pytorch/pytorch/pull/63689))\r\n    * Added inference for algebraic expressions ([#63822](https://github.com/pytorch/pytorch/pull/63822))\r\n* Provided function interface for `remove_duplicate_output_args` ([#65134](https://github.com/pytorch/pytorch/pull/65134))\r\n* Introduced helper function to generate an unique name for an attr in a module ([#64970](https://github.com/pytorch/pytorch/pull/64970))\r\n\r\n## ONNX\r\n\r\n* Added support for ONNX op set 14 ([#59486](https://github.com/pytorch/pytorch/pull/59486))\r\n* Added support for GRU RNNs with packed input in scripting mode ([#58691](https://github.com/pytorch/pytorch/pull/58691))\r\n* Enhanced shape inference ([#64585](https://github.com/pytorch/pytorch/pull/64585))\r\n* Added support for `torch.{linspace, new_ones, nn.LSTMCell, bernoulli, dot, nn.utils.spectral_norm,bernoulli, distributions.normal.Normal, roll}` ([#58854](https://github.com/pytorch/pytorch/pull/58854), [#59255](https://github.com/pytorch/pytorch/pull/59255), [#62757](https://github.com/pytorch/pytorch/pull/62757), [#62765](https://github.com/pytorch/pytorch/pull/62765), [#59536,](https://github.com/pytorch/pytorch/pull/59536)[#61560,](https://github.com/pytorch/pytorch/pull/61560)[#58697](https://github.com/pytorch/pytorch/pull/58697))\r\n\r\n## Infra (Releng)\r\n\r\n* Default Linux/Windows testing workflows were migrated to GitHub Actions. PyTorch Probot has been extended to support new set of rerun command with new set of labels that one can use to opt in and opt out of certain types of CI. More information can be found on [Continuous Integration](https://github.com/pytorch/pytorch/wiki/Continuous-Integration#user-guide) wiki page\r\n* Overall statistics and health of PyTorch CI/CD system can be viewed at [https://metrics.pytorch.org](https://metrics.pytorch.org/) ([#65157](https://github.com/pytorch/pytorch/pull/65157), [#61389](https://github.com/pytorch/pytorch/pull/61389), [#62217](https://github.com/pytorch/pytorch/pull/62217), [#64948](https://github.com/pytorch/pytorch/pull/64948), [#60026](https://github.com/pytorch/pytorch/pull/60026), [#61071](https://github.com/pytorch/pytorch/pull/61071), [#64303](https://github.com/pytorch/pytorch/pull/64303))\r\n* Improved mechanism for disabling tests via issues. Creating an issue which title begins with \u201cDISABLED\u201d followed by the test name will disable the test in question for all platforms, which could be refined by explicitly specifying list of platforms in the issue body. Comment from @pytorch-probot would indicate that issue format was recognized by the CI system and test is now disabled. Closing the issue re-enabled the specified test in CI. Disabled tests will be temporarily re-enabled while running CI for PR marked as fixing it ([#61427](https://github.com/pytorch/pytorch/pull/61427))\r\n* New documentation preview and new artifacts frontend. Using [https://hud.pytorch.org](https://hud.pytorch.org/), one can get an overview of PR/commit CI status, download build artifacts as well as read documentation associated with this build. See [Using HUD](https://github.com/pytorch/pytorch/wiki/Using-hud.pytorch.org) wiki page for more information ([#60711](https://github.com/pytorch/pytorch/pull/60711),  [#60792](https://github.com/pytorch/pytorch/pull/60792), [#60893](https://github.com/pytorch/pytorch/pull/60893))\r\n\r\n## Misc\r\n\r\n* Added support for `torch.fft.` operators on ARM-based platforms using pocket FFT ([#60976](https://github.com/pytorch/pytorch/pull/60976), [#62222](https://github.com/pytorch/pytorch/pull/62222), [#63714](https://github.com/pytorch/pytorch/pull/63714))\r\n* `torch.einsum`: added support for the \u201csublist\u201d format ([#56625](https://github.com/pytorch/pytorch/pull/56625))\r\n* `torch.linalg.det`: added support for complex autograd ([#58195](https://github.com/pytorch/pytorch/pull/58195))\r\n* Added autograd support for `Tensor.to_sparse` ([#58413](https://github.com/pytorch/pytorch/pull/58413))\r\n* Added more CUDA support for CSR layout: constructors ([#59010](https://github.com/pytorch/pytorch/pull/59010)), sparse_to_dense/add_sparse_csr ([#59011](https://github.com/pytorch/pytorch/pull/59011)), addmm/matvec ([#59012](https://github.com/pytorch/pytorch/pull/59012))\r\n* Vulkan: Added support for `max_pool2d`, `tanh`, `hardshrink`, `log_softmax`, `leaky_relu`, `softmax` ([#58806](https://github.com/pytorch/pytorch/pull/58806), [#60695](https://github.com/pytorch/pytorch/pull/60695), [#62870](https://github.com/pytorch/pytorch/pull/62870), [#63193](https://github.com/pytorch/pytorch/pull/63193), [#62239](https://github.com/pytorch/pytorch/pull/62239))\r\n* Enabled local run of clang-tidy and clang-format lint workflows ([#61121](https://github.com/pytorch/pytorch/pull/61121), [#61797](https://github.com/pytorch/pytorch/pull/61797), [#60745](https://github.com/pytorch/pytorch/pull/60745))\r\n\r\n# Improvements\r\n\r\n## Python API\r\n\r\n* Added clearer stack trace for `torch.floor_divide` deprecation warning ([#64034](https://github.com/pytorch/pytorch/pull/64034))\r\n* Use cascade-summation algorithm to improve `torch.nansum` accuracy ([#61082](https://github.com/pytorch/pytorch/pull/61082))\r\n* `torch.i0`: now promote integer inputs to float ([#52735](https://github.com/pytorch/pytorch/pull/52735))\r\n*  `torch.kthvalue:` added change to adjust output dim size for numpy compatibility ([#59214](https://github.com/pytorch/pytorch/pull/59214))\r\n* Added reduce variants for `torch.scatter` operation. ([#57015](https://github.com/pytorch/pytorch/pull/57015))\r\n* Added support for quantized tensors in `torch.testing.assert_close` ([#58926](https://github.com/pytorch/pytorch/pull/58926))\r\n* Improved error message for invalid value input to Distribution methods ([#61056](https://github.com/pytorch/pytorch/pull/61056))\r\n* `torch.isclose` upcast to most precise dtype within their category before the comparison ([#60536](https://github.com/pytorch/pytorch/pull/60536))\r\n* Added change to cast `alpha` to `acc_type` for `torch.add` and `torch.sub` ([#60227](https://github.com/pytorch/pytorch/pull/60227))\r\n* Fixed dimension in the error message for CUDA `torch.cat` shape check and removed unnecessary offending index information ([#64556](https://github.com/pytorch/pytorch/pull/64556)).\r\n* Improved DLPack support ([#57110](https://github.com/pytorch/pytorch/pull/57110)).\r\n* Added change to raise an error when empty index tensor is passed to `torch.gather` ([#65006](https://github.com/pytorch/pytorch/pull/65006)).\r\n* Added change to store `float64` in `tensorboard` instead of `float32` ([#59435](https://github.com/pytorch/pytorch/pull/59435)).\r\n* Added `use_strict_trace` to tensorboard `add_graph` method ([#63120](https://github.com/pytorch/pytorch/pull/63120)).\r\n* Add option to skip GH validation for `torch.hub` ([#62139](https://github.com/pytorch/pytorch/pull/62139))\r\n* Added a new kwarg `output_size` to `tensor.repeat_interleave`([#58881](https://github.com/pytorch/pytorch/pull/58881))\r\n* Add support for `torch.isclose` ([#63571](https://github.com/pytorch/pytorch/pull/63571))\r\n* Make the behavior of `torch.{testting.assert_close,is_close}` consistent with numpy ([#63841](https://github.com/pytorch/pytorch/pull/63841))\r\n\r\n## Autograd\r\n\r\n* Added warning about memory leak when `.backward()` is called with `create_graph=True` ([#59412](https://github.com/pytorch/pytorch/pull/59412))\r\n* Added warning when accessing `Tensor::grad()` on a non-leaf Tensor in the C++ API ([#59362](https://github.com/pytorch/pytorch/pull/59362))\r\n* Fixed error message formatting in `grad_output` creation for `.backward()` and `autograd.grad()` ([#59532](https://github.com/pytorch/pytorch/pull/59532))\r\n* Added change to raise `NotImplementedError` for forward and backward-mode AD formulas that are not implemented ([#59482](https://github.com/pytorch/pytorch/pull/59482), [#59483](https://github.com/pytorch/pytorch/pull/59483))\r\n* Reduced memory usage for `torch.relu` for common use cases ([#63089](https://github.com/pytorch/pytorch/pull/63089))\r\n* Added support for non-leaf inputs for `autograd.backward()` function `inputs` argument ([#60521](https://github.com/pytorch/pytorch/pull/60521))\r\n* Improved error message when a tensor with `requires_grad=True`  is passed to a non-differentiable function ([#60610](https://github.com/pytorch/pytorch/pull/60610))\r\n* Made `binary_cross_entropy` differentiable w.r.t. `target` ([#59447](https://github.com/pytorch/pytorch/pull/59447))\r\n\r\n## torch.nn\r\n\r\n* Added support for inputs with no batch dimensions for `nn.{AdaptiveAvgPool*d, AdaptiveMaxPool*d, AvgPool*d, CosineEmbeddingLoss, Dropout, FractionalMaxPool2d, Linear, LPPool1d, MaxPool*d, MaxUnpool*d, NLLLoss, PairwiseDistance, ReflectionPad*d, ReplicationPad*d, TripletMarginLoss, ZeroPad*d}`, most other loss modules, and all activation modules ([#61264](https://github.com/pytorch/pytorch/pull/61264), [#61847](https://github.com/pytorch/pytorch/pull/61847), [#61860](https://github.com/pytorch/pytorch/pull/61860), [#64590](https://github.com/pytorch/pytorch/pull/64590), [#61911](https://github.com/pytorch/pytorch/pull/61911), [#62490](https://github.com/pytorch/pytorch/pull/62490), [#60992](https://github.com/pytorch/pytorch/pull/60992), [#62190](https://github.com/pytorch/pytorch/pull/62190), [#62206](https://github.com/pytorch/pytorch/pull/62206), [#61984](https://github.com/pytorch/pytorch/pull/61984), [#61310](https://github.com/pytorch/pytorch/pull/61310), [#62651](https://github.com/pytorch/pytorch/pull/62651), [#64882](https://github.com/pytorch/pytorch/pull/64882), [#62183](https://github.com/pytorch/pytorch/pull/62183), [#61060](https://github.com/pytorch/pytorch/pull/61060), [#61262](https://github.com/pytorch/pytorch/pull/61262), [#62729](https://github.com/pytorch/pytorch/pull/62729), [#61300](https://github.com/pytorch/pytorch/pull/61300), [#61461](https://github.com/pytorch/pytorch/pull/61461), [#62726](https://github.com/pytorch/pytorch/pull/62726))\r\n* Added support for inputs with 0 batch size for `nn.{AdaptiveAvgPool*d, AdaptiveMaxPool*d, Bilinear, FractionalMaxPool*d, LocalResponseNorm, MaxPool*d, MaxUnpool*d, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer}` ([#62025](https://github.com/pytorch/pytorch/pull/62025), [#62088](https://github.com/pytorch/pytorch/pull/62088), [#47106](https://github.com/pytorch/pytorch/pull/47106), [#62083](https://github.com/pytorch/pytorch/pull/62083), [#62801](https://github.com/pytorch/pytorch/pull/62801), [#64082](https://github.com/pytorch/pytorch/pull/64082), [#62800](https://github.com/pytorch/pytorch/pull/62800))\r\n* Parametrization: Added support for nested parametrizations, parametrizations depending on several inputs, resizing of parametrized tensors, and the orthogonal parametrization ([#65167](https://github.com/pytorch/pytorch/pull/65167), [#60530](https://github.com/pytorch/pytorch/pull/60530), [#60418](https://github.com/pytorch/pytorch/pull/60418), [#62089](https://github.com/pytorch/pytorch/pull/62089))\r\n* `nn.AvgPool2d`: Added `channels_last` support on CPU ([#58725](https://github.com/pytorch/pytorch/pull/58725))\r\n* `nn.BatchNorm`: Use `resize_output` and `empty` instead of `empty_like` to improve flexibility in output memory format choice ([#63084](https://github.com/pytorch/pytorch/pull/63084))\r\n* `nn.Bilinear`: Added support for non-contiguous tensor inputs ([#38409](https://github.com/pytorch/pytorch/pull/38409))\r\n* `nn.GELU`: Added support for fp32/bfloat16 in CPU path using mkldnn implementation ([#58525](https://github.com/pytorch/pytorch/pull/58525))\r\n* `nn.GroupNorm`: Improved numerical stability by using the Welford algorithm and cascade summation ([#54921](https://github.com/pytorch/pytorch/pull/54921))\r\n* `nn.LayerNorm`: Improved numerical stability by using the Welford algorithm and pairwise sums ([#59987](https://github.com/pytorch/pytorch/pull/59987))\r\n* `nn.NLLLoss`: Added support for target of dtype `byte` ([#60308](https://github.com/pytorch/pytorch/pull/60308), [#60650](https://github.com/pytorch/pytorch/pull/60650))\r\n* `nn.SmoothL1Loss`: Added support for integral target within the backward pass ([#61112](https://github.com/pytorch/pytorch/pull/61112))\r\n* `nn.Transformer`: Added configurable pre/post LayerNorm placement ([#60593](https://github.com/pytorch/pytorch/pull/60593), [#61692](https://github.com/pytorch/pytorch/pull/61692))\r\n* Added check to verify non-zero sequence length for `nn.{RNN, LSTM, GRU}` ([#60269](https://github.com/pytorch/pytorch/pull/60269))\r\n* Added support for bfloat16 in CPU path to `nn.{LeakyReLU, RReLU}` ([#61514](https://github.com/pytorch/pytorch/pull/61514))\r\n* Added support for `channels_last` memory format in `nn.{AdaptiveMaxPool2d, GroupNorm}` ([#48920](https://github.com/pytorch/pytorch/pull/48920), [#49821](https://github.com/pytorch/pytorch/pull/49821))\r\n* Added callable activation function support to `nn.{MultiheadAttention, Transformer, TransformerDecoderLayer, TransformerEncoderLayer}` ([#61355](https://github.com/pytorch/pytorch/pull/61355), [#62342](https://github.com/pytorch/pytorch/pull/62342))\r\n\r\n## Profiler\r\n\r\n* Changed `profiler.profile` argument `with_flops`  when set to `True` to report total FLOPs rather than FLOP/s, and support more operators ([#62779](https://github.com/pytorch/pytorch/pull/62779), [#61895](https://github.com/pytorch/pytorch/pull/61895))\r\n* Improved memory profiling and Tensorboard memory view, enabling better understanding of memory usage by showing active memory allocations at various points of your program run as well as a memory usage trend chart.  ([#61282](https://github.com/pytorch/pytorch/pull/61282), [`#361`](https://github.com/pytorch/kineto/pull/361), [`#404`](https://github.com/pytorch/kineto/pull/404)[,](https://github.com/pytorch/kineto/pull/435/commits/36f069ad8f819255f5b575782e99b0c4573a6d0f)[`#416`](https://github.com/pytorch/kineto/pull/416)[,](https://github.com/pytorch/kineto/pull/435/commits/d6d28b719270b1ceb10fca1003cfb77a11e18c79)[`#421`](https://github.com/pytorch/kineto/pull/421))\r\n* Added flow arrows between ops in the forward pass and the corresponding ops in the backward pass in the trace view ([#62553](https://github.com/pytorch/pytorch/pull/62553), [#372](https://github.com/pytorch/kineto/pull/372))\r\n* Increased profiling coverage of backward pass ([#63619](https://github.com/pytorch/pytorch/pull/63619))\r\n* Made threads and GPU streams appear in a consistent sorted order in the trace view ([#399](https://github.com/pytorch/kineto/pull/399))\r\n* Added shapes and reg usage to the GPU kernel view ([`#351`](https://github.com/pytorch/kineto/pull/351)[)](https://github.com/pytorch/kineto/pull/402/commits/eed895ba7ce521deb457dee4678d7a6c8a4a7bd6)\r\n\r\n## Dataloader\r\n\r\n* Properly delegated indices called by `Subset` to dataset ([#59513](https://github.com/pytorch/pytorch/pull/59513))\r\n* Removed the restriction that input datasets in `ConcatDataset` must be `Sized` ([#64114](https://github.com/pytorch/pytorch/pull/64114))\r\n* Allowed annotation of `IterableDataset` to accept keyword-only arguments and `abc` class ([#58450](https://github.com/pytorch/pytorch/pull/58450))\r\n* Changed annotation of `DataLoader` to accept non-integer `Sampler` as input([#63500](https://github.com/pytorch/pytorch/pull/63500))\r\n\r\n## CUDA\r\n\r\n* Include function name in the error message for inputs being on different devices ([#58502](https://github.com/pytorch/pytorch/pull/58502))\r\n* Fix MAGMA initialization ([#58521](https://github.com/pytorch/pytorch/pull/58521))\r\n* Updated NCCL to 2.10 ([#62276](https://github.com/pytorch/pytorch/pull/62276))\r\n* Added deterministic path for `torch.scatter_add` for 1D tensors ([#58761](https://github.com/pytorch/pytorch/pull/58761))\r\n* Added CUDA support for mean reduction ([#59543](https://github.com/pytorch/pytorch/pull/59543))\r\n* Add missing CUDA kernel launch check ([#60114](https://github.com/pytorch/pytorch/pull/60114))\r\n* Improved CUDA extension building error/warning messages ([#59665](https://github.com/pytorch/pytorch/pull/59665), [#60592](https://github.com/pytorch/pytorch/pull/60592))\r\n* Added change to compute CUDA reduction buffer size in elements ([#63969](https://github.com/pytorch/pytorch/pull/63969))\r\n\r\n## TorchScript\r\n\r\n* Added change to simplify pass on arithmetic expressions for integers. ([#61444](https://github.com/pytorch/pytorch/pull/61444)) \r\n* Set future's error to current exception as is when `--torch_jit_enable_rethrow_caught_exception=true` ([#63348](https://github.com/pytorch/pytorch/pull/63348)) \r\n* Improved TorchScript module getattr() to be same as python class getattr() method ([#61599](https://github.com/pytorch/pytorch/pull/61599)) \r\n* Improved slicing for scripted version of `torch.nn.ModuleList` to support arbitrary step size ([#58361](https://github.com/pytorch/pytorch/pull/58361)) \r\n* Added parsing logic for `Tuple[()]` annotation ([#58340](https://github.com/pytorch/pytorch/pull/58340)) \r\n* Changed list striding kernel implementation to handle optional integers ([#58536](https://github.com/pytorch/pytorch/pull/58536)) \r\n* Added support for `torch.nn.Parameter` type for Profile-Directed-Typing ([#59249](https://github.com/pytorch/pytorch/pull/59249)) \r\n* Added change to annotate NoneType as Optional[type] ([#60383](https://github.com/pytorch/pytorch/pull/60383)) \r\n* Added support for default values on NamedTuple fields ([#54682](https://github.com/pytorch/pytorch/pull/54682)) \r\n* Improved JIT support for `torch.einsum` ([#59265](https://github.com/pytorch/pytorch/pull/59265)) \r\n* Added change to allow for heterogenous List and Dict values + Improve container typing algorithm ([#57137](https://github.com/pytorch/pytorch/pull/57137)) \r\n* Added support for eager mode use of `torch.jit.isinstance` with multiple types ([#60465](https://github.com/pytorch/pytorch/pull/60465)) \r\n* Allowed uncompiled strings as input to `checkScriptRaisesRegex` ([#63901](https://github.com/pytorch/pytorch/pull/63901))\r\n* Introduced more robust check of whether a class is defined in torch ([#64083](https://github.com/pytorch/pytorch/pull/64083)) \r\n* Added change to preserve types during empty container assignment ([#58911](https://github.com/pytorch/pytorch/pull/58911)) \r\n* Made JIT not assume that the device is CUDA. ([#54238](https://github.com/pytorch/pytorch/pull/54238)) \r\n* Updated `optimize_for_mobile` to preserve nodes\u2019 debug information ([#63106](https://github.com/pytorch/pytorch/pull/63106)) \r\n* Added support for device as Dict key ([#65079](https://github.com/pytorch/pytorch/pull/65079))  \r\n* Added support for Python C extension modules in `torch::deploy` ([#58117](https://github.com/pytorch/pytorch/pull/58117)) \r\n* Added a flag to suppress stacktrace in exception messages([#63073](https://github.com/pytorch/pytorch/pull/63073)) \r\n* Added API to change logging levels for JIT ([#58821](https://github.com/pytorch/pytorch/pull/58821)) \r\n* Provided API to preserve source range and callstack information during graph rewrite ([#58300](https://github.com/pytorch/pytorch/pull/58300)) \r\n* Re-enabled BatchNorm autodiff  ([#57321](https://github.com/pytorch/pytorch/pull/57321)) \r\n* Extracted element-wise ops supported by JIT fuser into a separate list ([#59579](https://github.com/pytorch/pytorch/pull/59579)) \r\n* Reworked requires_grad on DifferentiableGraphOp ([#57575](https://github.com/pytorch/pytorch/pull/57575)) \r\n\r\n## torch.package\r\n\r\n* Unified three categories of dependency handling error (broken, denied, unhandled) into a single \"error\" field in the node, with optional context ([#58572](https://github.com/pytorch/pytorch/pull/58572))\r\n* Renamed MockZipReader into DirectoryReader ([#59107](https://github.com/pytorch/pytorch/pull/59107))\r\n* Added change to silently skip cases where the __**import__** statement cannot be parsed ([#61148](https://github.com/pytorch/pytorch/pull/61148))\r\n* Make torch::deploy work with or without cuda ([#58493](https://github.com/pytorch/pytorch/pull/58493))\r\n\r\n## Mobile\r\n\r\n* Added check to ensure op name does not contain open parenthesis ([#58687](https://github.com/pytorch/pytorch/pull/58687))\r\n* Added handles and symbolicate exception callstack thrown from backend ([#55462](https://github.com/pytorch/pytorch/pull/55462), [#57441](https://github.com/pytorch/pytorch/pull/57441), [#57481](https://github.com/pytorch/pytorch/pull/57481))\r\n* Enabled implicit operator versioning via number of arguments ([#58852](https://github.com/pytorch/pytorch/pull/58852))\r\n* Cleaned up unused APIs and improve debugging experience for iOS GPU ([#60280](https://github.com/pytorch/pytorch/pull/60280), [#60281,](https://github.com/pytorch/pytorch/pull/60281)[#60282](https://github.com/pytorch/pytorch/pull/60282))\r\n* Added debug information to track memory allocation exception for Metal ([#59112](https://github.com/pytorch/pytorch/pull/59112))\r\n* Added print of IValue type name in error message for Android ([#64602](https://github.com/pytorch/pytorch/pull/64602))\r\n* Added print of error message when failing to load model file ([#63404](https://github.com/pytorch/pytorch/pull/63404))\r\n* Introduced multiple improvements in `torch.utils.model_dump` APIs: \r\n    * Make stdout argument for main kwarg-only ([#60699](https://github.com/pytorch/pytorch/pull/60699))\r\n    * Implement \"Hider\" properly ([#57654](https://github.com/pytorch/pytorch/pull/57654))\r\n    * Handle `torch.device` objects ([#57656](https://github.com/pytorch/pytorch/pull/57656))\r\n    * Handle dict rendering ([#57657](https://github.com/pytorch/pytorch/pull/57657))\r\n    * Add a section that summarizes tensor memory usage ([#57658](https://github.com/pytorch/pytorch/pull/57658))\r\n    * Handle invalid UTF-8 in pickles ([#57661](https://github.com/pytorch/pytorch/pull/57661))\r\n\r\n## Quantization\r\n\r\n* Added out variant for int8 `quantized::linear` ([#58282](https://github.com/pytorch/pytorch/pull/58282)) and `quantized::embedding_bag_byte_prepack` ([#64081](https://github.com/pytorch/pytorch/pull/64081))\r\n* FX graph mode quantization: improve `qconfig_dict` argument handling ([#59605](https://github.com/pytorch/pytorch/pull/59605), [#58566](https://github.com/pytorch/pytorch/pull/58566))\r\n* Added support to embedding trained in FP16 ([#60736](https://github.com/pytorch/pytorch/pull/60736))\r\n* Added support for `torch.index_select` on quantized tensors ([#61406](https://github.com/pytorch/pytorch/pull/61406))\r\n* Added a new fused MovingAvg Obs + FakeQuant operator ([#61570](https://github.com/pytorch/pytorch/pull/61570), [#61589](https://github.com/pytorch/pytorch/pull/61589), [#61691](https://github.com/pytorch/pytorch/pull/61691), [#62346](https://github.com/pytorch/pytorch/pull/62346), [#62863](https://github.com/pytorch/pytorch/pull/62863), [#62702](https://github.com/pytorch/pytorch/pull/62702), [#63043](https://github.com/pytorch/pytorch/pull/63043), [#64829](https://github.com/pytorch/pytorch/pull/64829))\r\n* Added support for dynamic linear + relu fusion (INT8) ([#63799](https://github.com/pytorch/pytorch/pull/63799),[#63826](https://github.com/pytorch/pytorch/pull/63826))\r\n* Enabled JIT tracing on quantizable LSTM ([#64438](https://github.com/pytorch/pytorch/pull/64438))\r\n\r\n## Distributed\r\n\r\n`DistributedDataParallel`\r\n\r\n* Added error logging to DDP logging API ([#59281](https://github.com/pytorch/pytorch/pull/59281), [#59284](https://github.com/pytorch/pytorch/pull/59284), [#59351,](https://github.com/pytorch/pytorch/pull/59351)[#65023](https://github.com/pytorch/pytorch/pull/65023))\r\n* Added `NCCL_ASYNC_ERROR_HANDLING` environment variable to control NCCL error handling ([#59109](https://github.com/pytorch/pytorch/pull/59109))\r\n* Communication hook APIs to always return single tensor ([#62074](https://github.com/pytorch/pytorch/pull/62074), [#62389](https://github.com/pytorch/pytorch/pull/62389), [#62457](https://github.com/pytorch/pytorch/pull/62457))\r\n* Added DDP bucket sizes in DDP logging API ([#62229](https://github.com/pytorch/pytorch/pull/62229), [#62232](https://github.com/pytorch/pytorch/pull/62232), [#62231](https://github.com/pytorch/pytorch/pull/62231), [#62625](https://github.com/pytorch/pytorch/pull/62625), \r\n* Improved rebuilding buckets logic  ([#62279](https://github.com/pytorch/pytorch/pull/62279), [#58097](https://github.com/pytorch/pytorch/pull/58097))\r\n* Allowed DDP uneven inputs work with communication hooks ([#61017](https://github.com/pytorch/pytorch/pull/61017), [#61018](https://github.com/pytorch/pytorch/pull/61018), [#61019](https://github.com/pytorch/pytorch/pull/61019), [#61020](https://github.com/pytorch/pytorch/pull/61020))\r\n* Added logging if graph is static at end of training ([#61871](https://github.com/pytorch/pytorch/pull/61871))\r\n* Added logging of unused param names under DETAIL debug mode. ([#62209](https://github.com/pytorch/pytorch/pull/62209))\r\n* Allowed tuning of first bucket in DDP ([#62748](https://github.com/pytorch/pytorch/pull/62748))\r\n* Added gradient ready order, host-side timestamps, and bucket indices to DDP logging ([#62751](https://github.com/pytorch/pytorch/pull/62751), [#62770](https://github.com/pytorch/pytorch/pull/62770))\r\n* Added a debug check in C++ fp16 gradient hook ([#63379](https://github.com/pytorch/pytorch/pull/63379))\r\n* Added a fallback to use `mul` and `copy_` instead of `mul`\u2019s `out=` variant when gradient tensor requires grad in DDP ([#63831](https://github.com/pytorch/pytorch/pull/63831))\r\n* Used `Tensor.set_` instead of directory assigning data in model averaging ([#63895](https://github.com/pytorch/pytorch/pull/63895))\r\n* Added more iterations for DDP logging ([#64071](https://github.com/pytorch/pytorch/pull/64071),  [#64411](https://github.com/pytorch/pytorch/pull/64411))\r\n\r\n`torch.distributed`\r\n\r\n* Introduced ProcessGroup wrapper and use it in debug mode([#58224](https://github.com/pytorch/pytorch/pull/58224), [#58281](https://github.com/pytorch/pytorch/pull/58281), [#60237](https://github.com/pytorch/pytorch/pull/60237))\r\n* Made a small change for `torch.distributed` launcher ([#59152](https://github.com/pytorch/pytorch/pull/59152))\r\n* Added complex number support for all_to_all/scatter ([#61299](https://github.com/pytorch/pytorch/pull/61299))\r\n* Made gloo communication profiling more accurate ([#61342](https://github.com/pytorch/pytorch/pull/61342))\r\n* Used generator instead of list to save memory in scatter ([#62516](https://github.com/pytorch/pytorch/pull/62516))\r\n* Provided failure reason from ProcessGroup when aborting NCCL communicator ([#64241](https://github.com/pytorch/pytorch/pull/64241))\r\n* Introduced error raised when capturing uncapturable NCCL in CUDA graphs. ([#64440](https://github.com/pytorch/pytorch/pull/64440))\r\n* Added Single-Machine Model Parallel Support to `torch.distributed.optim.ZeroRedundancyOptimizer` ([#61370](https://github.com/pytorch/pytorch/pull/61370))\r\n\r\n`torch.distributed.nn.RemoteModule`\r\n\r\n* Supported creating a RemoteModule by RRef ([#59242](https://github.com/pytorch/pytorch/pull/59242))\r\n* Supported switching RemoteModule between train/eval ([#59026](https://github.com/pytorch/pytorch/pull/59026))\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Added minor logging and error formatting improvements ([#63214](https://github.com/pytorch/pytorch/pull/63214),  [#62823](https://github.com/pytorch/pytorch/pull/62823))\r\n* Improved process termination logic ([#61602](https://github.com/pytorch/pytorch/pull/61602))\r\n* Added fqdn hostname to error printout ([#66662](https://github.com/pytorch/pytorch/pull/66662/))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Fix RPC initialization to avoid shutdown timeout ([#59801](https://github.com/pytorch/pytorch/pull/59801))\r\n* Supported RRefs that contain `threading.Locks` ([#57943](https://github.com/pytorch/pytorch/pull/57943)), `torch.cuda.Event` ([#61354](https://github.com/pytorch/pytorch/pull/61354))\r\n* Updated rpc tensorpipe logic for sparse tensors ([#64575](https://github.com/pytorch/pytorch/pull/64575))\r\n* Added rpc sparse tensor fix ([#59609](https://github.com/pytorch/pytorch/pull/59609), [#62794](https://github.com/pytorch/pytorch/pull/62794))\r\n* Added change to ensure that future completion doesn't swallow exception. ([#61094](https://github.com/pytorch/pytorch/pull/61094))\r\n* Set streams when invoking UDFs ([#59210](https://github.com/pytorch/pytorch/pull/59210))\r\n* Set and propagate devices in RRef completion Future ([#59211](https://github.com/pytorch/pytorch/pull/59211))\r\n* Made TensorPipe agent use streams from Future when sending response ([#59212](https://github.com/pytorch/pytorch/pull/59212))\r\n* Added change to leverage TensorPipe's automatic SHM address selection ([#63028](https://github.com/pytorch/pytorch/pull/63028))\r\n* Made Future store Storages instead of references to DataPtrs ([#60470](https://github.com/pytorch/pytorch/pull/60470), [#60943](https://github.com/pytorch/pytorch/pull/60943))\r\n* Added change to avoid re-doing CUDA stream sync in OwnerRRef ([#57355](https://github.com/pytorch/pytorch/pull/57355))\r\n\r\n`torch.distributed.Store`\r\n\r\n* Enhanced connect timeout error message ([#61390](https://github.com/pytorch/pytorch/pull/61390))\r\n* Added minor fixes in c10d for Windows ([#62953](https://github.com/pytorch/pytorch/pull/62953))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* Supported non-tensor inputs in pipeline parallel API ([#55441](https://github.com/pytorch/pytorch/pull/55441), [#57226](https://github.com/pytorch/pytorch/pull/57226), [#57325](https://github.com/pytorch/pytorch/pull/57325))\r\n* Added a `WithDevice` wrapper to specify device execution for a module. ([#65190](https://github.com/pytorch/pytorch/pull/65190))\r\n\r\n## torch.fx\r\n\r\n* Added users of a node to the serialized JSON ([#59357](https://github.com/pytorch/pytorch/pull/59357))\r\n* Added requires_grad to TensorMetadata ([#60972](https://github.com/pytorch/pytorch/pull/60972))\r\n* Added change to swap out Python's AnnAssign with an Assign node where the annotation function is called ([#60622](https://github.com/pytorch/pytorch/pull/60622))\r\n* Added type annotations for the `torch.nn.Module` constructor ([#61334](https://github.com/pytorch/pytorch/pull/61334))\r\n* Enabled `torch.deploy` for GraphModules with non-torch dependencies ([#61680](https://github.com/pytorch/pytorch/pull/61680))\r\n* Added change to allow FX tracer to trace control flow (if/while) statements when parameter shapes are in the conditionals ([#61820](https://github.com/pytorch/pytorch/pull/61820))\r\n* Added `torch.memory_format` as a BaseArgumentType ([#62593](https://github.com/pytorch/pytorch/pull/62593))\r\n* Added backwards compatibility guarantees for 1.10 ([#63888](https://github.com/pytorch/pytorch/pull/63888))\r\n    * Renamed reduce functions back to their old, public names ([#64324](https://github.com/pytorch/pytorch/pull/64324))\r\n    * Added change to ensure BC coverage for all of `torch.fx` passes ([#65081](https://github.com/pytorch/pytorch/pull/65081))\r\n* Add `__matmul__` to the magic methods for FX tracing ([#64512](https://github.com/pytorch/pytorch/pull/64512))\r\n\r\n## Composability\r\n\r\n* Added meta tensor support for `torch.{any, all, fmax, fmin, remainder, glu, argmax, argmin, avg_pool3d_backward, isposinf, isneginf, fmod, fmin, signbit, slow_conv_transpose2d, nll_loss_backward, cumprod, aminmax, addcmul, addcdiv, gather, hardshrink_backward, softshrink_backward, hardshrink, gelu, gelu_backward, avg_pool2d, avg_pool2d_backward, avg_pool3d, reflection_pad1d_backward, all, any, silu_backward, sgn, softplus, leaky_relu_backward, hardsigmoid_backward, elu_backward, eq, xlogy, ne, lt, gt, le, ge, sigmoid_backward, tanh_backward, logit_backward, bitwise_or, bitwise_xor, bitwise_and, nll_loss_forward, log_softmax, log_softmax_backward_data, prod, norm, sum.dim_IntList, clamp}` ([#64642](https://github.com/pytorch/pytorch/pull/64642), [#58458,](https://github.com/pytorch/pytorch/pull/58458)[#58732](https://github.com/pytorch/pytorch/pull/58732), [#61800](https://github.com/pytorch/pytorch/pull/61800), [#60363](https://github.com/pytorch/pytorch/pull/60363), [#60364](https://github.com/pytorch/pytorch/pull/60364), [#59084](https://github.com/pytorch/pytorch/pull/59084), [#60633](https://github.com/pytorch/pytorch/pull/60633), [#60809](https://github.com/pytorch/pytorch/pull/60809), [#60810](https://github.com/pytorch/pytorch/pull/60810), [#57936](https://github.com/pytorch/pytorch/pull/57936), [#55503](https://github.com/pytorch/pytorch/pull/55503), [#62144](https://github.com/pytorch/pytorch/pull/62144), [#61899](https://github.com/pytorch/pytorch/pull/61899), [#62401](https://github.com/pytorch/pytorch/pull/62401), [#62318](https://github.com/pytorch/pytorch/pull/62318), [#62319](https://github.com/pytorch/pytorch/pull/62319), [#63312](https://github.com/pytorch/pytorch/pull/63312), [#58662](https://github.com/pytorch/pytorch/pull/58662), [#58663](https://github.com/pytorch/pytorch/pull/58663), [#58664](https://github.com/pytorch/pytorch/pull/58664), [#58665](https://github.com/pytorch/pytorch/pull/58665), [#58987](https://github.com/pytorch/pytorch/pull/58987), [#59082](https://github.com/pytorch/pytorch/pull/59082), [#59083](https://github.com/pytorch/pytorch/pull/59083), [#59103](https://github.com/pytorch/pytorch/pull/59103), [#60360](https://github.com/pytorch/pytorch/pull/60360), [#60361](https://github.com/pytorch/pytorch/pull/60361), [#58661](https://github.com/pytorch/pytorch/pull/58661), [#58197](https://github.com/pytorch/pytorch/pull/58197), [#58482](https://github.com/pytorch/pytorch/pull/58482), [#58483](https://github.com/pytorch/pytorch/pull/58483), [#58484](https://github.com/pytorch/pytorch/pull/58484), [#58660](https://github.com/pytorch/pytorch/pull/58660), [#60177](https://github.com/pytorch/pytorch/pull/60177), [#60814](https://github.com/pytorch/pytorch/pull/60814), [#60942](https://github.com/pytorch/pytorch/pull/60942), [#60815](https://github.com/pytorch/pytorch/pull/60815), [#60816](https://github.com/pytorch/pytorch/pull/60816), [#60817](https://github.com/pytorch/pytorch/pull/60817), [#60811](https://github.com/pytorch/pytorch/pull/60811), [#60812](https://github.com/pytorch/pytorch/pull/60812), [#60813](https://github.com/pytorch/pytorch/pull/60813), [#61443](https://github.com/pytorch/pytorch/pull/61443), [#57374](https://github.com/pytorch/pytorch/pull/57374), [#62372](https://github.com/pytorch/pytorch/pull/62372), [#62024](https://github.com/pytorch/pytorch/pull/62024), [#62711](https://github.com/pytorch/pytorch/pull/62711), [#61642](https://github.com/pytorch/pytorch/pull/61642), [#61361](https://github.com/pytorch/pytorch/pull/61361))\r\n* PyObject preservation: Previously, tensors in python that no longer had any python-side references (but still had references in C++, e.g. if it\u2019s saved for autograd) would get deallocated, and we would create a new Python object to replace it next time it passes from C++ to Python. We now preserve the PyObject as long as there are any references on either the python or C++ side. This ensures that any metadata on the original python object is preserved. For example, tensor subclasses that were saved for autograd now get properly preserved. ([#56017](https://github.com/pytorch/pytorch/pull/56017))\r\n\r\n## Build_Frontend\r\n\r\n* Added a new include directory in BLIS search path ([#58166](https://github.com/pytorch/pytorch/pull/58166))\r\n* Added print to show full Python version in `torch.utils.collect_env` ([#59632](https://github.com/pytorch/pytorch/pull/59632))\r\n* Added change to respect `CMAKE_PREFIX_PATH` choice set by caller ([#61904](https://github.com/pytorch/pytorch/pull/61904))\r\n* Dropped incremental linking on Windows when REL_WITH_DEB_INFO=1. ([#64892](https://github.com/pytorch/pytorch/pull/64892))\r\n* Enabled kineto build for ROCm platform ([#58401](https://github.com/pytorch/pytorch/pull/58401))\r\n* Added support to system-provided Intel TBB ([#61934](https://github.com/pytorch/pytorch/pull/61934))\r\n* Added Pytorch build support with [Newlib](https://en.wikipedia.org/wiki/Newlib) c library ([#60345](https://github.com/pytorch/pytorch/pull/60345), [#60052](https://github.com/pytorch/pytorch/pull/60052))\r\n* Imrpove `torch.__version__` comparisons ([#61556](https://github.com/pytorch/pytorch/pull/61556), [#64565](https://github.com/pytorch/pytorch/pull/64565), [#63848](https://github.com/pytorch/pytorch/pull/63848))\r\n* CMake: added optional precompiled header support ([#61940](https://github.com/pytorch/pytorch/pull/61940))\r\n* Removed unnecessary Ubuntu version checks ([#61738](https://github.com/pytorch/pytorch/pull/61738))\r\n* Added GPU support to `bazel` builds ([#63604](https://github.com/pytorch/pytorch/pull/63604))\r\n\r\n## Infra (Releng)\r\n\r\n* Improved automated test sharding. ([#59727](https://github.com/pytorch/pytorch/pull/59727), [#60206](https://github.com/pytorch/pytorch/pull/60206))\r\n* Added change to strictly type everything in .github and tools ([#59117](https://github.com/pytorch/pytorch/pull/59117))\r\n* Upgraded Windows CI Python to 3.8 ([#59729](https://github.com/pytorch/pytorch/pull/59729)) and CUDA to 10.2 ([#65080](https://github.com/pytorch/pytorch/pull/65080))\r\n* Made change to use expecttest from PyPI ([#60658](https://github.com/pytorch/pytorch/pull/60658), [#63320](https://github.com/pytorch/pytorch/pull/63320))\r\n* Added option to run specified tests option to run_test.py ([#59649](https://github.com/pytorch/pytorch/pull/59649))\r\n* Enabled Metal in PyTorch MacOS/iOS nightly builds ([#63718](https://github.com/pytorch/pytorch/pull/63718), [#65075](https://github.com/pytorch/pytorch/pull/65075))\r\n* Added retries to flaky CI steps. ([#65013](https://github.com/pytorch/pytorch/pull/65013), [#65104](https://github.com/pytorch/pytorch/pull/65104), [#64120](https://github.com/pytorch/pytorch/pull/64120), [#60216](https://github.com/pytorch/pytorch/pull/60216), [#63319](https://github.com/pytorch/pytorch/pull/63319))\r\n* Allowed Docker build on macOS ([#60375](https://github.com/pytorch/pytorch/pull/60375))\r\n\r\n## Misc\r\n\r\n* Added support for MIOpen channel last convolution ([#63617](https://github.com/pytorch/pytorch/pull/63617))\r\n* Enabled kernel asserts on rocm ([#49624](https://github.com/pytorch/pytorch/pull/49624))\r\n* Added bool, float16, bfloat16 and complex support for to_dense for CSR sparse Tensors ([#60657](https://github.com/pytorch/pytorch/pull/60657))\r\n* Added complex dtype support for matrix multiplication of two COO sparse Tensors on CPU ([#59554](https://github.com/pytorch/pytorch/pull/59554))\r\n* Added the \u201cupper\u201d kwarg to `torch.linalg.cholesky` ([#62434](https://github.com/pytorch/pytorch/pull/62434))\r\n* Improved error message in ONNX when attempting to export dict modification ([#58696](https://github.com/pytorch/pytorch/pull/58696))\r\n* Migrated `THAllocator` to `MapAllocator` in ATen ([#60325](https://github.com/pytorch/pytorch/pull/60325))\r\n* Converted input type of `TensorOptions.device_index` from `int16_t` to to `c10::DeviceIndex` ([#60412](https://github.com/pytorch/pytorch/pull/60412))\r\n\r\n# Bug fixes\r\n\r\n## Python API\r\n\r\n* Added fix to recognize transposed dense tensors as a form of partial overlap ([#59014](https://github.com/pytorch/pytorch/pull/59014))\r\n* Fixed `torch.polygamma` incorrect behavior at infinites when n>=1 ([#61641](https://github.com/pytorch/pytorch/pull/61641))\r\n* Fixed for non-contiguous inputs for `torch.{sort,topk}` on CUDA ([#63029](https://github.com/pytorch/pytorch/pull/63029)), `torch.tensor_split` indices([#63390](https://github.com/pytorch/pytorch/pull/63390))\r\n* Fixed legacy constructor `torch.Tensor`  when given a scalar Tensor ([#58885](https://github.com/pytorch/pytorch/pull/58885))\r\n* Added change to not wrap `Tensor.{grad,_base}` by default for Tensor-like objects([#60464](https://github.com/pytorch/pytorch/pull/60464))\r\n* Fixed `torch.angle` on aarch64 ([#59832](https://github.com/pytorch/pytorch/pull/59832))\r\n* Fixed specialized convolution kernel on arm64 ([#60460](https://github.com/pytorch/pytorch/pull/60460))\r\n* `torch.normal`: fixed RuntimeError when standard deviation named arg is torch.empty [(#66524](https://github.com/pytorch/pytorch/pull/66524/))\r\n* Fixed random sampling on SGX platforms ([#60368](https://github.com/pytorch/pytorch/pull/60368))\r\n* Fixed testing when Scipy is not available ([#61699](https://github.com/pytorch/pytorch/pull/61699))\r\n* Fixed `torch.Tensor.copy_` when using large inputs and broadcasting ([#64425](https://github.com/pytorch/pytorch/pull/64425))\r\n* Fixed broadcasting behavior for `torch.trapezoid` ([#64054](https://github.com/pytorch/pytorch/pull/64054)).\r\n* Fixed dtype check of comparison ops ([#64267](https://github.com/pytorch/pytorch/pull/64267)).\r\n* Fixed `torch.median` crash on empty tensor ([#61698](https://github.com/pytorch/pytorch/pull/61698))\r\n* Fixed missing lazy initialization in `torch.get_num_threads` ([#64486](https://github.com/pytorch/pytorch/pull/64486))\r\n* Fixed check for empty named dims list to `torch.flatten` ([#61953](https://github.com/pytorch/pytorch/pull/61953))\r\n* Fixed `torch.hub.{list,help}` functions for Windows ([#63773](https://github.com/pytorch/pytorch/pull/63773))\r\n* Fixed `torch.{istft,rfft}` errors for special inputs ([#63469](https://github.com/pytorch/pytorch/pull/63469), [#63327](https://github.com/pytorch/pytorch/pull/63327))\r\n* Fixed type annotation\r\n    * `optim.lr_scheduler.CosineAnnealingWarmRestart` ([#61106](https://github.com/pytorch/pytorch/pull/61106))\r\n    * Fixed type annotation of `torch.hub.load` ([#63755](https://github.com/pytorch/pytorch/pull/63755))\r\n* `x[index] = value` no longer results in a RuntimeError if `x` and `value` are different devices.\r\n    ([#61612](https://github.com/pytorch/pytorch/pull/61612))\r\n* Fixed crash while creating new tensor if NumPy is not available ([#66433](https://github.com/pytorch/pytorch/pull/66433))\r\n* Handle exceptions from THPModule_setQEngine ([#60073](https://github.com/pytorch/pytorch/pull/60073))\r\n* Fixed `torch.Tensor.cauchy_` on CUDA for inf values ([#60186](https://github.com/pytorch/pytorch/pull/60186))\r\n\r\n## Autograd\r\n\r\n* `torch.{signbit,isin}` no longer raise an error when passed a tensor that requires grad ([#62529](https://github.com/pytorch/pytorch/pull/62529))\r\n* Fixed sub-gradient for `torch.a{max,min}` ([#59669](https://github.com/pytorch/pytorch/pull/59669))\r\n* Fixed segfaults when a tensor hook removes itself ([#61250](https://github.com/pytorch/pytorch/pull/61250))\r\n* Fixed double backward for `binary_cross_entropy` loss function when `reduction=sum`. ([#59479](https://github.com/pytorch/pytorch/pull/59479))\r\n* Made sure that TLS (grad mode, inference mode, dispatcher state, etc) are properly set in hooks being called during the backward pass ([#60067](https://github.com/pytorch/pytorch/pull/60067))\r\n\r\n## torch.nn\r\n\r\n* `nn.AdaptiveAvgPool2d`: Correctly dispatch to CUDA implementation ([#61851](https://github.com/pytorch/pytorch/pull/61851))\r\n* `nn.AdaptiveAvgPool3d`: Fixed gradient computation ([#60630](https://github.com/pytorch/pytorch/pull/60630))\r\n* `nn.BatchNorm`: Fixed mixed precision usage when `affine=False` ([#61962](https://github.com/pytorch/pytorch/pull/61962))\r\n* `nn.BatchNorm2d`: Fixed issue when input is non-contiguous ([#63392](https://github.com/pytorch/pytorch/pull/63392))\r\n* Fixed `batch_norm()` to preserve output memory layout based on input ([#62773](https://github.com/pytorch/pytorch/pull/62773))\r\n* `nn.MaxPool2d`: Use `channels_last` memory format for output and indices when input is channels_last ([#61245](https://github.com/pytorch/pytorch/pull/61245))\r\n* `nn.Module`: Fixed full backward hook when grad is disabled ([#65335](https://github.com/pytorch/pytorch/pull/65335))\r\n* `nn.Module`: Fixed `get_buffer()` to check buffers by name instead of value ([#61429](https://github.com/pytorch/pytorch/pull/61429))\r\n* `nn.Module`: Fixed pre-forward hooks for Lazy modules ([#60517](https://github.com/pytorch/pytorch/pull/60517))\r\n* `nn.Softmax`: Improve numerical stability by subtracting max value in vectorized CPU implementation ([#63132](https://github.com/pytorch/pytorch/pull/63132))\r\n* `F.cosine_similarity`: Fixed type promotion behavior and added input validation checks ([#62054](https://github.com/pytorch/pytorch/pull/62054), [#66191](https://github.com/pytorch/pytorch/pull/66191), [#62912](https://github.com/pytorch/pytorch/pull/62912), [#58559](https://github.com/pytorch/pytorch/pull/58559))\r\n* `F.embedding`: Added check to validate that weights are 2D ([#59314](https://github.com/pytorch/pytorch/pull/59314))\r\n* `F.interpolate`: Fixed output for edge case of single pixel without align_corners ([#61166](https://github.com/pytorch/pytorch/pull/61166))\r\n* `F.nll_loss`: Fixed regression for gradient computation ([#64203](https://github.com/pytorch/pytorch/pull/64203))\r\n* `F.pad`: Fixed type of default pad value to be floating point ([#62095](https://github.com/pytorch/pytorch/pull/62095))\r\n* Fixed issues with printing `torch._ops.ops.{atan, quantized}` modules ([#62447](https://github.com/pytorch/pytorch/pull/62447))\r\n* Fixed `torch.nn.utils.parametrizations.spectral_norm` so that it can be used twice in the same forward pass ([#62293](https://github.com/pytorch/pytorch/pull/62293))\r\n* Disabled cuDNN persistent RNN on A30 to avoid exceptions from hard-to-detect edge cases ([#59830](https://github.com/pytorch/pytorch/pull/59830))\r\n\r\n## Dataloader\r\n\r\n* Fixed `IterableFecher` to stop fetching data after `StopIterator` ([#59313](https://github.com/pytorch/pytorch/pull/59313))\r\n* Fixed `ExceptionWrapper` to re-raise Exception with multiple args ([#58131](https://github.com/pytorch/pytorch/pull/58131))\r\n\r\n## AMD\r\n\r\n* Fix ROCm compilation by properly marking c++ functions as CPU only ([#62628](https://github.com/pytorch/pytorch/pull/62628))\r\n* Fixed `torch.{i1,i1e}` ROCm failure: mark array as const so that it is available for host and device ([#59187](https://github.com/pytorch/pytorch/pull/59187))\r\n\r\n## CUDA\r\n\r\n* Fixed to not use deprecated data accessor in IndexKernel.cu ([#62268](https://github.com/pytorch/pytorch/pull/62268))\r\n* Fixed sign comparison ([#62194](https://github.com/pytorch/pytorch/pull/62194), [#62483](https://github.com/pytorch/pytorch/pull/62483))\r\n* Fixed `torch.manual_seed{_all}` memory leak ([#62534](https://github.com/pytorch/pytorch/pull/62534))\r\n* Fixed CUDA_KERNEL_ASSERT ambiguous symbol in NDEBUG mode ([#62527](https://github.com/pytorch/pytorch/pull/62527))\r\n* Changed to use long index type for `torch.index_add` deterministic implementation ([#59254](https://github.com/pytorch/pytorch/pull/59254))\r\n* Fixed illegal memory access on NHWC BN kernel ([#59981](https://github.com/pytorch/pytorch/pull/59981))\r\n* Fixed typo in Normalization.cu ([#62515](https://github.com/pytorch/pytorch/pull/62515))\r\n* Added change to ignore and clear errors related to cuda not being ready yet ([#61554](https://github.com/pytorch/pytorch/pull/61554))\r\n* Fixed segmentation fault due to access to destroyed global IPC variable([#56141](https://github.com/pytorch/pytorch/pull/56141))\r\n* Fixed reduction launch config ([#64304](https://github.com/pytorch/pytorch/pull/64304))\r\n* Fixed typo embedding_renorm_ cuda implementation ([#64542](https://github.com/pytorch/pytorch/pull/64542))\r\n* Added missing kernel checks ([#60635](https://github.com/pytorch/pytorch/pull/60635))\r\n* CUDA graphs: made sure graph mempool malloc counter pairs with frees for all allocations ([#61567](https://github.com/pytorch/pytorch/pull/61567))\r\n* Fix bug where some kernels would not properly call cuda lazy initialization ([#61882](https://github.com/pytorch/pytorch/pull/61882))\r\n* Added check for contiguous to dispatch to NHWC CUDA template ([#62839](https://github.com/pytorch/pytorch/pull/62839))\r\n* Moved grid_sampler to autocast promote list ([#58618](https://github.com/pytorch/pytorch/pull/58618))\r\n* Added check for memory overlap in sort for large input sizes ([#58327](https://github.com/pytorch/pytorch/pull/58327))\r\n\r\n## C++ API\r\n\r\n* Fixed `map` function for `vec256` to accept const pointer to function ([#59957](https://github.com/pytorch/pytorch/pull/59957))\r\n* Added `supports_as_strided` method to `Device` and fixed indices of `to_sparse()` contiguous on all devices ([#59370](https://github.com/pytorch/pytorch/pull/59370))\r\n* Removed redundant bitwise-and op in MT19937RNGEngine ([#63219](https://github.com/pytorch/pytorch/pull/63219))\r\n* Fixed subprocess encoding for cpp extension on Windows ([#63756](https://github.com/pytorch/pytorch/pull/63756))\r\n* Define the SYCL device version `__assert_fail` when the NDEBUG defined. ([#58906](https://github.com/pytorch/pytorch/pull/58906))\r\n\r\n## TorchScript\r\n\r\n* Fixed inconsistency between Python and JIT power operation ([#62842](https://github.com/pytorch/pytorch/pull/62842))\r\n* Added change to convert `__constants__` attribute in model to a set to be consistent ([#60003](https://github.com/pytorch/pytorch/pull/60003)) \r\n* Added change to Ignore unsupported attribute checker pass for `torch.jit.trace` ([#60200](https://github.com/pytorch/pytorch/pull/60200)) \r\n* Fixed missing element types and shapes when `torch.autograd.Function` has multiple tensor outputs ([#57966](https://github.com/pytorch/pytorch/pull/57966))\r\n* Fixed `Tensor.to` schema to reflect that the output may alias input ([#60001](https://github.com/pytorch/pytorch/pull/60001))  \r\n* Added change to turn off layer norm in jit symbolic differentiation ([#63816](https://github.com/pytorch/pytorch/pull/63816)) \r\n* Fixed name conflict by using a more specific prefix for lowered module name. ([#61007](https://github.com/pytorch/pytorch/pull/61007)) \r\n* Added change to allow disabling cache in autocast (automatic mixed precision) ([#63552](https://github.com/pytorch/pytorch/pull/63552)) \r\n* Fixed concat optimization to handle cases when input list is mutated after cat using AliasDb ([#60774](https://github.com/pytorch/pytorch/pull/60774)) \r\n* Fixed symbolic derivative of hardswish ([#59405](https://github.com/pytorch/pytorch/pull/59405)) \r\n\r\n## torch.package\r\n\r\n* Fixed a bug when using `importlib.resources.path` for python <3.8.8 ([#58718](https://github.com/pytorch/pytorch/pull/58718))\r\n* Fixed bugs when using `os` and `os.path` ([#60276](https://github.com/pytorch/pytorch/pull/60276))\r\n* Fixed storage serialization collision when saving a `ScriptModule` and then saving a `Tensor` owned by it. ([#61806](https://github.com/pytorch/pytorch/pull/61806))\r\n* Fixed use-after-free during autograd shutdown ([#64620](https://github.com/pytorch/pytorch/pull/64620))\r\n* Fixed non-determinism in naming scheme of serialized storages in export code paths and ABA ABA storage identity problem during serialization for `torch.package` ([#59735](https://github.com/pytorch/pytorch/pull/59735))\r\n* Fixed GIL issue when acquiring multiple sessions. ([#58584](https://github.com/pytorch/pytorch/pull/58584))\r\n\r\n## Mobile\r\n\r\n* Fixed Nnapi backend dangling pointer bug ([#63092](https://github.com/pytorch/pytorch/pull/63092))\r\n* Fixed missing constants archive in torchscript model after backport ([#58892](https://github.com/pytorch/pytorch/pull/58892))\r\n* Fixed type hints in optimize_for_mobile to be consistent with the default([#59282](https://github.com/pytorch/pytorch/pull/59282))\r\n* Fixed xnnpack hardswish memory issue ([#59577](https://github.com/pytorch/pytorch/pull/59577), [#61622](https://github.com/pytorch/pytorch/pull/61622))\r\n* Fixed the issue that model_dump didn\u2019t work with delegate models ([#61043](https://github.com/pytorch/pytorch/pull/61043))\r\n* Fixed concat shaders didn\u2019t work for certain iOS devices ([#61074](https://github.com/pytorch/pytorch/pull/61074))\r\n* Fixed the Metal `torch.clamp` shader function for x86_64 ([#63062](https://github.com/pytorch/pytorch/pull/63062))\r\n* Fixed callstack pointer serialization bug ([#63576](https://github.com/pytorch/pytorch/pull/63576))\r\n* Fixed model loading error for Vulkan backend in Java API ([#63402](https://github.com/pytorch/pytorch/pull/63402))\r\n* Fixed the issue that sub modules with same names are not serialized correctly in bytecode format ([#61933](https://github.com/pytorch/pytorch/pull/61933))\r\n\r\n## Quantization\r\n\r\n* Fixed crash when model outputs dicts or lists ([#58416](https://github.com/pytorch/pytorch/pull/58416))\r\n* QAT: Fixed the runtime run `cannot resize variables that require grad` ([#57068](https://github.com/pytorch/pytorch/pull/57068))\r\n* Fixed support for custom module ([#59041](https://github.com/pytorch/pytorch/pull/59041))\r\n* Fixed the \"tensors to be on the same device\" error in HistogramObserver ([#59234](https://github.com/pytorch/pytorch/pull/59234))\r\n* Fixed dimension for output of batchnorm 1d ([#59264](https://github.com/pytorch/pytorch/pull/59264))\r\n* Fixed quantized mean operator in QNNPACK backend ([#59761](https://github.com/pytorch/pytorch/pull/59761))\r\n* Fixed a bug in .to for qtensors so scale/zp move too ([#61576](https://github.com/pytorch/pytorch/pull/61576))\r\n* Fixed quantized Conv1d module parameters ([#62356](https://github.com/pytorch/pytorch/pull/62356))\r\n* Fixed quantization for tuple arguments ([#63376](https://github.com/pytorch/pytorch/pull/63376))\r\n* Fixed fuse qconfig comparison ([#63384](https://github.com/pytorch/pytorch/pull/63384))\r\n* Fixed the conversion of the quantizable RNN ([#63879](https://github.com/pytorch/pytorch/pull/63879))\r\n* Fixed quantization for sub_scalar ([#64603](https://github.com/pytorch/pytorch/pull/64603))\r\n* Fixed a bug for sub ([#65109](https://github.com/pytorch/pytorch/pull/65109))\r\n* Add change to ensure qconfig works for QAT with multiple modules ([#63343](https://github.com/pytorch/pytorch/pull/63343))\r\n\r\n## Distributed\r\n\r\n`DistributedDataParallel`\r\n\r\n* Fixed Pipe + DDP for unused parameters, static graph ([#60118](https://github.com/pytorch/pytorch/pull/60118))\r\n* Fixed case where new tensors with no grad_fn are returned in DDP forward. ([#60882](https://github.com/pytorch/pytorch/pull/60882))\r\n* Re-enabled the optimization of fusing copy and division when no comm hook is specified for both dense and sparse tensors ([#61379](https://github.com/pytorch/pytorch/pull/61379), [#61814](https://github.com/pytorch/pytorch/pull/61814))\r\n* Fixed fp16 C++ DDP gradient communication hook ([#63375](https://github.com/pytorch/pytorch/pull/63375))\r\n* Added change to ensure buffers are broadcasted properly when they are reassigned in module ([#64776](https://github.com/pytorch/pytorch/pull/64776))\r\n* Fixed GradBucket.is_last() logic ([#63768](https://github.com/pytorch/pytorch/pull/63768))\r\n\r\n\r\n`torch.distributed.Store`\r\n\r\n* torch.distributed and RPC cannot both be initialized with the same host:port pair ([#58328](https://github.com/pytorch/pytorch/pull/58328), [#58329](https://github.com/pytorch/pytorch/pull/58329), [#58330](https://github.com/pytorch/pytorch/pull/58330), [#58331](https://github.com/pytorch/pytorch/pull/58331))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Added change to run dist_autograd backward RPCs on appropriate CUDA streams. ([#60606](https://github.com/pytorch/pytorch/pull/60606))\r\n* Fixed race condition in TensorPipe agent ([#58753](https://github.com/pytorch/pytorch/pull/58753))\r\n* Fixed issue when some gradients are None for distributed optimizers ([#62249](https://github.com/pytorch/pytorch/pull/62249))\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Added change to ensure rendezvous timeout does not get overwritten ([#61471](https://github.com/pytorch/pytorch/pull/61471))\r\n* Fixed the edge case when no node is alive ([#59663](https://github.com/pytorch/pytorch/pull/59663))\r\n* Added change to cast timestamp type to int ([#59712](https://github.com/pytorch/pytorch/pull/59712))\r\n* Added properly formatted traceback on error ([#65041](https://github.com/pytorch/pytorch/pull/65041))\r\n\r\n`torch.distributed.autograd`\r\n\r\n* Updated GraphTask::owner_ in a single thread for DistEngine. ([#58625](https://github.com/pytorch/pytorch/pull/58625))\r\n* Introduced the deadlock fix ([#61588](https://github.com/pytorch/pytorch/pull/61588), [#61593](https://github.com/pytorch/pytorch/pull/61593))\r\n\r\n`torch.distributed`\r\n\r\n* Fixed the slowdown of _object_to_tensor since 1.9 (#65721) ([#65721](https://github.com/pytorch/pytorch/pull/65721))\r\n\r\n## torch.fx\r\n\r\n* Fixed retracing wrapped functions ([#58061](https://github.com/pytorch/pytorch/pull/58061))\r\n* Added override for call_function so that wrapped functions stay wrapped ([#60057](https://github.com/pytorch/pytorch/pull/60057))\r\n* Added fix to retain node.meta after normalizing args ([#60449](https://github.com/pytorch/pytorch/pull/60449))\r\n* Added change to skip the output nodes but process possible nodes after it, when creating a single partition  ([#60370](https://github.com/pytorch/pytorch/pull/60370))\r\n* Fixed fx patch module name ([#61062](https://github.com/pytorch/pytorch/pull/61062))\r\n* Fixed graph `copy.deepcopy` to propagate output type ([#61747](https://github.com/pytorch/pytorch/pull/61747))\r\n* Added change to allow starter nodes to depend on `get_attr` node ([#62234](https://github.com/pytorch/pytorch/pull/62234))\r\n* Added change to prevent implicit submodule inlining when submodule is a GraphModule ([#62436](https://github.com/pytorch/pytorch/pull/62436))\r\n* Added change to persist `tracer_cls` on `fx.Graph` when deep copying ([#63353](https://github.com/pytorch/pytorch/pull/63353))\r\n* Fixed GraphModule deepcopy to use deepcopied graph ([#63090](https://github.com/pytorch/pytorch/pull/63090))\r\n* Fixed constant folding for attrs in submodule hierarchies ([#64342](https://github.com/pytorch/pytorch/pull/64342))\r\n* Fixed some const fold cases with deep model hierarchy ([#64945](https://github.com/pytorch/pytorch/pull/64945))\r\n* Fixed tracing of bitwise and/or ([#65196](https://github.com/pytorch/pytorch/pull/65196))\r\n\r\n## ONNX\r\n\r\n* Added shape type inference fixes for control flow ([#60248](https://github.com/pytorch/pytorch/pull/60248))\r\n* Fixed sum export with attribute `keepdims` ([#60245](https://github.com/pytorch/pytorch/pull/60245))\r\n* Fixed shape inference for large model ([#60244](https://github.com/pytorch/pytorch/pull/60244))\r\n* Fixed split export in op set 13 ([#57605](https://github.com/pytorch/pytorch/pull/57605))\r\n* Fixed control-flow shape inference with contrib op ([#62762](https://github.com/pytorch/pytorch/pull/62762))\r\n* Updated `instance_norm2d` export to handle `track_running_stats=True` ([#58690](https://github.com/pytorch/pytorch/pull/58690))\r\n* Fixed the issue of converting empty list to sequence([#61558](https://github.com/pytorch/pytorch/pull/61558))\r\n* Fixed sum could not be exported for empty tensor ([#59537](https://github.com/pytorch/pytorch/pull/59537))\r\n* Fixed an issue that optimizations might adjust graph inputs unexpectedly ([#62763](https://github.com/pytorch/pytorch/pull/62763))\r\n\r\n## Vulkan\r\n\r\n* Fixed an issue where comparing equivalent descriptors would evaluate to `false` ([#60199](https://github.com/pytorch/pytorch/pull/60199))\r\n* Fixed asserts in Vulkan JIT passes to actually throw an exception ([#61495](https://github.com/pytorch/pytorch/pull/61495))\r\n\r\n## Performance_as_a_product\r\n\r\n* Added fix to ensure number of thread utilities are initialized before getting the number of threads ([#60185](https://github.com/pytorch/pytorch/pull/60185))\r\n* Added fix to ensure thread id is valid in nested parallel regions ([#60183](https://github.com/pytorch/pytorch/pull/60183))\r\n* Fixed parallel tbb build ([#60532](https://github.com/pytorch/pytorch/pull/60532))\r\n* Added change to make flags in the pytorch managed thread pool atomic. ([#58457](https://github.com/pytorch/pytorch/pull/58457))\r\n* Set mkl thread locally ([#62891](https://github.com/pytorch/pytorch/pull/62891))\r\n\r\n## Composability\r\n\r\n* Added a fix to ensure that the C++ API\u2019s that skip the dispatcher (such as `at::cpu::{op}` and `at::cuda::{op}` get external linkage, so they can be used outside of libtorch ([#58569](https://github.com/pytorch/pytorch/pull/58569))\r\n* Fixed bug where shared memory tensor file names can collide ([#60978](https://github.com/pytorch/pytorch/pull/60978))\r\n\r\n## Build_Frontend\r\n\r\n* Fixed binary building without python ([#66031](https://github.com/pytorch/pytorch/pull/66031))\r\n* Fixed Windows ninja builds when MAX_JOBS is specified ([#65444](https://github.com/pytorch/pytorch/pull/65444))\r\n* Skipped Bfloat16 support when building for VSX ([#61630](https://github.com/pytorch/pytorch/pull/61630))\r\n* Made change to use python3 alias in Makefile ([#58786](https://github.com/pytorch/pytorch/pull/58786))\r\n* Made change to use `pybind11` from `third_party` folder by default ([#58951](https://github.com/pytorch/pytorch/pull/58951))\r\n* Made change to ensure FindLAPACK finds the same BLAS library ([#49647](https://github.com/pytorch/pytorch/pull/49647))\r\n* Improved Python package detection in `torch.utils.collect_env` ([#63321](https://github.com/pytorch/pytorch/pull/63321))\r\n* Skipped SVE acceleration on M1 machine ([#58785](https://github.com/pytorch/pytorch/pull/58785))\r\n* Made `SciPy` dependency optional in PyTorch unary operators tests ([#59304](https://github.com/pytorch/pytorch/pull/59304))\r\n* Fixed error-handling when Python executable can not be found ([#61230](https://github.com/pytorch/pytorch/pull/61230))\r\n* Fixed `setup.py` re-run incremental build logic on Windows ([#59689](https://github.com/pytorch/pytorch/pull/59689))\r\n* Reduced binary size for CUDA-split build by establishing correct linking order ([#58287](https://github.com/pytorch/pytorch/pull/58287))\r\n* Fixed  `torch.utils.cpp_extension` behavior when older setuptools are used ([#61484](https://github.com/pytorch/pytorch/pull/61484))\r\n\r\n## Infra (Releng)\r\n\r\n* Fixed windows ci squid env ([#62353](https://github.com/pytorch/pytorch/pull/62353))\r\n* Introduced CI dependency pinning: ([#64922](https://github.com/pytorch/pytorch/pull/64922), [#65017](https://github.com/pytorch/pytorch/pull/65017))\r\n* Fixed breakpad build and add to more images ([#59236](https://github.com/pytorch/pytorch/pull/59236))\r\n* Updated certificate trust chain CI to depend on the linked commits ([#65934](https://github.com/pytorch/pytorch/pull/65934), [#66004](https://github.com/pytorch/pytorch/pull/66004))\r\n\r\n## LinAlg_Frontend\r\n\r\n* Fixed an issue where the \u201cinfo\u201d tensor returned by `torch.linalg.inv_ex` could sometimes be on the wrong device ([#59223](https://github.com/pytorch/pytorch/pull/59223))\r\n* Fixed an issue where `torch.linalg.norm` could return tensors with the wrong shape in some edge cases ([#60273](https://github.com/pytorch/pytorch/pull/60273))\r\n* Fixed an issue where `torch.linalg.svd` could return tensors with the wrong shape in some edge cases ([#62022](https://github.com/pytorch/pytorch/pull/62022))\r\n* Fixed an issue where `torch.matmul` would throw an error when attempting to multiply certain empty tensors ([#63359](https://github.com/pytorch/pytorch/pull/63359))\r\n\r\n## Sparse_Frontend\r\n\r\n* Fixed dtype inference in sparse_csr_tensor_ctor ([#58631](https://github.com/pytorch/pytorch/pull/58631))\r\n* Fixed addmm failure for CSR Tensors when MKL is not available ([#58768](https://github.com/pytorch/pytorch/pull/58768))\r\n* Fixed overflow of numel for sparse COO tensors after calling coalesce ([#57492](https://github.com/pytorch/pytorch/pull/57492))\r\n* Fixed multiplication of 0-dim Tensor and COO sparse Tensor and improved Error message for multiplication of dense and sparse COO tensor ([#61723](https://github.com/pytorch/pytorch/pull/61723))\r\n* Fixed internal assert error for CSR tensors crow_/col_indices methods in Debug build ([#63176](https://github.com/pytorch/pytorch/pull/63176))\r\n* Fixed support of torch.conj for zero-dimensional sparse COO Tensors ([#59553](https://github.com/pytorch/pytorch/pull/59553))\r\n\r\n## Misc\r\n\r\n* Added change to increase warmup for better steady state measurements. ([#58801](https://github.com/pytorch/pytorch/pull/58801))\r\n* Fixed bad use of channels last kernel in sync batch norm backward ([#64100](https://github.com/pytorch/pytorch/pull/64100))\r\n\r\n# Performance\r\n\r\n## Python API\r\n\r\n* `torch.special.{'i0', 'i0e', 'i1', 'i1e'}:` converted floating-point constants to input type in Bessel functions ([#59416](https://github.com/pytorch/pytorch/pull/59416))\r\n* Added change to speed up `torch.unique_consecutive()` ([#64835](https://github.com/pytorch/pytorch/pull/64835))\r\n* Made sure all graphs tests call `torch.cuda.empty_cache()` before capture to fix flaky tests ([#59233](https://github.com/pytorch/pytorch/pull/59233))\r\n* `torch.flip` : improved performance via TensorIterator ([#59509](https://github.com/pytorch/pytorch/pull/59509))\r\n* Added change to parallelize `torch.gelu` via tensoriterator ([#58950](https://github.com/pytorch/pytorch/pull/58950))\r\n* `torch.sum`: added change to accumulate 16-bit float sums in 32-bit accumulators for improved precision and performance ([#60387](https://github.com/pytorch/pytorch/pull/60387))\r\n* Added fast path for conjugated tensors for  `torch.`{`dot, vdot, mm, addmm, bmm, baddbmm}` ([#62915](https://github.com/pytorch/pytorch/pull/62915), [#59380](https://github.com/pytorch/pytorch/pull/59380))\r\n\r\n## Autograd\r\n\r\n* Faster `torch.cum{sum,prod}` backward formulas ([#60642](https://github.com/pytorch/pytorch/pull/60642))\r\n* Reduced overhead from `reshape` call if the tensor already has the right shape ([#61466](https://github.com/pytorch/pytorch/pull/61466))\r\n* Added change to speed up saving variables for backward ([#59837](https://github.com/pytorch/pytorch/pull/59837), [#61927](https://github.com/pytorch/pytorch/pull/61927))\r\n* Reduced number of TLS access when deciding if an op needs to be tracked by autograd or not ([#60740](https://github.com/pytorch/pytorch/pull/60740))\r\n* Improved code that detect when it is valid to re-use existing Tensors during the backward pass ([#59817](https://github.com/pytorch/pytorch/pull/59817))\r\n\r\n## torch.nn\r\n\r\n* `nn.utils.clip_grad_norm_`: Removed device syncs ([#61042](https://github.com/pytorch/pytorch/pull/61042))\r\n* `nn.BatchNorm2d`: Optimized performance for `channels_last` on CPU ([#59286](https://github.com/pytorch/pytorch/pull/59286))\r\n* `nn.Softmax`: Vectorized softmax calculation for the non-last-dimension case ([#59195](https://github.com/pytorch/pytorch/pull/59195), [#60371](https://github.com/pytorch/pytorch/pull/60371))\r\n* `nn.Transformer`: Faster `generate_square_subsequent_mask` ([#60631](https://github.com/pytorch/pytorch/pull/60631))\r\n\r\n## CUDA\r\n\r\n* Updated launch bounds for trilinear 3d ([#59999](https://github.com/pytorch/pytorch/pull/59999))\r\n* Migrated Embedding thrust sort to cub sort ([#62495](https://github.com/pytorch/pytorch/pull/62495))\r\n* Make `unique` call in embedding use cub instead of thrust ([#63042](https://github.com/pytorch/pytorch/pull/63042))\r\n* Migrated masked_scatter to use cub instead of thrust ([#56750](https://github.com/pytorch/pytorch/pull/56750))\r\n* Reverted D28547564: [pytorch][PR] masked_scatter thrust\u2192cub ([9e261de630](https://github.com/pytorch/pytorch/commit/9e261de630))\r\n* Make sort in EmbeddingBag use cub instead of thrust ([#64498](https://github.com/pytorch/pytorch/pull/64498))\r\n* Migrated Embedding thrust sort to cub sort ([#63806](https://github.com/pytorch/pytorch/pull/63806))\r\n* Removed cat, equal, and stack from autocast promote list ([#59497](https://github.com/pytorch/pytorch/pull/59497))\r\n* Add cublas and cusolver paths for LU solve ([#59148](https://github.com/pytorch/pytorch/pull/59148))\r\n* Fixed launch bounds for gathertopk kernel ([#60314](https://github.com/pytorch/pytorch/pull/60314))\r\n* Changed launch bounds, unrolled for loop for grid sampler 2d fwd and bwd ([#60405](https://github.com/pytorch/pytorch/pull/60405))\r\n* Changed launch bound to fix col2im kernel ([#60315](https://github.com/pytorch/pytorch/pull/60315))\r\n* Fixed launch bounds for grid sampler 3d ([#60385](https://github.com/pytorch/pytorch/pull/60385))\r\n* CUDA graphs: added change to not sync between replays for CUDA driver version 11.4+ ([#61063](https://github.com/pytorch/pytorch/pull/61063))\r\n* Changed launch bounds for upsample_linear1d fwd, bwd from 1024 to 512 ([#61307](https://github.com/pytorch/pytorch/pull/61307))\r\n* Added change to reduce max_num_threads for complex double ops in reduce_kernel ([#61438](https://github.com/pytorch/pytorch/pull/61438))\r\n* Added change to use `fastAtomicAdd` in EmbeddingBag (mode \"max\") backward ([#63298](https://github.com/pytorch/pytorch/pull/63298))\r\n* Added change to use multi-dimensional cuFFT transforms to improve FFT performance ([#61203](https://github.com/pytorch/pytorch/pull/61203))\r\n* `F.avg_pool3d` CUDA backward: use fast atomic adds ([#63387](https://github.com/pytorch/pytorch/pull/63387))\r\n* Add cuSOLVER path for LU factorization in CUDA. ([#56887](https://github.com/pytorch/pytorch/pull/56887))\r\n* Reverted launch bounds change in topK that induced a regression in perf ([#63431](https://github.com/pytorch/pytorch/pull/63431))\r\n* Added change to bring back old algorithm for sorting on small number of segments ([#64127](https://github.com/pytorch/pytorch/pull/64127))\r\n\r\n## Mobile\r\n\r\n* Added change to use channel-last to transform the weights for Metal ([#59113](https://github.com/pytorch/pytorch/pull/59113))\r\n* Implemented RoIAlign in Metal shaders using Sampler ([#56075](https://github.com/pytorch/pytorch/pull/56075))\r\n* Added cache operator lambda during model loading ([#61996](https://github.com/pytorch/pytorch/pull/61996))\r\n* Added Operator Call De-dup at TorchScript Serialization Level ([#64269](https://github.com/pytorch/pytorch/pull/64269))\r\n* Added change to speed up model loading by 1directly calling the C file API from FileAdapter ([#61997](https://github.com/pytorch/pytorch/pull/61997))\r\n* Moved from input ivalues in ByteCodeDeserializer ([#64029](https://github.com/pytorch/pytorch/pull/64029))\r\n* Fixed MobileDebugInfo vector copy ([#64030](https://github.com/pytorch/pytorch/pull/64030))\r\n* Added change to gate tls_local_dispatch_key_set off on iOS too ([#64753](https://github.com/pytorch/pytorch/pull/64753))\r\n* Added change to not store multiple kernels per key on mobile ([#64447](https://github.com/pytorch/pytorch/pull/64447))\r\n* Added OpCode cache in ByteCodeDeserializer ([#64110](https://github.com/pytorch/pytorch/pull/64110))\r\n* Reduced mobile model size by reusing constant and bump bytecode to v5 ([#59722](https://github.com/pytorch/pytorch/pull/59722))\r\n\r\n## Distributed\r\n\r\n* `torch.distributed:` replaced all_gather with more efficient collective api _all_gather_base ([#57769](https://github.com/pytorch/pytorch/pull/57769))\r\n* `torch.distributed.optim.ZeroRedundancyOptimizer: `Sorted params by size (decreasing) ([#59586](https://github.com/pytorch/pytorch/pull/59586))\r\n\r\n## Vulkan\r\n\r\n* Improved the performance of pointwise convolutions by having each shader invocation calculate a 4x4 output tile  ([#60760](https://github.com/pytorch/pytorch/pull/60760))\r\n* Implemented a simple scheme to set the local work group size adaptively ([#61170](https://github.com/pytorch/pytorch/pull/61170))\r\n\r\n## Performance_as_a_product\r\n\r\n* TensorIterator: added change to reduce serial_for_each static overhead ([#58909](https://github.com/pytorch/pytorch/pull/58909))\r\n* Added change to avoid using `std::regex` for device string parsing ([#63204](https://github.com/pytorch/pytorch/pull/63204))\r\n\r\n## Composability\r\n\r\n* Introduced some perf improvements for reduction ops ([#58655](https://github.com/pytorch/pytorch/pull/58655))\r\n* Added optimization to some internal representations of sizes ([#59333](https://github.com/pytorch/pytorch/pull/59333))\r\n* Reduced the number of tensor refcount bumps in many existing kernels ([#58303](https://github.com/pytorch/pytorch/pull/58303), [#59827](https://github.com/pytorch/pytorch/pull/59827), [#58273](https://github.com/pytorch/pytorch/pull/58273), [#58272](https://github.com/pytorch/pytorch/pull/58272), [#58276](https://github.com/pytorch/pytorch/pull/58276), [#58277](https://github.com/pytorch/pytorch/pull/58277), [#58279](https://github.com/pytorch/pytorch/pull/58279), [#60546](https://github.com/pytorch/pytorch/pull/60546), [#58280](https://github.com/pytorch/pytorch/pull/58280))\r\n* Added micro-optimizations to improve the time it takes to load pytorch ([#64784](https://github.com/pytorch/pytorch/pull/64784), [#64820](https://github.com/pytorch/pytorch/pull/64820), [#64821](https://github.com/pytorch/pytorch/pull/64821), [#64822](https://github.com/pytorch/pytorch/pull/64822), [#64838](https://github.com/pytorch/pytorch/pull/64838), [#64678](https://github.com/pytorch/pytorch/pull/64678), [#64682](https://github.com/pytorch/pytorch/pull/64682), [#64670](https://github.com/pytorch/pytorch/pull/64670))\r\n\r\n## Build_Frontend\r\n\r\n* Compiled BatchLinearAlgebra CUDA integration routines with host compiler ([#64146](https://github.com/pytorch/pytorch/pull/64146))\r\n* Sped-up compilation by splitting autogenerated files into smaller ones ([#62186](https://github.com/pytorch/pytorch/pull/62186))\r\n* Allowed [ninja-build](https://ninja-build.org/) to dynamically pick best parallel build option ([#64733](https://github.com/pytorch/pytorch/pull/64733), [#65162](https://github.com/pytorch/pytorch/pull/65162))\r\n\r\n## Infra (Releng)\r\n\r\n* .github: upload /download large artifacts to s3 ([#58506](https://github.com/pytorch/pytorch/pull/58506))\r\n* Made change to only run mem leak check on master ([#60023](https://github.com/pytorch/pytorch/pull/60023))\r\n* Enabled parallel clang-tidy on ec2 runner ([#60870](https://github.com/pytorch/pytorch/pull/60870))\r\n* Made change to skip magma library installation for Windows CPU builds ([#59619](https://github.com/pytorch/pytorch/pull/59619))\r\n\r\n## Sparse_Frontend\r\n\r\n* Sped up conversion of COO to CSR Tensor `to_sparse_csr` by writing custom CPU/GPU kernels ([#61340](https://github.com/pytorch/pytorch/pull/61340), [#61838](https://github.com/pytorch/pytorch/pull/61838))\r\n* Slightly sped up calculation of number of dense entries for sparse softmax via `c10::multiply_integers`  for COO Tensors ([#60872](https://github.com/pytorch/pytorch/pull/60872))\r\n* Slightly sped up sparse softmax for COO Tensors by improve usage of `std::vector` ([#60873](https://github.com/pytorch/pytorch/pull/60873))\r\n* Sped up index_select for sparse COO Tensor ([#63008](https://github.com/pytorch/pytorch/pull/63008))\r\n\r\n## Misc\r\n\r\n* Greatly reduced the post-processing time of the profiler ([#60432](https://github.com/pytorch/pytorch/pull/60432))\r\n* Saved some little memory in `default_collate` ([#61424](https://github.com/pytorch/pytorch/pull/61424))\r\n* Added new ops to the operator microbenchmark: `gelu`, `bmm`, `mm`, `einsum`, `log1p` ([#59334](https://github.com/pytorch/pytorch/pull/59334), [#59595](https://github.com/pytorch/pytorch/pull/59595), [#63654](https://github.com/pytorch/pytorch/pull/63654), [#64647](https://github.com/pytorch/pytorch/pull/64647), [#64032](https://github.com/pytorch/pytorch/pull/64032), [#64205](https://github.com/pytorch/pytorch/pull/64205))\r\n* Added AVX512 support in ATen & remove AVX support ([#61903](https://github.com/pytorch/pytorch/pull/61903))\r\n\r\n\r\nYou can also find the dev specific and documentation related changes in the forum post [here](https://dev-discuss.pytorch.org/t/pytorch-1-10-dev-release-notes/379)", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.10.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.10.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.10.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/51800581", "dateCreated": "2021-10-15T01:35:23Z", "datePublished": "2021-10-21T15:49:53Z"}, {"tagName": "v1.9.1", "name": "Small bug fix release", "authorName": "malfet", "authorType": "User", "body": "# PyTorch 1.9.1 Release Notes\r\n\r\n* Improvements\r\n* Bug Fixes\r\n* Documentation\r\n\r\n# Improvements\r\n\r\n* Stop warning on `.names()` access in `max_pool2d` #60059\r\n* Remove Caffe2 thread-pool leak warning #60318\r\n* Add option to skip GitHub tag validation for `torch.hub.load` #62139\r\n* Use `log.warning` in `torch.distributed.run`  to print OMP_NUM_THREADS warning #63953\r\n* TorchElastic: Pretty print the failure message captured by @record #64036\r\n* `torch.distribtued.run` to set `nproc_per_node` to 1 by default #61552\r\n* Remove experimental API warning  from `torch.distributed.elastic.utils.store` #60807\r\n* Deprecate  `use_env` in `torch.distributed.run` #59409\r\n* Better engineering changes for torch.distributed launcher #59152\r\n\r\n# Bug fixes\r\n\r\n## Distributed / TorchElastic\r\n\r\n* Make init_method=tcp:// compatible with `torch.distributed.run` #63910\r\n* Fix default parameters (number of restarts, log level, number of processes per node)  that regressed with the transition from `torch.distributed.launch` and `torch.distributed.run` and clarify the documentation accordingly #61294\r\n\r\n## Hub\r\n\r\n* Fix HTTP/403 error when calling `torch.hub.load` for TorchVision models #62072\r\n\r\n## Misc\r\n\r\n* `torch.mm` to check input matrix sizes shapes #61394\r\n\r\n# Documentation\r\n\r\n* Fix broken link in elastic launch doc #62378\r\n* Fix typo in `torch.distribtued.run` warning message #61127\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.9.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.9.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.9.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/50056192", "dateCreated": "2021-09-14T04:13:35Z", "datePublished": "2021-09-22T12:58:15Z"}, {"tagName": "v1.8.2", "name": "LTS 1.8.2, Wrap cub in its own namespace", "authorName": "seemethere", "authorType": "User", "body": "# **PyTorch 1.8.2 Release Notes** \r\n\r\n* Highlights\r\n* Bug Fixes\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.8.2. This is the first release we are making as part of the [Pytorch Enterprise Support Program](https://pytorch.org/enterprise-support-program). This release includes a bug fix requested by a customer in an LTS branch. \r\nWe'd like to thank Microsoft for their support and work on this release.\r\n\r\n# Bug Fixes\r\n* Wrap cub in its own namespace ([#55292](https://github.com/pytorch/pytorch/pull/55292)) ([#61605](https://github.com/pytorch/pytorch/pull/61605))", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.8.2", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.8.2", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.8.2", "url": "https://api.github.com/repos/pytorch/pytorch/releases/47996900", "dateCreated": "2021-07-23T18:17:46Z", "datePublished": "2021-08-17T18:33:00Z"}, {"tagName": "v1.9.0", "name": "PyTorch 1.9 Release, including Torch.Linalg and Mobile Interpreter", "authorName": "anjali411", "authorType": "User", "body": "# **PyTorch 1.9 Release Notes** \r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* Deprecations\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the release of PyTorch 1.9. The release is composed of more than 3,400 commits since 1.8, made by 398 contributors. Highlights include:\r\n\r\n* Major improvements to support scientific computing, including torch.linalg, torch.special, and Complex Autograd\r\n* Major improvements in on-device binary size with Mobile Interpreter\r\n* Native support for elastic-fault tolerance training through the upstreaming of TorchElastic into PyTorch Core\r\n* Major updates to the PyTorch RPC framework to support large scale distributed training with GPU support\r\n* New APIs to optimize performance and packaging for model inference deployment \r\n* Support for Distributed training, GPU utilization and SM efficiency in the PyTorch Profiler\r\n\r\nWe\u2019d like to thank the community for their support and work on this latest release. We\u2019d especially like to thank Quansight and Microsoft for their contributions.\r\n\r\nYou can find more details on all the highlighted features in the [_PyTorch 1.9 Release blogpost_](https://pytorch.org/blog/pytorch-1.9-released/). \r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n* **`torch.divide` with `rounding_mode='floor'` now returns infinity when a non-zero number is divided by zero (**[**#56893**](https://github.com/pytorch/pytorch/pull/56893)**).**\r\nThis fixes the `rounding_mode='floor'` behavior to return the same non-finite values as other rounding modes when there is a division by zero. Previously it would always result in a NaN value, but a non-zero number divided by zero should return +/- infinity in IEEE floating point arithmetic. Note this does not effect `torch.floor_divide` or the floor division operator, which currently use `rounding_mode='trunc'` (and are also deprecated for that reason).\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([-1.0, 0.0, 1.0])\r\n>>> b = torch.tensor([0.0])\r\n>>> torch.divide(a, b, rounding_mode='floor')\r\ntensor([nan, nan, nan])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([-1.0, 0.0, 1.0])\r\n>>> b = torch.tensor([0.0])\r\n>>> torch.divide(a, b, rounding_mode='floor')\r\ntensor([-inf, nan, inf])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n        \r\n\r\n* **Legacy tensor constructors and `Tensor.new` no longer support passing both `Tensor` and `device` as inputs  ([#58108](https://github.com/pytorch/pytorch/pull/58108)).**\r\nThis fixes a bug in which 1-element integer tensors were misinterpreted as specifying tensor size, yielding an uninitialized tensor. As noted in the error message, use the new-style `torch.tensor(...)` or `torch.as_tensor(...)` to copy or alias an existing tensor. If you want to create an uninitialized tensor, use `torch.empty(...)`. \r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([1])\r\n>>> torch.LongTensor(a, device='cpu') # uninitialized\r\ntensor([7022349217739848992])\r\n>>> a.new(a, device='cpu')\r\ntensor([4294967295]) # uninitialized\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([1])\r\n>>> torch.LongTensor(a, device='cpu')\r\nRuntimeError: Legacy tensor constructor of the form torch.Tensor(tensor, device=device) is\r\nnot supported. Use torch.tensor(...) or torch.as_tensor(...) instead.\r\n>>> a.new(a, device='cpu')\r\nRuntimeError: Legacy tensor new of the form tensor.new(tensor, device=device) is not\r\nsupported. Use torch.as_tensor(...) instead.\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>        \r\n        \r\n* **`torch.divide` with `rounding_mode='true'` is replaced with `rounding_mode=None` ([#51988](https://github.com/pytorch/pytorch/pull/51988)).**\r\n`torch.divide`'s undocumented `rounding_mode='true'` option has been removed, and instead `rounding_mode=None` should be passed to indicate no rounding should take place. This is equivalent to omitting the argument entirely.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a, b = torch.full((2,), 4.2), torch.full((2,), 2)\r\n>>> torch.divide(a, b, rounding_mode='true')\r\ntensor([2.1000, 2.1000])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a, b = torch.full((2,), 4.2), torch.full((2,), 2)\r\n>>> torch.divide(a, b, rounding_mode=None) # equivalent to  torch.divide(a, b, rounding_mode='true') from the prior release\r\ntensor([2.1000, 2.1000])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n* **`import torch.tensor as tensor` is no longer supported ([#53424](https://github.com/pytorch/pytorch/pull/53424)).**\r\nInstead, use `from torch import tensor`\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch.tensor as tensor\r\n>>> torch.tensor(1.)\r\ntensor(1.)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> import torch.tensor as tensor\r\nModuleNotFoundError: No module named 'torch.tensor'\r\n>>> from torch import tensor\r\n>>> tensor(1.)\r\ntensor(1.)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n* **binary release: `numpy` is no longer a required dependency**\r\nIf you require `numpy` (and don't already have it installed) you will need to install it separately.\r\n\r\n\r\n## Autograd\r\n\r\n* **`torch.autograd.gradcheck.get_numerical_jacobian` and `torch.autograd.gradcheck.get_analytical_jacobian` no longer support functions that return complex valued output as well as any other values of `grad_out` not equal to 1** ([#55692](https://github.com/pytorch/pytorch/pull/55692)).\r\nThis change is a part of a refactor of `gradcheck`\u2019s internals. Note that `gradcheck` itself still supports functions with complex output. This new restriction only applies to calls to the two internal helper functions. As a workaround, you can wrap your functions to return either the real or imaginary component of its output before calling these functions. Additionally these internal helpers no longer accept any other value except 1 for `grad_out` for any input function. Note that these helper functions are also being deprecated in this release.\r\n \r\n1.8.1:\r\n```python\r\nget_numerical_jacobian(torch.complex, (a, b), grad_out=2.0)\r\n```\r\n\r\n1.9.0:\r\n```python\r\n      def wrapped(fn):\r\n            def wrapper(*input):\r\n                return torch.real(fn(*input))\r\n            return wrapper\r\n        \r\n        get_numerical_jacobian(wrapped(torch.complex), (a, b), grad_out=1.0)\r\n```\r\n\r\n* **`torch.autograd.gradcheck` now throws `GradcheckError`** ([#55656](https://github.com/pytorch/pytorch/pull/55656)).\r\nThis change is a part of a refactor of `gradcheck`\u2019s internals. All errors that are able to be silenced by `raise_exception=False` now raise `GradcheckError` (which inherits from `RuntimeError`). If you explicitly check that the type of the error is `RuntimeError` you'll need to update your code to check for `GradcheckError` instead. Otherwise if you use something like `except` or `isinstance`, no changes are necessary.\r\n\r\n1.8.1:\r\n```python\r\n# An example of a situation that will now return GradcheckError instead of\r\n# RuntimeError is when there is a jacobian mismatch, which can happen\r\n# for example when you forget to specify float64 for your inputs.\r\ntry:\r\n    torch.autograd.gradcheck(torch.sin, (torch.ones(1, requires_grad=True),))\r\nexcept RuntimeError as e:\r\n    assert type(e) is RuntimeError # explicitly check type -> NEEDS UPDATE\r\n```\r\n\r\n1.9.0:\r\n```python\r\ntry:\r\n    torch.autograd.gradcheck(torch.sin, (torch.ones(1, requires_grad=True),)\r\nexcept RuntimeError as e:\r\n   # GradcheckError inherits from RuntimeError so you can still catch this\r\n   # with RuntimeError (No change necessary!)\r\n   \r\n   # BUT, if you explicitly check type...\r\n   assert type(e) is torch.autograd.GradcheckError\r\n```\r\n\r\n* **Finished deprecation cycle for in-place view error checks** ([#56093](https://github.com/pytorch/pytorch/pull/56093)).\r\n In-place modification of views will now raise an error if that view was created by a custom function or a function that returns multiple views, or if the view was created in no-grad mode. Modifying in-place a view created in the situations above are error-prone and have been deprecated since v1.5.0. Doing these in-place modifications are now forbidden. For more information on how to work around this, see the related sections the release notes linked below:\r\n    * [v1.5.0](https://github.com/pytorch/pytorch/releases?after=v1.5.1) (view created in custom autograd function, view created in no-grad block)\r\n    * [v1.7.0](https://github.com/pytorch/pytorch/releases?after=v1.8.0-rc3) (section on `split` and `chunk`, i.e., functions that return multiple views).\r\n\r\n## torch.nn\r\n\r\n* **Fixed regression for `nn.MultiheadAttention` to now apply bias flag to both in and out projection layers** ([#52537](https://github.com/pytorch/pytorch/pull/52537)).\r\nIn PyTorch 1.6, a regression was introduced that caused the `bias` flag of `nn.MultiheadAttention` only to apply to the input projection layer. This caused the output projection layer to always include a `bias` parameter, even with `bias=False` specified. The regression is now fixed in PyTorch 1.9, making the `bias` flag correctly apply to both the input and output projection layers. This fix is BC-breaking for the `bias=False` case as it will now result in no `bias` parameter for the output projection layer.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>v1.6 - v1.8.1:</th><th>pre 1.6 & 1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> mha = torch.nn.MultiheadAttention(4, 2, bias=False)\r\n>>> print(mha.out_proj.bias)\r\nParameter containing:\r\ntensor([0., 0., 0., 0.], requires_grad=True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> mha = torch.nn.MultiheadAttention(4, 2, bias=False)\r\n>>> print(mha.out_proj.bias)\r\nNone\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n* **Updated `nn.Module` to fire full backward hooks even when no input requires grad** ([#56693](https://github.com/pytorch/pytorch/pull/56693)).\r\nPrior to this release, full backward hooks were not fired when no input requires gradients. This has been changed so that full backward hooks will always fire during the backward pass, regardless of whether or not any input requires gradients. If you are using full backward hooks, be aware that they may fire more frequently than pre-1.9 due to this change.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1:</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> m = torch.nn.Linear(2, 3)\r\n>>> def hook(mod, grad_input, grad_output):\r\n>>> print('hook called:', grad_input, grad_output)\r\n>>> m.register_full_backward_hook(hook)\r\n>>> input_no_grad = torch.rand(1, 2, requires_grad=False)\r\n>>> m(input_no_grad).sum().backward()\r\n>>> input_grad = torch.rand(1, 2, requires_grad=True)\r\n>>> m(input_grad).sum().backward()\r\nhook called: (tensor([[0.1478, 0.6517]]),) (tensor([[1., 1., 1.]]),)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> m = torch.nn.Linear(2, 3)\r\n>>> def hook(mod, grad_input, grad_output):\r\n>>> print('hook called:', grad_input, grad_output)\r\n>>> m.register_full_backward_hook(hook)\r\n>>> input_no_grad = torch.rand(1, 2, requires_grad=False)\r\n>>> m(input_no_grad).sum().backward()\r\nhook called: (None,) (tensor([[1., 1., 1.]]),)\r\n>>> input_grad = torch.rand(1, 2, requires_grad=True)\r\n>>> m(input_grad).sum().backward()\r\nhook called: (tensor([[0.1478, 0.6517]]),) (tensor([[1., 1., 1.]]),)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Dataloader\r\n\r\n* **Add Numpy seeding to worker of DataLoader** ([#56488](https://github.com/pytorch/pytorch/pull/56488)).\r\n`DataLoader` with `num_workers > 0` will now set independent random seed for NumPy random functions on each worker by default. So, users now won\u2019t be required to set random seed for NumPy using `worker_init_fn` to force NumPy random operations deterministic and independent across `DataLoader` workers. This PR won\u2019t affect users who have already set random seed for NumPy random functions using `worker_init_fn`.\r\n```python \r\n        # dataset returns numpy.random.randint(1, 10000) \r\n        ctx = mp.get_context('fork')\r\n        gen = torch.Generator().manual_seed(0)\r\n        dl = DataLoader(dataset, batch_size=2, num_workers=2, multiprocessing_context=ctx, generator=gen)\r\n        for epoch in range(2):\r\n            print(\"=\" * 4, \"Epoch\", epoch, \"=\" * 4)\r\n            for batch in dl:\r\n                print(batch)\r\n```\r\n        \r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1:</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# When using fork, each worker has same random seed for NumPy random functions at each epoch.\r\n========== Epoch 0 ==========\r\ntensor([[ 0, 340],\r\n[ 1, 7512]])\r\ntensor([[ 2, 340],\r\n[ 3, 7512]])\r\n========== Epoch 1 ==========\r\ntensor([[ 0, 340],\r\n[ 1, 7512]])\r\ntensor([[ 2, 340],\r\n[ 3, 7512]])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# Random seeds for NumPy are different across `DataLoader` workers in each epoch.\r\n========== Epoch 0 ==========\r\ntensor([[ 0, 8715],\r\n[ 1, 5555]])\r\ntensor([[ 2, 6379],\r\n[ 3, 1432]])\r\n========== Epoch 1 ==========\r\ntensor([[ 0, 1374],\r\n[ 1, 996]])\r\ntensor([[ 2, 143],\r\n[ 3, 3507]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n* **Added static type checking enforce for DataPipe** ([#54020](https://github.com/pytorch/pytorch/pull/54020)).\r\n\r\nA new attribute named `type` has been introduced for `IterableDataset` using the typing annotation at each class declaration. By adding this attribute, we are able to extend `IterableDataset` to have type inference and lazy initialization to incorporate the new DataLoader architecture. But, several BC-breaking restrictions are introduced due to this feature.\r\n\r\n1.8.1:\r\n```python\r\n# Users can use string to bypass the invalid type annotation without any error. \r\n# And, incorrect type annotations attached to `__iter__` function are ignored.\r\n```\r\n\r\n1.9.0:\r\n```python\r\n# The following scenario will now raise different Exceptions\r\n# 1) The type annotation is required to be valid now. Previous workaround\r\n# like using string to  represent the invalid type annotation is not supported now.\r\n\r\n# Raises Exception from the evaluation `eval(\"invalid_type\", globals, locals)`\r\nclass DS(IterableDataset[\"invalid_type\"]):  \r\n     ...\r\n# Raises TypeError if the return type of __iter__ is not an Iterator\r\nclass DS(IterableDataset[str]):\r\n    def __iter__(self) -> str:\r\n      ...\r\n# Raise TypeError if the return type of __iter__ is of the form Iterator[X], \r\n# but the argument type X is not a subtype of the IterableDataset.type attribute.\r\nclass DS(IterableDataset[str]):\r\n    def __iter__(self) -> Iterator[int]:\r\n       ...\r\n\r\n#  IterableDatset now has a metaclass, which will conflict with\r\n#  existing user-defined metaclasses on IterableDatasets\r\nclass DS(IterableDataset[str], metaclass=MyMeta): \r\n    ...\r\n```\r\n\r\n\r\n## Meta API\r\n\r\n* **Given Tensor a non-trivial (for now) metaclass _TensorMeta** ([#56147](https://github.com/pytorch/pytorch/pull/56147)).\r\nTensor now has a non-trivial metaclass. This shouldn't be user observable, as Tensor already inherits from a C defined class (and is thus incompatible with other typical metaclasses), but there may be unanticipated interactions with other language features in Python. This PR changes the metaclass of torch.tensor. I.e. `type(type(torch.tensor([1])))` now prints `<class 'torch._C._TensorMeta'>` (used to be `<class 'type'>`)\r\n\r\n## C++ API\r\n\r\n* **Changed in-place resize functions to return const Tensor&** ([#55351](https://github.com/pytorch/pytorch/pull/55351)).\r\nThe C++ signature for `resize_`, `resize_as_`, `resize_as_sparse_`, `sparse_resize_`, and `sparse_resize_and_clear_` has changed to return a `const Tensor&` instead of a `Tensor&`. This may break users\u2019 TORCH_LIBRARY operators that called these functions but returned a non-const `Tensor&`. Ideally, users can change their operators to also consume and return `const Tensor&`, but simply casting the result of the changed function with `const_cast<Tensor&>` is also an option.\r\n\r\n1.8.1:\r\n```cpp\r\nconst at::Tensor a = at::randn({2, 2});\r\nconst at::Tensor b = at::ones({1, 4}, at::kInt);\r\nat::Tensor& out = at::resize_as_(a, b); # success\r\n```\r\n\r\n1.9.0:\r\n```cpp\r\nconst at::Tensor b = at::ones({1, 4}, at::kInt);\r\nat::Tensor& out = at::resize_as_(a, b); \r\n# error: binding value of type 'const at::Tensor' to reference to type 'at::Tensor' drops 'const' qualifier\r\nconst at::Tensor& out = at::resize_as_(a, b); # Success\r\n```\r\n\r\n* **Some ATen Reduction Ops as well as `kron_out` now throw an error when an undefined tensor is passed as input for `out` argument** ([#53218](https://github.com/pytorch/pytorch/pull/53218), [#53640](https://github.com/pytorch/pytorch/pull/53640)).\r\n    * C++ API for the reductions ops like `sum_out`, `nansum_out`, `prod_out`, `std_var_out` have been changed to require users allocating result Tensor before calling these ops. The C++ API `allocate_reduction_result` has changed to `resize_reduction_result` to disallow allocating result Tensor in these reduction ops.\r\n    * The following code can be compiled, but will raise a `c10::Error` when executed. This code compiled and executed successfully in the prior release.\r\n```cpp\r\nat::Tensor out;  # Undefined Tensor\r\nconst at::Tensor a = at::randn({2, 2});\r\nat::IntArrayRef dim = {1};\r\nat::sum_out(out, a, dim);\r\n# c10::Error: Expected a Tensor of type Variable but found an undefined Tensor for argument #4 'out'\r\n```\r\n\r\n* **The C++ API utility functions `expand_inplace` and `expand_outplace` now return `c10::MaybeOwned<Tensor>` instead of `std::tuple<Tensor>`** ([#55065](https://github.com/pytorch/pytorch/pull/55065), [#55245](https://github.com/pytorch/pytorch/pull/55245)). \r\nThe rationale for this change is to avoid unnecessary Tensor creation, thus improving performance. Functions in ExpandUtils return `c10::MaybeOwned<Tensor>` because expansion may not actually be needed, in which case we can improve efficiency by returning `c10::MaybeOwned<Tensor>::borrowed(to_expand)`. However, this means that you need to be careful: the returned `c10::MaybeOwned<Tensor> `must not outlive the original `Tensor` object that `to_expand` referred to! The deleted rvalue reference overloads of these functions help with this by preventing trivial use of a temporary resulting from a function call, but it is still possible to make a mistake. \r\n\r\n## TorchScript\r\n\r\n* **Added recursive scripting for class type module attributes** ([#55124](https://github.com/pytorch/pytorch/pull/55124)).\r\n    * This change is BC-breaking because it will result in class type module attributes being scripted when a module instance is scripted. In previous versions, such attributes were ignored unless their class type was also marked with `@torch.jit.script`. This new feature attempts to script the type, and falls back to the old behaviour of marking the class type attribute as \"failed\" if scripting fails. However, if the class definition does not have type annotations, the definition of the scripted class can different from users might expect (see code sample). If needed, users can explicitly disable the scripting of a class type attribute by adding its name to the `__jit_ignored_attributes__` class attribute of the module being scripted.\r\n\r\n1.8.1:\r\n```python\r\nclass MyClass:\r\n    def __init__(self, a):\r\n        self.attr = a\r\n        \r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        self.attr = MyClass(4)\r\n        \r\nsm = torch.jit.script(MyModule())\r\n```\r\n\r\n1.9.0:\r\n```python\r\nclass MyClass:\r\n    def __init__(self, a):\r\n        self.attr = a\r\n        \r\nclass MyModule(torch.nn.Module):\r\n    def __init__(self):\r\n        self.attr = MyClass(4)\r\n \r\n# RuntimeError: Could not cast attribute 'attr' to type Tensor: Unable to cast Python instance of type <class 'int'> to C++ type 'at::Tensor'         \r\nsm = torch.jit.script(MyModule()) \r\n```\r\n\r\nThis error occurs because `MyClass` is automatically scripted, but `self.attr` is inferred to be a `Tensor` instead of an `int` because `a` is not annotated. To fix this, annotate `a` with the right type `int`, or mark `attr` as an attribute that should be ignored by the scripting process and not recursively processed:\r\n```python\r\n       class MyModule(torch.nn.Module):\r\n            __jit_ignored_attributes__ = [\"attr\"]\r\n        \r\n            def __init__(self):\r\n                self.attr = MyClass(4)\r\n```\r\n                \r\n\r\n## Quantization\r\n\r\n*  **`torch.quantization.quantize_fx.convert_fx`\u2019s `debug` argument has been changed to `is_reference` ([#52179](https://github.com/pytorch/pytorch/pull/52179)).**\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.8.1:</th><th>1.9.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch.quantization.quantize_fx as quantize_fx\r\n>>> m = quantize_fx.convert_fx(m, debug=True)\r\n(Runs successfully)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> m = quantize_fx.convert_fx(m, is_reference=True) # Runs successfully\r\n>>> m = quantize_fx.convert_fx(m, debug=True)\r\nTraceback (most recent call last):\r\nFile \"<stdin>\", line 1, in <module>\r\nTypeError: convert_fx() got an unexpected keyword argument 'debug'\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n* **`torch.cat` is now quantized to `torch.cat` instead of `torch.ops.quantized.cat` ([#54924](https://github.com/pytorch/pytorch/pull/54924)).**\r\nPreviously, we produced torch.ops.quantize.cat which took inputs, dequantized them\r\n        and requantized them with new qparams. This behavior has been changed to produce `torch.cat` directly. [torch.cat](http://torch.cat/) uses the same observer/fake_quant instance for all inputs and output, assumes all inputs are sharing the same qparam, and produces a quantized Tensor with\r\n        the same qparam as all inputs. Using torch.cat is expected to be more efficient since it does not introduce extra quant/dequant.\r\n    * Version 1.8.1: `torch.cat` was quantized to  `torch.ops.quantized.cat.`\r\n    * Version 1.9: `torch.cat` is quantized to `torch.cat` (`torch.cat` works on both floating point and quantized Tensor).\r\n\r\n## Distributed\r\n\r\n* **`DistributedDataParallel`: Removed support for inter-process device replication in DDP ([#54454](https://github.com/pytorch/pytorch/pull/54454),  [#54825](https://github.com/pytorch/pytorch/pull/54825), [#54826](https://github.com/pytorch/pytorch/pull/54826), [#55212](https://github.com/pytorch/pytorch/pull/55212), [#55253](https://github.com/pytorch/pytorch/pull/55253)`, `[`#55353`](https://github.com/pytorch/pytorch/pull/55353)).**\r\n`DistributedDataParallel` now errors out when users attempt to use it in single-process multi-device mode, where a module is replicated across more than one device in a single process. This mode had been previously deprecated and is now removed. Use cases should switch to spawning a single process for each device that is used in replication, which is the performant way to use `DistributedDataParallel` and supports a variety of newly developed features.\r\n\r\n1.8.1:\r\n```python\r\n>>> # Assume the below is ran on 2 ranks in a distributed setting.\r\n>>> rank_to_devices = { 0: [0, 1], 1: [2, 3] }\r\n>>> # Each rank replicates model across 2 GPUs.\r\n>>> model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n        model,\r\n        device_ids=rank_to_devices[rank]\r\n    )\r\n>>> # No error is raised, but below warning is produced.\r\n>>> UserWarning: Single-Process Multi-GPU is not the recommended mode for DDP. In this mode, each DDP instance operates on multiple devices and creates multiple module replicas within one process. The overhead of scatter/gather and GIL contention in every forward pass can slow down training. Please consider using one DDP instance per device or per module replica by explicitly setting device_ids or CUDA_VISIBLE_DEVICES.\r\n```\r\n\r\n1.9.0:\r\n```python\r\n>>> # Assume the below is ran on 2 ranks in a distributed setting.\r\n>>> rank_to_devices = { 0: [0, 1], 1: [2, 3] }\r\n>>> # Each rank replicates model across 2 GPUs.\r\n>>> model_ddp = torch.nn.parallel.DistributedDataParallel(\r\n        model,\r\n        device_ids=rank_to_devices[rank]\r\n    )\r\n>>> # Single process multi-GPU mode now produces an error on initialization.\r\n>>> ValueError: device_ids can only be None or contain a single element.\r\n```\r\n\r\n* **`torch.distributed.elastic`: Replaced `torch.distributed.launch` with `torch.distributed.elastic_launch` ([#56037](https://github.com/pytorch/pytorch/pull/56037)`, `[`#56214`](https://github.com/pytorch/pytorch/pull/56214)).**\r\n        * --logdir \u2192 \u2014log_dir. The stdout and stderr log dir arg name and destination changed. The file destination changed from `$logdir/node_{}_local_rank_{}_stdout` to  `$log_dir/$rank/stdout.log`. If users used the `\u2014logdir` introduced in 1.8 pytorch version, they need to use` \u2014log_dir` parameter now.\r\n\r\n1.8.1:\r\n```python\r\n#!/bin/bash\r\n# Assumes training script train.py exists.\r\npython -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=\"29500\" --logdir test_logdir train.py\r\n# Logs are written to $logdir/node_{}_local_rank_{}_stdout\r\n```\r\n1.9.0:\r\n```python\r\n#!/bin/bash\r\n# Assumes training script train.py exists.\r\npython -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=\"29500\" --log_dir test_logdir train.py\r\n# Logs are written to $log_dir/$rank/stdout.log\r\n```\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n* **`torch.floor_divide` has been deprecated in favor of `torch.div(..., rounding_mode=\u2018floor\u2019)` ([#50281](https://github.com/pytorch/pytorch/pull/50281)).**\r\n    * `torch.floor_divide` incorrectly divides then truncates (rounds towards zero) instead of dividing then flooring (rounds \u201cdown\u201d). Use the `rounding_mode` argument of `torch.div` to indicate if you\u2019d like to continue performing truncation division or floor division, instead, since `torch.floor_divide` will be removed in a future PyTorch release.\r\n* **Older linear algebra operations have been deprecated in favor of their new linalg module counterparts. Namely:**\r\n    *  `torch.{cholesky, qr, symeig, chain_matmul, solve, eig, matrix_rank, lstsq}` have been deprecated in favor of `torch.linalg.{cholesky, qr, symeig, chain_matmul, solve, eig, matrix_rank, lstsq}` ([#57725,](https://github.com/pytorch/pytorch/pull/57725)[#57745](https://github.com/pytorch/pytorch/pull/57745), [#57732,](https://github.com/pytorch/pytorch/pull/57732)[#53453](https://github.com/pytorch/pytorch/pull/53453), [#57741](https://github.com/pytorch/pytorch/pull/57741), [#57727](https://github.com/pytorch/pytorch/pull/57727), [#57734](https://github.com/pytorch/pytorch/pull/57734), [#57743](https://github.com/pytorch/pytorch/pull/57743)).\r\n    * `torch.norm` has been deprecated in favor of the new linalg module norm functions: `torch.linalg.vector_norm`, `torch.linalg.matrix_norm`, and `torch.linalg.norm` ([#57986](https://github.com/pytorch/pytorch/pull/57986)).\r\n    * Aliased `torch.det`, `torch.slogdet`, `torch.matrix_power`, `torch.inverse`, and `torch.pinverse` to their linalg module counterparts ([#57821](https://github.com/pytorch/pytorch/pull/57821)).\r\n\r\n## Autograd\r\n\r\n* **[cpp] Renamed `AutoNonVariableTypeMode` to `AutoDispatchBelowAutograd` and added a warning. ([#56422](https://github.com/pytorch/pytorch/pull/56422))** \r\n`AutoNonVariableTypeMode` is deprecated and will be removed in 1.10 release. For kernel implementations,  please use `AutoDispatchBelowAutograd` instead. Check out more details on how to migrate your kernel [here](https://pytorch.org/cppdocs/notes/inference_mode.html#migration-guide-from-autononvariabletypemode). If you are looking for a user-facing API to enable running your inference-only workload, please use `c10::InferenceMode`. Using `AutoDispatchBelowAutogradMode` in user code is under risk of producing silently wrong result for some edge cases.\r\n    \r\n1.8.1:\r\n```cpp\r\n{\r\n  at::AutoNonVariableTypeMode guard(true);\r\n}\r\n```\r\n\r\n1.9.0:\r\n```\r\n{\r\n  c10::AutoDispatchBelowAutograd guard(true); // for kernel implementations\r\n  // c10::InferenceMode guard(true); --> consider inference mode if you are looking for a user-facing API\r\n\r\n}\r\n```\r\n\r\n* **Removed logic for old style custom autograd `Function` ([#57357](https://github.com/pytorch/pytorch/pull/57357)).**\r\nInstantiating a custom autograd function is now deprecated and will raise a warning. Users should call `.apply()` on the class itself because it is a static method.\r\n\r\n1.8.1:\r\n```python\r\n        # Instantiating custom function will raise a warning in 1.9\r\n        Func().apply\r\n```\r\n\r\n1.9.0:\r\n```python\r\n        # You should directly call the `apply` (classmethod) on the class\r\n        Func.apply\r\n```\r\n\r\n* **Deprecated `get_analytical_jacobian` and `get_numerical_jacobian` ([#54378](https://github.com/pytorch/pytorch/pull/54378), [#54049](https://github.com/pytorch/pytorch/pull/54049)).**\r\n`torch.autograd.gradcheck.get_analytical_jacobian`  and `torch.autograd.gradcheck.get_numerical_jacobian` are internal-facing functions that are not a part of our public API. We\u2019ve refactored some PyTorch internals to work without it and will\r\n        remove it in a future release. For gradient checking purposes, please use `torch.autograd.gradcheck`. \r\n\r\n## C++ API\r\n\r\n* **Removed the redundant `linalg_` prefix from `torch::linalg::linalg_det` and `torch::linalg::linalg_norm` C++ API ([#57464](https://github.com/pytorch/pytorch/pull/57464)).**\r\nC++ code that used to call `torch::linalg::{linalg_det, linalg_norm}` should be updated to call `torch::linalg::{det, norm}`\r\n\r\n## Distributed\r\n\r\n* **`torch.distributed.rpc`: Added a warning message to retire ProcessGroup RPC backend ([#55616](https://github.com/pytorch/pytorch/pull/55616))**\r\n    * ProcessGroup RPC backend is being deprecated and 1.9 is the last release which will carry it. The default RPC backend is TensorPipe which is the recommended backend to use over ProcessGroup.\r\n\r\n# \r\n\r\n# New features\r\n\r\n### Python API\r\n\r\n* Added BFloat16 support for `torch.{ceil, floor, frac, round, trunc, lerp, roll, diag, logaddexp, logaddexp2, nan_to_num, exp2, expm1, rsqrt, erfc, atan2, hypot}` on CUDA ([#57910](https://github.com/pytorch/pytorch/pull/57910), [#57907](https://github.com/pytorch/pytorch/pull/57907), [#57916](https://github.com/pytorch/pytorch/pull/57916), [#57908](https://github.com/pytorch/pytorch/pull/57908), [#58063](https://github.com/pytorch/pytorch/pull/58063), [#57913](https://github.com/pytorch/pytorch/pull/57913), [#57905](https://github.com/pytorch/pytorch/pull/57905)).\r\n* Added `torch.pow()` for `torch.{float16, BFloat16}` on CPU ([#55280](https://github.com/pytorch/pytorch/pull/55280)).\r\n* Added `torch.{index_select, argmax, argmin, min, max, amin, amax}` for `torch.{float16, BFloat16}` ([#53898](https://github.com/pytorch/pytorch/pull/53898), [#52582](https://github.com/pytorch/pytorch/pull/52582), [#51244](https://github.com/pytorch/pytorch/pull/51244), [#52579](https://github.com/pytorch/pytorch/pull/52579)).\r\n* Added `torch.dot` for `BFloat16` on CUDA ([#57903](https://github.com/pytorch/pytorch/pull/57903)).\r\n* Added support for tensor inputs for `min` and `max` arguments in `torch.clamp` ([#52695](https://github.com/pytorch/pytorch/pull/52695), [#56367](https://github.com/pytorch/pytorch/pull/56367)).\r\n* Added a new `torch.special` namespace similar to `scipy.special` ([#52296](https://github.com/pytorch/pytorch/pull/52296)).\r\n    * Added special.{`entr` ([#53500](https://github.com/pytorch/pytorch/pull/53500)),  `xlog1py` ([#55138](https://github.com/pytorch/pytorch/pull/55138)), `i0e` ([#54409](https://github.com/pytorch/pytorch/pull/54409)), `erfc`, `erfinv` ([#53260](https://github.com/pytorch/pytorch/pull/53260))}.\r\n    * Added aliases for `special.{expm1, exp2}` ([#54670](https://github.com/pytorch/pytorch/pull/54670)).\r\n    * Added aliases for `special.{sigmoid, logit}` ([#54759](https://github.com/pytorch/pytorch/pull/54759)).\r\n* Added the following new operators in PyTorch similar to those in NumPy:\r\n    * `torch.gradient` ([#54617](https://github.com/pytorch/pytorch/pull/54617))\r\n    * `torch.{hsplit, vsplit, dsplit}` ([#53536](https://github.com/pytorch/pytorch/pull/53536))\r\n    * `torch.positive` ([#55891](https://github.com/pytorch/pytorch/pull/55891))\r\n    * `torch.frexp` ([#51097](https://github.com/pytorch/pytorch/pull/51097))\r\n    * `torch.take_along_dim` ([#52833](https://github.com/pytorch/pytorch/pull/52833))\r\n* Added a new keyword argument `alpha` to `torch.index_add` ([#54176](https://github.com/pytorch/pytorch/pull/54176)).\r\n* Added `torch.assert_async` ([#53086](https://github.com/pytorch/pytorch/pull/53086))\r\n* Added a new keyword argument `interpolation` to `torch.quantile` ([#49267](https://github.com/pytorch/pytorch/pull/49267)).\r\n* Add correction parameter to std/var ([#50903](https://github.com/pytorch/pytorch/pull/50903))\r\n* Added overloads for `torch.{std, var, std_mean, var_mean}` with a correction argument specifying the difference between the sample size and number of degrees of freedom. \r\n* Add support for integer type for `torch.`{`logit, rad2deg, deg2rad, polygamma}` ([#52028](https://github.com/pytorch/pytorch/pull/52028), [#51853,](https://github.com/pytorch/pytorch/pull/51853)[#57462](https://github.com/pytorch/pytorch/pull/57462))\r\n* Added support for stable sort algorithm on CPU by a new kwarg `stable` ([#51790](https://github.com/pytorch/pytorch/pull/51790)).\r\n* The `torch.linalg` module, analogous to NumPy\u2019s linalg module but with several additional functions, is stable! Added `torch.linalg.{multi_dot, lstsq, vector_norm, matrix_norm, matrix_power, det, eig, eigvals, svdvals, cholesky_ex, inv_ex}` ([#51807](https://github.com/pytorch/pytorch/pull/51807), [#49093](https://github.com/pytorch/pytorch/pull/49093), [#51099](https://github.com/pytorch/pytorch/pull/51099), [#57127](https://github.com/pytorch/pytorch/pull/57127), [#52608](https://github.com/pytorch/pytorch/pull/52608), [#53119](https://github.com/pytorch/pytorch/pull/53119), [#52491](https://github.com/pytorch/pytorch/pull/52491), [#56684](https://github.com/pytorch/pytorch/pull/56684), [#56724](https://github.com/pytorch/pytorch/pull/56724), [#58039](https://github.com/pytorch/pytorch/pull/58039)).\r\n* Added a new `device=meta` API ([#53143](https://github.com/pytorch/pytorch/pull/53143))\r\n    * \u201cmeta\u201d is a new device, like CPU/CUDA, that doesn\u2019t allocate any memory for data. Operators that are passed meta tensor inputs will perform shape inference, without running the actually kernel computation. For example, `torch.ones(2, device='meta') + torch.ones(1, 2, device='meta')` will return a new meta tensor of size `[1, 2]` (performing broadcasting), without allocating memory or running an actual kernel.\r\n    * `device=meta` API is implemented for `upsample_linear1d`([#51917](https://github.com/pytorch/pytorch/pull/51917)), `upsample_bilinear2d` and `upsample_bicubic2d` ([#52012](https://github.com/pytorch/pytorch/pull/52012)), `upsample_nearest3d` ([#52065](https://github.com/pytorch/pytorch/pull/52065)), `sin`([#52277](https://github.com/pytorch/pytorch/pull/52277)), `mul`([#52692](https://github.com/pytorch/pytorch/pull/52692)), `pow`([#53669](https://github.com/pytorch/pytorch/pull/53669)), `sub`([#53679](https://github.com/pytorch/pytorch/pull/53679)), `div`([#53680](https://github.com/pytorch/pytorch/pull/53680)), `copysign`([#55040](https://github.com/pytorch/pytorch/pull/55040)), `atan2`([#55130](https://github.com/pytorch/pytorch/pull/55130)), `sinh`([#55538](https://github.com/pytorch/pytorch/pull/55538)), `acosh`([#55540](https://github.com/pytorch/pytorch/pull/55540)), `cosh`([#55563](https://github.com/pytorch/pytorch/pull/55563)), `cos` ([#55564](https://github.com/pytorch/pytorch/pull/55564)), `replication_padding1d` ([#55481](https://github.com/pytorch/pytorch/pull/55481)), `replication_padding3d` ([#55499](https://github.com/pytorch/pytorch/pull/55499)), `replication_pad1d_backward` ([#55537](https://github.com/pytorch/pytorch/pull/55537)), `fractional_max_pool2d` ([#55581](https://github.com/pytorch/pytorch/pull/55581)), `reflection_pad1d` ([#55531](https://github.com/pytorch/pytorch/pull/55531)), `replication_pad2d` ([#55511](https://github.com/pytorch/pytorch/pull/55511)), `addmv` ([#55746](https://github.com/pytorch/pytorch/pull/55746)), all unary float functions ([#56082](https://github.com/pytorch/pytorch/pull/56082)), `adaptive_max_pool2d`([#56317](https://github.com/pytorch/pytorch/pull/56317)), `adaptive_max_pool3d` ([#56320](https://github.com/pytorch/pytorch/pull/56320)), all non-float unary operators (and `rsqrt`) ([#56151](https://github.com/pytorch/pytorch/pull/56151)), `adaptive_max_pool2d_backward` ([#56799](https://github.com/pytorch/pytorch/pull/56799)), `adaptive_max_pool3d_backward` ([#56800](https://github.com/pytorch/pytorch/pull/56800)), `neg`([#57212](https://github.com/pytorch/pytorch/pull/57212)), `max_pool2d_with_indices`([#56459](https://github.com/pytorch/pytorch/pull/56459)), `trunc` ([#57350](https://github.com/pytorch/pytorch/pull/57350)), `floor` ([#57587](https://github.com/pytorch/pytorch/pull/57587)), `sign` ([#57588](https://github.com/pytorch/pytorch/pull/57588)), `ceil` ([#57589](https://github.com/pytorch/pytorch/pull/57589)), `gcd` ([#57624](https://github.com/pytorch/pytorch/pull/57624)), `nextafter` ([#57625](https://github.com/pytorch/pytorch/pull/57625)), `igamma` and `igammac`([#57626](https://github.com/pytorch/pytorch/pull/57626)), `hypot`([#57627](https://github.com/pytorch/pytorch/pull/57627)), `lcm` ([#57628](https://github.com/pytorch/pytorch/pull/57628)), `logaddexp` and `logaddexp2` ([#57629](https://github.com/pytorch/pytorch/pull/57629)), `maximum` and `minimum` ([#57630](https://github.com/pytorch/pytorch/pull/57630)), `topk` ([#57790](https://github.com/pytorch/pytorch/pull/57790)), `max_pool2d_with_indices_backward` ([#57797](https://github.com/pytorch/pytorch/pull/57797)), `threshold` ([#57810](https://github.com/pytorch/pytorch/pull/57810)), `addmm` ([#57417](https://github.com/pytorch/pytorch/pull/57417)), `heaviside` ([#57933](https://github.com/pytorch/pytorch/pull/57933)), `elu`([#57619](https://github.com/pytorch/pytorch/pull/57619)), `softplus` ([#57620](https://github.com/pytorch/pytorch/pull/57620)), `leaky_relu` ([#57621](https://github.com/pytorch/pytorch/pull/57621)), `hardsigmoid` ([#57622](https://github.com/pytorch/pytorch/pull/57622)), `softshrink` ([#57623](https://github.com/pytorch/pytorch/pull/57623)), `silu` ([#58050](https://github.com/pytorch/pytorch/pull/58050)), `empty_strided` ([#53397](https://github.com/pytorch/pytorch/pull/53397)), non-composite in-place operators ([#54901](https://github.com/pytorch/pytorch/pull/54901))\r\n\r\n### Complex Numbers\r\n\r\n* Added complex autograd support for `torch.{masked_fill, polar, cumsum, lerp, prod, rsub, unfold, symeig, index_copy}` ([#52483](https://github.com/pytorch/pytorch/pull/52483), [#52488](https://github.com/pytorch/pytorch/pull/52488), [#53240](https://github.com/pytorch/pytorch/pull/53240), [#53689](https://github.com/pytorch/pytorch/pull/53689), [#48125](https://github.com/pytorch/pytorch/pull/48125), [#53702](https://github.com/pytorch/pytorch/pull/53702), [#52999](https://github.com/pytorch/pytorch/pull/52999), [#55085](https://github.com/pytorch/pytorch/pull/55085), [#52203](https://github.com/pytorch/pytorch/pull/52203)).\r\n* Added complex support for torch.lerp ([#54129](https://github.com/pytorch/pytorch/pull/54129)) and torch.sigmoid ([#55975](https://github.com/pytorch/pytorch/pull/55975)) on CUDA. \r\n* Added complex support for `torch.index_copy` and `torch.{take}` and `torch.Tensor.put_` on both CPU and CUDA ([#52203](https://github.com/pytorch/pytorch/pull/52203), [#53356](https://github.com/pytorch/pytorch/pull/53356)).\r\n* Added complex support to TorchScript.\r\n    * Added logic to teach TorchScript frontend to parse complex literals, and complex lists. ([#52881](https://github.com/pytorch/pytorch/pull/52881)).\r\n    * Added TorchScript support for:\r\n        *  complex constructor and `torch.{add, mul, sub, as_tensor}` ([#52881](https://github.com/pytorch/pytorch/pull/52881)).\r\n        * `cmath` unary ops: `cmath.{phase, log, log10, sqrt, exp, sin, cos, tan, asin, acos, atan, sinh, cosh, tanh, asinh, acosh, atanh}` ([#54089](https://github.com/pytorch/pytorch/pull/54089)).\r\n        * `cmath.`{`infj, nanj}` ([#54328](https://github.com/pytorch/pytorch/pull/54328)).\r\n        *  `cmath.{isinf, isnan, isfinite, rect}` ([#54541](https://github.com/pytorch/pytorch/pull/54541)).\r\n        * real and imag tensor attributes (`tensor.real/imag`) ([#54692](https://github.com/pytorch/pytorch/pull/54692)).\r\n    * Fixed `test_variant_consistency_jit_addmm` for complex types ([#54917](https://github.com/pytorch/pytorch/pull/54917), [#57129](https://github.com/pytorch/pytorch/pull/57129)).\r\n* Added initial operator support for sparse complex tensors ([#57125](https://github.com/pytorch/pytorch/pull/57125)).\r\n    * Added complex support for `torch.{sparse_coo_tensor, coalesce, to_dense, to_sparse, sparse_add, sspaddmm, saddmm}`.\r\n* Added `torch.Tensor.{cfloat, cdouble}` functions ([#58137](https://github.com/pytorch/pytorch/pull/58137)).\r\n* Added complex support for all reductions for `torch.{std, var}` to return a real valued output tensor for complex inputs ([#58066](https://github.com/pytorch/pytorch/pull/58066)) .\r\n* Updated autograd formulas for many linear algebra operations support complex tensors:\r\n    * `eig`: faster and with complex support ([#52875](https://github.com/pytorch/pytorch/pull/52875))\r\n    * `lu`: more numerically stable and with complex support. ([#53994](https://github.com/pytorch/pytorch/pull/53994))\r\n\r\n### torch.nn\r\n\r\n* New `torch.nn` modules: `nn.LazyBatchNorm*d` ([#51862](https://github.com/pytorch/pytorch/pull/51862)), `nn.HuberLoss` ([#50553](https://github.com/pytorch/pytorch/pull/50553)), `nn.Mish` ([#58375](https://github.com/pytorch/pytorch/issues/58375)).\r\n* New parametrization functionality ([#33344](https://github.com/pytorch/pytorch/pull/33344), [#58142](https://github.com/pytorch/pytorch/pull/58142), [#55456](https://github.com/pytorch/pytorch/pull/55456), [#57784](https://github.com/pytorch/pytorch/pull/57784)).\r\n* `nn.Conv*d`: Added `padding='same'` mode for non-strided convolutions ([#45667](https://github.com/pytorch/pytorch/pull/45667)).\r\n* `nn.EmbeddingBag`: Added `padding_idx` support ([#49237](https://github.com/pytorch/pytorch/pull/49237), [#56065](https://github.com/pytorch/pytorch/pull/56065), [#56618](https://github.com/pytorch/pytorch/pull/56618)).\r\n* Added mish activation function ([#58648](https://github.com/pytorch/pytorch/pull/58648)).\r\n* [memory format] Added channels last support for `MaxPool2d` ([#56361](https://github.com/pytorch/pytorch/pull/56361)).\r\n* Added the option to build PyTorch with DNNL + AMD BLIS path ([#54953](https://github.com/pytorch/pytorch/pull/54953)).\r\n\r\n### Profiler\r\n\r\n* Added `skip_first` parameter to the default schedule ([#58025](https://github.com/pytorch/pytorch/pull/58025)).\r\n* Added support for trace metadata ([#56575](https://github.com/pytorch/pytorch/pull/56575)).\r\n* Added `gzip` format support for chrome tracing ([#56554](https://github.com/pytorch/pytorch/pull/56554)).\r\n* Added `sequenceNr` and `fwdThreadId` to the trace ([#57182](https://github.com/pytorch/pytorch/pull/57182)).\r\n* Enabled Kineto in CPU builds ([#53174](https://github.com/pytorch/pytorch/pull/53174)).\r\n\r\n### Autograd\r\n\r\n* Added new inference mode both in C++ ([](https://github.com/pytorch/pytorch/pull/58045)[#54403](https://github.com/pytorch/pytorch/pull/54403), [#53343](https://github.com/pytorch/pytorch/pull/53343)) and python ([#58045](https://github.com/pytorch/pytorch/pull/58045), [#57480](https://github.com/pytorch/pytorch/pull/57480)).\r\n* Added `fast_mode` argument to `autograd.gradcheck` ([#54480](https://github.com/pytorch/pytorch/pull/54480)).\r\n* Added support for non-Tensor inputs and outputs to `torch.utils.checkpoint` functions ([#52422](https://github.com/pytorch/pytorch/pull/52422)).\r\n\r\n### Dataloader\r\n\r\n* Implemented `FilterIterDataPipe` ([#51783](https://github.com/pytorch/pytorch/pull/51783)).\r\n* Added context manager for runtime type validation ([#55936](https://github.com/pytorch/pytorch/pull/55936)).\r\n* Added typing enforcement for `DataPipe` at construct-time ([#54066](https://github.com/pytorch/pytorch/pull/54066)).\r\n* Added typing Enforcement for `DataPipe` at runtime ([#54544](https://github.com/pytorch/pytorch/pull/54544)).\r\n* Implemented `issubtype` for `DataLoader` type hints ([#54299](https://github.com/pytorch/pytorch/pull/54299)).\r\n* Added type hint for SequentialSampler ([#56374](https://github.com/pytorch/pytorch/pull/56374)).\r\n* Added `ConcatDataPipe` ([#53301](https://github.com/pytorch/pytorch/pull/53301)).\r\n* Introduced deterministic context to `DataLoader` ([#53271](https://github.com/pytorch/pytorch/pull/53271)).\r\n* Added `ZipIterDataPipe` ([#53554](https://github.com/pytorch/pytorch/pull/53554)).\r\n* Added switch to guaranteed determinism & add option to non_deterministic ([#53532](https://github.com/pytorch/pytorch/pull/53532)).\r\n* Added `TransformsIterDataPipe` ([#52604](https://github.com/pytorch/pytorch/pull/52604)).\r\n* Renamed Callable to `MapIterDataPipe` ([#51879](https://github.com/pytorch/pytorch/pull/51879)).\r\n\r\n### CUDA\r\n\r\n* Added the following new features to CUDA Graphs:\r\n    *  Private mempools ([#54038](https://github.com/pytorch/pytorch/pull/54038))\r\n    * Support for RNN capture when cuDNN dropout is used ([#57373](https://github.com/pytorch/pytorch/pull/57373)).\r\n* Added support for `'max'` reduction for `torch.segment_reduce` ([#56704](https://github.com/pytorch/pytorch/pull/56704)).\r\n* Added support for CUDA allocator to handle multiple streams seamlessly ([#55860](https://github.com/pytorch/pytorch/pull/55860)).\r\n\r\n### C++ API\r\n\r\n* Added `torch::nn::functional::huber_loss` ([#50553](https://github.com/pytorch/pytorch/pull/50553)).\r\n* Added learning rate schedulers to C++ API ([#52268](https://github.com/pytorch/pytorch/pull/52268)).\r\n* Added `padding='same'` mode to `torch::conv{1,2,3}d` ([#45667](https://github.com/pytorch/pytorch/pull/45667)).\r\n* Added `padding_idx` argument to `EmbeddingBag` ([#49237](https://github.com/pytorch/pytorch/pull/49237)).\r\n* Added mish activation function ([#58648](https://github.com/pytorch/pytorch/pull/58648)) ([#58940](https://github.com/pytorch/pytorch/pull/58940)).\r\n\r\n### TorchScript\r\n\r\n* Added reductions to NNC python bindings ([#52492](https://github.com/pytorch/pytorch/pull/52492)).\r\n* Added Python bindings for ExternalCalls. ([#52905](https://github.com/pytorch/pytorch/pull/52905)).\r\n* Added an API to reorder multiple loops ([#55568](https://github.com/pytorch/pytorch/pull/55568)).\r\n* Added NNC support for `pow` on CPU ([#56308](https://github.com/pytorch/pytorch/pull/56308)).\r\n* Enabled horizontal fusion of all loops ([#56324](https://github.com/pytorch/pytorch/pull/56324)).\r\n* Added an API for Buffer Compression ([#55853](https://github.com/pytorch/pytorch/pull/55853)).\r\n* Added API to distribute loops ([#53865](https://github.com/pytorch/pytorch/pull/53865)).\r\n* Added `matmul` for NNC lowering/unified dtypes ([#56456](https://github.com/pytorch/pytorch/pull/56456)).\r\n* Added a method to compute `conv` without bias ([#57512](https://github.com/pytorch/pytorch/pull/57512)).\r\n* Added support for computing `conv` with dynamic shapes ([#57514](https://github.com/pytorch/pytorch/pull/57514)).\r\n* Added NNC lowerings for `t`/`transpose`/`permute`/`expand` ([#57426](https://github.com/pytorch/pytorch/pull/57426)).\r\n* Updated external functions for mobile build ([#56850](https://github.com/pytorch/pytorch/pull/56850)).\r\n* Added `GELU` To NNC ([#57753](https://github.com/pytorch/pytorch/pull/57753)).\r\n* Implemented `GELU` Backward ([#58249](https://github.com/pytorch/pytorch/pull/58249)).\r\n* Added a mobile NNC backend skeleton ([#56852](https://github.com/pytorch/pytorch/pull/56852)).\r\n* Added support for `torch.type` ([#51904](https://github.com/pytorch/pytorch/pull/51904))\r\n* Added `dict()` constructor ([#51934](https://github.com/pytorch/pytorch/pull/51934)).\r\n* Added a new `torch::deploy` to manage multiple python interpreters in a single\r\n    process to deploy PyTorch models packaged with torch.package ([#51754](https://github.com/pytorch/pytorch/pull/51754)).\r\n* Reintroduced static dispatch ([#51957](https://github.com/pytorch/pytorch/pull/51957)).\r\n* Added TS support for `torch.any` ([#52360](https://github.com/pytorch/pytorch/pull/52360)).\r\n* Added a demo backend with compiler ([#52603](https://github.com/pytorch/pytorch/pull/52603)).\r\n* Added MKLDNN fuser ([#51600](https://github.com/pytorch/pytorch/pull/51600)).\r\n* Added a context manager for hiding source ranges ([#53188](https://github.com/pytorch/pytorch/pull/53188)).\r\n* Implemented `embedding_bag` for SR ([#52429](https://github.com/pytorch/pytorch/pull/52429)).\r\n* Allowed the use of `AliasDb` in Python ([#51336](https://github.com/pytorch/pytorch/pull/51336)).\r\n* Added support for `DictConstruct` ([#54438](https://github.com/pytorch/pytorch/pull/54438))\r\n* Added `sliceHead`/`sliceTail` APIs with short parameter list ([#55115](https://github.com/pytorch/pytorch/pull/55115)).\r\n* Added logic to infer argument types in TorchScript ([#56832](https://github.com/pytorch/pytorch/pull/56832)).\r\n* Added support for  custom Python classes in `CUDAFuture` ([#56516](https://github.com/pytorch/pytorch/pull/56516)).\r\n* Added a concat optimization pass ([#55474](https://github.com/pytorch/pytorch/pull/55474)).\r\n* Added initial support for PEP-585 types ([#57363](https://github.com/pytorch/pytorch/pull/57363)).\r\n* Added logic to infer types for arguments of methods not invoked directly by `MonkeyType` ([#57202](https://github.com/pytorch/pytorch/pull/57202)).\r\n* Added support for `torch.jit.ignore` as a context manager ([#55172](https://github.com/pytorch/pytorch/pull/55172)).\r\n* Implemented `hardswish`/`hardsigmoid `on MKLDNN tensors ([#55218](https://github.com/pytorch/pytorch/pull/55218)).\r\n* Added `model_dump` tool for model inspection ([#56868](https://github.com/pytorch/pytorch/pull/56868))\r\n* Added static method support for TorchBind ([#51177](https://github.com/pytorch/pytorch/pull/51177))\r\n* Added TS support for `pow` ([#52374](https://github.com/pytorch/pytorch/pull/52374))\r\n* Added support for default argument values to `TorchBind` ([#51253](https://github.com/pytorch/pytorch/pull/51253)).\r\n* Added support for AST rewriting for submodules ([#52297](https://github.com/pytorch/pytorch/pull/52297)).\r\n* Added `optimize_for_inference` API ([#58193](https://github.com/pytorch/pytorch/pull/58193)).\r\n* Registered `aten::index_out` ([#51742](https://github.com/pytorch/pytorch/pull/51742)).\r\n* Added `PYTORCH_TENSOREXPR_DONT_FUSE` env variable to disable fusion on specified operators ([#55650](https://github.com/pytorch/pytorch/pull/55650)).\r\n\r\n### torch.package\r\n\r\n* Allow TorchScript models to be contained in the package format ([#54891,](https://github.com/pytorch/pytorch/pull/54891)[#56299,](https://github.com/pytorch/pytorch/pull/56299)[#54893](https://github.com/pytorch/pytorch/pull/54893), [#57573](https://github.com/pytorch/pytorch/pull/57573), [#54894](https://github.com/pytorch/pytorch/pull/54894), [#57678](https://github.com/pytorch/pytorch/pull/57678)).\r\n\r\n### Mobile\r\n\r\n* Added 8x1 block sparse kernels for ARM and AArch64 ([#51118](https://github.com/pytorch/pytorch/pull/51118), [#51119](https://github.com/pytorch/pytorch/pull/51119), [#51120](https://github.com/pytorch/pytorch/pull/51120)).\r\n* Made NNAPI converter handle binary ops combining NHWC+NCHW in some cases ([#48812](https://github.com/pytorch/pytorch/pull/48812)).\r\n* Improved support for multiple inputs and outputs in NNAPI ([#54697](https://github.com/pytorch/pytorch/pull/54697)).\r\n* Added flexible size support for NNAPI ([#54701](https://github.com/pytorch/pytorch/pull/54701)).\r\n* Added new ops for Metal (concat, mul/sub/div, transpose, view, reshape, mean, chunk, reflection_pad2d) ( [#53950](https://github.com/pytorch/pytorch/pull/53950), [#54107](https://github.com/pytorch/pytorch/pull/54107), [#54522](https://github.com/pytorch/pytorch/pull/54522), [#56073](https://github.com/pytorch/pytorch/pull/56073), [#56074](https://github.com/pytorch/pytorch/pull/56074), [#58263](https://github.com/pytorch/pytorch/pull/58263)).\r\n* Added python binding to use mobile cpu allocator ([#52323](https://github.com/pytorch/pytorch/pull/52323)).\r\n* Added lightweight RandomSampler for mobile ([#58201](https://github.com/pytorch/pytorch/pull/58201)).\r\n* Added support for:\r\n    * new ops to NNAPI converter (size, unsqueeze, cat, mean) ([#52026](https://github.com/pytorch/pytorch/pull/52026), [#48811](https://github.com/pytorch/pytorch/pull/48811)).\r\n    * multi-dimension tensors in Metal via MPSImage ([#54106](https://github.com/pytorch/pytorch/pull/54106)).\r\n    * multiple output tensors in Metal ([#56072](https://github.com/pytorch/pytorch/pull/56072)).\r\n    * methods other than forward in optimize_for_mobile  ([#53314](https://github.com/pytorch/pytorch/pull/53314)).\r\n    * ChannelsLast in TensorImageUtils on Android ([#48990](https://github.com/pytorch/pytorch/pull/48990)).\r\n    * loading \u201cextra files\u201d in Java/Android ([#55644](https://github.com/pytorch/pytorch/pull/55644)).\r\n    * loading \u201cextra files\u201d in Lite interpreter ([#52635](https://github.com/pytorch/pytorch/pull/52635)).\r\n    * querying bytecode version in Lite interpreter and bytecode models ([#56948](https://github.com/pytorch/pytorch/pull/56948), [#56948](https://github.com/pytorch/pytorch/pull/56948)).\r\n    * exporting some older bytecode versions for Lite interpreter ([#56802](https://github.com/pytorch/pytorch/pull/56802)).\r\n    * querying available ops ([#57570](https://github.com/pytorch/pytorch/pull/57570)).\r\n* Added SqueezeNet to PyTorch Playground (71d0b5632b).\r\n* Added libtorch lite build ([#51419](https://github.com/pytorch/pytorch/pull/51419)).\r\n\r\n### Distributed\r\n\r\n* `torch.distributed.Store`\r\n    * Added `compare_set` op ([#51815](https://github.com/pytorch/pytorch/pull/51815)).\r\n    * Added new `watchKey` method to register callbacks on a key ([#56217](https://github.com/pytorch/pytorch/pull/56217)).\r\n* `torch.distributed.rpc`\r\n    * Allowed passing `cpu` to CUDA RPC device maps ([#57019](https://github.com/pytorch/pytorch/pull/57019)).\r\n    *  Add a new `devices` argument to TensorPipe options to specify set of devices for TensorPipe ([#56405](https://github.com/pytorch/pytorch/pull/56405))\r\n* `DistributedDataParallel`\r\n    * Adds a flag to ddp `join` context manager that enables throwing an error across all ranks when this flag is specified ([#56755](https://github.com/pytorch/pytorch/pull/56755))\r\n    * Enable static graph training in DDP ([#55248](https://github.com/pytorch/pytorch/pull/55248), [#54995](https://github.com/pytorch/pytorch/pull/54995))\r\n    * Log unused parameter names in DDP when crashing due to unused parameters ([#55075](https://github.com/pytorch/pytorch/pull/55075))\r\n    * Introduce `torch.distributed.algorithms.default_hooks.fp16_compress_wrapper` wrapper that can be combined with other communication hooks ([#53808](https://github.com/pytorch/pytorch/pull/53808))\r\n    * Support loading a non-DP/DDP model from a DP/DDP state_dict ([#53224](https://github.com/pytorch/pytorch/pull/53224))\r\n    * Enhanced logging in DDP for performance metrics ([#52957](https://github.com/pytorch/pytorch/pull/52957), [#53145](https://github.com/pytorch/pytorch/pull/53145), [#54647](https://github.com/pytorch/pytorch/pull/54647))\r\n* `torch.distributed`\r\n    * Support `work.result` API for MPI backend ([#57168](https://github.com/pytorch/pytorch/pull/57168))\r\n    * Support `work.result` for `ProcessGroupGloo::AsyncWork` objects ([#57565](https://github.com/pytorch/pytorch/pull/57565))\r\n    * Support `work.get_future()` API for ProcessGroupMPI and ProcessGroupGloo [(](https://github.com/pytorch/pytorch/pull/57818)[#57818,](https://github.com/pytorch/pytorch/pull/57818)[#57214](https://github.com/pytorch/pytorch/pull/57214))\r\n    * New` torch.distributed.monitored_barrier` API (Gloo-only) ([#53773](https://github.com/pytorch/pytorch/pull/53773), [#53787](https://github.com/pytorch/pytorch/pull/53787), [#55009](https://github.com/pytorch/pytorch/pull/55009), [#55010](https://github.com/pytorch/pytorch/pull/55010), [#55197](https://github.com/pytorch/pytorch/pull/55197), [#55265](https://github.com/pytorch/pytorch/pull/55265), [#55989](https://github.com/pytorch/pytorch/pull/55989), [#55990](https://github.com/pytorch/pytorch/pull/55990))\r\n    * Allow passing `options` field to process group initialization APIs ([#53662](https://github.com/pytorch/pytorch/pull/53662), [#54090](https://github.com/pytorch/pytorch/pull/54090), [#53663](https://github.com/pytorch/pytorch/pull/53663))\r\n    * Enable profiling for distributed collectives ([#51822](https://github.com/pytorch/pytorch/pull/51822), , [#52004](https://github.com/pytorch/pytorch/pull/52004), [#52031](https://github.com/pytorch/pytorch/pull/52031), [#52949](https://github.com/pytorch/pytorch/pull/52949), [#55204](https://github.com/pytorch/pytorch/pull/55204), [#56412](https://github.com/pytorch/pytorch/pull/56412), [#56216](https://github.com/pytorch/pytorch/pull/56216), [#56427](https://github.com/pytorch/pytorch/pull/56427))\r\n    * Allow user to specify `TORCH_DISTRIBUTED_DEBUG `environment variable ([#52481](https://github.com/pytorch/pytorch/pull/52481))\r\n    * Added `compareSet` method for `torch.distributed.{HashStore, FileStore}` ([#53803](https://github.com/pytorch/pytorch/pull/53803)).\r\n* Added new `torch.distributed.elastic `module that upstreams `pytorch/elastic`\r\n    * Introduce RendezvousSettings ([#56537](https://github.com/pytorch/pytorch/pull/56537))\r\n    * Introduce a new from_backend static constructor for DynamicRendezvousHandler ([#57150](https://github.com/pytorch/pytorch/pull/57150))\r\n    * Introduce the implementation of DynamicRendezvousHandler ([#57151](https://github.com/pytorch/pytorch/pull/57151))\r\n    * add support for the new error file format ([#57084](https://github.com/pytorch/pytorch/pull/57084))\r\n    * Introduce the delay utility function ([#56533](https://github.com/pytorch/pytorch/pull/56533))\r\n    *  Make torchelastic launcher compatible with the caffe2.distributed.launch ([#55687](https://github.com/pytorch/pytorch/pull/55687))\r\n    * Introduce `PeriodicTimer` ([#55919](https://github.com/pytorch/pytorch/pull/55919))\r\n    * Introduce `DynamicRendezvousHandler` and `RendezvousBackend`. ([#55635](https://github.com/pytorch/pytorch/pull/55635))\r\n    * Introduce `C10dRendezvousBackend`. ([#55636](https://github.com/pytorch/pytorch/pull/55636))\r\n    * Introduce `EtcdRendezvousBackend`. ([#55637](https://github.com/pytorch/pytorch/pull/55637))\r\n    * Added `torch.distributed.elastic.launchers.api`, `torch.distributed.elastic.metrics`, `torch.distributed.events`, `torch.distributed.rendezvous`, `torch.distributed.elastic.agent` modules ([#55471](https://github.com/pytorch/pytorch/pull/55471), [#53870](https://github.com/pytorch/pytorch/pull/53870), [#53574](https://github.com/pytorch/pytorch/pull/53574), [#53760](https://github.com/pytorch/pytorch/pull/53760), [#53172](https://github.com/pytorch/pytorch/pull/53172), [#54343](https://github.com/pytorch/pytorch/pull/54343))\r\n    * Upstreamed timer and multiprocessing classes to `torch.distribute.elastic.timer` and `torch.distributed.elastic.multiprocessing` ([#53574](https://github.com/pytorch/pytorch/pull/53574))\r\n* `torch.distributed.nn.RemoteModule`: Enable RemoteModule to directly send GPU tensors over the wire on TensorPipe RPC backend if a device map is provided ([#57288](https://github.com/pytorch/pytorch/pull/57288))\r\n* `torch.distributed.optim`: \r\n    * Allow `torch.optim.Adamax`  to be used as a TorchScript functional optimizer in RPC ([#55833](https://github.com/pytorch/pytorch/pull/55833))\r\n    * Allow `torch.optim.Rprop` to be used as a TorchScript functional optimizer in RPC ([#55834](https://github.com/pytorch/pytorch/pull/55834))\r\n\r\n### torch.fx\r\n\r\n* Added `torch.fx.Node.format_node()` ([#51737](https://github.com/pytorch/pytorch/pull/51737)).\r\n* Added a `Transformer` to normalize args/kwargs of `torch.nn.functional` calls into only kwargs ([#51816](https://github.com/pytorch/pytorch/pull/51816)).\r\n* Added submodule manipulation APIs on `GraphModule` ([#52358](https://github.com/pytorch/pytorch/pull/52358)).\r\n* Added `Graph.eliminate_dead_code` ([#52658](https://github.com/pytorch/pytorch/pull/52658)).\r\n* Added a function to retrieve `inspect.Signature` instances for PyTorch operations ([#53830](https://github.com/pytorch/pytorch/pull/53830)).\r\n* Experimental type annotation pass using Python signatures ([#53831](https://github.com/pytorch/pytorch/pull/53831)).\r\n* Added a transformer to normalize `torch` namespace operations ([#53832](https://github.com/pytorch/pytorch/pull/53832)).\r\n* Extended `NormalizeArgs` to work on `torch` namespace operations ([#54236](https://github.com/pytorch/pytorch/pull/54236)).\r\n* Added FX `optimize_for_inference` for Intel CPUs ([#53805](https://github.com/pytorch/pytorch/pull/53805), [#58293](https://github.com/pytorch/pytorch/pull/58293)).\r\n* Added a metadata dict to `Node` and switch shape-prop to use that ([#54926](https://github.com/pytorch/pytorch/pull/54926)).\r\n* Added C-level monkey patching of `torch.randn` to capture it during tracing ([#54060](https://github.com/pytorch/pytorch/pull/54060)).\r\n* Added a new API replace_input_with to `Node` ([#55887](https://github.com/pytorch/pytorch/pull/55887)).\r\n* Added net splitter and net minimizer utilities ([#56201](https://github.com/pytorch/pytorch/pull/56201)).\r\n* Added PyTree support to FX through `concrete_args` ([#55888](https://github.com/pytorch/pytorch/pull/55888)).\r\n* Added support for proxy-able classes ([#56737](https://github.com/pytorch/pytorch/pull/56737)).\r\n\r\n### ONNX\r\n\r\n* Support onnxifi interface for set/get options ([#52388](https://github.com/pytorch/pytorch/pull/52388)).\r\n* Support --onnxifi_min_ops in AOT flow ([#52380](https://github.com/pytorch/pytorch/pull/52380)).\r\n* Redesign onnx pass to enable shape type dependent pattern conversion - cont ([#51795)](https://github.com/pytorch/pytorch/pull/51795) ([#53304)](https://github.com/pytorch/pytorch/pull/53304).\r\n* Support inplace operations on inplace indexing ([#52063)](https://github.com/pytorch/pytorch/pull/52063) ([#53306](https://github.com/pytorch/pytorch/pull/53306)).\r\n* Symbolic shape inference ([#51481](https://github.com/pytorch/pytorch/pull/51481)) ([#53307](https://github.com/pytorch/pytorch/pull/53307)).\r\n* Support repeat_interleave symbolic ([#52855](https://github.com/pytorch/pytorch/pull/52855)) ([#53312](https://github.com/pytorch/pytorch/pull/53312)).\r\n* Support primitive type input/outputs and attributes ([#53550](https://github.com/pytorch/pytorch/pull/53550)) ([#54864](https://github.com/pytorch/pytorch/pull/54864)).\r\n* Support outer export to onnx ([#53603](https://github.com/pytorch/pytorch/pull/53603)) ([#54869](https://github.com/pytorch/pytorch/pull/54869)).\r\n* Support hardsigmoid symbolic in opset 9 #49649 ([#54193](https://github.com/pytorch/pytorch/pull/54193)).\r\n* Support support for hann_window operator ([#54587](https://github.com/pytorch/pytorch/pull/54587)) ([#56163](https://github.com/pytorch/pytorch/pull/56163)).\r\n* Enable tensordot symbolic function ([#55654](https://github.com/pytorch/pytorch/pull/55654)) ([#56166](https://github.com/pytorch/pytorch/pull/56166)).\r\n* Support for prim::min ([#55259](https://github.com/pytorch/pytorch/pull/55259)) ([#56168](https://github.com/pytorch/pytorch/pull/56168)).\r\n* Support mv op ([#55470](https://github.com/pytorch/pytorch/pull/55470)) ([#56169](https://github.com/pytorch/pytorch/pull/56169)).\r\n* Support .item() export & NumberType to tensor conversion ([#55697](https://github.com/pytorch/pytorch/pull/55697)) ([#57594](https://github.com/pytorch/pytorch/pull/57594)).\r\n* Support a new operator for fill_() function ([#56859](https://github.com/pytorch/pytorch/pull/56859)) ([#57596](https://github.com/pytorch/pytorch/pull/57596)).\r\n* Support index_add_ function ([#56867](https://github.com/pytorch/pytorch/pull/56867)) ([#57830](https://github.com/pytorch/pytorch/pull/57830)).\r\n* Support tensor.to(device) ([#56857](https://github.com/pytorch/pytorch/pull/56857)) ([#57599](https://github.com/pytorch/pytorch/pull/57599)).\r\n* Support registering custom export for prim::PythonOp from torch.autograd.Function ([#55630](https://github.com/pytorch/pytorch/pull/55630)) ([#57600](https://github.com/pytorch/pytorch/pull/57600)).\r\n\r\n### Vulkan\r\n\r\n* Added the `hardswish` and `hardsigmoid` activation functions ([#53362](https://github.com/pytorch/pytorch/pull/53362)).\r\n* Added the `reflection_pad2d` op ([#53604](https://github.com/pytorch/pytorch/pull/53604)).\r\n* Added an implementation of Winograd convolutions ([#54639](https://github.com/pytorch/pytorch/pull/54639)).\r\n* Added the `sigmoid` activation function ([#57867](https://github.com/pytorch/pytorch/pull/57867)).\r\n\r\n### Misc\r\n\r\n* Android packages are now published to maven central ([#53568](https://github.com/pytorch/pytorch/pull/53568)).\r\n* Kineto is now supported on Windows ([#56323](https://github.com/pytorch/pytorch/pull/56323)).\r\n* Added a Gloo `TCP_TLS `transport ([#56442](https://github.com/pytorch/pytorch/pull/56442)).\r\n* Add ability to collect minidumps after the crash ([#59236](https://github.com/pytorch/pytorch/pull/59236)).\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Added nondeterministic alert for `index_put_` when `accumulate=False` ([#55827](https://github.com/pytorch/pytorch/pull/55827)).\r\n* Added deterministic path for `torch.index_add` on CUDA ([#56521](https://github.com/pytorch/pytorch/pull/56521)).\r\n* Added deterministic path for `torch.index_copy` on CPU ([#56900](https://github.com/pytorch/pytorch/pull/56900)).\r\n* Removed beta warning for use_deterministic_algorithms ([#58074](https://github.com/pytorch/pytorch/pull/58074))\r\n* Updated `torch.Tensor.unflatten` to be able to infer size value in `sizes` from -1 ([#51955](https://github.com/pytorch/pytorch/pull/51955)).\r\n* Added a safe cast and copy for `out=` input tensor for `torch.tensordot` ([#56286](https://github.com/pytorch/pytorch/pull/56286)).\r\n* Added cross-device check for `out` and `input` tensors for `torch.cat` ([#53004](https://github.com/pytorch/pytorch/pull/53004)).\r\n* Modified the order of asserts to correct the error message when nan appears in `torch.multinomial` on CUDA ([#53288](https://github.com/pytorch/pytorch/pull/53288)).\r\n* Converted a few more checks for unsupported device to raise `NotImplementedError` ([#53610](https://github.com/pytorch/pytorch/pull/53610)).\r\n* Made shared cache thread-safe for `torch.multiprocessing` ([#53750](https://github.com/pytorch/pytorch/pull/53750)).\r\n* Added support for `torch.int32` indices in `torch.repeat_interleave` ([#55102](https://github.com/pytorch/pytorch/pull/55102)).\r\n* Added a check to give a clear error message when a binary function is called for  non-complex inputs with complex valued alpha ([#54964](https://github.com/pytorch/pytorch/pull/54964)).\r\n* Propagate error message from `torch_shm_manager` when running `torch.multiprocessing` ([#57307](https://github.com/pytorch/pytorch/pull/57307), [#57310](https://github.com/pytorch/pytorch/pull/57310)).\r\n* Enabled deterministic path for `index_copy_cud`a with index_put ([#58144](https://github.com/pytorch/pytorch/pull/58144)).\r\n* Added support for uppercase letters in `torch.einsum` ([#56475](https://github.com/pytorch/pytorch/pull/56475)).\r\n* Added CUDA support for `torch.orgqr` ([#51348](https://github.com/pytorch/pytorch/pull/51348)) and  `torch.ormqr` ([#57316](https://github.com/pytorch/pytorch/pull/57316)).\r\n* Added support for batched as well as complex inputs for `torch.geqrf` on both CPU and CUDA ([#56249](https://github.com/pytorch/pytorch/pull/56249), [#56251](https://github.com/pytorch/pytorch/pull/56251)).\r\n\r\n### Complex Numbers\r\n\r\n* Fixed `torch.{linspace, logspace}` to correctly infer complex type and return a complex tensor when the `start` and (or) `end` values are complex numbers, and the `dtype` value is `None`  ([#38875](https://github.com/pytorch/pytorch/pull/38875)).\r\n\r\n### Autograd\r\n\r\n* Added support for single tensor in `inputs` argument for `.backward()` ([#53827](https://github.com/pytorch/pytorch/pull/53827)).\r\n* Added support for C++ optional arguments in autograd custom functions ([#54270](https://github.com/pytorch/pytorch/pull/54270)).\r\n* Added autograd support to `torch.orgqr` ([#52637](https://github.com/pytorch/pytorch/pull/52637)), `torch.segment_reduce` ([#56792](https://github.com/pytorch/pytorch/pull/56792)).\r\n* Added deterministic backward for `torch.gather` for `dim=1` ([#55573](https://github.com/pytorch/pytorch/pull/55573)).\r\n* Make detach return an alias even under inference mode ([#59633](https://github.com/pytorch/pytorch/pull/59633)).\r\n\r\n### torch.nn\r\n\r\n* Add 3D depthwise separable convolution ([#51027](https://github.com/pytorch/pytorch/pull/51027))\r\n* Make bias in lazy modules lazy and avoid creating empty tensors ([#52212](https://github.com/pytorch/pytorch/pull/52212)).\r\n* BFloat16: enable prepacked weights's inference ([#48922](https://github.com/pytorch/pytorch/pull/48922)).\r\n* Enable mkldnn conv2d backward to support mkldnn tensor input ([#48994](https://github.com/pytorch/pytorch/pull/48994)).\r\n* Add OneDNN pooling backward ([#49454](https://github.com/pytorch/pytorch/pull/49454)).\r\n* Add 64bit indexing support for softmax ([#52713](https://github.com/pytorch/pytorch/pull/52713)).\r\n* `nn.init._calculate_fan_in_and_fan_out`: Support usage with `__torch_function__` ([#53522](https://github.com/pytorch/pytorch/pull/53522)).\r\n* `nn.Transformer` / `nn.MultiheadAttention`: Add `batch_first` argument ([#55285](https://github.com/pytorch/pytorch/pull/55285)).\r\n* `nn.Transformer`: Add `layer_norm_eps` arg ([#54494](https://github.com/pytorch/pytorch/pull/54494)).\r\n* `nn.AvgPool2d`: Add channels_last support on CPU ([#48918](https://github.com/pytorch/pytorch/pull/48918)).\r\n* `clip_grad_norm_`: Add `error_if_nonfinite` flag ([#53843](https://github.com/pytorch/pytorch/pull/53843), [#55169](https://github.com/pytorch/pytorch/pull/55169)).\r\n* `Module.train`: Raise nicer error when called with invalid modes ([#58247](https://github.com/pytorch/pytorch/pull/58247)).\r\n* `nn.Linear`: Support 0 `in_features` ([#56505](https://github.com/pytorch/pytorch/pull/56505)).\r\n* `nn.EmbeddingBag`: Support mix of int32 and int64 offsets/indices ([#55189](https://github.com/pytorch/pytorch/pull/55189)).\r\n* `xnnpack::linear`: Handle 1D input ([#54986](https://github.com/pytorch/pytorch/pull/54986)).\r\n* `nn.Module`: Add `allow_duplicate` flag to `named_modules()` ([#54812](https://github.com/pytorch/pytorch/pull/54812)).\r\n* `nn.Module`: Add `to_empty()` function for moving to a device without copying storage ([#56610](https://github.com/pytorch/pytorch/pull/56610)).\r\n* Make `pad_sequence` callable from C++ API ([#57868](https://github.com/pytorch/pytorch/pull/57868)).\r\n\r\n### Dataloader\r\n\r\n* Added `generate_state` for NumPy seeding ([#56797](https://github.com/pytorch/pytorch/pull/56797)).\r\n* Modified construct_time_validation to argument_validation ([#55836](https://github.com/pytorch/pytorch/pull/55836)).\r\n* Added mode to `LoadFilesFromDisk` ([#57056](https://github.com/pytorch/pytorch/pull/57056)).\r\n* Added the ability to override *reduce_ex* function of `DataPipe` ([#52858](https://github.com/pytorch/pytorch/pull/52858)).\r\n* Added lambda support to `MapIterDataPipe` ([#52856](https://github.com/pytorch/pytorch/pull/52856)).\r\n* Added functional way of stacking DataPipes ([#52885](https://github.com/pytorch/pytorch/pull/52885)).\r\n\r\n### C++ API\r\n\r\n* Suppressed unsigned comparison warning ([#52653](https://github.com/pytorch/pytorch/pull/52653)).\r\n* Fixed constexpr **host** warning ([#52702](https://github.com/pytorch/pytorch/pull/52702)).\r\n* Introduced a fluent API to construct tensors from external data ([#54530](https://github.com/pytorch/pytorch/pull/54530)).\r\n\r\n### AMD\r\n\r\n* Allow PYTORCH_ROCM_ARCH in cpp_extension ([#54341](https://github.com/pytorch/pytorch/pull/54341)).\r\n* Added support for `torch.half` dtype RNNs with MIOpen ([#52475](https://github.com/pytorch/pytorch/pull/52475)).\r\n* Added support for the new `hiprtc` precompiler feature ([#54350](https://github.com/pytorch/pytorch/pull/54350)).\r\n* Improved reliability of `hipfft` and `rocfft` detection for ROCm build ([#53408](https://github.com/pytorch/pytorch/pull/53408)).\r\n\r\n### CUDA\r\n\r\n* Improved warning message when old GPU is detected ([#56621](https://github.com/pytorch/pytorch/pull/56621))\r\n* Made `torch.cuda.amp.GradScaler` scale updates in-place for better composability with graph capture ([#55562](https://github.com/pytorch/pytorch/pull/55562)).\r\n* Add `USE_MAGMA` build flag ([#55994](https://github.com/pytorch/pytorch/pull/55994)).\r\n* Change link order for BUILD_SPLIT_CUDA option ([#58437](https://github.com/pytorch/pytorch/pull/58437)).\r\n* Improve CUDA-11.X binary builds ([#58459](https://l.workplace.com/l.php?u=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F58459&h=AT1iY5lImKBK5tsZFPG9Ub57qOFMix4DslZFPlHNwT13OJnRq6Tvh_HehGQ-k4GF2bUDNhIQHBS578V8RQ-Sk2YYUv4Cys6KDIZPsunP7HzrwcYtfEZnPczt41cqT0KEIuvTSa1ZiOZ4SvvQV9F2xiYZ4cCJ7WIl2URyvg)).\r\n* Move CUDA async warning to suffix ([#59467](https://github.com/pytorch/pytorch/pull/59467)).\r\n\r\n### torch.fx\r\n\r\n* Make `torch.fx.map_arg` require a callable ([#51907](https://github.com/pytorch/pytorch/pull/51907)).\r\n* Generalize dict key check in `torch.fx.Tracer.create_arg` ([#51927](https://github.com/pytorch/pytorch/pull/51927)).\r\n* Customize traceback for calls to symbolically-traced code ([#51648](https://github.com/pytorch/pytorch/pull/51648)).\r\n* Allow `Transformer` to accept output result that is not Proxy ([#52473](https://github.com/pytorch/pytorch/pull/52473)).\r\n* Make `TracerBase._find_user_frame` private ([#53654](https://github.com/pytorch/pytorch/pull/53654)).\r\n* Improve buffer registration during `GraphModule` init ([#53444](https://github.com/pytorch/pytorch/pull/53444)).\r\n* Garbage collect values in `Interpreter` ([#54726](https://github.com/pytorch/pytorch/pull/54726)).\r\n* Improve placeholder matching in subgraph rewriter ([#54958](https://github.com/pytorch/pytorch/pull/54958)).\r\n* Record `stride` on `Node` during `ShapeProp` pass ([#55108](https://github.com/pytorch/pytorch/pull/55108)).\r\n* Record `memory_format` on `Node` during `ShapeProp` pass ([#55815](https://github.com/pytorch/pytorch/pull/55815)).\r\n* Put tensor metadata into a `NamedTuple` in `ShapeProp` ([#55930](https://github.com/pytorch/pytorch/pull/55930)).\r\n* Preserve node meta info in `split_module` ([#56212](https://github.com/pytorch/pytorch/pull/56212)).\r\n* Make `shape_prop` handle targets with aggregate outputs ([#56221](https://github.com/pytorch/pytorch/pull/56221)).\r\n* Make arg normalization a method on `Node` and not a pass (also augment tests to be exhaustive) ([#55992](https://github.com/pytorch/pytorch/pull/55992)).\r\n* Allow for args to be left as args in NormalizeArgs ([#55995](https://github.com/pytorch/pytorch/pull/55995)).\r\n* Maintain submodule references during subgraph rewriting ([#55463](https://github.com/pytorch/pytorch/pull/55463)).\r\n* Changes in order to move `PythonKey` out of tree ([#57427](https://github.com/pytorch/pytorch/pull/57427)).\r\n* Handle cases in `GraphDrawer` when shape, type or stride are not present ([#57845](https://github.com/pytorch/pytorch/pull/57845)).\r\n* Handle the case when output consumes `get_attr` directly in `split_by_tags` ([#57844](https://github.com/pytorch/pytorch/pull/57844)).\r\n* Let submodules be collected as `args/kwargs`  in symbolic tracing([#57840](https://github.com/pytorch/pytorch/pull/57840)).\r\n\r\n### Profiler\r\n\r\n* Expanded Kineto platform support ([#56323](https://github.com/pytorch/pytorch/pull/56323)).\r\n* Added profiler fallback ([#57612](https://github.com/pytorch/pytorch/pull/57612)).\r\n* Added CUDA event fallback ([#58133](https://github.com/pytorch/pytorch/pull/58133)).\r\n\r\n### TorchScript\r\n\r\n* Added a flag to enable CPU fusion in benchmarks ([#48612](https://github.com/pytorch/pytorch/pull/48612)).\r\n* Updated fusion to handle loops that have the same bounds as expressions ([#55997](https://github.com/pytorch/pytorch/pull/55997)).\r\n* Updated normalization transformation to be in-place ([#56158](https://github.com/pytorch/pytorch/pull/56158)).\r\n* Added check to only lower float `conv2d`s ([#56289](https://github.com/pytorch/pytorch/pull/56289)).\r\n* Added more python bindings for loopnest ([#56213](https://github.com/pytorch/pytorch/pull/56213)).\r\n* Updated `fuseLoops` API to return bool flag and not throw any exceptions ([#56353](https://github.com/pytorch/pytorch/pull/56353)).\r\n* Added `unroll` and `flatten` APIs which do not require return stmt pointer ([#56420](https://github.com/pytorch/pytorch/pull/56420)).\r\n* Updated `Buf` on mutation instead of creating a new one ([#57513](https://github.com/pytorch/pytorch/pull/57513)).\r\n* Updated `flatten` transformation to be in-place ([#56629](https://github.com/pytorch/pytorch/pull/56629)).\r\n* Added missing python bindings for NNC Stmts ([#55570](https://github.com/pytorch/pytorch/pull/55570)).\r\n* Allowed backend preprocessing to take place outside of the backend interface ([#51757](https://github.com/pytorch/pytorch/pull/51757))\r\n* Added an error message for the case when `with` item is not an object ([#52335](https://github.com/pytorch/pytorch/pull/52335)).\r\n* Enabled `ModuleList` non-literal indexing ([#53410](https://github.com/pytorch/pytorch/pull/53410)).\r\n* Added recursive scripting for class type module attributes ([#55124](https://github.com/pytorch/pytorch/pull/55124)).\r\n* Added support for `mypy` ignore annotation with particular rule specified ([#51675](https://github.com/pytorch/pytorch/pull/51675)).\r\n* Added support for comparing two bool variables ([#51844](https://github.com/pytorch/pytorch/pull/51844)).\r\n* Added MKLDNN GELU function ([#53615](https://github.com/pytorch/pytorch/pull/53615)).\r\n* Added `hardtanh(0,6)` to the set of MKLDNN fusible ops for mobilenetv2 ([#56203](https://github.com/pytorch/pytorch/pull/56203)).\r\n* Captured argument names for traced functions and modules ([#51775](https://github.com/pytorch/pytorch/pull/51775)).\r\n* Improved `has_bf16_support` ([#57408](https://github.com/pytorch/pytorch/pull/57408)).\r\n* Walk Python AST to check for unsupported attribute type annotations ([#51805](https://github.com/pytorch/pytorch/pull/51805)).\r\n* Added `out` version for sum ([#52225](https://github.com/pytorch/pytorch/pull/52225))\r\n* Added logic to trace `torch.nn.Linear` as `aten::linear` ([#51897](https://github.com/pytorch/pytorch/pull/51897)).\r\n* Made `is_tracing` scriptable ([#49853](https://github.com/pytorch/pytorch/pull/49853)).\r\n* Added support for builtin `sum` ([#52188](https://github.com/pytorch/pytorch/pull/52188)).\r\n* Fused `clip_ranges` and `gather_ranges` ([#52461](https://github.com/pytorch/pytorch/pull/52461)).\r\n* Added support for features from `to_backend` for the Lite Interpreter ([#52870](https://github.com/pytorch/pytorch/pull/52870)).\r\n* Added a filter to remove mutation ([#51923](https://github.com/pytorch/pytorch/pull/51923)).\r\n* Added logic functionalize ops which to be included in MKLDNN group ([#51924](https://github.com/pytorch/pytorch/pull/51924))\r\n* Extended subgraph utils to cover merging a node following a subgraph ([#52513](https://github.com/pytorch/pytorch/pull/52513))\r\n* Included max pool in fusion groups ([#52613](https://github.com/pytorch/pytorch/pull/52613)).\r\n* Registered both TupleConstruct and ListConstruct as out variants ([#52684](https://github.com/pytorch/pytorch/pull/52684)).\r\n* Added Alias analysis to Memory Management/Planning ([#50060](https://github.com/pytorch/pytorch/pull/50060)).\r\n* Included max pool in fusion groups ([#52613](https://github.com/pytorch/pytorch/pull/52613)).\r\n* Added property binding in TorchBind ([#50670](https://github.com/pytorch/pytorch/pull/50670)).\r\n* Registered `pow` out variant ([#52454](https://github.com/pytorch/pytorch/pull/52454)).\r\n* Made `torch.load()` aware of import path changes ([#53139](https://github.com/pytorch/pytorch/pull/53139)).\r\n* Added `aten::to` copy out variant ([#52343](https://github.com/pytorch/pytorch/pull/52343)).\r\n* Added more variants to `create_empty_from` ([#53333](https://github.com/pytorch/pytorch/pull/53333)).\r\n* Added support for parsing Ellipsis in JIT frontend ([#53576](https://github.com/pytorch/pytorch/pull/53576)).\r\n* Added a bool `is_available()` method to the backend contract ([#53068](https://github.com/pytorch/pytorch/pull/53068)).\r\n* Added parallel support for the LLVM backend. ([#53243](https://github.com/pytorch/pytorch/pull/53243)) / Resubmit: Add parallel support for the LLVM backend. ([#54122](https://github.com/pytorch/pytorch/pull/54122)).\r\n* Rewrote `functional.tensordot` to be TorchScript-able ([#53672](https://github.com/pytorch/pytorch/pull/53672)).\r\n* Added python bindings for missing loop transformations in `LoopNest` ([#54355](https://github.com/pytorch/pytorch/pull/54355)).\r\n* Added support for list insertion for mutation removal ([#54271](https://github.com/pytorch/pytorch/pull/54271)).\r\n* Added support for  `torch.bfloat16` in the fuser ([#54571](https://github.com/pytorch/pytorch/pull/54571)).\r\n* Added some functions for manipulating MKLDNN tensors to TORCH_API ([#56954](https://github.com/pytorch/pytorch/pull/56954)).\r\n* Merged CUDA Streams and Events ([#53902](https://github.com/pytorch/pytorch/pull/53902)).\r\n* Added python bindings for `TensorExprKernel` ([#54450](https://github.com/pytorch/pytorch/pull/54450)).\r\n* Added support for dtype-specific tensor subclasses (e.g. LongTensor) ([#54817](https://github.com/pytorch/pytorch/pull/54817)).\r\n* Added support for tuple `add` operator ([#52292](https://github.com/pytorch/pytorch/pull/52292)).\r\n* Disambiguated error message for working with not fully refined tuple types ([#55745](https://github.com/pytorch/pytorch/pull/55745)).\r\n* Allowed unpacking tuple and assigning unpacked values to SELECT-type expressions ([#55268](https://github.com/pytorch/pytorch/pull/55268)).\r\n* Made NoneType `annotation_str` emit `NoneType` instead of `None` ([#54746](https://github.com/pytorch/pytorch/pull/54746)).\r\n* Added CUDA device synchronization support in JIT ([#55469](https://github.com/pytorch/pytorch/pull/55469)).\r\n* Added `optimize_graph_output_memory` flag ([#55811](https://github.com/pytorch/pytorch/pull/55811)).\r\n* Added support for refinement for `torch.jit.Future` ([#56148](https://github.com/pytorch/pytorch/pull/56148)).\r\n* Added implicit conversion from null tensor to `NoneType `([#55823](https://github.com/pytorch/pytorch/pull/55823)).\r\n* Added `aten::matmul`s to TE fuser ([#54605](https://github.com/pytorch/pytorch/pull/54605)).\r\n* Put explicit error message on class attribute accesses ([#55723](https://github.com/pytorch/pytorch/pull/55723)).\r\n* Added support for constant tensors in tensorexpr kernel ([#56319](https://github.com/pytorch/pytorch/pull/56319)).\r\n* Added native support for `aten::getitem` ([#55310](https://github.com/pytorch/pytorch/pull/55310)).\r\n* Added stricter check for function schemas with varargs ([#56509](https://github.com/pytorch/pytorch/pull/56509)).\r\n* Added graceful failure handling of DataPtr extraction in CUDAFuture ([#56511](https://github.com/pytorch/pytorch/pull/56511)).\r\n* Enabled forward/backward compatibility in TS mobile ([#56079](https://github.com/pytorch/pytorch/pull/56079)).\r\n* Added binding for `aten::clamp_min_out` ([#56635](https://github.com/pytorch/pytorch/pull/56635)), `aten::argmin_out` ([#56638](https://github.com/pytorch/pytorch/pull/56638)), and `aten::norm_out` ([#56636](https://github.com/pytorch/pytorch/pull/56636)).\r\n* Enhanced error message for `Future.setErrorIfNeeded` ([#56631](https://github.com/pytorch/pytorch/pull/56631)).\r\n* Added type inference support for `nn.Module `methods using PDT ([#57165](https://github.com/pytorch/pytorch/pull/57165)).\r\n* Disabled conv-add-relu fusion for cuDNN7 when model uses `torch.float16` ([#56579](https://github.com/pytorch/pytorch/pull/56579)).\r\n* Enabled conv-add-relu fusion as a part of frozen graph optimization ([#56580](https://github.com/pytorch/pytorch/pull/56580)).\r\n* Reduced inline autodiff threshold to enable the capture of smaller fusions ([#57062](https://github.com/pytorch/pytorch/pull/57062)).\r\n* Added static runtime support for `aten::matmul` ([#57291](https://github.com/pytorch/pytorch/pull/57291)).\r\n* Added `device()` method to `c10::Event` ([#57293](https://github.com/pytorch/pytorch/pull/57293)).\r\n* Added support for normalization of `is` op ([#57862](https://github.com/pytorch/pytorch/pull/57862)).\r\n* Enabled `cat` without conditionals iff CPU ([#58026](https://github.com/pytorch/pytorch/pull/58026)).\r\n* Added `LowerSimpleTuples` for freeze tuples ([#57915](https://github.com/pytorch/pytorch/pull/57915)).\r\n* Added support for striding for list slicing ([#49352](https://github.com/pytorch/pytorch/pull/49352)).\r\n* Wrapped `torch::deploy` API functions in safe rethrow macros ([#58192](https://github.com/pytorch/pytorch/pull/58192)).\r\n* Added binding for `aten::div_out` ([#56653](https://github.com/pytorch/pytorch/pull/56653))\r\n* Added binding for `aten::sub_out` ([#56656](https://github.com/pytorch/pytorch/pull/56656)).\r\n* Supported `clamp.Tensor `([#58191](https://github.com/pytorch/pytorch/pull/58191)).\r\n* Added an out version for `aten::repeat` ([#57683](https://github.com/pytorch/pytorch/pull/57683)).\r\n* Added default arguments to CUDA stream and events ([#53025](https://github.com/pytorch/pytorch/pull/53025)).\r\n* Added support for linear in MKLDNN fusion ([#51484](https://github.com/pytorch/pytorch/pull/51484)).\r\n* Handled MKLDNN broadcasting in MKLDNN fuser ([#51736](https://github.com/pytorch/pytorch/pull/51736)).\r\n* Added 0-dim support for binary MKLDNN ops ([#51921](https://github.com/pytorch/pytorch/pull/51921)).\r\n* Added OneDNN relu backward and reshape backward ([#49455](https://github.com/pytorch/pytorch/pull/49455)).\r\n* Added OneDNN batch_norm backward ([#50460](https://github.com/pytorch/pytorch/pull/50460)).\r\n* Added support for `hardshrink` ([#57749](https://github.com/pytorch/pytorch/pull/57749)).\r\n* Added non mutator bundled inputs method ([#58408](https://github.com/pytorch/pytorch/pull/58408)).\r\n* Added support to compare devices ([#53045](https://github.com/pytorch/pytorch/pull/53045)).\r\n* Added support for `memory_arg` in `aten::clone` ([#58100](https://github.com/pytorch/pytorch/pull/58100)).\r\n* Implemented `aten::cat` without conditionals ([#53128](https://github.com/pytorch/pytorch/pull/53128)).\r\n* Added external function bindings ([#53420](https://github.com/pytorch/pytorch/pull/53420)).\r\n* Added out variant of `sigrid_transforms_torch_bind` and `ListUnpack` ([#54761](https://github.com/pytorch/pytorch/pull/54761)).\r\n\r\n### torch.package\r\n\r\n* Added a reliable method for determining if a file is part of Python\u2019s standard library  ([#51694](https://github.com/pytorch/pytorch/pull/51694)).\r\n* Made package code more composable with other parts of PyTorch (package GraphModule, load non-code files from package) ([#51674](https://github.com/pytorch/pytorch/pull/51674), [#51976](https://github.com/pytorch/pytorch/pull/51976)).\r\n* Improved debugging facilities (allow_empty flag, zip file viewer, deny instruction, dependency tracing, query if object is from a package)  ([#53232,](https://github.com/pytorch/pytorch/pull/53232)[#53233](https://github.com/pytorch/pytorch/pull/53233), [#52176](https://github.com/pytorch/pytorch/pull/52176), [#55167](https://github.com/pytorch/pytorch/pull/55167), [#56190](https://github.com/pytorch/pytorch/pull/56190), [#56238](https://github.com/pytorch/pytorch/pull/56238), [#56729](https://github.com/pytorch/pytorch/pull/56729)).\r\n* Allow save_module to accept module as arg ([#55996](https://github.com/pytorch/pytorch/pull/55996)).\r\n* Follow dependencies created by `__import__` calls ([#55153](https://github.com/pytorch/pytorch/pull/55153)).\r\n* Added hooks to exporters\u2019 mock and extern calls to take action when a module is matched ([#58000](https://github.com/pytorch/pytorch/pull/58000))\r\n* Turn the default behavior of packaging into an \u2018intern\u2019 action so that it can be ordered with repeat to mock, extern, and deny actions ([#57341](https://github.com/pytorch/pytorch/pull/57341)).\r\n\r\n### Quantization\r\n\r\n* Added support for keeping output quantized for list and dict ([#56391](https://github.com/pytorch/pytorch/pull/56391)).\r\n* Added `torch.float16` and `torch.float64` support to `fake_quantize_per_channel` ([#56894](https://github.com/pytorch/pytorch/pull/56894)).\r\n* Support preserving attributes in deepcopy of observed/quantized graphmodule ([#56550](https://github.com/pytorch/pytorch/pull/56550)).\r\n* Added support for packed params in state_dict ([#51639](https://github.com/pytorch/pytorch/pull/51639)).\r\n* Added support for fusing `Conv3d + BatchNorm3d + ReLU` operations ([#50003](https://github.com/pytorch/pytorch/pull/50003)).\r\n* Change back to `multiple_outputs_gpu_kernel` for learnable fake per-channel quantization ([#52017](https://github.com/pytorch/pytorch/pull/52017)).\r\n* Added `torch.float16` and `torch.float32` support to `fake_quantize_per_tensor` ([#52612](https://github.com/pytorch/pytorch/pull/52612)).\r\n* Support batched embeddings for 8 Bit embedding bag quantization ([#55343](https://github.com/pytorch/pytorch/pull/55343)).\r\n* Expose nbins and ratio for `quantized::embedding_bag_4bit_prepack` ([#50398](https://github.com/pytorch/pytorch/pull/50398)).\r\n\r\n### Mobile\r\n\r\n* Removed caching of inflated bundled inputs ([#55181](https://github.com/pytorch/pytorch/pull/55181)).\r\n* Improved exception reporting for Lite interpreter ([#54284](https://github.com/pytorch/pytorch/pull/54284), [#55062](https://github.com/pytorch/pytorch/pull/55062), [#55252](https://github.com/pytorch/pytorch/pull/55252)).\r\n* Improved forward/backward compatibility in Lite interpreter when adding new optional arguments to ops ([#56845](https://github.com/pytorch/pytorch/pull/56845)).\r\n* Added model size to logged metadata when loading a Lite interpreter model ([#53578](https://github.com/pytorch/pytorch/pull/53578)).\r\n* Benchmarking binary speed_benchmark_torch now supports Lite interpreter ([#55402](https://github.com/pytorch/pytorch/pull/55402)).\r\n\r\n### Distributed\r\n\r\n`torch.distributed.Store`\r\n\r\n* Update `compare_set` for other Store implementations to be the same as `TCPStore`. ([#57175](https://github.com/pytorch/pytorch/pull/57175))\r\n* `torch.distributed.Store`: Expose C++ `compare_set` API to python. ([#57191](https://github.com/pytorch/pytorch/pull/57191))\r\n* `torch.distributed.Store`: Add `timeout`, `host`, `port` to TCPStore\u2019s python API as accessors. ([#52784](https://github.com/pytorch/pytorch/pull/52784))\r\n* Allow `world_size` and `is_master` to be optional when constructing TCPStore. ([#51809](https://github.com/pytorch/pytorch/pull/51809))\r\n* Add `wait_for_worker` param to `TCPStore`\u2019s Python API([#52888](https://github.com/pytorch/pytorch/pull/52888))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Allow `RRef` to be created with a specified set of CUDA devices ([#57085](https://github.com/pytorch/pytorch/pull/57085))\r\n* Correctness fixes for CUDA support in RPC framework ([#54024](https://github.com/pytorch/pytorch/pull/54024), )\r\n* Refactor RPC agent to use `Store` to collect and verify name ([#53209](https://github.com/pytorch/pytorch/pull/53209), [#53202](https://github.com/pytorch/pytorch/pull/53202))\r\n\r\n`DistributedDataParallel`\r\n\r\n* Make unused parameter search show up in profiler output ([#57376](https://github.com/pytorch/pytorch/pull/57376))\r\n* Update DDP communication hooks to divide by world size before all_reduce to avoid overflow ([#57410](https://github.com/pytorch/pytorch/pull/57410))\r\n* Stabilize `torch.distributed.GradBucket` interface for gradient compression ([#53010](https://github.com/pytorch/pytorch/pull/53010), [#53098](https://github.com/pytorch/pytorch/pull/53098), [#53102](https://github.com/pytorch/pytorch/pull/53102), [#53009](https://github.com/pytorch/pytorch/pull/53009), [#53099](https://github.com/pytorch/pytorch/pull/53099))\r\n* Skip CPU to GPU input copy if input is already on the right device. ([#55624](https://github.com/pytorch/pytorch/pull/55624))\r\n* Record forward pass of `DistributedDataParallel` and `DataParallel` in profiler.([#55578](https://github.com/pytorch/pytorch/pull/55578))\r\n*  Make `orthogonalization_epsilon` flag configurable in `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState` ([#55738](https://github.com/pytorch/pytorch/pull/55738))\r\n* Set default value of `start_powerSGD_iter` to 1K iterations in \r\n    `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook. `([#55272](https://github.com/pytorch/pytorch/pull/55272))\r\n* Add a minimum compression rate threshold parameter for `torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook`   ([#52541](https://github.com/pytorch/pytorch/pull/52541))\r\n* Report compression rate for batched PowerSGD hook ([#55103](https://github.com/pytorch/pytorch/pull/55103))\r\n* Enable gradient compression hook testing on ROCm ([#52403](https://github.com/pytorch/pytorch/pull/52403))\r\n* Enhance warning for unused parameters in `DistributedDataParallel`. ([#52385](https://github.com/pytorch/pytorch/pull/52385))\r\n* Enhance error messages when crashing with unused parameters in `DistributedDataParallel`. ([#52391](https://github.com/pytorch/pytorch/pull/52391))\r\n\r\n`torch.distributed`\r\n\r\n* Add rank information on NCCL communicator abort ([#57974](https://github.com/pytorch/pytorch/pull/57974))\r\n* Enhance exception logging in NCCL ([#54557](https://github.com/pytorch/pytorch/pull/54557), [#54558](https://github.com/pytorch/pytorch/pull/54558), [#54117](https://github.com/pytorch/pytorch/pull/54117))\r\n\r\n`torch.distributed.nn.RemoteModule`\r\n\r\n* Create a separate remote module template when moving CPU tensors to a cuda device is not enabled ([#57413](https://github.com/pytorch/pytorch/pull/57413))\r\n* Allow passing `RemoteModule` as an argument over RPC ([#57695](https://github.com/pytorch/pytorch/pull/57695), [#58345](https://github.com/pytorch/pytorch/pull/58345))\r\n* Support async instantiation of RemoteModule ([#58052](https://github.com/pytorch/pytorch/pull/58052))\r\n* Place inputs on the appropriate devices in `RemoteModule` ([#56943](https://github.com/pytorch/pytorch/pull/56943))\r\n\r\n`torch.futures.Future`\r\n\r\n* Enable `torch.futures.Future` to be created with CUDA support ([#56517](https://github.com/pytorch/pytorch/pull/56517)) \r\n* `torch.futures`: Improve error propagation when using `then` API ([#54475](https://github.com/pytorch/pytorch/pull/54475))\r\n\r\n`torch.nn.SyncBatchNorm`\r\n\r\n* Migrate `apex.parallel.SyncBatchNorm` `channels_last` to PyTorch implementation ([#46906](https://github.com/pytorch/pytorch/pull/46906))\r\n* Fix `SyncBatchNorm`\u2019s forward pass to handle optional weight ([#54568](https://github.com/pytorch/pytorch/pull/54568))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* `torch.distributed.pipeline`: Merge pipeline partitions that are on the same device. ([#55973](https://github.com/pytorch/pytorch/pull/55973))\r\n\r\nAdded new `torch.distributed.elastic `module that upstreams `pytorch/elastic`\r\n\r\n* Rename `torch.distributed.elastic_launch` to `torch.distributed.run` ([#56831](https://github.com/pytorch/pytorch/pull/56831))\r\n* make process failure init error non-fatal ([#56739](https://github.com/pytorch/pytorch/pull/56739))\r\n* Reorder type definitions in dynamic_rendezvous.py ([#56534](https://github.com/pytorch/pytorch/pull/56534))\r\n* Revise the rendezvous handler registry logic. ([#55466](https://github.com/pytorch/pytorch/pull/55466))\r\n* Set error code in reply file when child process is terminated by signals. (f665a7f8a1)\r\n* Make sure torchelastic mp wait for queue to be drained before finishing the process ([#55412](https://github.com/pytorch/pytorch/pull/55412))\r\n* Revise the rendezvous exception types. ([#54803](https://github.com/pytorch/pytorch/pull/54803))\r\n* Expose a `stderr` parameter in `EtcdServer`. ([#54805](https://github.com/pytorch/pytorch/pull/54805))\r\n* Improve the implementation of the utility functions and add their unit tests. ([#54804](https://github.com/pytorch/pytorch/pull/54804))\r\n* Improve the implementation of `RendezvousParameters` and add its unit tests. ([#54807](https://github.com/pytorch/pytorch/pull/54807))\r\n\r\n`torch.distributed.optim.ZeroRedundancyOptimizer`\r\n\r\n* Add an option for buckets to be views of tensors and consolidate public interface ([#52987](https://github.com/pytorch/pytorch/pull/52987))\r\n* Make state dict for ZeroRedundancyOptimizer world size independent ([#52960](https://github.com/pytorch/pytorch/pull/52960))\r\n\r\nCombine backtrace print into one string to avoid interleaving ([#56961](https://github.com/pytorch/pytorch/pull/56961)).\r\nRaise exception rather than crash if GLOO_DEVICE_TRANSPORT is set to unknown value ([#58518](https://github.com/pytorch/pytorch/issues/58518)).\r\n\r\n### ONNX\r\n\r\n* Updated fuseLogSoftmaxNllLoss function to handle autocasting ([#51729](https://github.com/pytorch/pytorch/pull/51729)) ([#52349](https://github.com/pytorch/pytorch/pull/52349)).\r\n* Added support for sequence of tensor mutations in blocks ([#51577](https://github.com/pytorch/pytorch/pull/51577)) ([#52347](https://github.com/pytorch/pytorch/pull/52347)).\r\n* Updated LayerNorm symbolic to handle autocasting ([#52199](https://github.com/pytorch/pytorch/pull/52199)) ([#52350](https://github.com/pytorch/pytorch/pull/52350)).\r\n* Restored fast path in `OnnxifiOp::adjustOutputBatchSize` ([#52498](https://github.com/pytorch/pytorch/pull/52498)).\r\n* Improved index_put symbolic to handle singular Bool updates ([#53690](https://github.com/pytorch/pytorch/pull/53690)) ([#54863](https://github.com/pytorch/pytorch/pull/54863)).\r\n* Replaced decomposeLinear pre process pass with a symbolic ([#53077](https://github.com/pytorch/pytorch/pull/53077)) ([#54866](https://github.com/pytorch/pytorch/pull/54866)).\r\n* Improved assign input shape for tuple inputs & primitive type inputs ([#54112](https://github.com/pytorch/pytorch/pull/54112)) ([#56164](https://github.com/pytorch/pytorch/pull/56164)).\r\n* Updated repeat_interleave symbolic ([#54312](https://github.com/pytorch/pytorch/pull/54312)) ([#56165](https://github.com/pytorch/pytorch/pull/56165)).\r\n* Enabled `word_language_model` GRU and LSTM scripting ([#54310](https://github.com/pytorch/pytorch/pull/54310)) ([#56170](https://github.com/pytorch/pytorch/pull/56170)).\r\n* Added standardOps match more input type in ORT ([#53813](https://github.com/pytorch/pytorch/pull/53813)) ([#56172](https://github.com/pytorch/pytorch/pull/56172)).\r\n* Redesigned in-place conversion ([#55033](https://github.com/pytorch/pytorch/pull/55033)) ([#56173](https://github.com/pytorch/pytorch/pull/56173)).\r\n* Handled PackedParams inputs for _propagate_and_assign_input_shapes ([#56449](https://github.com/pytorch/pytorch/pull/56449)) ([#57079](https://github.com/pytorch/pytorch/pull/57079)).\r\n* Added a warning for the case when *len* is used to calculate tensor shape ([#55151](https://github.com/pytorch/pytorch/pull/55151)) ([#57595](https://github.com/pytorch/pytorch/pull/57595)).\r\n* Added special post processing for `onnx::Cast` and `onnx::ConstantOfShape` shape type inference ([#55962](https://github.com/pytorch/pytorch/pull/55962)) ([#57597](https://github.com/pytorch/pytorch/pull/57597)).\r\n* Handled NoneType in Assign Output Shapes ([#54623](https://github.com/pytorch/pytorch/pull/54623)) ([#57602](https://github.com/pytorch/pytorch/pull/57602)).\r\n* ListUnpack on dynamic tensor list ([#56592](https://github.com/pytorch/pytorch/pull/56592)) ([#57603](https://github.com/pytorch/pytorch/pull/57603)).\r\n* Handled mixed mask, index input for index_put ([#57604](https://github.com/pytorch/pytorch/pull/57604)).\r\n* Handled incorrect format for example_outputs ([#55802](https://github.com/pytorch/pytorch/pull/55802)) ([#57829](https://github.com/pytorch/pytorch/pull/57829)).\r\n* Enabled several script unit tests using new jit passes ([#51722](https://github.com/pytorch/pytorch/pull/51722)) ([#53309](https://github.com/pytorch/pytorch/pull/53309)).\r\n\r\n### Vulkan\r\n\r\n* Enabled broadcasting for arithmetic ops (add, sub, mul, and div) ([#52842](https://github.com/pytorch/pytorch/pull/52842)).\r\n* Reduced size of compiled shaders by using the `-Os` flag when calling `glslc` ([#57199](https://github.com/pytorch/pytorch/pull/57199)).\r\n* The vulkan optimization JIT pass now adds an `optimized_for_vulkan` attribute to the model ([#56414](https://github.com/pytorch/pytorch/pull/56414)).\r\n\r\n### Benchmark\r\n\r\n* Quality of life improvements to Timer ([#53294](https://github.com/pytorch/pytorch/pull/53294))\r\n* Add repeats to Timer.collect_callgrind(...) ([#54484](https://github.com/pytorch/pytorch/pull/54484))\r\n\r\n### Misc\r\n\r\n* Auto-detect ccache to speed up developer builds ([#49389](https://github.com/pytorch/pytorch/pull/49389)).\r\n* Catch and ignore tracebacks for compilation errors ([#55986](https://github.com/pytorch/pytorch/pull/55986)).\r\n* Register DefaultBackend implementations for functional/inplace structured operators ([#53037](https://github.com/pytorch/pytorch/pull/53037)).\r\n* Improved support for oneDNN on AArch64 when building from src ([#55913](https://github.com/pytorch/pytorch/pull/55913)).\r\n\r\n\r\n# Bug fixes\r\n\r\n### Python API\r\n\r\n* Updated `torch.lerp`  to make `weights` tensor broadcast-able ([#52319](https://github.com/pytorch/pytorch/pull/52319)).\r\n* Fixed print for negative torch.int8 tensors on ARM64 ([#52616](https://github.com/pytorch/pytorch/pull/52616)).\r\n* Fixed type annotation for `as_tuple` to clearly determine what `torch.nonzero` will resolve to ([#51635](https://github.com/pytorch/pytorch/pull/51635)).\r\n* Fixed `torch.logcumsumexp` to correctly handle infs and nans ([#52947](https://github.com/pytorch/pytorch/pull/52947)).\r\n* Fixed `torch.topk` for k=0 on CUDA by skipping the kernel launch in this case ([#58086](https://github.com/pytorch/pytorch/pull/58086)).\r\n* Fixed a bug for optimizers to have the hyper parameters be still defined when all parameters have no grad ([#52944](https://github.com/pytorch/pytorch/pull/52944)).\r\n* Fixed type promotion issue for `torch.pow` ([#54085](https://github.com/pytorch/pytorch/pull/54085)).\r\n* Fixed `torch.min()` and `torch.max()` to work on a non-empty dimension for tensors with 0 elements ([#52565](https://github.com/pytorch/pytorch/pull/52565)).\r\n* Fixed the upper bound computation for `torch.randperm` ([#56967](https://github.com/pytorch/pytorch/pull/56967)).\r\n* Allowed `std=0` in `torch.normal`, and added checks to consistently error out if `std<0` ([#51317](https://github.com/pytorch/pytorch/pull/51317))\r\n* Fixed  `torch.index_fill` to output 0-dim tensor for a 0-dim input tensor ([#52209](https://github.com/pytorch/pytorch/pull/52209)).\r\n* Fixed mul_() to correctly work for Mkldnn tensors ([#51758](https://github.com/pytorch/pytorch/pull/51758)).\r\n* Fixed temp file/bind race condition in torch_shm_manager for `torch.multiprocessing` ([#57309](https://github.com/pytorch/pytorch/pull/57309)).\r\n* Fixed tempfile address binding in torch_shm_manager to be destructed correctly for `torch.multiprocessing` ([#57566](https://github.com/pytorch/pytorch/pull/57566)).\r\n* Fixed `torch.multinomial` to never select an element with 0 weight for `torch.half` (already works correctly for other datatypes) ([#53480](https://github.com/pytorch/pytorch/pull/53480)).\r\n* Fixed a bug in `assertRaises` `NotImplemented` handling when no exception is thrown ([#54126](https://github.com/pytorch/pytorch/pull/54126)).\r\n* Fixed override for `__iter__` ([#54702](https://github.com/pytorch/pytorch/pull/54702)).\r\n* Fixed segmentation fault for `torch.floor_divide` when compiling on ARM64 ([#55608](https://github.com/pytorch/pytorch/pull/55608)).\r\n* Fixed `torch.digamma`\u2019s inconsistency with SciPy\u2019s digamma ([#56689](https://github.com/pytorch/pytorch/pull/56689)).\r\n* Fixed `torch.cat` to return correct result for non-contiguous tensors ([#57177](https://github.com/pytorch/pytorch/pull/57177)).\r\n* Fixed distributions for `torch.distributions.log_prob` which don't properly honor `validate_args=False` ([#53600](https://github.com/pytorch/pytorch/pull/53600)).\r\n* De-prioritized `Dimname` and `DimnameList` in python overload resolution ([#51350](https://github.com/pytorch/pytorch/pull/51350)).\r\n* Fixed the handling of scalar and zero dimensional inputs as well to `torch.take()` and `torch.Tensor.put_` on both CPU and CUDA ([#53356](https://github.com/pytorch/pytorch/pull/53356)).\r\n* Fixed a bug to not rebuild extensions for every import ([#56015](https://github.com/pytorch/pytorch/pull/56015)).\r\n* Fixed error message for `torch.as_strided` ([#53198](https://github.com/pytorch/pytorch/pull/53198)).\r\n* Added correct handling for tensor allocation for large tensors when using `torch.resize` on CUDA ([#52672](https://github.com/pytorch/pytorch/pull/52672)).\r\n* Fixed an illegal memory access that could happen when computing the inverse of a batch of matrices on CUDA ([#53064](https://github.com/pytorch/pytorch/pull/53064)).\r\n* Fixed a bug where `torch.sparse.addmm` would compute the wrong results for CUDA inputs when beta was not zero or one ([#56160](https://github.com/pytorch/pytorch/pull/56160)).\r\n* Fixed a bug where `torch.sparse.sparse_coo_tensor`\u2019s gradient could be calculated incorrectly ([#50361](https://github.com/pytorch/pytorch/pull/50361)).\r\n* `pow`: Fixed a bug caused for mixed cpu/cuda input tensors ([#53669](https://github.com/pytorch/pytorch/pull/53669)).\r\n* `sub`: Fixed a `sub.Scalar` bug ([#53679](https://github.com/pytorch/pytorch/pull/53679)).\r\n* Fixed `torch.unique` for discontiguous inputs ([#59003](https://github.com/pytorch/pytorch/pull/59003)).\r\n* Fixed `torch.randperm` on CUDA ([#59352](https://github.com/pytorch/pytorch/pull/59352)).\r\n* Fix `torch.reciprocal` for `torch.float32` on ARMv8 ([#59361](https://github.com/pytorch/pytorch/pull/59361)).\r\n* Disable overloading of std::max & std::min for inputs of different types, which could cause accuracy loss ([#55638](https://github.com/pytorch/pytorch/pull/55638))\r\n\r\n### Complex Numbers\r\n\r\n* Added custom implementation for `sqrt` and `acos` to be used if `libc++` is used to reduce numerical error for edge cases. ([#52018](https://github.com/pytorch/pytorch/pull/52018), [#54820](https://github.com/pytorch/pytorch/pull/54820), [#52287](https://github.com/pytorch/pytorch/pull/52287)).\r\n\r\n### Autograd\r\n\r\n* Fixed\r\n    * `torch.autograd.gradgradcheck` when outputs are independent of the inputs ([#58049](https://github.com/pytorch/pytorch/pull/58049)).\r\n    * `torch.utils.checkpoint` to behave properly when an error happens during forward ([#51746](https://github.com/pytorch/pytorch/pull/51746)).\r\n    * autograd\u2019s graph discovery when output is a leaf that requires gradients ([#51940](https://github.com/pytorch/pytorch/pull/51940)).\r\n    * some cases where `torch.autograd.gradcheck` did not return the correct value when `raise_exception=False`  ([#53916](https://github.com/pytorch/pytorch/pull/53916)) .\r\n    * thread local state not being properly propagated for some operations during the backward pass ([#56174](https://github.com/pytorch/pytorch/pull/56174)).\r\n    * `torch.index_fill_` formula to support duplicate indices ([#57101](https://github.com/pytorch/pytorch/pull/57101)).\r\n    * derivative of `torch.sinc` around `x=0` ([#56763](https://github.com/pytorch/pytorch/pull/56763), [#56986](https://github.com/pytorch/pytorch/pull/56986)).\r\n    * `torch.cdist` backward formula to correctly support broadcasting ([#56605](https://github.com/pytorch/pytorch/pull/56605)) and empty inputs ([#56606](https://github.com/pytorch/pytorch/pull/56606)).\r\n    * view creation metadata for functions that return multiple views in `no_grad` or inference mode. ([#57842](https://github.com/pytorch/pytorch/pull/57842)).\r\n    * `autograd.functional.*` functions to work in no_grad mode ([#47543](https://github.com/pytorch/pytorch/pull/47543)).\r\n    * rare deadlocks on exit due to autograd worker threads ([#53170](https://github.com/pytorch/pytorch/pull/53170)).\r\n\r\n### torch.nn\r\n\r\n* `nn.AdaptiveAveragePooling`: Fix crash for integral inputs ([#51443](https://github.com/pytorch/pytorch/pull/51443)).\r\n* `F.normalize`: Fix to make it properly scriptable ([#51909](https://github.com/pytorch/pytorch/pull/51909)).\r\n* `nn.parallel.scatter_gather.gather`: Fix to handle `NamedTuple`s and moving output to CPU ([#51104](https://github.com/pytorch/pytorch/pull/51104)).\r\n* `fractional_max_pool{2/3}d` : Fix segfaults for incorrect `kernel_size` and `output_size` ([#51626](https://github.com/pytorch/pytorch/pull/51626)).\r\n* `nn.CosineEmbeddingLoss`: Validate target has correct shape ([#53110](https://github.com/pytorch/pytorch/pull/53110)).\r\n* Fix multiprocessing serialization for integer parameters on CUDA ([#56529](https://github.com/pytorch/pytorch/pull/56529)).\r\n* `nn.Softplus`: Fix backwards computation by comparing `input` against `beta * threshold` ([#56484](https://github.com/pytorch/pytorch/pull/56484)).\r\n* `addmm_`: Add check to disallow resizing the input tensor for the in-place variation on CPU ([#56452](https://github.com/pytorch/pytorch/pull/56452)).\r\n* `nn.InstanceNorm*d`: Fix to perform correct input size check ([#56659](https://github.com/pytorch/pytorch/pull/56659)).\r\n* `nn.CTCLoss`: Fix backward pass regression on cuDNN ([#56639](https://github.com/pytorch/pytorch/pull/56639)).\r\n* `nn.ConvTranspose*d`: Fix regression that broke padding with a list of values ([#54911](https://github.com/pytorch/pytorch/pull/54911)).\r\n* `F.max_pool3d`: Fix illegal memory access for large inputs on CUDA by doing multiplication in `int64` ([#52828](https://github.com/pytorch/pytorch/pull/52828)).\r\n* `F.embedding`: Support `__torch_function__` ([#54478](https://github.com/pytorch/pytorch/pull/54478)).\r\n* `nn.ChannelShuffle`: Remove `NamedTensor` warnings ([#55911](https://github.com/pytorch/pytorch/pull/55911)).\r\n* `mkldnn_linear`: Fix incorrect results for non-contiguous inputs ([#51713](https://github.com/pytorch/pytorch/pull/51713)).\r\n* `nn.ModuleList` / `nn.ModuleDict`: Raise `NotImplementedError` for `forward()` ([#48785](https://github.com/pytorch/pytorch/pull/48785)).\r\n* Change `maybe_resize_storage_cpu` `new_size` arg to unsigned ([#52671](https://github.com/pytorch/pytorch/pull/52671)).\r\n* `nn.LSTM`: Fix regression that broke loading older serialized modules ([#57558](https://github.com/pytorch/pytorch/pull/57558)).\r\n* `F.reflection_pad2d`: Fix CUDA launch error ([#56451](https://github.com/pytorch/pytorch/pull/56451)).\r\n* Fix wrong detection of depthwise convolution on neon ([#55794](https://github.com/pytorch/pytorch/pull/55794)).\r\n* Re-enable fast winograd convolution on IOS ([#56021](https://github.com/pytorch/pytorch/pull/56021)).\r\n* `gaussian_nll_loss`: Fix incorrect `reduction=\u2018none\u2019` behavior ([#56469](https://github.com/pytorch/pytorch/pull/56469)).\r\n* Fix misaligned access #56325 ([#56403](https://github.com/pytorch/pytorch/pull/56403)).\r\n* Use native CTC loss for target length 256 ([#53557](https://github.com/pytorch/pytorch/pull/53557)).\r\n* `register_full_backward_hook`: Fix crash when first argument doesn't require a gradient ([#57945](https://github.com/pytorch/pytorch/pull/57945)).\r\n* Remove asserts of Tensor type and ignore mypy checks to support `__torch_function__` usage ([#57458](https://github.com/pytorch/pytorch/pull/57458)).\r\n* Handle stride > 1 with im2col in CUDA thnn conv2d ([#54080](https://github.com/pytorch/pytorch/pull/54080)).\r\n* Add device id to ConvolutionParams ([#50892](https://github.com/pytorch/pytorch/pull/50892)).\r\n* Enabling OneDNN for group convolution ([#54890](https://github.com/pytorch/pytorch/pull/54890)).\r\n* `nn.AdaptiveAveragePooling3d`: Add `AccumulateType` for CUDA ([#53607](https://github.com/pytorch/pytorch/pull/53607)).\r\n* Do not use depthwise3x3 conv in grad mode for ARM ([#56889](https://github.com/pytorch/pytorch/pull/56889)).\r\n* Fix type annotations for `state_dict()` override ([#55704](https://github.com/pytorch/pytorch/pull/55704)).\r\n* Pass contiguous weight to NNPACK convolution ([#56569](https://github.com/pytorch/pytorch/pull/56569)).\r\n* `nn.EmbeddingBag`: Mark backward as non-deterministic for max mode rather than all reducing modes ([#55574](https://github.com/pytorch/pytorch/pull/55574)).\r\n* `nn.EmbeddingBag`: Initialize `bag_size` output with zeros to make it deterministic ([#56661](https://github.com/pytorch/pytorch/pull/56661)).\r\n* `nn.EmbeddingBag`: Support the empty bag case on CPU ([#57446](https://github.com/pytorch/pytorch/pull/57446)).\r\n* Fix `nn.MHA` + `quantized` scriptability ([#58727](https://github.com/pytorch/pytorch/pull/58727)).\r\n* Fixes cuDNN performance on A100 ([#58287](https://github.com/pytorch/pytorch/pull/58287), [#59721](https://github.com/pytorch/pytorch/pull/59721), [#59744](https://github.com/pytorch/pytorch/pull/59744), [#59802](https://github.com/pytorch/pytorch/pull/59802)).\r\n\r\n### Dataloader\r\n\r\n* Fixed type hints of the callable DataLoader arguments ([#52924](https://github.com/pytorch/pytorch/pull/52924)).\r\n* Added a keyword arg to meta and support `abc` for typing ([#58450](https://github.com/pytorch/pytorch/pull/58450)).\r\n* Fixed a bug to use `generator` instead of `self.generator` in the `RandomSampler` ([#52956](https://github.com/pytorch/pytorch/pull/52956)).\r\n\r\n### C++ API\r\n\r\n* Fixed the lifetime of `PyTensorType` ([#51649](https://github.com/pytorch/pytorch/pull/51649)).\r\n* Fixed linker failure with ambiguous namespaces ([#45736](https://github.com/pytorch/pytorch/pull/45736)).\r\n* Fix Scalar output formatting ([#53229](https://github.com/pytorch/pytorch/pull/53229))\r\n* Fix printing of optional string arguments in schemas ([#55196](https://github.com/pytorch/pytorch/pull/55196))\r\n\r\n### AMD\r\n\r\n* Fixed `hipfft` transform type error ([#53411](https://github.com/pytorch/pytorch/pull/53411)).\r\n* Load only hipfft for ROCm > 4.1 ([#54349](https://github.com/pytorch/pytorch/pull/54349)).\r\n\r\n### CUDA\r\n\r\n* Added `torch.scatter_add` to `torch.cuda.amp` promote list ([#52133](https://github.com/pytorch/pytorch/pull/52133)).\r\n* Fixed segfault in distributed process group due to IPC ([#53080](https://github.com/pytorch/pytorch/pull/53080)).\r\n* Fixed multinomial CUDA misalignment and non-deterministic behavior ([#55364](https://github.com/pytorch/pytorch/pull/55364)).\r\n* Replaced raw cudaMalloc in `torch.sparse` code ([#57083](https://github.com/pytorch/pytorch/pull/57083)).\r\n* [CUDA graphs] Added proper sync after replay ([#57556](https://github.com/pytorch/pytorch/pull/57556)).\r\n* Fixed NVRTC versioning for CUDA 11.X (X>=3), CUDA 12 and later ([#57204](https://github.com/pytorch/pytorch/pull/57204)).\r\n* Fixed a correctness issue of CUDA channels-last `nn.SyncBatchNorm` ([#57077](https://github.com/pytorch/pytorch/pull/57077)).\r\n* Fixed CUDA caching allocator when trying to allocate ~2^64 memory ([#57571](https://github.com/pytorch/pytorch/pull/57571)).\r\n* Fixed raw_deleter() bug with PYTORCH_NO_CUDA_MEMORY_CACHING=1 ([#54775](https://github.com/pytorch/pytorch/pull/54775)).\r\n* Fixed undefined symbol for CUDA 11.1 Windows ([#52506](https://github.com/pytorch/pytorch/pull/52506)).\r\n* Automatically set BUILD_SPLIT_CUDA for cpp extensions ([#52503](https://github.com/pytorch/pytorch/pull/52503)).\r\n* Adds grid_sampler to the list of operations that can autocast `torch.float32` ([#58679](https://github.com/pytorch/pytorch/pull/58679)).\r\n\r\n### Dispatcher\r\n\r\n* Fix boxing/unboxing for `Scalar` bool values ([#53228](https://github.com/pytorch/pytorch/pull/53228))\r\n* Fix inaccurate dispatch table for `fill_` ([#53611](https://github.com/pytorch/pytorch/pull/53611))\r\n* Fix inaccurate dispatch tables ([#54127](https://github.com/pytorch/pytorch/pull/54127))\r\n* Fix issue with dispatch key: `AutogradXPU` ([#56336](https://github.com/pytorch/pytorch/pull/56336))\r\n* Modify `DispatchKeyExtractor` to also work for optional Tensors ([#58283](https://github.com/pytorch/pytorch/pull/58283))\r\n* Extract dispatch keys from optional Tensors (unboxed) ([#58296](https://github.com/pytorch/pytorch/pull/58296))\r\n\r\n### torch.fx\r\n\r\n* Preserve leaf modules in `Transformer` ([#51998](https://github.com/pytorch/pytorch/pull/51998)).\r\n* Fix tuple type annotations in FX codebase ([#52010](https://github.com/pytorch/pytorch/pull/52010)).\r\n* Fix type correctness on `GraphModule.graph` ([#54305](https://github.com/pytorch/pytorch/pull/54305)).\r\n* Remove `forward` from `forward.__globals__` to facilitate retracing ([#54011](https://github.com/pytorch/pytorch/pull/54011)).\r\n* Fix `ScriptMethod` dispatch on `__torch_function__` ([#56103](https://github.com/pytorch/pytorch/pull/56103)).\r\n* Fix `type_matches` for `Optional[List[int]]` arguments to make `NormalizeArgs` more permissive ([#56790](https://github.com/pytorch/pytorch/pull/56790)).\r\n* Fix `NormalizeArgs` issues with lists of tensors ([#57004](https://github.com/pytorch/pytorch/pull/57004)).\r\n* Changed parametric type error in `NormalizeArgs` to a warning ([#57183](https://github.com/pytorch/pytorch/pull/57183)).\r\n* Make `NormalizeArgs` not save output node in the `node_map` ([#58058](https://github.com/pytorch/pytorch/pull/58058)).\r\n\r\n### Profiler\r\n\r\n* Fixed intermittent CUDA activity flush issue (https://github.com/pytorch/kineto/pull/95).\r\n* Handled empty trace ([#58013](https://github.com/pytorch/pytorch/pull/58013)).\r\n* Added cuda synchronization points ([#56651](https://github.com/pytorch/pytorch/pull/56651)).\r\n* Removed usage of onEachDevice from legacy profiler ([#54125](https://github.com/pytorch/pytorch/pull/54125)).\r\n* Fixed double printing of FLOPs ([#56974](https://github.com/pytorch/pytorch/pull/56974)).\r\n\r\n### TorchScript\r\n\r\n* Fixed `jit.trace` mishandling of InterfaceType ([#53052](https://github.com/pytorch/pytorch/pull/53052)).\r\n* Made `reshape`/`flatten` deterministic ([#54353](https://github.com/pytorch/pytorch/pull/54353)).\r\n* Added logic to use `is_buffer` in `BufferPolicy::valid` ([#49588](https://github.com/pytorch/pytorch/pull/49588)).\r\n* Updated NNC to sanitize input names ([#52786](https://github.com/pytorch/pytorch/pull/52786)).\r\n* Handled ExternalCalls in LoadStore analysis and Inliner ([#52628](https://github.com/pytorch/pytorch/pull/52628)).\r\n* Fixed output restriding of size-1 dimensions ([#58256](https://github.com/pytorch/pytorch/pull/58256)).\r\n* Handled non literal constant bounds in Unroll ([#53029](https://github.com/pytorch/pytorch/pull/53029)).\r\n* Fixed a case where inlining wouldn't work because dim-size was 1 ([#53254](https://github.com/pytorch/pytorch/pull/53254)).\r\n* Removed cached argv from LLVMCodeGen to fix race condition ([#54286](https://github.com/pytorch/pytorch/pull/54286)).\r\n* Lowered scalar constants as doubles/longs ([#54824](https://github.com/pytorch/pytorch/pull/54824)).\r\n* Added a check to not try to vectorize kernels that use float16 ([#55970](https://github.com/pytorch/pytorch/pull/55970)).\r\n* Added a check to not fuse `torch.float16` on CPU ([#56119](https://github.com/pytorch/pytorch/pull/56119)).\r\n* Fixed `float->bool` conversion on CPU ([#57798](https://github.com/pytorch/pytorch/pull/57798)).\r\n* Fixed handling of the arguments of `aten::to` ([#58028](https://github.com/pytorch/pytorch/pull/58028)).\r\n* Don\u2019t error on 0-dim in convolution ([#51922](https://github.com/pytorch/pytorch/pull/51922)).\r\n* Allow `__exit__` to have a return value ([#52336](https://github.com/pytorch/pytorch/pull/52336)).\r\n* Added metacompile of ternary if ([#51789](https://github.com/pytorch/pytorch/pull/51789)).\r\n* Keep alive graph when creating iterators from it ([#51951](https://github.com/pytorch/pytorch/pull/51951)).\r\n* Fixed return value of `IValue::to` for Tensor/String ([#51463](https://github.com/pytorch/pytorch/pull/51463)).\r\n* Added function to check for memory leak ([#52342](https://github.com/pytorch/pytorch/pull/52342)).\r\n* Ignore user annotated ignored attributes ([#52367](https://github.com/pytorch/pytorch/pull/52367)).\r\n* Fixed `jit.trace` mishandling of InterfaceType ([#53052](https://github.com/pytorch/pytorch/pull/53052)).\r\n* Fixed tracing support for TorchBind ([#52884](https://github.com/pytorch/pytorch/pull/52884)).\r\n* Use correct warning type for tracer warnings ([#53460](https://github.com/pytorch/pytorch/pull/53460)).\r\n* Removed the assumption that `forward` exists in freeze_module ([#52918](https://github.com/pytorch/pytorch/pull/52918)).\r\n* Removed notion of \"level\" from `Module::dump_to_str` ([#52539](https://github.com/pytorch/pytorch/pull/52539)).\r\n* Made `IValue::toTensor()` inline-able ([#53213](https://github.com/pytorch/pytorch/pull/53213)).\r\n* Consider `normal_` as a special operation in the remove mutation pass ([#52175](https://github.com/pytorch/pytorch/pull/52175)).\r\n* Updated `set_stream` API to change the device ([#53741](https://github.com/pytorch/pytorch/pull/53741)).\r\n* Only run `ReplaceWithCopy` pass when `enable_out_variant` is true ([#54111](https://github.com/pytorch/pytorch/pull/54111)).\r\n* Disable dfusion group that is not supported by XPU device ([#54239](https://github.com/pytorch/pytorch/pull/54239)).\r\n* Don\u2019t require same-sized `src`/`dest` in `reshape_copy` ([#54467](https://github.com/pytorch/pytorch/pull/54467)).\r\n* Fixed `TupleType.annotation_str` to conform to `typing` module syntax for empty tuple type ([#54641](https://github.com/pytorch/pytorch/pull/54641)).\r\n* Made NoneType `annotation_str` emit `NoneType` instead of `None` ([#54642](https://github.com/pytorch/pytorch/pull/54642)).\r\n* Made sure the copy version of the op exists in `ReplaceWithCopy` ([#55337](https://github.com/pytorch/pytorch/pull/55337)).\r\n* Included `conv3d` in `conv-add-relu` fusion ([#54772](https://github.com/pytorch/pytorch/pull/54772)).\r\n* Added `cond-add-relu` matching pattern to cover in-place ops ([#55458](https://github.com/pytorch/pytorch/pull/55458)).\r\n* Fixed `TupleType.annotation_str` to conform to `typing` module syntax for empty tuple type ([#54745](https://github.com/pytorch/pytorch/pull/54745)).\r\n* Fixed `Optional[Tensor]` type in autodiff ([#55565](https://github.com/pytorch/pytorch/pull/55565)).\r\n* Raise TypeErrors when `IValue::getSubValues` fails ([#56510](https://github.com/pytorch/pytorch/pull/56510)).\r\n* Fixed num args for `to_copy` ([#56441](https://github.com/pytorch/pytorch/pull/56441))\r\n* Fixed error in JIT CUDA on ROCm ([#55243](https://github.com/pytorch/pytorch/pull/55243)).\r\n* Fixed a bug in `emitUse` to drop all values that are marked as drop ([#56652](https://github.com/pytorch/pytorch/pull/56652)).\r\n* Fixed default dtype for `randperm` and `triu`/`tril_indices` inside TorchScript ([#57105](https://github.com/pytorch/pytorch/pull/57105)).\r\n* Don't allow create() on singleton types ([#56807](https://github.com/pytorch/pytorch/pull/56807)).\r\n* Fix GIL mutithreading issue exposed by `torch::jit::toIValue()` ([#57688](https://github.com/pytorch/pytorch/pull/57688)).\r\n* Fold `NaiveSyncBatchNorm` when folding batch norm ([#57823](https://github.com/pytorch/pytorch/pull/57823)).\r\n* Fix UB in `LoopNest::distribute` ([#57883](https://github.com/pytorch/pytorch/pull/57883)).\r\n* Fix a condition when we use a native depthwise `conv2d` lowering ([#57906](https://github.com/pytorch/pytorch/pull/57906)).\r\n* Ensure `torch.save()` has deterministic output ([#57536](https://github.com/pytorch/pytorch/pull/57536))\r\n* Fixed `hasattr` support type ([#57950](https://github.com/pytorch/pytorch/pull/57950))\r\n* Return nullptr if the number of input args doesn't match ([#58018](https://github.com/pytorch/pytorch/pull/58018)).\r\n* Added fix for missing ops `aten::sorted.str` ([#58339](https://github.com/pytorch/pytorch/pull/58339)).\r\n* Fixed deadlock in `Future` due to lock inversion with GIL ([#58382](https://github.com/pytorch/pytorch/pull/58382)).\r\n* Added logic to prevent lock inversions with GIL in `Future` ([#58391](https://github.com/pytorch/pytorch/pull/58391)).\r\n* Fixed `MKLDNN_add` in-place behavior ([#51687](https://github.com/pytorch/pytorch/pull/51687)).\r\n* Use MKLDNN copy for `copy_ when` self and src are MKLDNN layout ([#54248](https://github.com/pytorch/pytorch/pull/54248)) .\r\n* Fixed default to align with documentation in `fuser.py` ([#53457](https://github.com/pytorch/pytorch/pull/53457)).\r\n* Fixed upcoming changes that are part of ROCm 4.2 and affect PyTorch JIT ([#57400](https://github.com/pytorch/pytorch/pull/57400)).\r\n* Fix for improper mobile and torch.package serialization ([#59642](https://github.com/pytorch/pytorch/pull/59642)).\r\n\r\n### torch.package\r\n\r\n* Add cpython as a dependency for torch_python_obj ([#56740](https://github.com/pytorch/pytorch/pull/56740)).\r\n* Catch exceptions where dependency resolution gets invalid imports ([#58573](https://github.com/pytorch/pytorch/pull/58573)).\r\n* Simplifications to broken dependency handling ([#58572](https://github.com/pytorch/pytorch/pull/58572)).\r\n\r\n### Quantization\r\n\r\n* Fixed conv packed param serialization in `state_dict` ([#52787](https://github.com/pytorch/pytorch/pull/52787)).\r\n* Fixed `torch.float16` dynamic quant for functional linear ([#52369](https://github.com/pytorch/pytorch/pull/52369)).\r\n* Fixed prepacking for `F.conv1d` ([#55311](https://github.com/pytorch/pytorch/pull/55311)).\r\n* MHA tensor assignment fix ([#53031](https://github.com/pytorch/pytorch/pull/53031)).\r\n* Fixed `conv` transpose with `qconfig == None` ([#52844](https://github.com/pytorch/pytorch/pull/52844)).\r\n* Quant norm layers: move scale + zp to buffers ([#52861](https://github.com/pytorch/pytorch/pull/52861)).\r\n* Handled the case when observed node has no users ([#53210](https://github.com/pytorch/pytorch/pull/53210)).\r\n* Only insert observers for fixed qparam ops ([#53330](https://github.com/pytorch/pytorch/pull/53330)).\r\n* Fixed a condition check for `CopyNode` ([#53585](https://github.com/pytorch/pytorch/pull/53585)).\r\n* Fix for `x.ndim` followed by `sub` ([#53120](https://github.com/pytorch/pytorch/pull/53120)).\r\n* Fixed using size of quant layer in `torch._assert` ([#53187](https://github.com/pytorch/pytorch/pull/53187)).\r\n* Fixed fx quant for `quant_layer -> stack -> sum` ([#53196](https://github.com/pytorch/pytorch/pull/53196)).\r\n* Fixed `deepcopy` on quantized `ConvNd` ([#56154](https://github.com/pytorch/pytorch/pull/56154))\r\n* Fixed `getitem` for unmatched nodes ([#57173](https://github.com/pytorch/pytorch/pull/57173)).\r\n* Made quantizeable MHA work with `torch.jit.script` ([#57774](https://github.com/pytorch/pytorch/pull/57774)).\r\n* Fixed `quantize_per_tensor` on CUDA ([#57703](https://github.com/pytorch/pytorch/pull/57703)).\r\n* Fixed a bug to handle bias in rowwise quantization of FC ([#58022](https://github.com/pytorch/pytorch/pull/58022)).\r\n* Skipped inserting observer for boolean Tensors ([#57375](https://github.com/pytorch/pytorch/pull/57375)).\r\n* Fixed `torch.float16` reference patterns for linear ([#55727](https://github.com/pytorch/pytorch/pull/55727)).\r\n* FX Quant:\r\n    * Fixed edge case with copynode after user function ([#55710](https://github.com/pytorch/pytorch/pull/55710)).\r\n    * Fixed subtle bug in BinaryOpQuantizeHanlder logic in matching ([#56294](https://github.com/pytorch/pytorch/pull/56294)).\r\n    * Fixed bug with fusion patterns and disabling quantization ([#54654](https://github.com/pytorch/pytorch/pull/54654)).\r\n* Fixed overflow issue in quantized instance_norm/layer_norm/group_norm ([#54872](https://github.com/pytorch/pytorch/pull/54872)).\r\n* Fixed zero_point rounding for _fake_quantize_learnable_per_channel_affine ([#52290](https://github.com/pytorch/pytorch/pull/52290)).\r\n* Bug fix to update requantization and zp parameters of input ([#52797](https://github.com/pytorch/pytorch/pull/52797)).\r\n* Fix embedding bag bug accessing unaligned memory ([#53300](https://github.com/pytorch/pytorch/pull/53300)).\r\n* Fix out variant for 4bit embedding bag ([#55096](https://github.com/pytorch/pytorch/pull/55096)).\r\n* Avoid tensor refcount bumps on embedding bag ([#55023](https://github.com/pytorch/pytorch/pull/55023)).\r\n\r\n### Mobile\r\n\r\n* Fixed some bugs in the implementation of various functions on iOS GPU:\r\n    * `max_pool_2d` when padding is used ([#52431](https://github.com/pytorch/pytorch/pull/52431)).\r\n    * `softmax` ([#54519](https://github.com/pytorch/pytorch/pull/54519)).\r\n    * binary element-wise ops to handle inputs with different number of dimensions ([#58262](https://github.com/pytorch/pytorch/pull/58262)).\r\n* Removed duplication of constant tensors in model when using Lite interpreter ([#58182](https://github.com/pytorch/pytorch/pull/58182), [#56002](https://github.com/pytorch/pytorch/pull/56002)).\r\n* Banned mutating operators in mobile GPU models ([#56070](https://github.com/pytorch/pytorch/pull/56070)).\r\n* Use lite interpreter as default and bump model version ([#58630](https://github.com/pytorch/pytorch/pull/58630))\r\n\r\n### Distributed\r\n\r\n`torch.distributed.Store`\r\n\r\n* Fix flag specifying whether there is more data for `TCPStore` delete key ([#53886](https://github.com/pytorch/pytorch/pull/53886))\r\n* Properly enforce timeout for `PrefixStore`. ([#53928](https://github.com/pytorch/pytorch/pull/53928))\r\n* Fix `TCPStore` `wait` hang when key is previously set ([#53860](https://github.com/pytorch/pytorch/pull/53860))\r\n* Properly order `TCPStore`\u2019s `compare_set` parameters in Python API ([#52696](https://github.com/pytorch/pytorch/pull/52696))\r\n* Fix resource leak bug in TCPStore constructor ([#52860](https://github.com/pytorch/pytorch/pull/52860))\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Several fixes for CUDA support in the RPC framework ([#57926](https://github.com/pytorch/pytorch/pull/57926), [#57432](https://github.com/pytorch/pytorch/pull/57432), [#57394](https://github.com/pytorch/pytorch/pull/57394), [#57443](https://github.com/pytorch/pytorch/pull/57443), [#57487](https://github.com/pytorch/pytorch/pull/57487), [#58384](https://github.com/pytorch/pytorch/pull/58384), [#51820](https://github.com/pytorch/pytorch/pull/51820), [#57792](https://github.com/pytorch/pytorch/pull/57792), [#56895](https://github.com/pytorch/pytorch/pull/56895), [#54932](https://github.com/pytorch/pytorch/pull/54932))\r\n* Fix possible reference cycle by passing reference to parent future in RPC callbacks ([#57635](https://github.com/pytorch/pytorch/pull/57635))\r\n* Fix RPC `get_worker_info` for rank 0 ([#52804](https://github.com/pytorch/pytorch/pull/52804))\r\n* Fix crash when TensorPipe agent tries to double-set errors. ([#52837](https://github.com/pytorch/pytorch/pull/52837))\r\n\r\n`torch.distributed`\r\n\r\n* Fix path handling on Windows during rendezvous process ([#57000](https://github.com/pytorch/pytorch/pull/57000))\r\n* Fix and re-enable `ProcessGroupMPITest` ([#56709](https://github.com/pytorch/pytorch/pull/56709))\r\n\r\n`DistributedDataParallel`\r\n\r\n*  Correct the usage of min_compression_rate in gradient compression communication hooks ([#52979](https://github.com/pytorch/pytorch/pull/52979))\r\n* Fix mapping of parameter to parameter names when certain parameters don\u2019t require gradient ([#57771](https://github.com/pytorch/pytorch/pull/57771))\r\n* Skip rebuild buckets in `DistributedDataParallel` when running under `no_grad` mode. ([#54159](https://github.com/pytorch/pytorch/pull/54159))\r\n* Fix a race condition in `DistributedDataParallel` when all parameters are used but running with `find_unused_parameters=True`. ([#53160](https://github.com/pytorch/pytorch/pull/53160))\r\n* In `DistributedDataParallel`, pass in `process_group` argument into `dist.get_rank` calls ([#53793](https://github.com/pytorch/pytorch/pull/53793))\r\n* Fix `DistributedDataParallel`\u2019s process for verifying model consistency during initialization. ([#52887](https://github.com/pytorch/pytorch/pull/52887))\r\n\r\n`torch.distributed`\r\n\r\n* Check vector boundaries in `torch::cuda::scatter` ([#53057](https://github.com/pytorch/pytorch/pull/53057))\r\n* Release GIL before destructing ProcessGroup classes ([#56381](https://github.com/pytorch/pytorch/pull/56381))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* Fix hang in `pipeline` destructor by removing `join_workers` ([#53433](https://github.com/pytorch/pytorch/pull/53433))\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Resolve bug around incorrect rendezvous handler resolution ([#56386](https://github.com/pytorch/pytorch/pull/56386))\r\n\r\n`torch.nn.SyncBatchNorm`\r\n\r\n* Ensure `SyncBatchNorm` behaves like a regular `BatchNorm` layer in eval mode. ([#56982](https://github.com/pytorch/pytorch/pull/56982))\r\n\r\n`torch.distributed.optim.ZeroRedundancyOptimizer`\r\n\r\n* Typing fixes([#53165](https://github.com/pytorch/pytorch/pull/53165))\r\n\r\nFix monitored_barrier with wait_all_ranks ([#58702](https://github.com/pytorch/pytorch/pull/58702)).\r\n\r\n### ONNX\r\n\r\n* Removed the last Cast in pow symbolic_opset9 ([#52646](https://github.com/pytorch/pytorch/pull/52646)) ([#53305](https://github.com/pytorch/pytorch/pull/53305)).\r\n* Fixed export of `copy_` operator ([#53046](https://github.com/pytorch/pytorch/pull/53046)) ([#53310](https://github.com/pytorch/pytorch/pull/53310)) ([#51938](https://github.com/pytorch/pytorch/pull/51938)) ([#54870](https://github.com/pytorch/pytorch/pull/54870)).\r\n* Fixed export of embedding with `padding_idx` ([#53053](https://github.com/pytorch/pytorch/pull/53053)) ([#53530](https://github.com/pytorch/pytorch/pull/53530)).\r\n* Fixed onnx warning message ([#54371](https://github.com/pytorch/pytorch/pull/54371)).\r\n* Improved error message during Glow ONNXIFI ([#58069](https://github.com/pytorch/pytorch/pull/58069)).\r\n* Fixed if output shape mismatch error & graph input directly used as output ([#53219](https://github.com/pytorch/pytorch/pull/53219)) ([#54865](https://github.com/pytorch/pytorch/pull/54865)).\r\n* Fixed ComputeShapeFromReshape when `input_shape_size < reshape_size` ([#56171](https://github.com/pytorch/pytorch/pull/56171)).\r\n* Fixed -Wrange-loop-construct in onnx_exporter.cc ([#56759](https://github.com/pytorch/pytorch/pull/56759)).\r\n* Print `onnxifi` failed status code in readable format ([#53648](https://github.com/pytorch/pytorch/pull/53648)).\r\n\r\n### Vulkan\r\n\r\n* Fixed kernel registration errors in Vulkan test and benchmark binaries by adding `nonVarTypeModeGuard` ([#52535](https://github.com/pytorch/pytorch/pull/52535)).\r\n* Fixed the `glslc` path in CMake for desktop builds ([#56507](https://github.com/pytorch/pytorch/pull/56507)).\r\n* Fixed build failures caused by `warnings-treated-as-error` for Linux builds. ([#52781](https://github.com/pytorch/pytorch/pull/52781)).\r\n* Remove constant duplication for Vulkan optimize_for_mobile ([#59276](https://github.com/pytorch/pytorch/pull/59276)).\r\n\r\n### Benchmark\r\n\r\n* Fix timer overflow on small, fast snippets ([#55200](https://github.com/pytorch/pytorch/pull/55200))\r\n\r\n### Misc\r\n\r\n* [memory format] Fixed channels last bug in upsample kernels to now correctly pass `memory_format` information from the input to the output tensors ([#53535](https://github.com/pytorch/pytorch/pull/53535)).\r\n* [memory format] Fixed silent correctness bug for CUDA upsample kernels to correctly handle `torch.channels_last` contiguous tensors ([#54744](https://github.com/pytorch/pytorch/pull/54744)).\r\n* Workaround intermittent gcc-7.5 ICE in cpp tests ([#57016](https://github.com/pytorch/pytorch/pull/57016)).\r\n* Improve build quality on Windows ([#52729](https://github.com/pytorch/pytorch/pull/52729), [#53562](https://github.com/pytorch/pytorch/pull/53562), [#54132](https://github.com/pytorch/pytorch/pull/54132), [#55275](https://github.com/pytorch/pytorch/pull/55275)).\r\n* Search for static OpenBLAS compiled with OpenMP ([#59428](https://github.com/pytorch/pytorch/pull/59428)).\r\n\r\n\r\n# Performance\r\n\r\n### Python API\r\n\r\n* Optimized memory usage for `out=` version of `torch`.`logsumexp` ([#51239](https://github.com/pytorch/pytorch/pull/51239)).\r\n* Added vectorization for `torch.floor_divide` ([#55380](https://github.com/pytorch/pytorch/pull/55380)).\r\n* Reimplemented `torch.flip()` using advanced indexing ([#56713](https://github.com/pytorch/pytorch/pull/56713)).\r\n* Improved performance for `torch.take()` and `torch.Tensor.put_` on both CPU and CUDA ([#53356](https://github.com/pytorch/pytorch/pull/53356))\r\n* Generic performance improvement for operations performed on non-contiguous 2-dimensional tensors ([#53613](https://github.com/pytorch/pytorch/pull/53613)).\r\n* Added vectorization for `torch.copysign` on CPU ([#51792](https://github.com/pytorch/pytorch/pull/51792)).\r\n* Improved performance for bilinear interpolation on CPU ([#51653](https://github.com/pytorch/pytorch/pull/51653)).\r\n* Improved performance for backward computations on `torch.cumsum` and `torch.cumprod` on both CPU and CUDA ([#53711](https://github.com/pytorch/pytorch/pull/53711)).\r\n* Improved performance for `torch.Tensor.copy_`  when performing copies between small tensors of `torch.float` and `torch.half` data types ([#53800](https://github.com/pytorch/pytorch/pull/53800)).\r\n* Enabled vectorization for `torch.Tensor.copy_` and `torch.cat` for BFloat16 tensors ([#54671](https://github.com/pytorch/pytorch/pull/54671), [#54674](https://github.com/pytorch/pytorch/pull/54674)).\r\n* Added a fast path for a common case for `torch.addmm` on CUDA ([#55026](https://github.com/pytorch/pytorch/pull/55026)).\r\n* In collaboration with NVIDIA, the CUDA performance of many linear algebra operations has been improved by increasing use of the cuSOLVER and cuBLAS libraries\r\n    * Added cuBLAS support for `torch.triangular_solve` ([#53147](https://github.com/pytorch/pytorch/pull/53147)) and batched `torch.geqrf` ([#56253](https://github.com/pytorch/pytorch/pull/56253)).\r\n    * Added cuSOLVER support for `torch.linalg.eigh/eigvalsh` ([#53040](https://github.com/pytorch/pytorch/pull/53040)), `torch.cholesky_solve` ([#54315](https://github.com/pytorch/pytorch/pull/54315)), `torch.cholesky_inverse` ([#54676](https://github.com/pytorch/pytorch/pull/54676)), and `torch.linalg.q`r ([#56256](https://github.com/pytorch/pytorch/pull/56256)).\r\n    * Added cuBLAS and cuSOLVER support for `torch.linalg.lstsq` ([#57317](https://github.com/pytorch/pytorch/pull/57317)).\r\n* Improved performance for `torch.nonzero` ([#58468](https://github.com/pytorch/pytorch/pull/58468)).\r\n* Removed device check from a few indexing methods ([#58800](https://github.com/pytorch/pytorch/pull/58800)).\r\n\r\n### Complex Numbers\r\n\r\n* Added a faster path for `torch.is_complex()` by skipping unnecessary  dispatch ([#50054](https://github.com/pytorch/pytorch/pull/50054)).\r\n\r\n### Autograd\r\n\r\n* Sped up autograd\u2019s graph discovery algorithm by skipping some nodes using sequence number ([#52180](https://github.com/pytorch/pytorch/pull/52180), [#52057](https://github.com/pytorch/pytorch/pull/52057)).\r\n* Added a new fast gradcheck ([#54480](https://github.com/pytorch/pytorch/pull/54480)).\r\n\r\n### torch.nn\r\n\r\n* `Module.forward`: Add fast path for the case of no hooks ([#52576](https://github.com/pytorch/pytorch/pull/52576)).\r\n* Fix `mkldnn` heuristic for multithreaded convolution ([#52909](https://github.com/pytorch/pytorch/pull/52909)).\r\n* `linear`: Remove one refcount bump ([#54936](https://github.com/pytorch/pytorch/pull/54936)).\r\n* Improve `native_batch_norm_backward` performance on CUDA ([#58240](https://github.com/pytorch/pytorch/pull/58240)).\r\n* `nll_loss`: Use cascade summation on CPU ([#55841](https://github.com/pytorch/pytorch/pull/55841)).\r\n* `nn.BatchNorm1d`: Improve training performance on CPU ([#57033](https://github.com/pytorch/pytorch/pull/57033)).\r\n* Simplify convolution double backward gradInput formulas ([#54840](https://github.com/pytorch/pytorch/pull/54840)).\r\n* Move RNN cell size check to cpp ([#51964](https://github.com/pytorch/pytorch/pull/51964)).\r\n* Remove syncs in `one_hot` ([#57902](https://github.com/pytorch/pytorch/pull/57902)).\r\n* Enable and enhance bf16 threshold ([#54384](https://github.com/pytorch/pytorch/pull/54384)).\r\n* `nn.Conv3d`: Enable `channels_last_3d` for cuDNN ([#48430](https://github.com/pytorch/pytorch/pull/48430)).\r\n* Increase token count threshold for calling thrust sort in embedding backward ([#49913](https://github.com/pytorch/pytorch/pull/49913)).\r\n* CPU convolution benchmark harness for some popular models ([#56455](https://github.com/pytorch/pytorch/pull/56455)).\r\n* Improved performance for `torch.nn.BatchNorm1d` on both CPU and CUDA ([#57033](https://github.com/pytorch/pytorch/pull/57033), [#57786](https://github.com/pytorch/pytorch/pull/57786)).\r\n* Added optimized generic interpolation for `torch.nn.functional.{upsample_nearest`, `upsample_bicubic}` and speed up for channels first and last cases ([#54500](https://github.com/pytorch/pytorch/pull/54500)).\r\n* Added shape documentation for CosineEmbeddingLoss ([#58403](https://github.com/pytorch/pytorch/pull/58403)).\r\n\r\n### C++ API\r\n\r\n* Fixed nest openmp performance bug in `thnn_conv2d` ([#52577](https://github.com/pytorch/pytorch/pull/52577)).\r\n* Added c10::MaybeOwned and Tensor::expect_contiguous ([#53317](https://github.com/pytorch/pytorch/pull/53317))\r\n* Added DimVector variant of infer_size ([#54882](https://github.com/pytorch/pytorch/pull/54882))\r\n* Added logic to use `DimVector` for inputs to `as_strided `that don't grow dim ([#55016](https://github.com/pytorch/pytorch/pull/55016)).\r\n* Reduce ref-counting by borrowing in/out Tensors in TensorIterator ([#55690](https://github.com/pytorch/pytorch/pull/55690)).\r\n* Reduce ref-counting by migrating add operators to borrow Tensors in TensorIteratorBase ([#55691](https://github.com/pytorch/pytorch/pull/55691)).\r\n* Reduce ref-counting by migrating copy_ operators to borrow input/output Tensors ([#56031](https://github.com/pytorch/pytorch/pull/56031)).\r\n* Added logic to use `expect_contiguous` in `layer_norm` ([#58067](https://github.com/pytorch/pytorch/pull/58067)).\r\n\r\n### CUDA\r\n\r\n* Construct only necessary elements in OffsetCalculator ([#55107](https://github.com/pytorch/pytorch/pull/55107)).\r\n* Migrated `torch.index_put` to use cub instead of thrust ([#55693](https://github.com/pytorch/pytorch/pull/55693)).\r\n* Added cuSOLVER `potrf` and `potrfBatched` to the backend of `torch.cholesky_decomposition` ([#53104](https://github.com/pytorch/pytorch/pull/53104)).\r\n* Implemented `torch.sort` with cub::DeviceSegmentedRadixSort ([#56821](https://github.com/pytorch/pytorch/pull/56821)).\r\n* Added cuSOLVER path for `torch.geqrf` ([#56252](https://github.com/pytorch/pytorch/pull/56252)).\r\n* Enabled cuSOLVER `torch.potrf` batched for Cholesky decomposition when CUDA >= 11.3 ([#57788](https://github.com/pytorch/pytorch/pull/57788)).\r\n* Fewer CUDA sync in unique by using cub instead of thrust ([#57323](https://github.com/pytorch/pytorch/pull/57323)).\r\n* Removed sync for `randperm` on small tensors ([#54113](https://github.com/pytorch/pytorch/pull/54113)).\r\n* Simplify convolution double backward gradInput formulas ([#54840](https://github.com/pytorch/pytorch/pull/54840)).\r\n\r\n### Composability\r\n\r\n* We\u2019ve landed lots of performance optimizations for 1.9, both large and small. See individual PRs for details:\r\n    * Inline `tensor.device()` ([#50848](https://github.com/pytorch/pytorch/pull/50848))\r\n    * Skip a second call to `shouldUseRecordFunction` for BackendSelect ops ([#50891](https://github.com/pytorch/pytorch/pull/50891))\r\n    * Re-order `TensorImpl` fields to save a word ([#50920](https://github.com/pytorch/pytorch/pull/50920))\r\n    * Devirtualize `TensorImpl::storage()` ([#51050](https://github.com/pytorch/pytorch/pull/51050))\r\n    * Reduce template expansion in `call_functor_with_args_from_stack` (build time) ([#51313](https://github.com/pytorch/pytorch/pull/51313))\r\n    * Eliminate `WrapFunctionIntoRuntimeFunctor `use in CppFunction constructors ([#51315](https://github.com/pytorch/pytorch/pull/51315))\r\n    * Remove `reference_cast` in `make_boxed_from_unboxed_functor` (build time) ([#51319](https://github.com/pytorch/pytorch/pull/51319))\r\n    * Debug-gate `static_assert` in `KernelFunction::makeFromUnboxedFunctor` (build time) ([#51367](https://github.com/pytorch/pytorch/pull/51367))\r\n    * Use real `if constexpr` behind macro in hot template (build time) ([#51368](https://github.com/pytorch/pytorch/pull/51368), [#52420](https://github.com/pytorch/pytorch/pull/52420))\r\n    * Outline `DispatchStub::get_call_ptr()` ([#51908](https://github.com/pytorch/pytorch/pull/51908))\r\n    * Use `torchCheckFail` in `TORCH_INTERNAL_ASSERT` ([#52086](https://github.com/pytorch/pytorch/pull/52086))\r\n    * Add `Storage::set_data_ptr_noswap` and use where possible ([#52244](https://github.com/pytorch/pytorch/pull/52244))\r\n    * Make shared empty string static instead of thread_local ([#52220](https://github.com/pytorch/pytorch/pull/52220))\r\n    * Avoid `std::string` in `TORCH_CHECK` when possible ([#52221](https://github.com/pytorch/pytorch/pull/52221))\r\n    * Make `c10::str(const char*)` return `const char*` ([#52222](https://github.com/pytorch/pytorch/pull/52222))\r\n    * Sync `TORCH_INTERNAL_ASSERT` optimizations with `TORCH_CHECK` ([#52226](https://github.com/pytorch/pytorch/pull/52226))\r\n    * Save a single add instruction in the dispatcher ([#52543](https://github.com/pytorch/pytorch/pull/52543))\r\n    * Inline `TensorIteratorConfig` setters ([#52661](https://github.com/pytorch/pytorch/pull/52661))\r\n    * Use `DimVector` for sizes and strides in `view` ([#53001](https://github.com/pytorch/pytorch/pull/53001))\r\n    * Avoid TLS in `has_names` ([#53003](https://github.com/pytorch/pytorch/pull/53003))\r\n    * Don't inline `Dispatcher::call` on mobile (binary size) ([#53197](https://github.com/pytorch/pytorch/pull/53197))\r\n    * Skip dispatch for `is_floating_point` ([#53242](https://github.com/pytorch/pytorch/pull/53242))\r\n    * Move non-template part of `TensorImpl::Resize` to cpp (binary size, build time) ([#53388](https://github.com/pytorch/pytorch/pull/53388))\r\n    * Don't copy vector arguments to `Tensor::Resize` ([#53389](https://github.com/pytorch/pytorch/pull/53389))\r\n    * Skip dispatch trip for CPU in `resize_` ([#53575](https://github.com/pytorch/pytorch/pull/53575))\r\n    * Pass `Scalar` by reference ([#53583](https://github.com/pytorch/pytorch/pull/53583))\r\n    * Don't use static for template declarations in headers (binary size) ([#53602](https://github.com/pytorch/pytorch/pull/53602))\r\n    * Boxing logic forwards arguments to stack ([#53624](https://github.com/pytorch/pytorch/pull/53624))\r\n    * `Speed up Tensor::data_ptr by using static item size (`[`#53723`](https://github.com/pytorch/pytorch/pull/53723)`)`\r\n    * `Skip dispatch for is_signed (`[`#53847`](https://github.com/pytorch/pytorch/pull/53847)`)`\r\n    * Allow inlining of more Tensor methods ([#53905](https://github.com/pytorch/pytorch/pull/53905))\r\n    * `Tensor::register_hook`: Avoid wrapping hook in two levels of `std::function` ([#53917](https://github.com/pytorch/pytorch/pull/53917))\r\n    * Take advantage of string literals in `TORCH_WARN` ([#54032](https://github.com/pytorch/pytorch/pull/54032))\r\n    * Inline `Tensor` keyset-checking methods & similar getters ([#54806](https://github.com/pytorch/pytorch/pull/54806))\r\n    * `TensorIterator::output` returns const reference ([#54811](https://github.com/pytorch/pytorch/pull/54811))\r\n    * Avoid refcount bump in `TensorArg` ([#54934](https://github.com/pytorch/pytorch/pull/54934))\r\n    * Move `Tensor::has_names` inline ([#54965](https://github.com/pytorch/pytorch/pull/54965))\r\n    * `OperandInfo` ctor should take rvalue reference ([#54972](https://github.com/pytorch/pytorch/pull/54972))\r\n    * Don't bother with `SmallVector` in `TensorMaker` ([#55125](https://github.com/pytorch/pytorch/pull/55125))\r\n    * Eliminate device guard in generic dispatch key kernel wrappers ([#55131](https://github.com/pytorch/pytorch/pull/55131))\r\n    * Move logic to skip a redispatch directly inside of `resize_output` ([#55162](https://github.com/pytorch/pytorch/pull/55162))\r\n    * Use `infer_size_dimvector` in `ExpandUtils` ([#55180](https://github.com/pytorch/pytorch/pull/55180))\r\n    * Don't create intermediate Tensor for `at::result_type` w/Scalar ([#55232](https://github.com/pytorch/pytorch/pull/55232))\r\n    * Use `sizes()[x]` instead of `size(x)` in `addr` ([#55247](https://github.com/pytorch/pytorch/pull/55247))\r\n    * Add & use `inferExpandGeometry_dimvector` ([#55316](https://github.com/pytorch/pytorch/pull/55316))\r\n    * Mark borrowed case as `C10_LIKELY` in `MaybeOwned` ([#55553](https://github.com/pytorch/pytorch/pull/55553))\r\n    * Avoid double indirection in `MaybeOwned`'s borrowed state ([#55685](https://github.com/pytorch/pytorch/pull/55685))\r\n    * Make `VariableVersion::DISABLED` the default constructor for `VariableVersion`. ([#55572](https://github.com/pytorch/pytorch/pull/55572))\r\n    * Don't set `version_counter` on inference tensor for `unsafe_` ops. ([#55819](https://github.com/pytorch/pytorch/pull/55819))\r\n    * Add & document `borrow_from_optional_tensor` ([#56647](https://github.com/pytorch/pytorch/pull/56647))\r\n    * Migrate hacky wrapper removal to `borrow_from_optional_tensor` ([#56648](https://github.com/pytorch/pytorch/pull/56648))\r\n    * Optimize `at::repeat` ([#56994](https://github.com/pytorch/pytorch/pull/56994))\r\n    * Optimize `intrusive_ptr(TTarget*) ` ctor (`pybind`) ([#57053](https://github.com/pytorch/pytorch/pull/57053))\r\n\r\n### torch.fx\r\n\r\n* Use precompiled regex in graph name processing ([#52853](https://github.com/pytorch/pytorch/pull/52853)).\r\n* Optimize module path finding in `Tracer` ([#52990](https://github.com/pytorch/pytorch/pull/52990)).\r\n* Speed up `_Namespace.create_name` ([#55580](https://github.com/pytorch/pytorch/pull/55580)).\r\n\r\n### Profiler\r\n\r\n* Sped up post processing ([#58021](https://github.com/pytorch/pytorch/pull/58021)).\r\n\r\n### TorchScript\r\n\r\n* Generate arithmetic vs logical right shift as appropriate ([#51749](https://github.com/pytorch/pytorch/pull/51749))\r\n* Introduced likely/unlikely `CompareSelect` hint ([#51751](https://github.com/pytorch/pytorch/pull/51751)).\r\n* Implemented log approximation using the VML approach ([#51752](https://github.com/pytorch/pytorch/pull/51752)).\r\n* Updated `TensorExpr` to use `LLVM` as the default backend ([#52314](https://github.com/pytorch/pytorch/pull/52314)).\r\n* Added support for `aten::hardtanh` (a hot operation in mobilenet v2/v3) ([#52394](https://github.com/pytorch/pytorch/pull/52394))\r\n* Implemented `hardtanh` ([#57750](https://github.com/pytorch/pytorch/pull/57750)).\r\n* Add `aten::batch_norm` into fuser when in inference mode ([#54204](https://github.com/pytorch/pytorch/pull/54204)).\r\n* NNC\r\n    * Added a new API to perform loop fusion ([#54461](https://github.com/pytorch/pytorch/pull/54461)).\r\n    * Implemented depthwise `conv2d` ([#54920](https://github.com/pytorch/pytorch/pull/54920)).\r\n    * Integrated NNC `conv2d` with fuser ([#55213](https://github.com/pytorch/pytorch/pull/55213)).\r\n    * Added logic to use NNC to generate `logit`, `relu` and `tanh` ([#52322](https://github.com/pytorch/pytorch/pull/52322)).\r\n    * Use VML-inspired logarithm with NNC, tweak scheduling ([#52423](https://github.com/pytorch/pytorch/pull/52423)).\r\n    * Generate `sigmoid` with NNC ([#52424](https://github.com/pytorch/pytorch/pull/52424)).\r\n    * Enabled CPU fusion only when `num_threads == 1` ([#56120](https://github.com/pytorch/pytorch/pull/56120)).\r\n    * Use NNC's `call_raw` API to reduce call overheads. ([#57553](https://github.com/pytorch/pytorch/pull/57553)).\r\n    * Started codegen\u2019ing some external calls ([#58118](https://github.com/pytorch/pytorch/pull/58118)).\r\n* Reduce memory use for inference path in `OneDNN MaxPooling` ([#52728](https://github.com/pytorch/pytorch/pull/52728)).\r\n* Removed redundant `gather_ranges` when fusing ([#53323](https://github.com/pytorch/pytorch/pull/53323)).\r\n* Optimized `sigrid_hash` ([#53065](https://github.com/pytorch/pytorch/pull/53065)).\r\n* Updated `create_empty_from` to directly use the native version of `at::empty` ([#53216](https://github.com/pytorch/pytorch/pull/53216)).\r\n* Added a minimum fusion group size ([#50217](https://github.com/pytorch/pytorch/pull/50217)).\r\n* Added CUDNN `Conv-Add-Relu` fusion for Frozen Model Optimization ([#52102](https://github.com/pytorch/pytorch/pull/52102)).\r\n* Avoid dispatch overhead in call to MKLDNN convolution ([#52614](https://github.com/pytorch/pytorch/pull/52614)).\r\n* Added re-inplacing to MKLDNN subgraphs ([#53908](https://github.com/pytorch/pytorch/pull/53908)).\r\n* Set `requires_gradient` to help autodiff prune unneeded gradients ([#54374](https://github.com/pytorch/pytorch/pull/54374)).\r\n* Use type cache in erasing shape information ([#55828](https://github.com/pytorch/pytorch/pull/55828)).\r\n* Added heuristic to avoid perf incompatible MKLDNN formats for binary ops ([#56089](https://github.com/pytorch/pytorch/pull/56089))\r\n* Added `adaptive_avgpool2d` to the set of fusible ops ([#56180](https://github.com/pytorch/pytorch/pull/56180)).\r\n* Lazily initialize `AliasDb` in `remove_mutation` opt ([#55949](https://github.com/pytorch/pytorch/pull/55949))\r\n* Made DataPtr extraction in CUDAFuture faster for Python values ([#56918](https://github.com/pytorch/pytorch/pull/56918)).\r\n* Lazily initialize `AliasDb` in DCE ([#56649](https://github.com/pytorch/pytorch/pull/56649)).\r\n* Add explicit checks for in-place ops in `ReplaceWithCopy` ([#54657](https://github.com/pytorch/pytorch/pull/54657)).\r\n    \r\n\r\n### Quantization\r\n\r\n* Optimized quantized `torch.cat` ([#54813](https://github.com/pytorch/pytorch/pull/54813)).\r\n\r\n### Mobile\r\n\r\n* Enabled `QNNPACK` for Apple Silicon builds ([#52308](https://github.com/pytorch/pytorch/pull/52308)).\r\n* Sped up model loading for per-channel quantized models using `QNNPACK` ([#53726](https://github.com/pytorch/pytorch/pull/53726)).\r\n* Added `XNNPACK` implementations for various operationss (`hardswish, global average pool`) ([#56714](https://github.com/pytorch/pytorch/pull/56714), [#56715](https://github.com/pytorch/pytorch/pull/56715), [#55791](https://github.com/pytorch/pytorch/pull/55791)).\r\n* Made various performance improvements for iOS GPU (Metal) ([#57664](https://github.com/pytorch/pytorch/pull/57664), [#57665](https://github.com/pytorch/pytorch/pull/57665), [#57666](https://github.com/pytorch/pytorch/pull/57666), [#57667](https://github.com/pytorch/pytorch/pull/57667), [#57668](https://github.com/pytorch/pytorch/pull/57668)).\r\n\r\n### Distributed\r\n\r\n`torch.distributed`\r\n\r\n* Avoid 2 extra copies when reducing sparse tensors ([#57822](https://github.com/pytorch/pytorch/pull/57822))\r\n\r\n### Vulkan\r\n\r\n* Switched to a more performant implementation of matrix multiplication ([#49609](https://github.com/pytorch/pytorch/pull/49609)).\r\n* Updated the version of Vulkan Memory Allocator used ([#52938](https://github.com/pytorch/pytorch/pull/52938)).\r\n* Increased the command buffer submission rate ([#57196](https://github.com/pytorch/pytorch/pull/57196)).\r\n* Updated the Vulkan tensors to use 2D textures whenever possible, instead of always using 3D textures ([#57198](https://github.com/pytorch/pytorch/pull/57198)).\r\n* Updated convolution shaders to receive the bias tensor as a texture as opposed to a buffer ([#57201](https://github.com/pytorch/pytorch/pull/57201)).\r\n\r\n# Docs\r\n\r\n### Python API\r\n\r\n* Added `torch.testing` docs ([#57247](https://github.com/pytorch/pytorch/pull/57247)).\r\n* Updated docs to mention CUDA support for Future ([#50048](https://github.com/pytorch/pytorch/pull/50048)).\r\n* Included `memory_format` , an already accepted argument, in `torch.empty` doc ([#54664](https://github.com/pytorch/pytorch/pull/54664)).\r\n* Improved the documentation for torch.matrix_exp() ([#55626](https://github.com/pytorch/pytorch/pull/55626)).\r\n* Updated use_deterministic_algorithms docs ([#55413](https://github.com/pytorch/pytorch/pull/55413)).\r\n* Added the `generator`  argument to `torch.rand` and `torch.randn` docs ([#56242](https://github.com/pytorch/pytorch/pull/56242)).\r\n* Added an example to show how to use learning rate schedulers in Optimizers ([#56705](https://github.com/pytorch/pytorch/pull/56705)).\r\n* Corrected the torch.ceil formula in docs ([#55039](https://github.com/pytorch/pytorch/pull/55039))\r\n* Fixed docs to use autosummary on tensors.rst ([#55042](https://github.com/pytorch/pytorch/pull/55042))\r\n* Improved testing documentation in `CONTRIBUTING.md` ([#54904](https://github.com/pytorch/pytorch/pull/54904))\r\n* Updated `torch.fft` docs to include `out=` argument ([#56732](https://github.com/pytorch/pytorch/pull/56732)).\r\n* Updated rounding_mode documentation to remove `\"true\"` ([#52202](https://github.com/pytorch/pytorch/pull/52202)).\r\n* Added a note about error handling for non-chained futures ([#53212](https://github.com/pytorch/pytorch/pull/53212)).\r\n* Updated `torch.stft` documentation to clarify output shape ([#54877](https://github.com/pytorch/pytorch/pull/54877)).\r\n* Added an example for `torch.is_tensor` and `torch.is_storage` ([#55052](https://github.com/pytorch/pytorch/pull/55052)).\r\n\r\n### Autograd\r\n\r\n* Added a note describing gradcheck internals ([#55966](https://github.com/pytorch/pytorch/pull/55966)).\r\n* Split up autograd documentation into separate pages ([#55672](https://github.com/pytorch/pytorch/pull/55672)).\r\n* `torch.utils.checkpoint` : Updated docs to state that `input` flag in `.backward()` is disallowed when checkpointing ([#51746](https://github.com/pytorch/pytorch/pull/51746)).\r\n* Added section in autograd mechanics note describing how to use inference/no_grad ([#58513](https://github.com/pytorch/pytorch/pull/58513)).\r\n* Added doc string for `torch.is_inference_mode_enabled` and `torch.is_grad_enabled` ([#59047](https://github.com/pytorch/pytorch/pull/59047)).\r\n* Added no-grad inference mode note ([#58513](https://github.com/pytorch/pytorch/pull/58513)).\r\n* Add docstring for is_inference_mode_enabled ([#59047](https://github.com/pytorch/pytorch/pull/59047)).\r\n\r\n### torch.nn\r\n\r\n* `nn.TripletMarginLoss` / `torch.reciprocal`: Fix formatting in docs ([#51650](https://github.com/pytorch/pytorch/pull/51650))\r\n* `nn.FractionalMaxPool3d`: Add to pooling layer docs ([#52556](https://github.com/pytorch/pytorch/pull/52556))\r\n* `F.fractional_max_pool`: Add to `nn.functional` docs ([#52557](https://github.com/pytorch/pytorch/pull/52557))\r\n* `Module.share_memory`: Add link to `Tensor.share_memory_` in docs ([#52561](https://github.com/pytorch/pytorch/pull/52561))\r\n* `nn.SiLU`: Mention alternative name of Swish within docs ([#53239](https://github.com/pytorch/pytorch/pull/53239))\r\n* Remove redundant hardsigmoid() in docstring to show up `inplace` parameter ([#52559](https://github.com/pytorch/pytorch/pull/52559))\r\n* Clarify docs for lazy modules ([#53495](https://github.com/pytorch/pytorch/pull/53495))\r\n* `torch.nn`: Grammatically update docs ([#54370](https://github.com/pytorch/pytorch/pull/54370))\r\n* `nn.Sequential`: Expand docs, including comparison with `nn.ModuleList` ([#53380](https://github.com/pytorch/pytorch/pull/53380))\r\n* `F.embedding_bag`: Fix formatting in docs ([#54666](https://github.com/pytorch/pytorch/pull/54666))\r\n* `F.group_norm`: Add to docs ([#54673](https://github.com/pytorch/pytorch/pull/54673))\r\n* Add separate autosummary for flatten layer docs ([#54663](https://github.com/pytorch/pytorch/pull/54663))\r\n* `LazyModuleMixin`: Add missing attr in docs to improve formatting ([#53363](https://github.com/pytorch/pytorch/pull/53363))\r\n* `conv1d`: Fix example error in docs ([#57356](https://github.com/pytorch/pytorch/pull/57356))\r\n* `nn.functional`: Split docs into a table-of-contents page and a sub-page per function ([#55038](https://github.com/pytorch/pytorch/pull/55038))\r\n* `nn.LSTM` / `nn.RNN` / `nn.GRU`: Clarify `batch_first` behavior ([#58809](https://github.com/pytorch/pytorch/pull/58809))\r\n* `nn.CosineEmbeddingLoss`: Add shape info to docs ([#58403](https://github.com/pytorch/pytorch/pull/58403))\r\n* Add doc warnings for default SELU gain ([#54057](https://github.com/pytorch/pytorch/pull/54057)).\r\n* Clarify batch_first behavior for `nn.LSTM, nn.RNN, and nn.GRU` ([#58809](https://github.com/pytorch/pytorch/pull/58809)).\r\n* Add UninitializedBuffer to nn docs ( [#59021](https://github.com/pytorch/pytorch/pull/59021)).\r\n* Document factory_kwargs in nn.Quantize + remove Attributes section ([#59025](https://github.com/pytorch/pytorch/pull/59025)).\r\n\r\n### Dataloader\r\n\r\n* Added DataPipes Typing Doc ([#54773](https://github.com/pytorch/pytorch/pull/54773)).\r\n* Added docs to document the default NumPy seed for DataLoader workers ([#56528](https://github.com/pytorch/pytorch/pull/56528)).\r\n\r\n### AMD\r\n\r\n* Added HIP semantics doc ([#57871](https://github.com/pytorch/pytorch/pull/57871)).\r\n\r\n### CUDA\r\n\r\n* Added `scatter_add` to amp docs ([#54908](https://github.com/pytorch/pytorch/pull/54908)) \r\n* Added `reset_peak_memory_stats` in cuda.rst ([#54668](https://github.com/pytorch/pytorch/pull/54668)).\r\n\r\n### torch.fx\r\n\r\n* Make some modifications to limitation section ([#51928](https://github.com/pytorch/pytorch/pull/51928))\r\n* Added docstring for concrete_args on `Tracer.trace` ([#53151](https://github.com/pytorch/pytorch/pull/53151)).\r\n* Change Dynamic Control Flow example to a *more* dynamic version ([#53250](https://github.com/pytorch/pytorch/pull/53250)).\r\n* Render inherited methods in fx.Tracer API reference ([#53630](https://github.com/pytorch/pytorch/pull/53630)).\r\n* Add docs for `ShapeProp` ([#54554](https://github.com/pytorch/pytorch/pull/54554)).\r\n* Hide module paths leaking in the documentation. ([#54585](https://github.com/pytorch/pytorch/pull/54585)).\r\n\r\n### Profiler\r\n\r\n* Updated profiler recipe doc (https://github.com/pytorch/tutorials/pull/1528).\r\n\r\n### TorchScript\r\n\r\n* Added NNC IR specification ([#52912](https://github.com/pytorch/pytorch/pull/52912)).\r\n* Added starter content for new TorchScript language reference ([#53837](https://github.com/pytorch/pytorch/pull/53837)).\r\n* Added documentation for `torch.jit.Attribute` and `torch.jit.annotate` ([#54485](https://github.com/pytorch/pytorch/pull/54485)).\r\n* Updated TorchScript language reference section for types ([#53673](https://github.com/pytorch/pytorch/pull/53673)).\r\n* Documented the TorchScript type system ([#53244](https://github.com/pytorch/pytorch/pull/53244)).\r\n* Added language reference for Python builtin functions, statements,  and values in TorchScript ([#52847](https://github.com/pytorch/pytorch/pull/52847), [#52830](https://github.com/pytorch/pytorch/pull/52830)).\r\n* Added `torch.*` API section for TorchScript language reference ([#53236](https://github.com/pytorch/pytorch/pull/53236)).\r\n* Added \u201cConditionals in TE\u201d doc ([#56949](https://github.com/pytorch/pytorch/pull/56949)).\r\n    \r\n\r\n### torch.package\r\n\r\n* Added API reference ([#55812](https://github.com/pytorch/pytorch/pull/55812), [#56547](https://github.com/pytorch/pytorch/pull/56547)).\r\n* Add explanation, tutorial, and preamble sections for `torch.package` ([#59833](https://github.com/pytorch/pytorch/pull/59833), [#59503](https://github.com/pytorch/pytorch/pull/59503), [#59499](https://github.com/pytorch/pytorch/pull/59499), [#59491](https://github.com/pytorch/pytorch/pull/59491), [#59842](https://github.com/pytorch/pytorch/pull/59842), [#59843](https://github.com/pytorch/pytorch/pull/59843), [#59602](https://github.com/pytorch/pytorch/pull/59602)).\r\n* Add pickle security warning to package docs ([#59959](https://github.com/pytorch/pytorch/pull/59959)).\r\n\r\n### Quantization\r\n\r\n* Added docs for storage and tensors for quantized Tensor ([#51817](https://github.com/pytorch/pytorch/pull/51817)).\r\n* Fixed FX Graph Mode Quantization tutorial link ([#54715](https://github.com/pytorch/pytorch/pull/54715)).\r\n* Added fx graph mode quant api doc ([#55306](https://github.com/pytorch/pytorch/pull/55306)).\r\n* FX Graph Mode Quantization - fixed preamble ([#52192](https://github.com/pytorch/pytorch/pull/52192)).\r\n* Fixed broken link to fx graph quant guide in quantization.rst ([#56776](https://github.com/pytorch/pytorch/pull/56776)).\r\n\r\n### Mobile\r\n\r\n* Added doc string for lite interpreter related API in Android ([#53136](https://github.com/pytorch/pytorch/pull/53136)).\r\n* Improved `export_opnames` Documentation ([#52333](https://github.com/pytorch/pytorch/pull/52333)).\r\n\r\n### Distributed\r\n\r\n`torch.distributed.Store`\r\n\r\n* Documentation for TCPStore\u2019s `compare_set` API ([#57203](https://github.com/pytorch/pytorch/pull/57203))\r\n\r\n`torch.distributed.optim`\r\n\r\n* Update distributed optimizer documentation ([#58084](https://github.com/pytorch/pytorch/pull/58084))\r\n* Update and expose ZeroRedundancyOptimizer docs ([#53112](https://github.com/pytorch/pytorch/pull/53112), [#53113](https://github.com/pytorch/pytorch/pull/53113))\r\n\r\n\r\n`torch.distributed.elastic`\r\n\r\n* Upstream `torchelastic` documentation to PyTorch. ([#56811](https://github.com/pytorch/pytorch/pull/56811))\r\n* Revise the note section of RendezvousHandler doc ([#57723](https://github.com/pytorch/pytorch/pull/57723))\r\n* Update the rendezvous documentation ([#57973](https://github.com/pytorch/pytorch/pull/57973))\r\n\r\n\r\n`DistributedDataParallel`\r\n\r\n* Add register_comm_hook API to DDP communication hooks documentation page ([#51846](https://github.com/pytorch/pytorch/pull/51846),[](https://github.com/pytorch/pytorch/pull/51986)[#51986](https://github.com/pytorch/pytorch/pull/51986))\r\n* Enhance documentation around `DistributedDataParallel` uneven input support ([#57448](https://github.com/pytorch/pytorch/pull/57448))\r\n* Enhance communication hook documentation ([#58170](https://github.com/pytorch/pytorch/pull/58170), [#58168](https://github.com/pytorch/pytorch/pull/58168), [#53253](https://github.com/pytorch/pytorch/pull/53253), [#53855](https://github.com/pytorch/pytorch/pull/53855), [#53596,](https://github.com/pytorch/pytorch/pull/53596)[#53955](https://github.com/pytorch/pytorch/pull/53955), [#54052](https://github.com/pytorch/pytorch/pull/54052). [#55031](https://github.com/pytorch/pytorch/pull/55031))\r\n\r\n\r\n`torch.distributed.rpc`\r\n\r\n* Add a disclaimer about limited CUDA support in RPC ([#58023](https://github.com/pytorch/pytorch/pull/58023)) \r\n* `torch.distributed.rpc`:  Add a link to the tutorial in RemoteModule docstring ([#57875](https://github.com/pytorch/pytorch/pull/57875))\r\n* `torch.distributed.rpc`:  Mentioned `RemoteModule` in RPC documentation ([#57876](https://github.com/pytorch/pytorch/pull/57876))\r\n\r\n\r\n`torch.distributed.nn.RemoteModule`\r\n\r\n* Add RemoteModule to master RPC docs. ([#53084](https://github.com/pytorch/pytorch/pull/53084))\r\n* Add `remote_parameters` and `get_module_rref` to RemoteModule docs. ([#54645](https://github.com/pytorch/pytorch/pull/54645))\r\n\r\n`torch.distributed.pipeline`\r\n\r\n* Enhance Pipe docs to explicitly mention RPC initialization. ([#55187](https://github.com/pytorch/pytorch/pull/55187))\r\n* Add tutorials to pipeline docs. ([#55209](https://github.com/pytorch/pytorch/pull/55209))\r\n\r\n`torch.distributed`\r\n\r\n* Update documentation for `get_future` support ([#58107](https://github.com/pytorch/pytorch/pull/58107))\r\n* Mention distributed profiling in documentation ([#58286](https://github.com/pytorch/pytorch/pull/58286))\r\n* Update distributed doc table for `alltoall`  ([#54277](https://github.com/pytorch/pytorch/pull/54277))\r\n*  fix docstring signature in `all_reduce_multigpu` ([#54665](https://github.com/pytorch/pytorch/pull/54665))\r\n* `torch.distributed`: Improve dist.new_group doc ([#55660](https://github.com/pytorch/pytorch/pull/55660))\r\n\r\n### ONNX\r\n\r\n* Updated ONNX documentation ([#51362](https://github.com/pytorch/pytorch/pull/51362)) ([#53313](https://github.com/pytorch/pytorch/pull/53313)).\r\n* Updated scripting docs ([#54634](https://github.com/pytorch/pytorch/pull/54634)) ([#54868](https://github.com/pytorch/pytorch/pull/54868)).\r\n* Fixed docstring signature of torch.{onnx,utils} ([#54662](https://github.com/pytorch/pytorch/pull/54662)).\r\n* onnx.symbolic_helper.parse_args: document and clean up ([#56956](https://github.com/pytorch/pytorch/pull/56956)) ([#57598](https://github.com/pytorch/pytorch/pull/57598)).\r\n\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.9.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.9.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.9.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/43927405", "dateCreated": "2021-06-11T20:43:27Z", "datePublished": "2021-06-15T16:06:52Z"}, {"tagName": "v1.8.1", "name": "Small bug fix release ", "authorName": "albanD", "authorType": "User", "body": "# PyTorch 1.8.1 Release Notes\r\n\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Documentation\r\n\r\n# New Features\r\n\r\n### Revamp of profiling tools in `torch.profiler`\r\n\r\nThe [`torch.profiler`](https://pytorch.org/docs/stable/profiler.html) submodule is now available. It leveraged the newly released kineto library for profiling.\r\nYou can find more details in this blogpost: https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/\r\n\r\n### Enable use of autocast for pytorch xla ([#48570](https://github.com/pytorch/pytorch/pull/48570))\r\n\r\nThe `torch.cuda.autocast` package can now be used in conjunction with torch xla to provide easy mixed-precision training.\r\n\r\n# Improvements\r\n\r\n* Make `torch.` submodule import more autocomplete-friendly ([#52339](https://github.com/pytorch/pytorch/pull/52339))\r\n* Add support in ONNX for `torch.{isinf,any,all}` ([#53529](https://github.com/pytorch/pytorch/pull/53529))\r\n* Replace thrust with cub in GPU implementation of `torch.randperm` for performance ([#54537](https://github.com/pytorch/pytorch/pull/54537))\r\n\r\n# Bug fixes\r\n\r\n## Misc\r\n\r\n* Fixes for `torch.distributions` validation checks ([](https://github.com/pytorch/pytorch/commit/e991cdaf58bda3169a284e2dead254262b450787)[#53763](https://github.com/pytorch/pytorch/pull/53763)[](https://github.com/pytorch/pytorch/commit/e991cdaf58bda3169a284e2dead254262b450787))\r\n* Allow changing the padding vector for `nn.Embedding` ([#53447](https://github.com/pytorch/pytorch/pull/53447))\r\n* Fix TensorPipe for large copies and interoperability with CUDA ([#53804](https://github.com/pytorch/pytorch/pull/53804))\r\n* Properly de-sugar `Ellipsis` in TorchScript ([#53766](https://github.com/pytorch/pytorch/pull/53766))\r\n* Stop using OneDNN for group convolutions when groups size is a multiple of `24` ([#54015](https://github.com/pytorch/pytorch/pull/54015))\r\n* Use `int8_t` instead of `char` in `{load,store}_scalar` ([#52616](https://github.com/pytorch/pytorch/pull/52616))\r\n* Make ideep honor `torch.set_num_thread` ([#53871](https://github.com/pytorch/pytorch/pull/53871))\r\n* Fix dimension out of range in `pixel_{un}shuffle` ([#54178](https://github.com/pytorch/pytorch/pull/54178))\r\n* Update kineto to fix libtorch builds ([#54205](https://github.com/pytorch/pytorch/pull/54205))\r\n* Fix distributed autograd CUDA stream synchronization for send/recv operations ([#54358](https://github.com/pytorch/pytorch/pull/54358))\r\n\r\n## ONNX\r\n\r\n* Update error handling in ONNX to avoid `ValueError` ([#53548](https://github.com/pytorch/pytorch/pull/53548))\r\n* Update assign output shape for nested structure and dict output ([#53311](https://github.com/pytorch/pytorch/pull/53311))\r\n* Update embedding export wrt `padding_idx` ([#53931](https://github.com/pytorch/pytorch/pull/53931))\r\n\r\n# Documentation\r\n\r\n* Doc update for `torch.fx` ([#53674](https://github.com/pytorch/pytorch/pull/53674))\r\n* Fix `distributed.rpc.options.TensorPipeRpcBackendOptions.set_device_map` ([#53508](https://github.com/pytorch/pytorch/pull/53508))\r\n* Update example for `nn.LSTMCell` ([#51983](https://github.com/pytorch/pytorch/pull/51983))\r\n* Update doc for the `padding_idx` argument for `nn.Embedding` ([#53809](https://github.com/pytorch/pytorch/pull/53809))\r\n* Update general doc template ([#54141](https://github.com/pytorch/pytorch/pull/54141))\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.8.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.8.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.8.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/40418343", "dateCreated": "2021-03-24T02:28:21Z", "datePublished": "2021-03-25T16:07:08Z"}, {"tagName": "v1.8.0", "name": "PyTorch 1.8 Release, including Compiler and Distributed Training updates, New Mobile Tutorials and more", "authorName": "albanD", "authorType": "User", "body": "# PyTorch 1.8.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nWe are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:\r\n\r\n1. Support for doing python to python functional transformations via `torch.fx`;\r\n2. Added or stabilized APIs to support FFTs (`torch.fft`), Linear Algebra functions (`torch.linalg`), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and\r\n3. Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression. See the full release notes [here](https://github.com/pytorch/pytorch/releases).\r\n\r\nAlong with 1.8, we are also releasing major updates to PyTorch libraries including [TorchCSPRNG](https://github.com/pytorch/csprng), [TorchVision](https://github.com/pytorch/vision), [TorchText](https://github.com/pytorch/text) and [TorchAudio](https://github.com/pytorch/audio). For more on the library releases, see the post [here](http://pytorch.org/blog/pytorch-1.8-new-library-releases). As previously noted, features in PyTorch releases are classified as Stable, Beta and Prototype. You can learn more about the definitions in the post [here](https://pytorch.org/blog/pytorch-1.8-new-library-releases).\r\n\r\nYou can find more details on all the highlighted features in the [PyTorch 1.8 Release blogpost](https://pytorch.org/blog/pytorch-1.8-released/).\r\n\r\n# Backwards Incompatible changes\r\n\r\n### Fix Tensor inplace modulo in python ([#49390](https://github.com/pytorch/pytorch/pull/49390))\r\n\r\nInplace modulo in python `%=` was wrongfully done out of place for Tensors. This change fixes the behavior.\r\nPrevious code that was relying on this operation being done out of place should be updated to use the out of place version `t = t % other` instead of `t %= other`.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.7.1</th><th>1.8.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.arange(0, 10)\r\n>>> b = a\r\n>>> b %= 3\r\n>>> print(a)\r\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n>>> print(b)\r\ntensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.arange(0, 10)\r\n>>> b = a\r\n>>> b %= 3\r\n>>> print(a)\r\ntensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\r\n>>> print(b)\r\ntensor([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Standardize `torch.clamp` edge cases ([#43288](https://github.com/pytorch/pytorch/pull/43288))\r\n\r\nFor ease of exposition let `a_min` be the value of the \"min\" argument to clamp, and `a_max` be the value of the \"max\" argument to clamp.\r\n\r\nThis PR changes the behavior of torch.clamp to always compute `min(max(a, a_min), a_max)`. `torch.clamp` currently computes this in its vectorized CPU implementation but uses different approaches for other backends.\r\nThese implementations are the same when `a_min < a_max`, but divergent when `a_min > a_max`. This divergence is easily triggered:\r\n\r\n```python\r\n>>> t = torch.arange(200).to(torch.float)\r\n>>> torch.clamp(t, 4, 2)[0]\r\ntensor(2.)\r\n\r\n>>> torch.clamp(t.cuda(), 4, 2)[0]\r\ntensor(4., device='cuda:0')\r\n\r\n>>> torch.clamp(torch.tensor(0), 4, 2)\r\ntensor(4)\r\n```\r\n\r\nThis PR makes the behavior consistent with NumPy's `clip`. C++'s `std::clamp`'s behavior is undefined when `a_min > a_max`. Python has no standard clamp implementation.\r\n\r\n### Tensor deepcopy now properly copies the `.grad` field ([#50663](https://github.com/pytorch/pytorch/pull/50663))\r\n\r\nThe deepcopy protocol will now properly copy the `.grad` field of Tensors when it exists.\r\nThe old behavior can be recovered by setting the `.grad` field to `None` after doing the deepcopy.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.7.1</th><th>1.8.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.grad\r\ntensor([0.8883, 0.5765])\r\n>>> deepcopy(t).grad\r\nNone\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.grad\r\ntensor([0.8883, 0.5765])\r\n>>> deepcopy(t).grad\r\ntensor([0.8883, 0.5765])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Fix `torch.fmod` type promotion ([#47323](https://github.com/pytorch/pytorch/pull/47323), [#48278](https://github.com/pytorch/pytorch/pull/48278))\r\n\r\n1.7.1\r\nRaises RuntimeError for integral tensor and floating-point tensor.\r\nThe dtype of output is determined by the first input.\r\n\r\n```python\r\n>>> x = torch.arange(start=1, end=6, dtype=torch.int32) # tensor([1, 2, 3, 4, 5])\r\n>>> y = torch.arange(start=1.1, end=2.1, step=0.2, dtype=torch.float32) # tensor([1.1, 1.3, 1.5, 1.7, 1.9])\r\n>>> torch.fmod(x, y)\r\nRuntimeError: result type Float can't be cast to the desired output type Int\r\n>>> z = torch.arange(start=0.2, end=1.1, step=0.2, dtype=torch.float64) # tensor([0.2, 0.4, 0.6, 0.8, 1.], dtype=torch.float64)\r\n>>> torch.fmod(y, z).dtype\r\ntorch.float32\r\n>>> torch.fmod(z, y).dtype\r\ntorch.float64\r\n>>> torch.fmod(x, 1.2)\r\ntensor([0, 0, 0, 0, 0], dtype=torch.int32)\r\n```\r\n\r\n\r\n1.8.0:\r\nSupport integral tensor and floating-point tensor as inputs.\r\nThe dtype of output is determined by both inputs.\r\n\r\n```python\r\n>>> x = torch.arange(start=1, end=6, dtype=torch.int32) # tensor([1, 2, 3, 4, 5])\r\n>>> y = torch.arange(start=1.1, end=2.1, step=0.2, dtype=torch.float32) # tensor([1.1, 1.3, 1.5, 1.7, 1.9])\r\n>>> torch.fmod(x, y)\r\ntensor([1.0000, 0.7000, 0.0000, 0.6000, 1.2000])\r\n>>> z = torch.arange(start=0.2, end=1.1, step=0.2, dtype=torch.float64) # tensor([0.2, 0.4, 0.6, 0.8, 1.], dtype=torch.float64)\r\n>>> torch.fmod(y, z).dtype\r\ntorch.float64\r\n>>> torch.fmod(z, y).dtype\r\ntorch.float64\r\n>>> torch.fmod(x, 1.2)\r\ntensor([1.0000, 0.8000, 0.6000, 0.4000, 0.2000])\r\n```\r\n\r\n### Preserve non-dense or overlapping tensor's layout in *_like functions ([#46046](https://github.com/pytorch/pytorch/pull/46046))\r\n\r\nAll the `*_like` factory functions will now generate the same striding as out of place operations would.\r\nThis means in particular that non-contiguous tensors will produce non-contiguous outputs.\r\nIf you require a contiguous output, you can pass the `memory_format=torch.contiguous` keyword argument to the factory function. Such factory functions include `clone`, `to`, `float`, `cuda,` `*_like`, `zeros`, `rand{n}`, etc.\r\n\r\n### Make output of `torch.norm` and `torch.linalg.norm` consistent for complex inputs ([#48284](https://github.com/pytorch/pytorch/pull/48284))\r\n\r\nPreviously, when given a complex input, `torch.linalg.norm` and `torch.norm` would return a complex output. `torch.linalg.cond` would sometimes return a complex output and sometimes return a real output when given a complex input, depending on its `p` argument. This PR changes this behavior to match `numpy.linalg.norm` and `numpy.linalg.cond`, so that a complex input will result in a real number type, consistent with NumPy.\r\n\r\n### Make `torch.svd` return `V`, not `V.conj()` for complex inputs ([#51012](https://github.com/pytorch/pytorch/pull/51012))\r\n\r\n`torch.svd` added support for complex inputs in PyTorch 1.7, but was not documented as doing so. The complex `V` tensor returned was actually the complex conjugate of what's expected. This PR fixes the discrepancy.\r\nUsers that were already using the previous version of `torch.svd` with complex inputs can recover the previous behavior by taking the complex conjugate of the returned `V`.\r\n\r\n### `torch.angle`: properly handle pure real numbers ([#49163](https://github.com/pytorch/pytorch/pull/49163))\r\n\r\nThis PR updates PyTorch's `torch.angle` operator to be consistent with NumPy's. Previously `torch.angle` would return zero for all real inputs (including NaN). Now angle returns `pi` for negative real inputs, zero for non-negative real inputs, and propagates NaNs.\r\n\r\n### Enable distribution validation by default for `torch.distributions` ([#48743](https://github.com/pytorch/pytorch/pull/48743))\r\n\r\nThis may slightly slow down some models. Concerned users may disable validation by using `torch.distributions.Distribution.set_default_validate_args(False)` or by disabling individual distribution validation via `MyDistribution(..., validate_args=False)`.\r\n\r\nThis may cause new `ValueErrors` in models that rely on unsupported behavior, e.g. `Categorical.log_prob()` applied to continuous-valued tensors (only {0,1}-valued tensors are supported).\r\nSuch models should be fixed but the previous behavior can be recovered by disabling argument validation using the methods mentioned above.\r\n\r\n### Prohibit assignment to a sparse tensor ([#50040](https://github.com/pytorch/pytorch/pull/50040))\r\n\r\nAssigning to a sparse Tensor did not work properly and resulted in a no-op. The following code now properly raises an error:\r\n```python\r\n>>> t = torch.rand(10).to_sparse()\r\n>>> t[0] = 42\r\nTypeError: Cannot assign to a sparse tensor\r\n```\r\n\r\n### C++ API: operators that take a list of optional `Tensor`s cannot be called with `ArrayRef<Tensor>` anymore ([#49138](https://github.com/pytorch/pytorch/pull/49138))\r\n\r\nThis PR changes the C++ API representation of lists of optional Tensors (e.g. in the `Tensor::``index` method) from `ArrayRef<Tensor>` to  `List<optional<Tensor>>`. This change breaks backwards compatibility, since there is no implicit conversion from `ArrayRef<Tensor>` to `List<optional<Tensor>>`. \r\n\r\nA common call pattern is `tensor.index({indices_tensor})`, where `indices_tensor` is a `Tensor`. This will continue to work because the `{}` initializer_list constructor for `List<optional<Tensor>>` can take `Tensor` elements that are implicitly converted to `optional<Tensor>`. \r\n\r\nHowever, another common call pattern is `tensor.index(indices_tensor)`, where previously the `Tensor` got implicitly converted to an `ArrayRef<Tensor>`. To implicitly convert `Tensor` -> `optional<Tensor>` -> `List<optional<Tensor>>` would chain two implicit conversions, which C++ doesn't allow. So those call sites should be rewritten to use the  `tensor.index({indices_tensor})` pattern.\r\n\r\n### Autograd view creation informations are now properly propagated when views are chained\r\n\r\nAfter this fix, an error will properly be thrown to avoid wrong gradients when an in-place operation is performed on a view of a view, when in-place operation were not allowed on the first view.\r\nThis means that code that used to return wrong gradients in 1.7.1 (such as `t.unbind()[0].select(0, 0).add_(1)`) will now properly raise an error.\r\n\r\n### End of deprecation cycle for spectral ops in the torch. namespace ([#48594](https://github.com/pytorch/pytorch/pull/48594))\r\n\r\nThis PR removes the deprecated `torch.{fft,rfft,ifft,irfft}` and their corresponding methods on `torch.Tensor`. PyTorch programs using these functions must now update to use the `torch.fft` namespace.\r\n\r\n### `torch.digamma` : properly handle all inputs ([#48302](https://github.com/pytorch/pytorch/pull/48302))\r\n\r\nThis PR updates PyTorch's `torch.digamma` function to be consistent with SciPy's `special.digamma` function. This changes the result of the `torch.digamma` function on the nonpositive integers, where the gamma function is not defined. Since the gamma function is undefined at these points, the (typical) derivative of the logarithm of the gamma function is also undefined at these points, and for negative integers this PR updates `torch.digamma` to return `NaN`. For zero, however, it returns `-inf` to be consistent with SciPy.\r\n\r\nInterestingly, SciPy made a similar change, which was noticed by at least one user: [scipy/scipy#9663](https://github.com/scipy/scipy/issues/9663#issue-396587679)\r\n\r\nSciPy's returning of negative infinity at zero is intentional:\r\nhttps://github.com/scipy/scipy/blob/59347ae8b86bcc92c339efe213128f64ab6df98c/scipy/special/cephes/psi.c#L163\r\n\r\nThis change is consistent with the C++ standard for the gamma function:\r\nhttps://en.cppreference.com/w/cpp/numeric/math/tgamma\r\n\r\n### Fix `torch.remainder` type promotion ([#48668](https://github.com/pytorch/pytorch/pull/48668))\r\n\r\n1.7.1:\r\nIn the case where the second argument is a python number, the result is casted to the dtype of the first argument.\r\n\r\n```python\r\n>>> torch.remainder(x, 1.2)\r\ntensor([0, 0, 0, 0, 0], dtype=torch.int32)\r\n```\r\n\r\n\r\n1.8.0\r\nIn the case where the second argument is a python number, the dtype of result is determined by type promotion of both inputs.\r\n\r\n```python\r\n>>> torch.remainder(x, 1.2)\r\ntensor([1.0000, 0.8000, 0.6000, 0.4000, 0.2000])\r\n```\r\n\r\n### Changes to onnx export API to better handle named arguments ([#47367](https://github.com/pytorch/pytorch/pull/47367))\r\n\r\nThe `args` input argument of the `torch.onnx.export` function is updated to better support optional arguments. An optional dictionary can be passed in addition as the last argument in the `args` tuple, specifying inputs with the corresponding named parameter. Note that this is backward breaking for cases where the last input is also of a dictionary type. In the new API, for such cases, it is mandatory to have an empty dictionary as the last argument in the `args` tuple.\r\nMore details can be found at: https://pytorch.org/docs/1.8.0/onnx.html?highlight=onnx#using-dictionaries-to-handle-named-arguments-as-model-inputs.\r\n\r\n### Update signature of `torch.quantization.quantize` function [#48537](https://github.com/pytorch/pytorch/pull/48537)\r\n\r\nThe `run_args` argument must now contain a list or tuple containing the positional arguments, even if there is only a single argument.\r\nIn particular, code like: `qmodel = quantize(float_model, default_eval_fn, img_data)` that was working in 1.7.1 will now raise the error: `TypeError: default_eval_fn() takes 2 positional arguments but 3 were given`.\r\nYou should update this code to provide the image in a list for example: `qmodel = quantize(float_model, default_eval_fn, [img_data])`\r\n\r\n### Change the way we quantize relu, leaky relu and sigmoid([#47415](https://github.com/pytorch/pytorch/pull/47415), [#48038](https://github.com/pytorch/pytorch/pull/48038), [#45702,](https://github.com/pytorch/pytorch/pull/45702)[#45711](https://github.com/pytorch/pytorch/pull/45711), [#45883](https://github.com/pytorch/pytorch/pull/45883) [#45883](https://github.com/pytorch/pytorch/pull/45883), [#45882](https://github.com/pytorch/pytorch/pull/45882), [#47660](https://github.com/pytorch/pytorch/pull/47660)**)**\r\n\r\nStarting with version 1.8.0, in the eager mode quantization flow, relu is not observed anymore as it is not needed.\r\nIn previous versions, quantized `leaky_relu` and `sigmoid` did not require observation and just inherited the quantization parameters from their input, but that does not work very well in eager mode quantization. Starting with version 1.8.0, they are observed operator so that they work better in eager mode quantization.\r\n\r\n### Update direction numbers to 21201 dims in the SobolEngine ([#49710](https://github.com/pytorch/pytorch/pull/49710))\r\n\r\nThis update is BC-breaking because the values drawn by the engine will be different from the ones drawn in 1.7.1 even with the same seed.\r\n\r\n\r\n<p align=\"center\">\r\n  <table  align=\"center\">\r\n    <tr><th>1.7.1</th><th>1.8.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> from torch.quasirandom import SobolEngine\r\n>>> eng = SobolEngine(1)\r\n>>> eng.draw(3)\r\ntensor([[0.5000],\r\n            [0.7500],\r\n            [0.2500]])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> from torch.quasirandom import SobolEngine\r\n>>> eng = SobolEngine(1)\r\n>>> eng.draw(3)\r\ntensor([[0.0000],\r\n            [0.5000],\r\n            [0.7500]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### Deprecate old style `nn.Module` backward hooks ([#46163](https://github.com/pytorch/pytorch/pull/46163))\r\n\r\nOld style `nn.Module` backward hooks have been broken for a long time (they do not behave as advertised in the documentation). We now have new `nn.Module.register_full_backward_hook` that provide a fully working implementation of these hooks.\r\nThe old function should not be used and migrated to the new full version.\r\n\r\nAn example of this discrepancy is shown in the example below where a Linear layer takes as input a single Tensor of size 5 and returns a single Tensor of size 5 but old style hook would return two gradients with respect to the input for only one input.\r\n\r\n1.7.1:\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nmod = nn.Linear(5, 5)\r\ndef hook(mod, grad_inp, grad_out):\r\n    print(f\"grad input size: \" + \" \".join(str(g.size()) for g in grad_inp))\r\n    print(f\"grad output size: \" + \" \".join(str(g.size()) for g in grad_out))\r\nmod.register_backward_hook(hook)\r\n\r\nmod(torch.rand(5, requires_grad=True)).sum().backward()\r\n>>> `grad input size: torch.Size([5]) torch.Size([5]) # One too many\r\n>>> grad output size: torch.Size([5])`\r\n```\r\n\r\n1.8.0:\r\nOld style hooks are deprecated and will warn when providing wrong result.\r\n\r\n```python\r\nimport torch\r\nfrom torch import nn\r\n\r\nmod = nn.Linear(5, 5)\r\ndef hook(mod, grad_inp, grad_out):\r\n    print(f\"grad input size: \" + \" \".join(str(g.size()) for g in grad_inp))\r\n    print(f\"grad output size: \" + \" \".join(str(g.size()) for g in grad_out))\r\nmod.register_backward_hook(hook)\r\n\r\nmod(torch.rand(5, requires_grad=True)).sum().backward()\r\n>>> grad input size: torch.Size([5]) torch.Size([5]) # One too many\r\n>>> grad output size: torch.Size([5])\r\n>>> `UserWarning: Using a non-full backward hook when the forward contains multiple\r\nautograd Nodes is deprecated and will be removed in future versions. This hook\r\nwill be missing some grad_input.`\r\n```\r\n\r\nFull hooks should be used to get the proper result all the time and avoid warnings\r\n\r\n```python\r\nmod.register_full_backward_hook(hook)\r\n\r\nmod(torch.rand(5, requires_grad=True)).sum().backward()\r\n>>> grad input size: torch.Size([5])\r\n>>> grad output size: torch.Size([5])\r\n```\r\n\r\n### `torch.stft`: Deprecate default value of the `require_complex` argument ([#49022](https://github.com/pytorch/pytorch/pull/49022), [#50102](https://github.com/pytorch/pytorch/pull/50102))\r\n\r\nPreviously `torch.stft` took an optional `return_complex` parameter that indicated whether the output would be a real tensor or a complex tensor. `return_complex` has the default value of `False`. This default value is deprecated (meaning that this optional argument is becoming mandatory) and will be removed in future versions. You can pass this argument explicitly to avoid this deprecation.\r\n\r\n### Deprecate `torch.set_deterministic` in favor of `torch.use_deterministic_algorithms` ([#49904](https://github.com/pytorch/pytorch/pull/49904))\r\n\r\nThis beta feature is being renamed for improved clarity. Users should migrate to use the new name.\r\n\r\n### Deprecate `torch.*` linear algebra functions in favor of the `torch.linalg.*` variant for `cholesky` ([#51460](https://github.com/pytorch/pytorch/pull/51460)), `slogdet` ([#51354](https://github.com/pytorch/pytorch/pull/51354)), `inverse` ([#51672](https://github.com/pytorch/pytorch/pull/51672)), `pinverse` ([#51671](https://github.com/pytorch/pytorch/pull/51671))\r\n\r\nAll the linear algebra functions are being moved to the `torch.linalg` submodule that provided a compatible API with NumPy. These new functions have the same set of features as the `torch.` ones and should be used instead.\r\n\r\n# New features\r\n\r\n### Python API\r\n\r\n* New functions (most of them to improve numpy compatibility): `torch.nan_to_num` ([#44592](https://github.com/pytorch/pytorch/pull/44592)), `torch.tensor_split` ([#45168](https://github.com/pytorch/pytorch/pull/45168)), `torch.nanmedian` ([#45847](https://github.com/pytorch/pytorch/pull/45847)), `torch.ravel` ([#46098](https://github.com/pytorch/pytorch/pull/46098)), `torch.igamma` ([#46183](https://github.com/pytorch/pytorch/pull/46183)), `torch.igammac` ([#48171](https://github.com/pytorch/pytorch/pull/48171)), `torch.{column_stack,row_stack}` ([#46313](https://github.com/pytorch/pytorch/pull/46313)), `torch.kron` ([#45358](https://github.com/pytorch/pytorch/pull/45358)), `torch.copysign` ([#46396](https://github.com/pytorch/pytorch/pull/46396)), `Tensor.new_empty_strided` ([#47225](https://github.com/pytorch/pytorch/pull/47225)), `torch.{swapdims,swapaxes}` ([#46041](https://github.com/pytorch/pytorch/pull/46041)), `torch.tile` ([#47974](https://github.com/pytorch/pytorch/pull/47974)), `torch.float_power` ([#44937](https://github.com/pytorch/pytorch/pull/44937)), `torch.moveaxis` ([#48581](https://github.com/pytorch/pytorch/pull/48581)), `torch.inner` ([#46716](https://github.com/pytorch/pytorch/pull/46716)), `torch.msort` ([#48440](https://github.com/pytorch/pytorch/pull/48440)), `torch.sinc` ([#48740](https://github.com/pytorch/pytorch/pull/48740)), `torch.broadcast_to` ([#48997](https://github.com/pytorch/pytorch/pull/48997)), `torch.xlogy` ([#48777](https://github.com/pytorch/pytorch/pull/48777)), `torch.f{max,min}` ([#49312](https://github.com/pytorch/pytorch/pull/49312)), `torch.diff` ([#50569](https://github.com/pytorch/pytorch/pull/50569)), `torch.ldexp` ([#45370](https://github.com/pytorch/pytorch/pull/45370)), `torch.broadcast_shapes` ([#43935](https://github.com/pytorch/pytorch/pull/43935)), \r\n* `torch.fft` new features: 2D FFT functions ([#45164](https://github.com/pytorch/pytorch/pull/45164)), use new FFT operators in stft ([#47601](https://github.com/pytorch/pytorch/pull/47601)), helper functions ([#44877](https://github.com/pytorch/pytorch/pull/44877)), fuzzing benchmark ([#47872](https://github.com/pytorch/pytorch/pull/47872))\r\n* `torch.linalg` new features: `linalg.tensorsolve` ([#46142](https://github.com/pytorch/pytorch/pull/46142)), `linalg.cholesky` ([#46083](https://github.com/pytorch/pytorch/pull/46083)), `linalg.tensorinv` ([#45969](https://github.com/pytorch/pytorch/pull/45969)), `linalg.{eigh,eigvalsh}` ([#45526](https://github.com/pytorch/pytorch/pull/45526)), `linalg.matrix_rank` ([#48206](https://github.com/pytorch/pytorch/pull/48206)), `linalg.solve` ([#48456](https://github.com/pytorch/pytorch/pull/48456)), `linalg.qr` ([#47764](https://github.com/pytorch/pytorch/pull/47764),  [#50046](https://github.com/pytorch/pytorch/pull/50046)), `linalg.svd` ([#45562](https://github.com/pytorch/pytorch/pull/45562)), `linalg.inv` ([#48261](https://github.com/pytorch/pytorch/pull/48261)), `linalg.pinv` ([#48399](https://github.com/pytorch/pytorch/pull/48399)), `linalg.slogdet` ([#49194](https://github.com/pytorch/pytorch/pull/49194)), `linalg.cond` ([#45832](https://github.com/pytorch/pytorch/pull/45832))\r\n* New `torch.nn` Modules: `nn.PixelUnshuffle` ([#49334](https://github.com/pytorch/pytorch/pull/49334)), `nn.GaussianNLLLoss` ([#50886](https://github.com/pytorch/pytorch/pull/50886))\r\n* Automatic shape inference in `torch.nn`: new `nn.LazyLinear` ([#44538](https://github.com/pytorch/pytorch/pull/44538)), `nn.LazyConv{1,2,3}d` and `nn.LazyConvTranspose{1,2,3}d` ([#47350](https://github.com/pytorch/pytorch/pull/47350))\r\n* Add channels last support for `torch.nn.AdaptiveAvgPool2d` ([#48916](https://github.com/pytorch/pytorch/pull/48916))\r\n* Add option to produce standalone executable with `cpp_extensions` ([#47862](https://github.com/pytorch/pytorch/pull/47862))\r\n* Add sparse-sparse matrix multiplication support ([#39526](https://github.com/pytorch/pytorch/pull/39526))\r\n* Add `torch.futures.Future.add_done_callback` ([#45675](https://github.com/pytorch/pytorch/pull/45675))\r\n* Add `three_phase` optional argument to `torch.optim.lr_scheduler.OneCycleLR` ([#42715](https://github.com/pytorch/pytorch/pull/42715))\r\n* Add `bicubic` option for the `mode` argument of `torch.nn.functional.grid_sampler` ([#44780](https://github.com/pytorch/pytorch/pull/44780))\r\n* Add new distributions to `torch.distributions`:  `Kumaraswamy` ([#48285](https://github.com/pytorch/pytorch/pull/48285)), `LKJCholesky` ([#48798](https://github.com/pytorch/pytorch/pull/48798))\r\n* Add reparameterization support to `torch.distributions.OneHotCategorical` ([#46610](https://github.com/pytorch/pytorch/pull/46610))\r\n* Add new transforms to `torch.distributions`: `CorrCholeskyTransform` ([#48041](https://github.com/pytorch/pytorch/pull/48041))\r\n* Add new constraint to `torch.distributions`: `independent` ([#50547](https://github.com/pytorch/pytorch/pull/50547), [#50302](https://github.com/pytorch/pytorch/pull/50302))\r\n* Add zero annealing epochs to SWA optimizer ([#47579](https://github.com/pytorch/pytorch/pull/47579))\r\n* Add `close` method to `torch.hub.tqdm` mock ([#46040](https://github.com/pytorch/pytorch/pull/46040))\r\n* Add support for pruning based on custom importance scores via the `importance_scores` keyword argument ([#48378](https://github.com/pytorch/pytorch/pull/48378))\r\n* Add torch vitals ([#51047](https://github.com/pytorch/pytorch/pull/51047))\r\n\r\n### Complex Numbers\r\n\r\n* Complex Number support on CPU and CUDA for `torch.symeig` ([#45121](https://github.com/pytorch/pytorch/pull/45121)), `torch.pinverse` ([#45819](https://github.com/pytorch/pytorch/pull/45819)), `torch.det` ([#45980](https://github.com/pytorch/pytorch/pull/45980)), `torch.diagflat`  ([#47564](https://github.com/pytorch/pytorch/pull/47564)), `torch.{addcmul, addcdiv} `([#46639](https://github.com/pytorch/pytorch/pull/46639)), `torch.lu_solve` ([#48028](https://github.com/pytorch/pytorch/pull/48028)), `torch.matrix_exp` ([#48363](https://github.com/pytorch/pytorch/pull/48363)), `torch.eig` ([#49168](https://github.com/pytorch/pytorch/pull/49168)), `torch.{acosh, asinh, atanh}`  ([#50387](https://github.com/pytorch/pytorch/pull/50387)), `torch.masked_scatter` ([#51281](https://github.com/pytorch/pytorch/pull/51281)),  `torch.bmm` and `torch.baddbmm` ([#42553](https://github.com/pytorch/pytorch/pull/42553)), `torch.orgqr` ([#50502](https://github.com/pytorch/pytorch/pull/50502)), `torch.index_fill_` ([#50578](https://github.com/pytorch/pytorch/pull/50578)), `torch.cholesky_inverse` ([#50269](https://github.com/pytorch/pytorch/pull/50269))\r\n* Complex Number support on CUDA for  `torch.qr` ([#45032](https://github.com/pytorch/pytorch/pull/45032)), `torch.lu (`[`#45898`](https://github.com/pytorch/pytorch/pull/45898)`), torch.prod`([#45980](https://github.com/pytorch/pytorch/pull/45980)), `torch.triangular_solve `([#46916](https://github.com/pytorch/pytorch/pull/46916)), `torch.solve `([#47045](https://github.com/pytorch/pytorch/pull/47045)),  `torch.cholesky_solve` ([#47047](https://github.com/pytorch/pytorch/pull/47047)), `torch.mean` ([#47048](https://github.com/pytorch/pytorch/pull/47048)), `torch.svd` ([#45795](https://github.com/pytorch/pytorch/pull/45795)), `torch.inverse` ([#47595](https://github.com/pytorch/pytorch/pull/47595)),  `torch.Tensor.index_put_` ([#51148](https://github.com/pytorch/pytorch/pull/51148))\r\n* Complex Number support on CPU for  `torch.trace` ([#50380](https://github.com/pytorch/pytorch/pull/50380))\r\n* Complex Number support for `torch.nn.DataParallel` ([#48686](https://github.com/pytorch/pytorch/pull/48686)),  `torch.nn.L1Loss` ([#49912](https://github.com/pytorch/pytorch/pull/49912)), Padding functions ([#50594](https://github.com/pytorch/pytorch/pull/50594))\r\n* Complex Number support  for `torch.distributed.{all_reduce, all_gather}`  ([#45879](https://github.com/pytorch/pytorch/pull/45879), [#46270](https://github.com/pytorch/pytorch/pull/46270))\r\n* Complex Autograd support for `torch.{atan, log, log10, log1p, log2, reciprocal, tan, pow, rsqrt, tanh, asinh, acosh}` ([#46275](https://github.com/pytorch/pytorch/pull/46275)),  `torch.{cholesky, triangular_solve, mm, mv, ger} `([#45737](https://github.com/pytorch/pytorch/pull/45737)),  `torch.take(), torch.Tensor.fill_()` ([#46860](https://github.com/pytorch/pytorch/pull/46860)), `torch.matrix_exp` ([#48363](https://github.com/pytorch/pytorch/pull/48363)), `torch.{baddbmm, addbmm, addmm, addmv}` ([#50632](https://github.com/pytorch/pytorch/pull/50632)), `torch.qr` ([#48489](https://github.com/pytorch/pytorch/pull/48489)), `torch.svd` and `torch.pinverse` ([#47761](https://github.com/pytorch/pytorch/pull/47761)), `torch.sqrt` ([#49461](https://github.com/pytorch/pytorch/pull/49461)), `torch.diag` ([#51268](https://github.com/pytorch/pytorch/pull/51268)), `torch.trace` ([#51537](https://github.com/pytorch/pytorch/pull/51537)), `torch.exp` ([#47194](https://github.com/pytorch/pytorch/pull/47194)), `torch.mean` ([#47566](https://github.com/pytorch/pytorch/pull/47566)),  `torch.addr` ([#50667](https://github.com/pytorch/pytorch/pull/50667)), torch.{`stack, gather, index_select}, torch.Tensor.index_add_`([#49552](https://github.com/pytorch/pytorch/pull/49552)), `torch.{masked_scatter, masked_select}` ([#51281](https://github.com/pytorch/pytorch/pull/51281)),  `torch.{addcmul, addcdiv} `([#46639](https://github.com/pytorch/pytorch/pull/46639)),  `torch.{acosh, asinh, atanh}`  ([#50387](https://github.com/pytorch/pytorch/pull/50387)), `torch.solve `([#47045](https://github.com/pytorch/pytorch/pull/47045)), `torch.cholesky_solve` ([#47047](https://github.com/pytorch/pytorch/pull/47047)), `torch.inverse` ([#47595](https://github.com/pytorch/pytorch/pull/47595))\r\n* Add complex autograd support for named tensors ([#47289](https://github.com/pytorch/pytorch/pull/47289))\r\n* Allow converting parameters and buffers of `torch.nn.Module` to complex dtypes ([#44788](https://github.com/pytorch/pytorch/pull/44788))\r\n* Add complex support to IValues ([#50883](https://github.com/pytorch/pytorch/pull/50883), [#51476](https://github.com/pytorch/pytorch/pull/51476))\r\n* Add TorchScript type annotation logic for complex numbers ([#50884](https://github.com/pytorch/pytorch/pull/50884))\r\n* Add serialization logic for complex numbers ([#51287](https://github.com/pytorch/pytorch/pull/51287))\r\n* Add support for complex number lists in JIT ([#51145](https://github.com/pytorch/pytorch/pull/51145))\r\n* Add support for complex valued keys for dict in TorchScript ([#51472](https://github.com/pytorch/pytorch/pull/51472))\r\n* Add `scalar.conj()` ([#46596](https://github.com/pytorch/pytorch/pull/46596))\r\n* Add `Tensor.copy_()` for `ComplexHalf` tensors ([#45339](https://github.com/pytorch/pytorch/pull/45339))\r\n\r\n### Profiler\r\n\r\n* New profiler API ([#48280](https://github.com/pytorch/pytorch/pull/48280))\r\n* Use libkineto in profiler ([#46470](https://github.com/pytorch/pytorch/pull/46470))\r\n* Add FLOPS computation support to the new profiler API ([#51734](https://github.com/pytorch/pytorch/pull/51734))\r\n* Add high level profiling trace for dataloading and optimizer ([#47655](https://github.com/pytorch/pytorch/pull/47655))\r\n* Add support for SVG visualization ([#48438](https://github.com/pytorch/pytorch/pull/48438))\r\n\r\n### Autograd\r\n\r\n* Add `inputs` argument to `autograd.backward()` both in python and c++ ([#46855](https://github.com/pytorch/pytorch/pull/46855), [#47214](https://github.com/pytorch/pytorch/pull/47214))\r\n* Add support for Tensor-like objects in `torch.autograd.gradcheck` ([#45732](https://github.com/pytorch/pytorch/pull/45732))\r\n* Add experimental `vectorize` flag to `torch.autograd.functional.{jacobian, hessian}` ([#50915](https://github.com/pytorch/pytorch/pull/50915), [#51638](https://github.com/pytorch/pytorch/pull/51638))\r\n* Add anomaly mode in C++ API ([#46981](https://github.com/pytorch/pytorch/pull/46981), [#47164](https://github.com/pytorch/pytorch/pull/47164))\r\n* Make `torch.lu` differentiable. ([#46284](https://github.com/pytorch/pytorch/pull/46284))\r\n* Add support for generators in autograd decorators like `torch.no_grad` ([#49017](https://github.com/pytorch/pytorch/pull/49017))\r\n\r\n### Dataloader\r\n\r\n* Add `BufferedShuffleDataset` ([#45290](https://github.com/pytorch/pytorch/pull/45290))\r\n* Add warning if DataLoader is going to create excessive number of thread ([#46867](https://github.com/pytorch/pytorch/pull/46867))\r\n* Add prototype of `BatchIterDataPipe` ([#49186, #51880](https://github.com/pytorch/pytorch/pull/49186))\r\n* Add prototype of `SamplerIterDataPipe` ([#49363, #52104](https://github.com/pytorch/pytorch/pull/49363))\r\n* Implement `BucketBatchIterDataPipe` ([#51126, #51880](https://github.com/pytorch/pytorch/pull/51126))\r\n* Add Tar DataPipe-s ([#51398](https://github.com/pytorch/pytorch/pull/51398))\r\n* Add `MapIterDataPipe` ([#51488](https://github.com/pytorch/pytorch/pull/51488)[](https://github.com/pytorch/pytorch/commit/9eb70c3c78ca971bf0277b9991f0932b4896bfed)[#51879](https://github.com/pytorch/pytorch/pull/51879))\r\n\r\n### CUDA\r\n\r\n* Allow user to specify a fraction of the GPU memory with `set_per_process_memory_fraction`. ([#48172](https://github.com/pytorch/pytorch/pull/48172))\r\n* CUDA BFloat16 TopK ([#44755](https://github.com/pytorch/pytorch/pull/44755))\r\n* Add LazyNVRTC ([#45674](https://github.com/pytorch/pytorch/pull/45674))\r\n* Enable CUDA Fuser for ROCm ([#45965](https://github.com/pytorch/pytorch/pull/45965))\r\n* Define the record_stream method in native_functions.yaml ([#44301](https://github.com/pytorch/pytorch/pull/44301))\r\n* Add CUDA 11.1 docker build ([#46283](https://github.com/pytorch/pytorch/pull/46283))\r\n* Add nvtx.range() context manager ([#42925](https://github.com/pytorch/pytorch/pull/42925))\r\n* CUDA BFloat16 gelu, hardswish, hardsigmoid ([#44997](https://github.com/pytorch/pytorch/pull/44997))\r\n* [ROCm] enable stream priorities ([#47136](https://github.com/pytorch/pytorch/pull/47136))\r\n* Add bfloat support for torch.randn and torch.norm ([#47143](https://github.com/pytorch/pytorch/pull/47143))\r\n* CUDA BFloat16 Dropout ([#45005](https://github.com/pytorch/pytorch/pull/45005)), batchnorm (non-cuDNN) ([#44994](https://github.com/pytorch/pytorch/pull/44994)), backwards ([#48809](https://github.com/pytorch/pytorch/pull/48809)), sparse ([#48807](https://github.com/pytorch/pytorch/pull/48807)),  indexing ([#48801](https://github.com/pytorch/pytorch/pull/48801)), embedding ([#44848](https://github.com/pytorch/pytorch/pull/44848)), signal windows ([#45155](https://github.com/pytorch/pytorch/pull/45155)), norm ([#48806](https://github.com/pytorch/pytorch/pull/48806)), isinf and isfinite ([#49356](https://github.com/pytorch/pytorch/pull/49356)), gemms on arch other than ampere ([#50442](https://github.com/pytorch/pytorch/pull/50442)), clamp, remainder, lshift, rshift ([#45247](https://github.com/pytorch/pytorch/pull/45247))\r\n* Make CUDAGeneratorImpl capturable ([#48694](https://github.com/pytorch/pytorch/pull/48694))\r\n* Adding support for CuDNN-based LSTM with projections ([#47725](https://github.com/pytorch/pytorch/pull/47725))\r\n* Add `torch.cuda.can_device_access_peer` ([#50446](https://github.com/pytorch/pytorch/pull/50446))\r\n* Add torch::cuda::ncll::all2all ([#45900](https://github.com/pytorch/pytorch/pull/45900)) \r\n\r\n### C++ API\r\n\r\n* Add distance-agnostic triplet margin loss ([#45377](https://github.com/pytorch/pytorch/pull/45377))\r\n* Add `torch::nn::ModuleDict` ([#47707](https://github.com/pytorch/pytorch/pull/47707))\r\n* Add `torch::cuda::synchronize` ([#50072](https://github.com/pytorch/pytorch/pull/50072))\r\n* Add new XPU backend type for Intel heterogeneous computation platform. ([#49786](https://github.com/pytorch/pytorch/pull/49786))\r\n\r\n### TorchScript\r\n\r\n* `torch::jit::freeze` C++ api introduced ([#52337](https://github.com/pytorch/pytorch/pull/52337), [#52392](https://github.com/pytorch/pytorch/pull/52392))\r\n* Add API for ignoring arbitrary module attributes during compilation ([#45262](https://github.com/pytorch/pytorch/pull/45262))\r\n* Support tracing tensor `__setitem__` with dynamic shape ([#45828](https://github.com/pytorch/pytorch/pull/45828))\r\n* Expose script_if_tracing as public API ([#46494](https://github.com/pytorch/pytorch/pull/46494))\r\n* Support %-based string formatting ([#45976](https://github.com/pytorch/pytorch/pull/45976))\r\n* Add `torch.jit.isinstance` support for typed containers ([#46062](https://github.com/pytorch/pytorch/pull/46062))\r\n* Allow for source code comments at any level of indentation ([#46548](https://github.com/pytorch/pytorch/pull/46548))\r\n* Support hashing of various data types by implementing generic hashing for IValues ([#46441](https://github.com/pytorch/pytorch/pull/46441))\r\n* Support doc string for TorchBind custom classes ([#46576](https://github.com/pytorch/pytorch/pull/46576))\r\n* Add API for selective lowering of modules to custom JIT backend ([#43613](https://github.com/pytorch/pytorch/pull/43613))\r\n* add list() support ([#42382](https://github.com/pytorch/pytorch/pull/42382))\r\n* Support using lambda function as TorchBind constructor ([#47819](https://github.com/pytorch/pytorch/pull/47819))\r\n* Support user defined classes as constants ([#45556](https://github.com/pytorch/pytorch/pull/45556))\r\n* Allow del statements with multiple targets ([#48876](https://github.com/pytorch/pytorch/pull/48876))\r\n* Tuple Slice with both negative and positive stepped size ([#48660](https://github.com/pytorch/pytorch/pull/48660))\r\n* Expose run_async function on torch::jit::Method ([#48607](https://github.com/pytorch/pytorch/pull/48607))\r\n* Add flag torch_jit_disable_warning_prints to allow disabling all warnings.warn ([#49313](https://github.com/pytorch/pytorch/pull/49313))\r\n* Add dict comprehension ([#47774](https://github.com/pytorch/pytorch/pull/47774))\r\n* Adding support for bitwise augassignment operators (`+=` style statements) ([#44621](https://github.com/pytorch/pytorch/pull/44621))\r\n* Support the `in` operator with str ([#47057](https://github.com/pytorch/pytorch/pull/47057))\r\n* Adding JIT support for cuda streams and events ([#48020](https://github.com/pytorch/pytorch/pull/48020))\r\n* Add `Type::{castRaw,expectRef}` ([#50061](https://github.com/pytorch/pytorch/pull/50061))\r\n* Allow arbitrary docstrings to be inside torchscript interface methods ([#50271](https://github.com/pytorch/pytorch/pull/50271))\r\n* Change list striding parameters to take optional integer ([#48719](https://github.com/pytorch/pytorch/pull/48719))\r\n* Add support for scripting and running module level hooks in JIT ([#49544](https://github.com/pytorch/pytorch/pull/49544), [#49975](https://github.com/pytorch/pytorch/pull/49975), [#49545](https://github.com/pytorch/pytorch/pull/49545), [#49546](https://github.com/pytorch/pytorch/pull/49546), [#49547](https://github.com/pytorch/pytorch/pull/49547))\r\n* Support default argument values of a method ([#48863](https://github.com/pytorch/pytorch/pull/48863))\r\n* Graceful invalidation of Python Node/Value/Block when C++ object is deleted ([#50326](https://github.com/pytorch/pytorch/pull/50326))\r\n* Support `Union[NoneType, T]` as input type ([#51605](https://github.com/pytorch/pytorch/pull/51605))\r\n* Allow implicit boolean conversion of lists, strings, and dictionaries ([#51683](https://github.com/pytorch/pytorch/pull/51683))\r\n\r\n### Mobile\r\n\r\n* Add instance_key into mobile stats logging. ([#45517](https://github.com/pytorch/pytorch/pull/45517))\r\n* Profiling allocator for mobile. ([#43951](https://github.com/pytorch/pytorch/pull/43951))\r\n* [Metal] Add Metal/MPSCNN support on iOS ([#46112](https://github.com/pytorch/pytorch/pull/46112))\r\n* [Metal] Introduce USE_PYTORCH_METAL ([#46383](https://github.com/pytorch/pytorch/pull/46383))\r\n* [Metal] Support Resnet models (b63ddd6f57)\r\n* PyTorch NNAPI integration prototype ([#46780](https://github.com/pytorch/pytorch/pull/46780))\r\n* [Metal] Enable Metal on macosx ([#47635](https://github.com/pytorch/pytorch/pull/47635))\r\n* [Metal] Enable optimize_for_mobile on Linux ([#46384](https://github.com/pytorch/pytorch/pull/46384))\r\n* [Android] Fix YUV camera image to tensor ([#50871](https://github.com/pytorch/pytorch/pull/50871))\r\n* [Android] turn on USE_VULKAN for android builds by default ([#51291](https://github.com/pytorch/pytorch/pull/51291))\r\n* Add windows JNI support ([#44257](https://github.com/pytorch/pytorch/pull/44257))\r\n* \r\n* Enable partial loading of GPU models on linux CPU machines ([#51236](https://github.com/pytorch/pytorch/pull/51236))\r\n\r\n### Distributed\r\n\r\n* Support `send` and `recv` in c10d NCCL backend ([#44921](https://github.com/pytorch/pytorch/pull/44921), [#44922](https://github.com/pytorch/pytorch/pull/44922))\r\n* Add support for NCCL alltoall ([#44374](https://github.com/pytorch/pytorch/pull/44374))\r\n* Upstream `fairscale.nn.Pipe` into PyTorch as `torch.distributed.pipeline` ([#44090](https://github.com/pytorch/pytorch/pull/44090))\r\n* Add a `--logdir` option to log subprocess output to files in DDP launcher. ([#33193](https://github.com/pytorch/pytorch/pull/33193))\r\n* Support `RRef.backward()` for local RRefs. ([#46568](https://github.com/pytorch/pytorch/pull/46568)) and Owner RRefs. ([#46641](https://github.com/pytorch/pytorch/pull/46641))\r\n* Support C++ implementation for DDP communication hook. ([#46566](https://github.com/pytorch/pytorch/pull/46566))\r\n* Provide 2 default C++ comm hooks for DDP ([#46701](https://github.com/pytorch/pytorch/pull/46701))\r\n* Support remote device format `\"worker_name/device\"` ([#46773](https://github.com/pytorch/pytorch/pull/46773))\r\n* Enable creation and transfer of `ScriptModule` over RPC ([#48293](https://github.com/pytorch/pytorch/pull/48293))\r\n* Enable TCPStore on Windows ([#47749](https://github.com/pytorch/pytorch/pull/47749))\r\n* Support `torch.distributed.irecv(src=None, ...)` as `recv_anysource` ([#49383](https://github.com/pytorch/pytorch/pull/49383))\r\n* Implement layer-wise PowerSGD as a DDP comm hook ([#49639](https://github.com/pytorch/pytorch/pull/49639))\r\n* Support `alltoall_single` in TorchScript ([#48345](https://github.com/pytorch/pytorch/pull/48345))\r\n* Enable GPU-to-GPU comm in `TensorPipeAgent` ([#44418](https://github.com/pytorch/pytorch/pull/44418))\r\n* Support timeout in `rref._get_type()` ([#50498](https://github.com/pytorch/pytorch/pull/50498))\r\n* Support timeout for RRef proxy functions ([#50499](https://github.com/pytorch/pytorch/pull/50499))\r\n* Add optimizer state sharding as `ZeroRedundancyOptimizer` ([#46750](https://github.com/pytorch/pytorch/pull/46750))\r\n* Add distributed functional `Adam` optimizer ([#50624](https://github.com/pytorch/pytorch/pull/50624)),  `sgd` optimizer ([#50618](https://github.com/pytorch/pytorch/pull/50618)),  `Adadelta` optimizer ([#50623](https://github.com/pytorch/pytorch/pull/50623)),  `RMSprop` optimizer ([#50619](https://github.com/pytorch/pytorch/pull/50619)), l `AdamW` optimizer ([#50620](https://github.com/pytorch/pytorch/pull/50620))\r\n* Create a DDPLoggingData struct and expose it to python interface ([#50622](https://github.com/pytorch/pytorch/pull/50622))\r\n* Implement autograd functions for c10d communication operations ([#40762](https://github.com/pytorch/pytorch/pull/40762))\r\n* Enable TensorPipe's SHM transport ([#50760](https://github.com/pytorch/pytorch/pull/50760))\r\n* Support device map for distributed autograd while using TensorPipe. ([#44859](https://github.com/pytorch/pytorch/pull/44859))\r\n* Create PyTorch DDP logging APIs for applications to use ([#50637](https://github.com/pytorch/pytorch/pull/50637))\r\n* Add `set_exception` API in `torch.futures.Future` ([#50983](https://github.com/pytorch/pytorch/pull/50983))\r\n* Add `scatter_object_list` API for c10d ([#43930](https://github.com/pytorch/pytorch/pull/43930))\r\n* Provide parameter to pass GPU ID in barrier function ([#49069](https://github.com/pytorch/pytorch/pull/49069))\r\n* Enable TensorPipe CUDA fallback channel ([#50675](https://github.com/pytorch/pytorch/pull/50675))\r\n* Enable TensorPipe's InfiniBand transport ([#50761](https://github.com/pytorch/pytorch/pull/50761))\r\n\r\n### torch.fx\r\n\r\n* allow custom behavior for args, kwargs, and bool ([#45193](https://github.com/pytorch/pytorch/pull/45193))\r\n* Mutable Graph APIs ([#45227](https://github.com/pytorch/pytorch/pull/45227))\r\n* Make output a non-special Node ([#45599](https://github.com/pytorch/pytorch/pull/45599))\r\n* Make `Tracer.trace()` just return a Graph ([#45704](https://github.com/pytorch/pytorch/pull/45704))\r\n* Preserve type annotations on generated code in Graph ([#45880](https://github.com/pytorch/pytorch/pull/45880))\r\n* Make `graph_copy` examine existing values in val_map ([#46104](https://github.com/pytorch/pytorch/pull/46104))\r\n* Allow tracing free functions ([#46268](https://github.com/pytorch/pytorch/pull/46268))\r\n* Make sure args/kwargs are immutable ([#46325](https://github.com/pytorch/pytorch/pull/46325))\r\n* Make wrapped functions traceable ([#46692](https://github.com/pytorch/pytorch/pull/46692))\r\n* Added `GraphModule.to_folder` ([#47544](https://github.com/pytorch/pytorch/pull/47544))\r\n* Support default args in symbolic tracing ([#47615](https://github.com/pytorch/pytorch/pull/47615))\r\n* Add `Node.all_input_nodes` ([#48270](https://github.com/pytorch/pytorch/pull/48270))\r\n* Support torchbind as attribute in torch.fx symbolic tracing ([#48732](https://github.com/pytorch/pytorch/pull/48732))\r\n* Create subgraph rewriter API ([#49540](https://github.com/pytorch/pytorch/pull/49540))\r\n* Make len traceable and scriptable with wrap ([#50184](https://github.com/pytorch/pytorch/pull/50184))\r\n* Add Interpreter and Transformer APIs ([#50420](https://github.com/pytorch/pytorch/pull/50420))\r\n* Add alternative prettyprinting method to `Graph` ([#50878](https://github.com/pytorch/pytorch/pull/50878))\r\n* Move some heavily used passes out of experimental ([#51392](https://github.com/pytorch/pytorch/pull/51392))\r\n* Added partial concrete values for symbolic tracing ([#51609](https://github.com/pytorch/pytorch/pull/51609))\r\n\r\n### Quantization\r\n\r\n* Quantized Operators and Modules\r\n    *  Embedding and EmbeddingBag operator support\r\n        * creating quint4x2 dtype for quantized tensors ([#44678](https://github.com/pytorch/pytorch/pull/44678))\r\n        * PerChannelFloatQParams support for quint4x2 dtype ([#45594](https://github.com/pytorch/pytorch/pull/45594))\r\n        * Add 4-bit embedding_bag prepack/unpack support using quint4x2 ([#45751](https://github.com/pytorch/pytorch/pull/45751))\r\n        * Support 4-bit embedding_bag operators using the dtype quint4x2 ([#45752](https://github.com/pytorch/pytorch/pull/45752))\r\n        * Support for 4-bit quantized EmbeddingBag module ([#45865](https://github.com/pytorch/pytorch/pull/45865))\r\n        * Refactor qembeddingbag to remove duplicate code ([#45881](https://github.com/pytorch/pytorch/pull/45881))\r\n        * Rename the sparse argument for embedding_bag ops ([#46003](https://github.com/pytorch/pytorch/pull/46003))\r\n        * Add support for pruned weights in embedding_bag_byte lookup ([#47329](https://github.com/pytorch/pytorch/pull/47329))\r\n        * fp16 -> fp32 EmbeddingBag moved into CPU impl ([#47076](https://github.com/pytorch/pytorch/pull/47076))\r\n        * Add non-fbgemm fallback implementation for embedding lookup ops ([#50706](https://github.com/pytorch/pytorch/pull/50706))\r\n        * Out variant for embedding_bag_4bit_rowwise_offsets ([#51324](https://github.com/pytorch/pytorch/pull/51324))\r\n        * Using int32 as indices for embedding_bag operators ([#45878](https://github.com/pytorch/pytorch/pull/45878))\r\n    * Add transposed conv support for fbgemm backend for 1d, 2d, 3d ([#46607](https://github.com/pytorch/pytorch/pull/46607), [#46608](https://github.com/pytorch/pytorch/pull/46608))\r\n    * Add quantized flip dispatch ([#46235](https://github.com/pytorch/pytorch/pull/46235))\r\n    * Add support for ReflectionPad2d ([#48036](https://github.com/pytorch/pytorch/pull/48036))\r\n    * Dynamic GRU quantization support ([#49448](https://github.com/pytorch/pytorch/pull/49448))\r\n    * Quantizable LSTM ([#49671](https://github.com/pytorch/pytorch/pull/49671))\r\n* Quantization Flow/API\r\n    * quantization: Linear + BatchNorm1d fusion ([#50748](https://github.com/pytorch/pytorch/pull/50748))\r\n    * compare_model_stub_fx API implementation ([#48951](https://github.com/pytorch/pytorch/pull/48951))\r\n    * Add additional_fuser_method_mapping to config ([#46355](https://github.com/pytorch/pytorch/pull/46355))\r\n    * Compare Weights FX Implementation ([#48056](https://github.com/pytorch/pytorch/pull/48056))\r\n    * Numeric Suite: Swap with shadow modules only for quantized part of model ([#51052](https://github.com/pytorch/pytorch/pull/51052))\r\n* FX Graph Mode Quantization\r\n    * Add prepare_custom_config_dict and convert_custom_config_dict ([#46223](https://github.com/pytorch/pytorch/pull/46223), [#46364](https://github.com/pytorch/pytorch/pull/46364))\r\n    * Add FixedQParamsFakeQuantize module ([#46657](https://github.com/pytorch/pytorch/pull/46657))\r\n    * Add support for additional_fuse_method_mapping ([#46345](https://github.com/pytorch/pytorch/pull/46345)), additional_{fusion/quant}_pattern ([#46346](https://github.com/pytorch/pytorch/pull/46346))\r\n    * Support in qat sigmoid/hardsigmoid/tanh ([#46871](https://github.com/pytorch/pytorch/pull/46871)), convbn{relu}1d ([#47248](https://github.com/pytorch/pytorch/pull/47248)),  FloatFunctional ([#46634](https://github.com/pytorch/pytorch/pull/46634))\r\n    * custom_module support static/dynamic/weight_only quant ([#46786](https://github.com/pytorch/pytorch/pull/46786))\r\n    * Support standalone_module_class ([#47705](https://github.com/pytorch/pytorch/pull/47705))\r\n    * Embedding/EmbeddingBag works in static quant qconfig ([#48062](https://github.com/pytorch/pytorch/pull/48062))\r\n    * Add MatchAllNode in pattern matching ([#48979](https://github.com/pytorch/pytorch/pull/48979))\r\n    * Add support for dynamic quant for RNN and RNNCell ([#49126](https://github.com/pytorch/pytorch/pull/49126)), ConvTranspose{n}d ([#49717](https://github.com/pytorch/pytorch/pull/49717)), quantizing functional linear + {functional relu/module relu} ([#50975](https://github.com/pytorch/pytorch/pull/50975)), functional conv2d + relu ([#51079](https://github.com/pytorch/pytorch/pull/51079)), functional conv1d and conv3d (#51155) ([#51254](https://github.com/pytorch/pytorch/pull/51254)), Scalar as first input for add/mul ([#46751](https://github.com/pytorch/pytorch/pull/46751)), leaky relu ([#45712](https://github.com/pytorch/pytorch/pull/45712)), Embedding ([#46677](https://github.com/pytorch/pytorch/pull/46677)), EmbeddingBag ([#46678](https://github.com/pytorch/pytorch/pull/46678))\r\n    * Remove inplace option for convert_fx ([#46955](https://github.com/pytorch/pytorch/pull/46955))\r\n    * Support non_traceable_module/module_class ([#46298](https://github.com/pytorch/pytorch/pull/46298))\r\n    * Add additional_object_mapping argument to convert ([#46338](https://github.com/pytorch/pytorch/pull/46338))\r\n    * Keep linear op unchanged when qconfig is not supported ([#48067](https://github.com/pytorch/pytorch/pull/48067))\r\n    * Move {input|output}_quantized_idxs cfg from convert to prepare ([#49238](https://github.com/pytorch/pytorch/pull/49238))\r\n    * Allow user to specify qconfig for call_method ([#49621](https://github.com/pytorch/pytorch/pull/49621))\r\n    * Do not observe bias on F.conv and F.linear ([#49623](https://github.com/pytorch/pytorch/pull/49623), [#49628](https://github.com/pytorch/pytorch/pull/49628))\r\n    * Linear work with float_qparam_dynamic_qconfig ([#47068](https://github.com/pytorch/pytorch/pull/47068))\r\n    * Fix error that DefaultQuantizer is not inserted after a module configured with None qconfig ([#47316](https://github.com/pytorch/pytorch/pull/47316))\r\n    * Scope support for call_method in QuantizationTracer ([#50173](https://github.com/pytorch/pytorch/pull/50173))\r\n    * Support preserved_attributes in prepare_fx ([#50306](https://github.com/pytorch/pytorch/pull/50306))\r\n    * Add option to leave graph inputs and/or outputs quantized ([#48624](https://github.com/pytorch/pytorch/pull/48624))\r\n    * Support quantization for custom module ([#44074](https://github.com/pytorch/pytorch/pull/44074))\r\n    * Remove `inplace` option for fuse_fx ([#46953](https://github.com/pytorch/pytorch/pull/46953)) and prepare_fx ([#46954](https://github.com/pytorch/pytorch/pull/46954))\r\n    * Scope support for call_function in QuantizationTracer ([#51086](https://github.com/pytorch/pytorch/pull/51086))\r\n\r\n### ONNX\r\n\r\n* Preprocess index_put with bool inputs to `torch.masked_{scatter,fill}` ([#45584](https://github.com/pytorch/pytorch/pull/45584))\r\n* Export `torch.{var,var_mean,std_mean}` ops ([#45678](https://github.com/pytorch/pytorch/pull/45678))\r\n* Enable NoneType inputs to export API ([#45792](https://github.com/pytorch/pytorch/pull/45792))\r\n* Add export of prim::dtype, prim::tolist ([#46019](https://github.com/pytorch/pytorch/pull/46019))\r\n* Enable onnx shape inference in export by default ([#46629](https://github.com/pytorch/pytorch/pull/46629))\r\n* Add `torch.silu` operator support for onnx ([#51519](https://github.com/pytorch/pytorch/pull/51519))\r\n* Support list remove for onnx export ([#51526](https://github.com/pytorch/pytorch/pull/51526))\r\n* Added `torch.hardswish` symbolic in opset 9 ([#48423](https://github.com/pytorch/pytorch/pull/48423))\r\n* Add export of `aten::is_floating` point ([#46442](https://github.com/pytorch/pytorch/pull/46442))\r\n* Add `torch.logical_{and,or,xor}` torch op support in pytorch exporter ([#50909](https://github.com/pytorch/pytorch/pull/50909))\r\n* Add `torch.binary_cross_entropy_with_logits` op to ONNX opset version 12 ([#50908](https://github.com/pytorch/pytorch/pull/50908))\r\n* Support opset13 `nn.Squeeze` and `nn.Unsqueeze` ([#50906](https://github.com/pytorch/pytorch/pull/50906))\r\n* Add export of `prim::data` ([#45747](https://github.com/pytorch/pytorch/pull/45747))\r\n* Support `torch.nonzero(*, as_tuple=True)` export ([#47421](https://github.com/pytorch/pytorch/pull/47421))\r\n* Update Reducesum operator for opset 13 ([#50907](https://github.com/pytorch/pytorch/pull/50907))\r\n\r\n### Misc\r\n\r\n* Enable python code coverage on windows ([#44548](https://github.com/pytorch/pytorch/pull/44548)) and onnx ([#47387](https://github.com/pytorch/pytorch/pull/47387))\r\n* Fix PyTorch compilation on Apple M1 chips ([#48275](https://github.com/pytorch/pytorch/pull/48275), [#49701](https://github.com/pytorch/pytorch/pull/49701))\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Add integer support (by promoting integer to float) to `torch.{cos,sin,tan}` ([#45733](https://github.com/pytorch/pytorch/pull/45733), [#46706](https://github.com/pytorch/pytorch/pull/46706)), `torch.log{2,10}` ([#46810](https://github.com/pytorch/pytorch/pull/46810)), `torch.{a}tanh` ([#47064](https://github.com/pytorch/pytorch/pull/47064)), `torch.a{cos, tan}` ([#47005](https://github.com/pytorch/pytorch/pull/47005)), `torch.a{cosh, sinh}` ([#47152](https://github.com/pytorch/pytorch/pull/47152)), `torch.sqrt` ([#47293](https://github.com/pytorch/pytorch/pull/47293)), `torch.log1p`  ([#48002](https://github.com/pytorch/pytorch/pull/48002)). `torch.erf{c}` ([#48472](https://github.com/pytorch/pytorch/pull/48472)), `torch.asin` ([#48461](https://github.com/pytorch/pytorch/pull/48461)), `torch.sigmoid` ([#47551](https://github.com/pytorch/pytorch/pull/47551)), `torch.sinh` ([#48644](https://github.com/pytorch/pytorch/pull/48644)), `torch.cosh` ([#48923](https://github.com/pytorch/pytorch/pull/48923)), `torch.exp{2, m1}`([#48926](https://github.com/pytorch/pytorch/pull/48926)), `torch.reciprocal` ([#49102](https://github.com/pytorch/pytorch/pull/49102)), `torch.erfinv` ([#49155](https://github.com/pytorch/pytorch/pull/49155)), `torch.rsqrt` ([#47909](https://github.com/pytorch/pytorch/pull/47909)), `torch.exp`  ([#50093](https://github.com/pytorch/pytorch/pull/50093)), `torch.lgamma` ([#50140](https://github.com/pytorch/pytorch/pull/50140))\r\n* Add optional `dtype` argument to `Tensor.view` ([#47951](https://github.com/pytorch/pytorch/pull/47951))\r\n* Add `out` optional arguments to `torch.{reshape,flatten}`  ([#51249](https://github.com/pytorch/pytorch/pull/51249)), `torch.tensordot` ([#47278](https://github.com/pytorch/pytorch/pull/47278)), `torch.fft.*` ([#49335](https://github.com/pytorch/pytorch/pull/49335)), `torch.narrow_copy` ([#49502](https://github.com/pytorch/pytorch/pull/49502))\r\n* Add support for int32 indices and offset in `nn.Embedding` and `nn.EmbeddingBag` ([#46758](https://github.com/pytorch/pytorch/pull/46758))\r\n* Add boolean type support to `torch.where` ([#47454](https://github.com/pytorch/pytorch/pull/47454)), `torch.mul` and `Tensor.__mul__` ([#48637](https://github.com/pytorch/pytorch/pull/48637)), `torch.diag` ([#47455](https://github.com/pytorch/pytorch/pull/47455)), `torch.{all,any}` ([#44790](https://github.com/pytorch/pytorch/pull/44790)), ` Tensor.to_dense` ([#50019](https://github.com/pytorch/pytorch/pull/50019))\r\n* Add inplace version of `torch.cum{sum,prod}_` ([#47651](https://github.com/pytorch/pytorch/pull/47651))\r\n* Add sparse support to `torch.sqrt` ([#50088](https://github.com/pytorch/pytorch/pull/50088))\r\n* Add support for both `dtype` and `ord` arguments in `torch.linalg.norm` ([#46637](https://github.com/pytorch/pytorch/pull/46637))\r\n* Make `torch.nn` Module accept batch size of 0: `nn.ReplicationPad` ([#39137](https://github.com/pytorch/pytorch/pull/39137)), `nn.Unfold` ([#40689](https://github.com/pytorch/pytorch/pull/40689)), `nn.PixelShuffle`  ([#49187](https://github.com/pytorch/pytorch/pull/49187)), `nn.AvgPool{1,2,3}d`  ([#50008](https://github.com/pytorch/pytorch/pull/50008)), `nn.MultiLabelMarginLoss` and `nn.MultiMarginLoss` ([#50007](https://github.com/pytorch/pytorch/pull/50007))\r\n* `utils.cpp_extensions` Ensure default extra_compile_args are properly handled ([#45956](https://github.com/pytorch/pytorch/pull/45956))\r\n* `torch.LongTensor` legacy construction improved error message ([#46147](https://github.com/pytorch/pytorch/pull/46147))\r\n* `torch.utils.checkpoint` allow having Tensors that don\u2019t require gradients ([#45934](https://github.com/pytorch/pytorch/pull/45934))\r\n* `torch.nan_to_num`: fix deprecated warnings ([#46309](https://github.com/pytorch/pytorch/pull/46309))\r\n* Remove more use of \u201cblacklist\u201d  ([#45512](https://github.com/pytorch/pytorch/pull/45512), [#45781](https://github.com/pytorch/pytorch/pull/45781))\r\n* Add type annotation to submodules: `torch.nn.cpp` ([#46490](https://github.com/pytorch/pytorch/pull/46490)), `torch.nn.parallel.comm` ([#46736](https://github.com/pytorch/pytorch/pull/46736)), `torch.nn.modules.*` ([#46828](https://github.com/pytorch/pytorch/pull/46828), [#45772](https://github.com/pytorch/pytorch/pull/45772), [#46013](https://github.com/pytorch/pytorch/pull/46013), [#49957](https://github.com/pytorch/pytorch/pull/49957), [#49479](https://github.com/pytorch/pytorch/pull/49479), [#49045](https://github.com/pytorch/pytorch/pull/49045), [#49035](https://github.com/pytorch/pytorch/pull/49035), [#49494](https://github.com/pytorch/pytorch/pull/49494), [#48969](https://github.com/pytorch/pytorch/pull/48969)), autograd functions from c++ ([#46622](https://github.com/pytorch/pytorch/pull/46622)), `torch.distributed` functions from c++  ([#46623](https://github.com/pytorch/pytorch/pull/46623)), `torch.storage`  ([#46876](https://github.com/pytorch/pytorch/pull/46876)), `torch._tensor_str` ([#48463](https://github.com/pytorch/pytorch/pull/48463), [#48584](https://github.com/pytorch/pytorch/pull/48584)), `torch.nn.modules.pooling` ([#48412](https://github.com/pytorch/pytorch/pull/48412)), `common_nn` ([#48190](https://github.com/pytorch/pytorch/pull/48190)), `torch.lobpcg` ([#47680](https://github.com/pytorch/pytorch/pull/47680)), `torch.nn.functional`  ([#50106](https://github.com/pytorch/pytorch/pull/50106)), `torch.overrides` ([#50824](https://github.com/pytorch/pytorch/pull/50824)), `torch.generate_torch_version`  ([#51637](https://github.com/pytorch/pytorch/pull/51637)), `torch.distributions` ([#45689](https://github.com/pytorch/pytorch/pull/45689)), `torch.quantization.quantize_jit` ([#45548](https://github.com/pytorch/pytorch/pull/45548)), `torch.utils.tensorboard` ([#49834](https://github.com/pytorch/pytorch/pull/49834)), `torch.multiprocessing` ([#47756](https://github.com/pytorch/pytorch/pull/47756)), `torch.cuda` ([#47134](https://github.com/pytorch/pytorch/pull/47134)), `torch._C._distributed_rpc` ([#46624](https://github.com/pytorch/pytorch/pull/46624)), `torch.distributed.*` ([#47531](https://github.com/pytorch/pytorch/pull/47531), [#47532](https://github.com/pytorch/pytorch/pull/47532), [#47533](https://github.com/pytorch/pytorch/pull/47533), [#47534](https://github.com/pytorch/pytorch/pull/47534)), `torch.nn.parallel._functions` ([#49687](https://github.com/pytorch/pytorch/pull/49687))\r\n* Make comparison fail when dtypes don\u2019t match ([#47288](https://github.com/pytorch/pytorch/pull/47288))\r\n* Allow large inputs for `torch.svd` ([#47440](https://github.com/pytorch/pytorch/pull/47440))\r\n* Add nondeterministic alerts to `torch.index_copy`, `torch.median` on CUDA and `torch.kthvalue` on CUDA  ([#46942](https://github.com/pytorch/pytorch/pull/46942))\r\n* Add float16 and  bfloat16 support to `torch.where` ([#49004](https://github.com/pytorch/pytorch/pull/49004)), `torch.matmul` ([#47873](https://github.com/pytorch/pytorch/pull/47873))\r\n* Add float16 support for CPU and bfloat16 support for CPU & CUDA to `torch.flip` and `torch.flip{lr, ud}` ([#49895](https://github.com/pytorch/pytorch/pull/49895))\r\n* Add support for providing `indices` as a Tensor for `torch.tensor_split` ([#49169](https://github.com/pytorch/pytorch/pull/49169))\r\n* Add support for SELU activation in `torc.nn.init.calculate_gain` ([#50664](https://github.com/pytorch/pytorch/pull/50664))\r\n* Add function version of `torch.optim` optimizers and refactor existing classes to use the functional version: SGD ([#45597](https://github.com/pytorch/pytorch/pull/45597)), Adadelta ([#50409](https://github.com/pytorch/pytorch/pull/50409)), RMSProp ([#50410](https://github.com/pytorch/pytorch/pull/50410)), AdamW ([#50411](https://github.com/pytorch/pytorch/pull/50411))\r\n* Improve error message when window is on wrong device for `torch.fft.stft` ([#51128](https://github.com/pytorch/pytorch/pull/51128))\r\n* Add rounding_mode selection to `torch.div` ([#51706](https://github.com/pytorch/pytorch/pull/51706), [#52242](https://github.com/pytorch/pytorch/pull/52242))\r\n* Remove spurious numpy writable warning ([#47271](https://github.com/pytorch/pytorch/pull/47271))\r\n* Enable deterministic mode for rocBLAS ([#48654](https://github.com/pytorch/pytorch/pull/48654))\r\n* Hipify submodule revamp and improved integration with cpp_extensions ([#48715](https://github.com/pytorch/pytorch/pull/48715))\r\n* Remove warning about saving state in `torch.optim.lr_scheduler.LambdaLR` ([#46813](https://github.com/pytorch/pytorch/pull/46813))\r\n* Improve typing of `torch.nn.Unflatten` ([#49838](https://github.com/pytorch/pytorch/pull/49838))\r\n* Add exception classification to `torch.multiprocessing.spawn`\r\n\r\n### Autograd\r\n\r\n* Add double backward checks for the `torch.fft` submodule ([#46004](https://github.com/pytorch/pytorch/pull/46004))\r\n* Detect inplace modifications of views of leaf Tensors earlier to improve error ([#46204](https://github.com/pytorch/pytorch/pull/46204))\r\n\r\n### torch.utils\r\n\r\n* `data.TensorDataset`: Add more specific error message ([#46905](https://github.com/pytorch/pytorch/pull/46905))\r\n* `data.DistributedSampler`: Additional validation ([#48865](https://github.com/pytorch/pytorch/pull/48865))\r\n\r\n### Complex Numbers\r\n\r\n* Improve error message thrown by `torch.sign` for complex tensors ([#43280](https://github.com/pytorch/pytorch/pull/43280))\r\n* Remove unnecessary dtype checks for complex types and disable complex dispatch for CPU `torch.{min,max}` pointwise ops ([#50465](https://github.com/pytorch/pytorch/pull/50465))\r\n\r\n### CUDA\r\n\r\n* Allow consumer ops to sync on autograd engine base gradient ([#45787](https://github.com/pytorch/pytorch/pull/45787))\r\n* Add `torch::cuda::nccl::{send,recv}` ([#45926](https://github.com/pytorch/pytorch/pull/45926))\r\n* Cusolver inverse check info ([#46625](https://github.com/pytorch/pytorch/pull/46625))\r\n* Make numpy optional dependency for `torch.cuda.amp` ([#48154](https://github.com/pytorch/pytorch/pull/48154))\r\n* Support all visible cards when building a cuda extension ([#48891](https://github.com/pytorch/pytorch/pull/48891))\r\n* Enable using `torch.utils.checkpoint.checkpoint` and `torch.cuda.amp` at the same time ([#49757](https://github.com/pytorch/pytorch/pull/49757))\r\n* Make `DeviceCachingAllocator`'s error handling more defensive and a bit easier to read ([#51158](https://github.com/pytorch/pytorch/pull/51158))\r\n\r\n### Distributed\r\n\r\n* Create NCCL communicator for send/recv on demand ([#44922](https://github.com/pytorch/pytorch/pull/44922))\r\n* Reduce the peak memory of fp16 compression DDP comm hook by avoiding converting to fp32 ([#46078](https://github.com/pytorch/pytorch/pull/46078))\r\n* Allow RPC framework to use rank in addition to `WorkerInfo` and name. ([#46221](https://github.com/pytorch/pytorch/pull/46221))\r\n* Add to the `HashStore` `getNumKeys()`  ([#46048](https://github.com/pytorch/pytorch/pull/46048)) and `deleteKey()` ([#46049](https://github.com/pytorch/pytorch/pull/46049))\r\n* Print exception message on both RPC caller and callee ([#46372](https://github.com/pytorch/pytorch/pull/46372))\r\n* Add RRef proxy support for `ScriptModule` methods ([#48339](https://github.com/pytorch/pytorch/pull/48339))\r\n* Support retrieving the RRef to the remote module ([#48983](https://github.com/pytorch/pytorch/pull/48983))\r\n* Add a c++ interface in processGroup to get its backend name ([#51066](https://github.com/pytorch/pytorch/pull/51066))\r\n* Enable `NamedTuple` data type to work with DDP ([#44220](https://github.com/pytorch/pytorch/pull/44220))\r\n* Support send/recv to/from self when communicator is created on demand ([#45873](https://github.com/pytorch/pytorch/pull/45873))\r\n* Add Error log when ProcessGroupNCCL takes down a process ([#44988](https://github.com/pytorch/pytorch/pull/44988))\r\n* Provide additional information about NCCL error codes. ([#45950](https://github.com/pytorch/pytorch/pull/45950))\r\n* Avoid scatter for single-device case in DDP ([#46304](https://github.com/pytorch/pytorch/pull/46304))\r\n* Use Blocking Wait if both Blocking Wait and Async Error Handling Are Set ([#47926](https://github.com/pytorch/pytorch/pull/47926))\r\n* Providing more information while crashing a process in async error handling ([#47246](https://github.com/pytorch/pytorch/pull/47246))\r\n* Add PowerSGD comm hook ([#48060](https://github.com/pytorch/pytorch/pull/48060))\r\n* Define a customized state for PowerSGD comm hook ([#48348](https://github.com/pytorch/pytorch/pull/48348))\r\n* Add a random generator to PowerSGD state for initializing low-rank matrix Q ([#48507](https://github.com/pytorch/pytorch/pull/48507))\r\n* Replace the key of `error_dict` in PowerSGD state with bucket index ([#48867](https://github.com/pytorch/pytorch/pull/48867))\r\n* Make `CUDAFuture` remember and restore current device in callback ([#48789](https://github.com/pytorch/pytorch/pull/48789))\r\n* Update pipeline API to accept arbitrary sequence of Tensors and not just Tuple ([#48467](https://github.com/pytorch/pytorch/pull/48467))\r\n* Use `group.WORLD` appropriately in process group initialization. ([#48767](https://github.com/pytorch/pytorch/pull/48767))\r\n* Add error feedback to layerwise PowerSGD ([#49418](https://github.com/pytorch/pytorch/pull/49418))\r\n* Warm-start of PowerSGD by reusing states from previous iteration is possible ([#49451](https://github.com/pytorch/pytorch/pull/49451))\r\n* Change `wait()` to `value()` in some callbacks of PowerSGD communication hook ([#49709](https://github.com/pytorch/pytorch/pull/49709))\r\n* Ensure DDP + Pipe works with `find_unused_parameters`. ([#49908](https://github.com/pytorch/pytorch/pull/49908))\r\n* Enable TensorPipe CUDA sending to self ([#50674](https://github.com/pytorch/pytorch/pull/50674)) and  GDR channel ([#50763](https://github.com/pytorch/pytorch/pull/50763))\r\n* Add warning to distributed optimizer ([#50630](https://github.com/pytorch/pytorch/pull/50630))\r\n* Make python object collective API args consistent ([#50625](https://github.com/pytorch/pytorch/pull/50625))\r\n* Add option to make `rref.get_type` non-blocking. ([#50977](https://github.com/pytorch/pytorch/pull/50977))\r\n* Unescape string in RPC error message ([#49373](https://github.com/pytorch/pytorch/pull/49373))\r\n* Event Logging for NCCL Async Error Handling Process Crash ([#47244](https://github.com/pytorch/pytorch/pull/47244))\r\n* Remove `balance` and `devices` parameter from Pipe. ([#48432](https://github.com/pytorch/pytorch/pull/48432))\r\n* Error feedback for PowerSGD DDP comm hook ([#48670](https://github.com/pytorch/pytorch/pull/48670))\r\n* Add an index field to `GradBucket` for PowerSGD ([#48757](https://github.com/pytorch/pytorch/pull/48757))\r\n* Have `FutureNCCL` record streams w/ allocator in addCallback ([#48496](https://github.com/pytorch/pytorch/pull/48496)) and events in current stream ([#48497](https://github.com/pytorch/pytorch/pull/48497))\r\n* Use fresh stream from pool for each `FutureNCCL` callback ([#48498](https://github.com/pytorch/pytorch/pull/48498))\r\n* Record CUDA events for \"follow-up\" `FutureNCCL` inside `markCompleted()` ([#48499](https://github.com/pytorch/pytorch/pull/48499))\r\n* Fix `FutureNCCL`'s `completed()` disagreeing with `wait()` ([#48503](https://github.com/pytorch/pytorch/pull/48503))\r\n* Fix `FutureNCCL` not recording `DataPtr`s with caching alloc in `wait()` ([#48563](https://github.com/pytorch/pytorch/pull/48563))\r\n* Add multi-GPU support to `FutureNCCL` ([#48500](https://github.com/pytorch/pytorch/pull/48500))\r\n* Don't store device indices separately on `FutureNCCL` ([#48501](https://github.com/pytorch/pytorch/pull/48501))\r\n* Support wider range of types in `FutureNCCL` ([#48502](https://github.com/pytorch/pytorch/pull/48502))\r\n* Split `FutureNCCL`'s CUDA-specific parts from generic future logic ([#48504](https://github.com/pytorch/pytorch/pull/48504))\r\n* Merge common parts of FutureNCCL into `at::ivalue::Future` ([#48505](https://github.com/pytorch/pytorch/pull/48505))\r\n* Split out reusable `CUDAFuture` from `FutureNCCL` ([#48506](https://github.com/pytorch/pytorch/pull/48506))\r\n* Cache the `DataPtr`s in `CUDAFuture` ([#48788](https://github.com/pytorch/pytorch/pull/48788))\r\n* Modify `Pipe` to return an RRef. ([#47829](https://github.com/pytorch/pytorch/pull/47829))\r\n* Cleanup APIs for pipeline parallelism. ([#48630](https://github.com/pytorch/pytorch/pull/48630))\r\n* Fix TCPStore type coercion ([#49685](https://github.com/pytorch/pytorch/pull/49685))\r\n* Simplify the implementation of error feedback and warm-start ([#50981](https://github.com/pytorch/pytorch/pull/50981))\r\n* Explicitly specify the `dtype` of the error tensor ([#50985](https://github.com/pytorch/pytorch/pull/50985))\r\n* Check `start_PowerSGD_iter > 1` and add guidance on tuning PowerSGD configs. ([#51427](https://github.com/pytorch/pytorch/pull/51427))\r\n* Check if the backend is NCCL when a DDP communication hook is registered ([#51759](https://github.com/pytorch/pytorch/pull/51759))\r\n\r\n### TorchScript\r\n\r\n* Add multiline string dedent support ([#45580](https://github.com/pytorch/pytorch/pull/45580))\r\n* Add string versions of argument funcs in jit Node ([#45464](https://github.com/pytorch/pytorch/pull/45464))\r\n* Make sure each `warnings.warn` only executes once inside TorchScript. ([#45382](https://github.com/pytorch/pytorch/pull/45382))\r\n* Allow slicing multiple dimensions with indexes if not Tuple ([#45239](https://github.com/pytorch/pytorch/pull/45239))\r\n* Change type inferred from empty annotation ([#45360](https://github.com/pytorch/pytorch/pull/45360))\r\n* Fix stride printing/parsing formatting ([#45156](https://github.com/pytorch/pytorch/pull/45156))\r\n* Make objects throw Python AttributeError on nonexistant attr access ([#45911](https://github.com/pytorch/pytorch/pull/45911))\r\n* Make InsertInstruction overflow check a warning instead of fatal ([#46369](https://github.com/pytorch/pytorch/pull/46369))\r\n* Add an option to getWriteableTensorData to avoid copy CUDA tensor to CPU ([#46524](https://github.com/pytorch/pytorch/pull/46524))\r\n* Add error messages and workaround for RET failure of containers with a torch class type ([#46543](https://github.com/pytorch/pytorch/pull/46543))\r\n* Correctly mark unannotated NamedTuple field to be inferred TensorType ([#46969](https://github.com/pytorch/pytorch/pull/46969))\r\n* Enable ModuleDict non-literal indexing ([#45716](https://github.com/pytorch/pytorch/pull/45716))\r\n* Add an attribute to the torchscript model exported by metal ([#47174](https://github.com/pytorch/pytorch/pull/47174))\r\n* Print out interface mismatch for prim::ModuleDictIndex ([#47300](https://github.com/pytorch/pytorch/pull/47300))\r\n* better message for bad type annotation ([#47464](https://github.com/pytorch/pytorch/pull/47464))\r\n* Resolve string literal type annotations using `Resolver::resolveType` ([#47731](https://github.com/pytorch/pytorch/pull/47731))\r\n* Resolve `torch.device` in recursive compilation of classes ([#47734](https://github.com/pytorch/pytorch/pull/47734))\r\n* Metacompile boolean constants ([#46721](https://github.com/pytorch/pytorch/pull/46721))\r\n* Allow JIT unpickler to accept CUDA DataPtr from read_record_ ([#46827](https://github.com/pytorch/pytorch/pull/46827))\r\n* Skip None submodule during JIT-tracing ([#49765](https://github.com/pytorch/pytorch/pull/49765))\r\n* Add `__prepare_scriptable__` duck typing to allow replacing `nn.Module`s with scriptable preparations (#45645) ([#49242](https://github.com/pytorch/pytorch/pull/49242))\r\n* Fix deprecation warning in scalar_type_analysis ([#50218](https://github.com/pytorch/pytorch/pull/50218))\r\n* Support scripting classmethod called with object instances ([#49967](https://github.com/pytorch/pytorch/pull/49967))\r\n* Use FileStore in TorchScript for store registry ([#50248](https://github.com/pytorch/pytorch/pull/50248))\r\n* Treat has_torch_function and object_has_torch_function as static False when scripting ([#48966](https://github.com/pytorch/pytorch/pull/48966))\r\n* Print better error when class attribute IValue conversion fails ([#50255](https://github.com/pytorch/pytorch/pull/50255))\r\n* Clean up some type annotations in test/jit/...../test_class_type.py ([#50156](https://github.com/pytorch/pytorch/pull/50156))\r\n* Type annotations in test/jit ([#50293](https://github.com/pytorch/pytorch/pull/50293))\r\n* Eliminate static default_extra_files_mobile from header import.h ([#50832](https://github.com/pytorch/pytorch/pull/50832))\r\n* Dump torch::jit::AliasDb objects as Graphviz files ([#50452](https://github.com/pytorch/pytorch/pull/50452))\r\n* Fix test_jit_cuda_archflags on machine with more than one arch ([#50405](https://github.com/pytorch/pytorch/pull/50405))\r\n* Provide more info when attribute fails to convert ([#50870](https://github.com/pytorch/pytorch/pull/50870))\r\n* Adding correct error message for for..else ([#51258](https://github.com/pytorch/pytorch/pull/51258))\r\n* Handle error during dict expansion ([#51374](https://github.com/pytorch/pytorch/pull/51374))\r\n\r\n### Mobile\r\n\r\n* Update default output extension in optimize_for_mobile.cc ([#45598](https://github.com/pytorch/pytorch/pull/45598))\r\n* Add named tuple's error message and workaround for RET failure ([#46347](https://github.com/pytorch/pytorch/pull/46347))\r\n* [Metal] Add metal backend type ([#46455](https://github.com/pytorch/pytorch/pull/46455))\r\n* [Metal] Add the Python binding for optimize_for_mobile ([#46456](https://github.com/pytorch/pytorch/pull/46456))\r\n* [Metal] Add pin_memory check in empty_strided ([#47228](https://github.com/pytorch/pytorch/pull/47228))\r\n* [Metal] Calculate strides for metal tensors ([#50309](https://github.com/pytorch/pytorch/pull/50309))\r\n* [Metal] Clean up the operator tests ([#50311](https://github.com/pytorch/pytorch/pull/50311))\r\n* Add an overload for deserialize() that doesn't accept the extra_files map. ([#50932](https://github.com/pytorch/pytorch/pull/50932))\r\n* bundled_inputs: Preserve bundled input related methods when calling optimize_for_mobile ([#49170](https://github.com/pytorch/pytorch/pull/49170))\r\n* bundled_inputs: Preserved all functions generated by bundled inputs ([#51496](https://github.com/pytorch/pytorch/pull/51496))\r\n* bundled_inputs: Expanded Bundled Inputs To Any Public Function ([#51153](https://github.com/pytorch/pytorch/pull/51153))\r\n* Expose _export_operator_list to python ([#51312](https://github.com/pytorch/pytorch/pull/51312))\r\n\r\n### Quantization\r\n\r\n* Quantized Operators and Modules\r\n    * Add reflection padding to conv ([#49011](https://github.com/pytorch/pytorch/pull/49011))\r\n    * Add support for 2D indices for quantized embedding operators ([#47766](https://github.com/pytorch/pytorch/pull/47766))\r\n    * quantize_tensor_per_channel ARM implementation ([#46018](https://github.com/pytorch/pytorch/pull/46018))\r\n    * Support either min or max in qclamp ([#45937](https://github.com/pytorch/pytorch/pull/45937))\r\n    * Add preliminary support for advanced indexing ([#49346](https://github.com/pytorch/pytorch/pull/49346))\r\n    * Add backend_independent option for quantized linear module ([#48192](https://github.com/pytorch/pytorch/pull/48192))\r\n    * Add out-variant for the reflection pad ([#48037](https://github.com/pytorch/pytorch/pull/48037))\r\n    * Support 2 dim input in quantized batchnorm 1d ([#51597](https://github.com/pytorch/pytorch/pull/51597))\r\n* Typing, Formatting, Error Messages, Logging and Tests\r\n    * numeric suite: add types to eager ([#51168](https://github.com/pytorch/pytorch/pull/51168))\r\n    * Enable type check for torch.quantization.fake_quantize ([#45701](https://github.com/pytorch/pytorch/pull/45701))\r\n    * Type check for `torch.quantization.observer` ([#45630](https://github.com/pytorch/pytorch/pull/45630)), `torch.quantization._numeric_suite` ([#46330](https://github.com/pytorch/pytorch/pull/46330)), `torch.quantization.stubs` ([#46475](https://github.com/pytorch/pytorch/pull/46475)), `quantization.fx.Quantizer` ([#48343](https://github.com/pytorch/pytorch/pull/48343)), `quantization.fx.Quantizer` ([#48350](https://github.com/pytorch/pytorch/pull/48350)), `quantization_mappings.py` ([#49179](https://github.com/pytorch/pytorch/pull/49179)), `fusion_patterns.py` ([#49606](https://github.com/pytorch/pytorch/pull/49606)), `torch/nn/quantized/modules` ([#49941](https://github.com/pytorch/pytorch/pull/49941)), quantization-related files in `torch/jit` ([#49939](https://github.com/pytorch/pytorch/pull/49939)), fuser ([#48844](https://github.com/pytorch/pytorch/pull/48844)), quantization_patterns ([#48851](https://github.com/pytorch/pytorch/pull/48851)), observed_module.py ([#49607](https://github.com/pytorch/pytorch/pull/49607)), quantization ([#49942](https://github.com/pytorch/pytorch/pull/49942))\r\n    * Enable mypy on `torch/quantization/fx/*` ([#48331](https://github.com/pytorch/pytorch/pull/48331))\r\n    * Make each line of fx/quantize.py <=80 chars ([#48357](https://github.com/pytorch/pytorch/pull/48357))\r\n    * Add more typehints ([#48774](https://github.com/pytorch/pytorch/pull/48774), [#48794](https://github.com/pytorch/pytorch/pull/48794), [#48792](https://github.com/pytorch/pytorch/pull/48792))\r\n    * Nice error message on convtranspose with per-channel weight ([#49899](https://github.com/pytorch/pytorch/pull/49899))\r\n    * Throw a nice error message for allclose with quantized inputs ([#49802](https://github.com/pytorch/pytorch/pull/49802))\r\n    * Add type annotations to torch.nn.quantized.modules.conv ([#49702](https://github.com/pytorch/pytorch/pull/49702))\r\n    * Add type annotations to conv_fused/blas_compare/blas_compare_setup ([#51235](https://github.com/pytorch/pytorch/pull/51235))\r\n    * Add API usage logging to numeric suite ([#46504](https://github.com/pytorch/pytorch/pull/46504)) and quantization ([#46095](https://github.com/pytorch/pytorch/pull/46095))\r\n* Sparsity\r\n    * Block Sparse kernel ([#50585](https://github.com/pytorch/pytorch/pull/50585))\r\n    * Add A matrix pretransformed based sparse kernels for linear ([#50587](https://github.com/pytorch/pytorch/pull/50587))\r\n    * Add dyanmic linear sparse kernel for arm64 ([#50591](https://github.com/pytorch/pytorch/pull/50591))\r\n* Others\r\n    * Use tensor's quantized properties directly in pickler ([#46267](https://github.com/pytorch/pytorch/pull/46267))\r\n    * Remove register api and rename get_*mapping to get_default*_mapping ([#46337](https://github.com/pytorch/pytorch/pull/46337))\r\n    * Update HistogramObserver to be scriptable ([#51081](https://github.com/pytorch/pytorch/pull/51081))\r\n    * Support varying size input in numeric suite ([#47391](https://github.com/pytorch/pytorch/pull/47391))\r\n    * Backend string for the quantized types ([#49965](https://github.com/pytorch/pytorch/pull/49965))\r\n    * Disable pruning on embedding look up operators when compressed_indices_mapping = {0} ([#48672](https://github.com/pytorch/pytorch/pull/48672))\r\n    * Support out variant of embedding_bag_byte_rowwise_offsets_out ([#49561](https://github.com/pytorch/pytorch/pull/49561))\r\n\r\n### ONNX\r\n\r\n* Update embedding_bag export ([#44693](https://github.com/pytorch/pytorch/pull/44693))\r\n* Improve error handling for adaptive_pool ([#45874](https://github.com/pytorch/pytorch/pull/45874))\r\n* Support nd mask index in opset >= 11 ([#45252](https://github.com/pytorch/pytorch/pull/45252))\r\n* Update peephole pass for prim::ListUnpack ([#46264](https://github.com/pytorch/pytorch/pull/46264))\r\n* Slightly improve indexing with ellipsis under scripting ([#46571](https://github.com/pytorch/pytorch/pull/46571))\r\n* Update batch_norm symbolic to handle track_running_stats=False ([#47135](https://github.com/pytorch/pytorch/pull/47135))\r\n* Cast Gather index to Long if needed ([#47653](https://github.com/pytorch/pytorch/pull/47653))\r\n* Handle dynamic input axes for prim_ConstantChunk ([#48176](https://github.com/pytorch/pytorch/pull/48176))\r\n* Remove usage of isCompleteTensor() in symbolic functions ([#48162](https://github.com/pytorch/pytorch/pull/48162))\r\n* Changes to export API to better handle named arguments ([#47367](https://github.com/pytorch/pytorch/pull/47367))\r\n* Modified var_mean symbolic to support more combinations of dims ([#48949](https://github.com/pytorch/pytorch/pull/48949))\r\n* Support gelu for fp16 export ([#50911](https://github.com/pytorch/pytorch/pull/50911))\r\n* Enable Constant Folding for ONNX Opset 13 ([#51523](https://github.com/pytorch/pytorch/pull/51523))\r\n* Export and shape inference for prim uninitialized in If subblock ([#46094](https://github.com/pytorch/pytorch/pull/46094))\r\n* Scripting support for inputs to index_put ([#46866](https://github.com/pytorch/pytorch/pull/46866))\r\n* Track and list model params for scripting ([#47348](https://github.com/pytorch/pytorch/pull/47348))\r\n* Modifications in remove inplace ops passes to better handle binary inplace ops ([#51572](https://github.com/pytorch/pytorch/pull/51572))\r\n* Improve error message for parse_arg in symbolic functions ([#51516](https://github.com/pytorch/pytorch/pull/51516))\r\n* Update error message that displays when encountering an op unsupported for ONNX export ([#51522](https://github.com/pytorch/pytorch/pull/51522))\r\n* Preserve param names during in-place op removal ([#50955](https://github.com/pytorch/pytorch/pull/50955))\r\n* Handle sequence output shape and type inference ([#50599](https://github.com/pytorch/pytorch/pull/50599))\r\n* Update constant-folding of Gather op to include cases where rank of indices input is 0 ([#51514](https://github.com/pytorch/pytorch/pull/51514))\r\n* Update unsafe_chunk() method to support new version 13 of Split operator ([#51524](https://github.com/pytorch/pytorch/pull/51524))\r\n* Replace optional parameters of Resize with placeholder for ops13 ([#50954](https://github.com/pytorch/pytorch/pull/50954))\r\n\r\n### Vulkan\r\n\r\nThis release brings about a complete rewrite of PyTorch\u2019s Vulkan backend with primary focus on improved performance, robustness, and better code structure and organization.  These changes are transparent to the end user.  Considering that this is a rewrite, many of these changes also qualify as performance improvements.\r\n\r\n* Add Vulkan Tensor factory. ([#44016](https://github.com/pytorch/pytorch/pull/44016))\r\n* Redo Vulkan command and descriptor pools. ([#44496](https://github.com/pytorch/pytorch/pull/44496))\r\n* Add low level utilities image sampler ([#45037](https://github.com/pytorch/pytorch/pull/45037)), fence ([#45148](https://github.com/pytorch/pytorch/pull/45148)), tensor copy ([#46481](https://github.com/pytorch/pytorch/pull/46481)), job dispatch and flush ([#46008](https://github.com/pytorch/pytorch/pull/46008)), \r\n* Add more ops Add ([#44017](https://github.com/pytorch/pytorch/pull/44017)), Mul ([#47021](https://github.com/pytorch/pytorch/pull/47021)), Mm, Pool, Upsample ([#47063](https://github.com/pytorch/pytorch/pull/47063)), Conv2D ([#46900](https://github.com/pytorch/pytorch/pull/46900), [#48266](https://github.com/pytorch/pytorch/pull/48266), [#48816](https://github.com/pytorch/pytorch/pull/48816)), clamp ([#47196](https://github.com/pytorch/pytorch/pull/47196)), reshape ([#47252](https://github.com/pytorch/pytorch/pull/47252)), mean ([#47312](https://github.com/pytorch/pytorch/pull/47312)), \r\n* Add CMake option to enable Vulkan [v2] API. ([#46503](https://github.com/pytorch/pytorch/pull/46503))\r\n* Add `Tensor.is_vulkan` ([#46655](https://github.com/pytorch/pytorch/pull/46655))\r\n\r\n### Misc\r\n\r\n* Factory operators (at::empty, at::zeroes,...) now have a new overload in the C++ API that takes ScalarType, Layout, Device and pin_memory parameters separately, in addition to the previously existing overload that takes one TensorOptions argument. ([#44087](https://github.com/pytorch/pytorch/pull/44087))\r\n\r\n# Bug fixes\r\n\r\n### Python API\r\n\r\n* Fix `torch.nn.BatchNorm{1,2,3}d` channels_last contiguity check ([#50659](https://github.com/pytorch/pytorch/pull/50659))\r\n* Fix `torch.nn.ConstantPadNd` not preserving memory format ([#50898](https://github.com/pytorch/pytorch/pull/50898))\r\n* Fix dtype of first sample in `torch.quasirandom.SobolEngine` ([#51578](https://github.com/pytorch/pytorch/pull/51578))\r\n* Fixes bug in `torch.sspaddmm` ([#45963](https://github.com/pytorch/pytorch/pull/45963))\r\n* Check `support_as_strided` before using `torch.empty_strided` ([#46746](https://github.com/pytorch/pytorch/pull/46746))\r\n* Fix internal assert for `torch.heaviside` with cuda tensor and cpu scalar tensor ([#46831](https://github.com/pytorch/pytorch/pull/46831))\r\n* Fix negative column numbers for `torch.eye` ([#46841](https://github.com/pytorch/pytorch/pull/46841))\r\n* Fix segfault with `torch.orgqr` ([#46700](https://github.com/pytorch/pytorch/pull/46700))\r\n* Fix `torch.nn.functional.embedding` padding_idx behavior ([#46714](https://github.com/pytorch/pytorch/pull/46714))\r\n* Fix `torch.nn.Embedding.from_pretrained` to properly handle the `padding_idx` argument ([#47184](https://github.com/pytorch/pytorch/pull/47184))\r\n* Fix functions not handling discontiguous Tensors properly: `torch.dropout` ([#47552](https://github.com/pytorch/pytorch/pull/47552)), `torch.median` ([#46917](https://github.com/pytorch/pytorch/pull/46917))\r\n* Fix max_pool2d with ceil_mode ([#46558](https://github.com/pytorch/pytorch/pull/46558))\r\n* Fix type promotion for `torch.trace` on CPU ([#47305](https://github.com/pytorch/pytorch/pull/47305))\r\n* Fix `torch.kthvalue` error for scalar input ([#47600](https://github.com/pytorch/pytorch/pull/47600))\r\n* Fix multinomial when input has 0 probability ([#47386](https://github.com/pytorch/pytorch/pull/47386))\r\n* Fix incorrect warnings in `torch.nn.Parameter{List,Dict}` ([#48315](https://github.com/pytorch/pytorch/pull/48315))\r\n* Fix printing of `torch.device` ([#48655](https://github.com/pytorch/pytorch/pull/48655))\r\n* Fix parameter generator exhaustion in `torch.optim.SparseAdam` ([#47724](https://github.com/pytorch/pytorch/pull/47724))\r\n* Fix `torch.pow` bug for complex exponents ([#49809](https://github.com/pytorch/pytorch/pull/49809))\r\n* Fix gradient for `torch.norm` when `p=+inf` ([#48611](https://github.com/pytorch/pytorch/pull/48611))\r\n* Fix `SyncBatchNorm` when stats tracking is disabled ([#50126](https://github.com/pytorch/pytorch/pull/50126))\r\n* Fix `torch.elu` backward when alpha is negative ([#49272](https://github.com/pytorch/pytorch/pull/49272))\r\n* Fix pickling for Tensor-like objects ([#47732](https://github.com/pytorch/pytorch/pull/47732))\r\n* Fix `torch.distributions.Half{Cauchy,Normal}` support for `validate_args=True` ([#50403](https://github.com/pytorch/pytorch/pull/50403), [#50492](https://github.com/pytorch/pytorch/pull/50492))\r\n* Fix `torch.distributions.CatTransform` for `event_dim` > 0 ([#49111](https://github.com/pytorch/pytorch/pull/49111))\r\n* Fix `torch.distributions.Binomial` to retain lazy logit initialization ([#46055](https://github.com/pytorch/pytorch/pull/46055))\r\n* Fix `torch.pow` when exponent is provided as a scalar Tensor and on different device ([#46185](https://github.com/pytorch/pytorch/pull/46185), [#46320](https://github.com/pytorch/pytorch/pull/46320))\r\n* Fix classmethod override argument passing for Tensor-like objects ([#47114](https://github.com/pytorch/pytorch/pull/47114))\r\n* Fix internal assert when inputs are on the wrong device for `torch.`{`maximum, minimum}` ([#48446](https://github.com/pytorch/pytorch/pull/48446))\r\n* Fix `torch.distributions.utils.broadcast_all` crashing on Tensor-like objects ([#48169](https://github.com/pytorch/pytorch/pull/48169))\r\n* Fix vectorized conversion of `-nan` from float16 to float32 ([#41280](https://github.com/pytorch/pytorch/pull/41280))\r\n* Fix `torch.silu` backward for all backends other than CPU and CUDA ([#49439](https://github.com/pytorch/pytorch/pull/49439))\r\n* Fix wrong output when `torch.kthvalue` `out=` argument overlaps with input ([#48254](https://github.com/pytorch/pytorch/pull/48254))\r\n* Fix advanced indexing for Tensor-like objects ([#49324](https://github.com/pytorch/pytorch/pull/49324))\r\n* Fix `torch.distributions.TransformedDistribution` shape logic([#50581](https://github.com/pytorch/pytorch/pull/50581))\r\n* Fix `torch.nn.functional.interpolate` backward on GPU for nearest interpolation ([#51240](https://github.com/pytorch/pytorch/pull/51240))\r\n* Fix `torch.svd` ignoring `some` keyword argument for empty inputs ([#51109](https://github.com/pytorch/pytorch/pull/51109))\r\n* Fix `torch.distributions.Dirichlet` `arg_constraints` ([#51369](https://github.com/pytorch/pytorch/pull/51369))\r\n* Use deterministic implementation of `torch.index_put` and `torch.index` backward CPU in deterministic mode ([#51388](https://github.com/pytorch/pytorch/pull/51388))\r\n* Removes spurious warning in `torch.nonzero` ([#51618](https://github.com/pytorch/pytorch/pull/51618))\r\n* Fix calculation of number of elements to not overflow in many c++ implementations ([#46997](https://github.com/pytorch/pytorch/pull/46997))\r\n* Fix Parameter detection as Tensor in c++ backend ([#48963](https://github.com/pytorch/pytorch/pull/48963))\r\n* Fix bug in miopen findAlgorithm ([#46852](https://github.com/pytorch/pytorch/pull/46852))\r\n\r\n### Autograd\r\n\r\n* Fix deadlock on Windows due to bad thread termination in autograd engine ([#43532](https://github.com/pytorch/pytorch/pull/43532))\r\n* Fix deadlock in tsan builds due to bad locking in the engine ([#45867](https://github.com/pytorch/pytorch/pull/45867))\r\n* Avoid NaN values in `torch.cdist` backward for p<1 ([#45720](https://github.com/pytorch/pytorch/pull/45720))\r\n* Fix handling of `requires_grad` arg for `torch.new_`{`full,empty,zeros}` ([#46486](https://github.com/pytorch/pytorch/pull/46486))\r\n* Fix inplace check logic to be triggered when written-to Tensor does not require gradients ([#46296](https://github.com/pytorch/pytorch/pull/46296))\r\n* Set proper output differentiability for `torch.unique` ([#47930](https://github.com/pytorch/pytorch/pull/47930)), `torch.count_nonzero` ([#50866](https://github.com/pytorch/pytorch/pull/50866))\r\n* Fix race in autograd engine that lead can lead to `std::out_of_range` error ([#50164](https://github.com/pytorch/pytorch/pull/50164), [#50372](https://github.com/pytorch/pytorch/pull/50372))\r\n* Fix autograd thread crash on destruction with python-3.9 ([#50998](https://github.com/pytorch/pytorch/pull/50998))\r\n* Fix autograd side effects when printing ([#51364](https://github.com/pytorch/pytorch/pull/51364))\r\n* Fix memory leak in anomaly mode ([#51610](https://github.com/pytorch/pytorch/pull/51610))\r\n* fix `torch.hardsigmoid` backward at boundary values ([#51454](https://github.com/pytorch/pytorch/pull/51454))\r\n\r\n### CUDA\r\n\r\n* Fix incorrect CUDA `torch.nn.Embedding` result when `max_norm` is not `None` and indices are not sorted ([#45248](https://github.com/pytorch/pytorch/pull/45248))\r\n* Ensure kernel launches are checked ([#46474](https://github.com/pytorch/pytorch/pull/46474), [#46727](https://github.com/pytorch/pytorch/pull/46727))\r\n* Fix bit math ([#46837](https://github.com/pytorch/pytorch/pull/46837))\r\n* Fix test_inverse_singular for cublas path; fix cusolver inverse multi-stream issue ([#47026](https://github.com/pytorch/pytorch/pull/47026))\r\n* Fix indices computation for trilinear interpolate backwards ([#50084](https://github.com/pytorch/pytorch/pull/50084))\r\n* Fix for possible RNG offset calculation bug in cuda vectorized dropout with VEC=2 ([#50110](https://github.com/pytorch/pytorch/pull/50110))\r\n* Disable cuDNN persistent RNN on `sm_86` devices ([#49534](https://github.com/pytorch/pytorch/pull/49534))\r\n* Fix Error with `torch.flip` for cuda tensors when `dims=()` ([#50325](https://github.com/pytorch/pytorch/pull/50325))\r\n* Fix replication_pad CUDA launch configuration ([#50565](https://github.com/pytorch/pytorch/pull/50565))\r\n* Workaround for MAGMA accessing illegal memory in batched cholesky ([#50957](https://github.com/pytorch/pytorch/pull/50957))\r\n* Fix `torch.cdist` backward CUDA error due to illegal gridDim setting ([#51569](https://github.com/pytorch/pytorch/pull/51569))\r\n* Prevent CUDAFuture from using uninitialized device index ([#51505](https://github.com/pytorch/pytorch/pull/51505))\r\n* Fix incorrect usage of CUDACachingAllocator ([#48817](https://github.com/pytorch/pytorch/pull/48817))\r\n* Fix `torch.cuda.memory_allocated` to return `{}` if not initialized ([#51179](https://github.com/pytorch/pytorch/pull/51179))\r\n* Fix crash when trying to reset memory stats when no cuda device is available ([#48406](https://github.com/pytorch/pytorch/pull/48406))\r\n\r\n### torch.utils\r\n\r\n* `data.DistributedSampler`: Fix possible padding length overflow ([#45329](https://github.com/pytorch/pytorch/pull/45329))\r\n* `data.DataLoader`: Fix hang with large sampler ([#48669](https://github.com/pytorch/pytorch/pull/48669))\r\n* `data.DataLoader`: Fix unintended error when worker force kill happens #43455 ([#43462](https://github.com/pytorch/pytorch/pull/43462))\r\n* `data.DataLoader`: Fix persistent_workers + pin_memory ([#48543](https://github.com/pytorch/pytorch/pull/48543))\r\n\r\n### Complex Number\r\n\r\n* Make `torch.view_as_real` raise a proper error for backends where it is not supported ([#47018](https://github.com/pytorch/pytorch/pull/47018))\r\n* Fix bug in `toComplexWithDefault` ([#43841](https://github.com/pytorch/pytorch/pull/43841))\r\n* Fix `torch.cat` backward formula to return correct gradient values for R -> C case ([#51681](https://github.com/pytorch/pytorch/pull/51681))\r\n* Update backward formulas for `torch.{add, sub}` to correctly handle R -> C case. ([#46596](https://github.com/pytorch/pytorch/pull/46596))\r\n* Add custom implementation for `torch.csqrt` if libc++ is used ([#52018](https://github.com/pytorch/pytorch/pull/52018))\r\n\r\n### C++ API\r\n\r\n* Refine `ConvParams::use_nnpack()` to allow NNPACK convolution algorithm only be used for kernels up to 16x16.([#49464](https://github.com/pytorch/pytorch/pull/49464))\r\n\r\n### Distributed\r\n\r\n* Record FutureNCCL callback stream on CUDA caching allocator ([#45318](https://github.com/pytorch/pytorch/pull/45318))\r\n* Fix object-based collectives API to use `torch.cuda.current_device` instead of rank ([#46897](https://github.com/pytorch/pytorch/pull/46897))\r\n* Explicitly restrict the scope of `torch.cuda.synchronize` to the current device in PowerSGD ([#49711](https://github.com/pytorch/pytorch/pull/49711))\r\n* Fix Hang in Async Error Handling due to Work logging ([#46265](https://github.com/pytorch/pytorch/pull/46265))\r\n* Add missing `recordStream` in `ProcessGroupNCCL::alltoall_base` ([#46603](https://github.com/pytorch/pytorch/pull/46603))\r\n* Allow DataParallel to run zero input Module ([#46565](https://github.com/pytorch/pytorch/pull/46565))\r\n* Fix DDP issue where parameters share same `grad_accumulator` ([#46755](https://github.com/pytorch/pytorch/pull/46755))\r\n* Fix ProcessGroupNCCL profiling when profiler is not run with `use_cuda` ([#48946](https://github.com/pytorch/pytorch/pull/48946))\r\n* Refactor RPC `matchBuiltInOp` to get rid of exception swallowing ([#49009](https://github.com/pytorch/pytorch/pull/49009))\r\n* Solve zombie process problem in DDP launcher ([#49305](https://github.com/pytorch/pytorch/pull/49305))\r\n* Fix memory leak in TensorPipeAgent. ([#50564](https://github.com/pytorch/pytorch/pull/50564))\r\n* Fix warm-start for PowerSGD layer-wise compression ([#50283](https://github.com/pytorch/pytorch/pull/50283))\r\n* Fix CUDA RPC Stream Synchronization ([#50949](https://github.com/pytorch/pytorch/pull/50949))\r\n* Fix `benchmarks/distributed/ddp/benchmark.py` ([#51095](https://github.com/pytorch/pytorch/pull/51095))\r\n* Fix store based barrier to only use `add` ([#49930](https://github.com/pytorch/pytorch/pull/49930))\r\n\r\n### Mobile\r\n\r\n* Fix out-of-bounds access for caching allocator calls ([#46439](https://github.com/pytorch/pytorch/pull/46439))\r\n* Fix CPUCaching allocator guard bug ([#46922](https://github.com/pytorch/pytorch/pull/46922))\r\n* [Metal] Make the dst tensor contiguous when copying from metal (25833e5d1c)\r\n* [Metal] Fix the broken strides value for 2d transpose ([#50310](https://github.com/pytorch/pytorch/pull/50310))\r\n* [Android] Fix yuv conversion ([#50951](https://github.com/pytorch/pytorch/pull/50951))\r\n\r\n### TorchScript\r\n\r\n* Fix bugs in a number of ops in CUDA fuser ([#47795](https://github.com/pytorch/pytorch/pull/47795), [#49143,](https://github.com/pytorch/pytorch/pull/49143) [#49396](https://github.com/pytorch/pytorch/pull/49396) ,[#48329](https://github.com/pytorch/pytorch/pull/48329) and others)\r\n* Fix dict update ([#45857](https://github.com/pytorch/pytorch/pull/45857))\r\n* Fix Dict bug in constant hashing ([#45929](https://github.com/pytorch/pytorch/pull/45929))\r\n* Fix TypeError when `torch.jit.load` is passed a pathlib.Path ([#45825](https://github.com/pytorch/pytorch/pull/45825))\r\n* Fix missing call to `__setstate__` when cloning modules ([#45858](https://github.com/pytorch/pytorch/pull/45858))\r\n* Prevent caching of `graph` attribute. ([#46960](https://github.com/pytorch/pytorch/pull/46960))\r\n* Fix traced training attribute ([#47211](https://github.com/pytorch/pytorch/pull/47211))\r\n* Correctly compare Stream IValues ([#47303](https://github.com/pytorch/pytorch/pull/47303))\r\n* Correctly print out sign of near-zero double values ([#47081](https://github.com/pytorch/pytorch/pull/47081))\r\n* Properly serialize types that only appear at function input ([#47775](https://github.com/pytorch/pytorch/pull/47775))\r\n* Fix bug in get_annotation_str for ast.Subscript ([#48741](https://github.com/pytorch/pytorch/pull/48741))\r\n* Fix include files for out-of-tree compilation ([#48827](https://github.com/pytorch/pytorch/pull/48827))\r\n* Fix constant propagation schemas ([#49605](https://github.com/pytorch/pytorch/pull/49605))\r\n* Fix return type Any for Ternary ops ([#49165](https://github.com/pytorch/pytorch/pull/49165))\r\n* Fix for module_has_exports ([#50680](https://github.com/pytorch/pytorch/pull/50680))\r\n* Properly convert Python strings implictly to device ([#51340](https://github.com/pytorch/pytorch/pull/51340))\r\n* Add missing support for `torch.jit.Final` in python 3.6 ([#47393](https://github.com/pytorch/pytorch/pull/47393))\r\n\r\n### torch.fx\r\n\r\n* Fix recursion depth issue on Graph deepcopy ([#46669](https://github.com/pytorch/pytorch/pull/46669))\r\n* Fix handling of `inf` and `nan` literals ([#46894](https://github.com/pytorch/pytorch/pull/46894))\r\n* Fix corner case in name sanitization ([#46958](https://github.com/pytorch/pytorch/pull/46958))\r\n* Fix submodule naming for subgraph split ([#47869](https://github.com/pytorch/pytorch/pull/47869))\r\n* Fix create_arg for NamedTuple ([#48986](https://github.com/pytorch/pytorch/pull/48986))\r\n* Fix python code having spurious newlines from placeholders ([#49720](https://github.com/pytorch/pytorch/pull/49720))\r\n* Make `split_module` results deterministic ([#50470](https://github.com/pytorch/pytorch/pull/50470))\r\n* Fix tracing a free function with embedded constant ([#50639](https://github.com/pytorch/pytorch/pull/50639))\r\n* Fix using `fx.wrap` as a decorator ([#50677](https://github.com/pytorch/pytorch/pull/50677))\r\n* Fix annotation in generated code ([#50777](https://github.com/pytorch/pytorch/pull/50777), [#52021](https://github.com/pytorch/pytorch/pull/52021))\r\n\r\n### Quantization\r\n\r\n* Remove fake_quant after add/mul nodes during eager mode QAT ([#49213](https://github.com/pytorch/pytorch/pull/49213))\r\n* `torch.mean` add path for unsupported QNNPACK modes ([#45533](https://github.com/pytorch/pytorch/pull/45533))\r\n* Set type for GetAttr nodes in remapTypes ([#46250](https://github.com/pytorch/pytorch/pull/46250))\r\n* Avoid inserting fakequant for sigmoid/hardsigmoid/tanh in eval mode ([#47297](https://github.com/pytorch/pytorch/pull/47297))\r\n* Ensure observer respects device affinity ([#47514](https://github.com/pytorch/pytorch/pull/47514))\r\n* Fix quant type classification for float_qparam qconfig ([#48069](https://github.com/pytorch/pytorch/pull/48069))\r\n* Fix quant_type classification for fp16, fp16 ([#48073](https://github.com/pytorch/pytorch/pull/48073))\r\n* Fix a bug in leakyReLU ([#48265](https://github.com/pytorch/pytorch/pull/48265))\r\n* Fix quantization for qat.ConvBnReLU1d ([#48059](https://github.com/pytorch/pytorch/pull/48059))\r\n* Add bias once in conv_fused ([#48593](https://github.com/pytorch/pytorch/pull/48593))\r\n* Do not return unitialized qschame from getQSchemeAndQParamVector ([#49391](https://github.com/pytorch/pytorch/pull/49391))\r\n* Fix quantization for DeQuantStub ([#49428](https://github.com/pytorch/pytorch/pull/49428))\r\n* Ensure observers do not crash for empty Tensors ([#49800](https://github.com/pytorch/pytorch/pull/49800))\r\n* fake_quant: fix device affinity and buffer resizing for state_dict ([#50868](https://github.com/pytorch/pytorch/pull/50868))\r\n* Fix memory leak in qnnpack ops ([#51612](https://github.com/pytorch/pytorch/pull/51612))\r\n* Remove set_quantizer_ from native_functions.yaml ([#49463](https://github.com/pytorch/pytorch/pull/49463))\r\n* Make choose_qparams_optimized return Tensors to preserve dtype ([#45530](https://github.com/pytorch/pytorch/pull/45530))\r\n* Use PlaceholderObserver as default dynamic quant observer ([#45343](https://github.com/pytorch/pytorch/pull/45343))\r\n* FixedQParamsFakeQuantize: adjust default quant_min and quant_max ([#47423](https://github.com/pytorch/pytorch/pull/47423))\r\n* Add bias once in conv_fused (#48593) ([#48661](https://github.com/pytorch/pytorch/pull/48661))\r\n* Fix unused var warning when building for different archs. ([#48730](https://github.com/pytorch/pytorch/pull/48730))\r\n* Make the CUDA fake quantize logic consistent with CPU fake quantize logic ([#49808](https://github.com/pytorch/pytorch/pull/49808))\r\n* eager quant: fix error with removing forward hooks ([#49813](https://github.com/pytorch/pytorch/pull/49813))\r\n\r\n### ONNX\r\n\r\n* Fix `torch.flatten` operator ([#45632](https://github.com/pytorch/pytorch/pull/45632))\r\n* Reimplement _var_mean to ensure non-negative ([#47240](https://github.com/pytorch/pytorch/pull/47240))\r\n* Fix scripting of `torch.{rand,randn,where}` ([#45793](https://github.com/pytorch/pytorch/pull/45793))\r\n* Fix `torch.eye` export ([#47016](https://github.com/pytorch/pytorch/pull/47016))\r\n* Fix dtype for log_softmax export ([#46627](https://github.com/pytorch/pytorch/pull/46627))\r\n* Fix graph position to insert clone node for inplace op removal ([#51520](https://github.com/pytorch/pytorch/pull/51520))\r\n* Fix graph sequence output from loop node ([#51521](https://github.com/pytorch/pytorch/pull/51521))\r\n* Do not dereference nullptr in scalar type analysis ([#50237](https://github.com/pytorch/pytorch/pull/50237))\r\n* Fix bug in `torch.unfold` symbolic ([#51515](https://github.com/pytorch/pytorch/pull/51515))\r\n* Fix opset 11 ConstantChunk with negative dim ([#51525](https://github.com/pytorch/pytorch/pull/51525))\r\n* Fix bug in scatter_add ([#51527](https://github.com/pytorch/pytorch/pull/51527))\r\n\r\n### Vulkan\r\n\r\n* Fix interval midpoint calculation ([#46839](https://github.com/pytorch/pytorch/pull/46839))\r\n* Fix Vulkan `torch.empty` (and family) breakage as a result of API update. ([#47937](https://github.com/pytorch/pytorch/pull/47937))\r\n* Fix Addmm prepacking to persist after GPU flush ([#48313](https://github.com/pytorch/pytorch/pull/48313))\r\n* Properly forbid dilation > 1 for conv2d ([#48800](https://github.com/pytorch/pytorch/pull/48800))\r\n\r\n### Misc\r\n\r\n* Fix c++ extension ninja CUDA build ([#49344](https://github.com/pytorch/pytorch/pull/49344))\r\n* Only include dataclasses for py < 3.8 to make `setup.py` compatible with older python versions ([#45611](https://github.com/pytorch/pytorch/pull/45611))\r\n\r\n# Performance\r\n\r\n### Python API\r\n\r\n* Rewrite `torch.kron` to improve performance and support more dtypes ([#50927](https://github.com/pytorch/pytorch/pull/50927))\r\n* Enable the faster combined weight branch in MHA when query/key/value is same object with NaN ([#48126](https://github.com/pytorch/pytorch/pull/48126))\r\n\r\n### Autograd\r\n\r\n* `autograd.gradcheck` update to reduce computations ([#45757](https://github.com/pytorch/pytorch/pull/45757))\r\n* Reduce memory usage for `torch.mm` when only one input requires gradient ([#45777](https://github.com/pytorch/pytorch/pull/45777))\r\n* Reduce autograd engine startup cost ([#47592](https://github.com/pytorch/pytorch/pull/47592))\r\n* Make `torch.svd` backward formula more memory and computationally efficient. ([#50109](https://github.com/pytorch/pytorch/pull/50109))\r\n\r\n### CUDA\r\n\r\n* Fix perfornance issue of GroupNorm on CUDA when feature map is small. ([#46170](https://github.com/pytorch/pytorch/pull/46170))\r\n* Concat fast path with empty tensor ([#46805](https://github.com/pytorch/pytorch/pull/46805))\r\n* Support the strided tensor on input for `torch.cat` ([#46859](https://github.com/pytorch/pytorch/pull/46859))\r\n* Pin destination memory for `cuda_tensor.to(\"cpu\", non_blocking=True)` ([#46878](https://github.com/pytorch/pytorch/pull/46878))\r\n* Add proper maximum number of threads per block for sm_86 as 1536 ([#45889](https://github.com/pytorch/pytorch/pull/45889)) \r\n* Use MTA for amp grad unscaling, enforce op math type in MTA functors, and allow op lambdas ([#44778](https://github.com/pytorch/pytorch/pull/44778))\r\n* Improve performance of CUDA trilinear interpolate backward  ([#52649](https://github.com/pytorch/pytorch/pull/52649))\r\n\r\n### C++ API\r\n\r\n* Avoid computing AutogradKey if not needed to speed up low level C++ calls ([#46252](https://github.com/pytorch/pytorch/pull/46252))\r\n* VariableKernel calls into scattered C++ api ([#44158](https://github.com/pytorch/pytorch/pull/44158))\r\n* Make validate debug-only in Device constructor ([#49123](https://github.com/pytorch/pytorch/pull/49123))\r\n* Add macro to optionally devirtualize `TensorImpl::numel()` ([#49766](https://github.com/pytorch/pytorch/pull/49766)) and `TensorImpl::sizes()` ([#50176](https://github.com/pytorch/pytorch/pull/50176))\r\n* Inline access to low level Dispatcher ([#50644](https://github.com/pytorch/pytorch/pull/50644))\r\n\r\n### Distributed\r\n\r\n* Only track variables with grad accumulator for find_unused_parameters=True in DDP to save memory ([#45942](https://github.com/pytorch/pytorch/pull/45942))\r\n* Benchmark combining Distributed Data Parallel and Distributed RPC ([#46993](https://github.com/pytorch/pytorch/pull/46993))\r\n* Drop FutureNCCL in favor of vanilla CUDAFuture ([#49014](https://github.com/pytorch/pytorch/pull/49014))\r\n* Pytorch Distributed RPC Reinforcement Learning Benchmark (Throughput and Latency) ([#46901](https://github.com/pytorch/pytorch/pull/46901))\r\n\r\n### TorchScript\r\n\r\n* Optimized hot path in JIT graph executor ([#47465](https://github.com/pytorch/pytorch/pull/47465), [#48061](https://github.com/pytorch/pytorch/pull/48061),[#48034](https://github.com/pytorch/pytorch/pull/48034))\r\n* Added support for `is_nan`, `to`, and `lgamma` in CUDA fuser([#45791](https://github.com/pytorch/pytorch/pull/45791), [#48973](https://github.com/pytorch/pytorch/pull/48973), [#48976](https://github.com/pytorch/pytorch/pull/48976))\r\n* Added additional optimizations as part of `torch.jit.freeze` (Conv-Batchnorm, Conv-Add, and Conv-Mul folding, Dropout Removal) ([#50222](https://github.com/pytorch/pytorch/pull/50222)).\r\n* Fast TypeMeta/ScalarType conversion ([#45544](https://github.com/pytorch/pytorch/pull/45544))\r\n* Fix getCustomClassType() perf ([#48981](https://github.com/pytorch/pytorch/pull/48981))\r\n* Avoid move-constructing a List in listConstruct ([#49355](https://github.com/pytorch/pytorch/pull/49355))\r\n* Specialize `list_element_from` for `IValue` to avoid extra move/copy ([#50124](https://github.com/pytorch/pytorch/pull/50124))\r\n\r\n### Mobile\r\n\r\n* Avoid inlining kernel lambdas on mobile ([#46249](https://github.com/pytorch/pytorch/pull/46249))\r\n* Free original weight after prepacking in XNNPACK based op ([#46541](https://github.com/pytorch/pytorch/pull/46541))\r\n* [Metal] Make permuteWeights inline ([#47634](https://github.com/pytorch/pytorch/pull/47634))\r\n* [Metal] Use MPSCNN kernels for binary elementwise ops (c18403a693)\r\n\r\n### Vulkan\r\n\r\n* Enable prepacked addmm/mm for linear layers ([#47815](https://github.com/pytorch/pytorch/pull/47815))\r\n* Tweak memory use. ([#47728](https://github.com/pytorch/pytorch/pull/47728))\r\n* Add linear memory allocator. ([#48569](https://github.com/pytorch/pytorch/pull/48569))\r\n* Optimize Vulkan command buffer submission rate. ([#49112](https://github.com/pytorch/pytorch/pull/49112))\r\n\r\n### torch.fx\r\n\r\n* Speed up non-parameter tensor lookup ([#47325](https://github.com/pytorch/pytorch/pull/47325))\r\n\r\n### Quantization\r\n\r\n* Parallelize the quantization conversion operators ([#45536](https://github.com/pytorch/pytorch/pull/45536))\r\n* Add a more memory efficient version of fake quant ([#50561](https://github.com/pytorch/pytorch/pull/50561))\r\n* mem-efficient learnable fake quantization ([#49315](https://github.com/pytorch/pytorch/pull/49315), [#51255](https://github.com/pytorch/pytorch/pull/51255), [#51159](https://github.com/pytorch/pytorch/pull/51159))\r\n* Remove contiguous calls in qembeddingbag ([#48993](https://github.com/pytorch/pytorch/pull/48993))\r\n* Update embedding module to not store qweight ([#50418](https://github.com/pytorch/pytorch/pull/50418))\r\n\r\n### Misc\r\n\r\n* Extra sampling of record function events for the profiler ([#49114](https://github.com/pytorch/pytorch/pull/49114))\r\n\r\n# Documentation\r\n\r\n### Python API\r\n\r\n* Add information how to control randomness in `DataLoader` ([#45749](https://github.com/pytorch/pytorch/pull/45749))\r\n* Revamp reproducibility notes ([#45748](https://github.com/pytorch/pytorch/pull/45748))\r\n* Revamp `torch.optim` doc for better understanding ([#45944](https://github.com/pytorch/pytorch/pull/45944))\r\n* Revamp `torch.sparse` tensor documentation. ([#45400](https://github.com/pytorch/pytorch/pull/45400))\r\n* Add doc for `torch.overrides` submodule. ([#48170](https://github.com/pytorch/pytorch/pull/48170))\r\n* Add note on `nn.Module` overview and design principles ([#51536](https://github.com/pytorch/pytorch/pull/51536))\r\n* Add helper functions section to `torch.fft` doc ([#46032](https://github.com/pytorch/pytorch/pull/46032))\r\n* Add object-based collective APIs to public docs ([#48909](https://github.com/pytorch/pytorch/pull/48909))\r\n* Fix diverse typos and rendering issues in `torch.` doc ([#46328](https://github.com/pytorch/pytorch/pull/46328), [#46589](https://github.com/pytorch/pytorch/pull/46589), [#47545](https://github.com/pytorch/pytorch/pull/47545), [#48316](https://github.com/pytorch/pytorch/pull/48316), [#48328](https://github.com/pytorch/pytorch/pull/48328), [#48673](https://github.com/pytorch/pytorch/pull/48673), [#48787](https://github.com/pytorch/pytorch/pull/48787), [#47762](https://github.com/pytorch/pytorch/pull/47762), [#48970](https://github.com/pytorch/pytorch/pull/48970), [#49136](https://github.com/pytorch/pytorch/pull/49136), [#49388](https://github.com/pytorch/pytorch/pull/49388), [#49413](https://github.com/pytorch/pytorch/pull/49413), [#49584](https://github.com/pytorch/pytorch/pull/49584), [#49667](https://github.com/pytorch/pytorch/pull/49667), [#41887](https://github.com/pytorch/pytorch/pull/41887), [#50254](https://github.com/pytorch/pytorch/pull/50254), [#51053](https://github.com/pytorch/pytorch/pull/51053), [#51212](https://github.com/pytorch/pytorch/pull/51212), [#51439](https://github.com/pytorch/pytorch/pull/51439), [#51286](https://github.com/pytorch/pytorch/pull/51286), [#49648](https://github.com/pytorch/pytorch/pull/49648))\r\n* Fix diverse typo and rendering issues in `torch.nn` doc ([#45662](https://github.com/pytorch/pytorch/pull/45662), [#45660](https://github.com/pytorch/pytorch/pull/45660), [#45587](https://github.com/pytorch/pytorch/pull/45587), [#45763](https://github.com/pytorch/pytorch/pull/45763), [#46853](https://github.com/pytorch/pytorch/pull/46853), [#48577](https://github.com/pytorch/pytorch/pull/48577), [#48775](https://github.com/pytorch/pytorch/pull/48775), [#49950](https://github.com/pytorch/pytorch/pull/49950), [#50430](https://github.com/pytorch/pytorch/pull/50430), [#48596](https://github.com/pytorch/pytorch/pull/48596))\r\n* Fix diverse typo and rendering issues in `torch.linalg` doc ([#51459](https://github.com/pytorch/pytorch/pull/51459), [#51353](https://github.com/pytorch/pytorch/pull/51353), [#51620](https://github.com/pytorch/pytorch/pull/51620), [#51641](https://github.com/pytorch/pytorch/pull/51641), [#51651](https://github.com/pytorch/pytorch/pull/51651), [#51658](https://github.com/pytorch/pytorch/pull/51658), [#51659](https://github.com/pytorch/pytorch/pull/51659), [#51660](https://github.com/pytorch/pytorch/pull/51660))\r\n* Update docs for `torch.nn`:  in-place modification of weight in `nn.Embedding` ([#45595](https://github.com/pytorch/pytorch/pull/45595))\r\n* Update docs for `torch.distributions`: `NegativeBinomial` ([#45693](https://github.com/pytorch/pytorch/pull/45693)), `Categorical` ([#45804](https://github.com/pytorch/pytorch/pull/45804)), `LKJCholesky` ([#52904](https://github.com/pytorch/pytorch/pull/52904))\r\n* Improve `torch.matmul` doc regarding broadcasting ([#45699](https://github.com/pytorch/pytorch/pull/45699))\r\n* Add function signature for `torch.pixel_shuffle` ([#45661](https://github.com/pytorch/pytorch/pull/45661))\r\n* Fix signature for `torch.poisson` ([#45656](https://github.com/pytorch/pytorch/pull/45656))\r\n* Add 3D reduction example to `torch.tensordot` ([#45697](https://github.com/pytorch/pytorch/pull/45697))\r\n* Fix `torch.matrix_exp` ([#45909](https://github.com/pytorch/pytorch/pull/45909))\r\n* Fix typo in `torch.load` docstring for the `f` parameter ([#49350](https://github.com/pytorch/pytorch/pull/49350))\r\n* Document fix for `torch.logspace` and `torch.linspace` ([#46056](https://github.com/pytorch/pytorch/pull/46056))\r\n* Improve clarity of `torch.norm` ([#42696](https://github.com/pytorch/pytorch/pull/42696))\r\n* Fix info on the shape of pivots in `torch.lu` ([#46844](https://github.com/pytorch/pytorch/pull/46844))\r\n* Add `generator` param in `torch.randperm` doc ([#47231](https://github.com/pytorch/pytorch/pull/47231))\r\n* Updated doc for `torch.{v}dot` ([#47242](https://github.com/pytorch/pytorch/pull/47242))\r\n* Update doc of `torch.eig` about backward([#47598](https://github.com/pytorch/pytorch/pull/47598))\r\n* Fix `torch.swap{dim/axes}` to properly appear in doc ([#48376](https://github.com/pytorch/pytorch/pull/48376))\r\n* Add global `nn.Module` hooks to nn doc ([#48374](https://github.com/pytorch/pytorch/pull/48374))\r\n* Added `torch.linalg.cond` to doc([#48941](https://github.com/pytorch/pytorch/pull/48941))\r\n* Improve new_group example in the context of `torch.nn.SyncBatchNorm` ([#48897](https://github.com/pytorch/pytorch/pull/48897))\r\n* Update `is_floating_point()` docs to mention bfloat16 ([#49611](https://github.com/pytorch/pytorch/pull/49611))\r\n* Improve docs for `torch.{scatter,gather}` ([#49679](https://github.com/pytorch/pytorch/pull/49679))\r\n* Rename \"Arguments:\" to \"Args:\" in all doc ([#49736](https://github.com/pytorch/pytorch/pull/49736))\r\n* Fix a KaTeX crash and many docstring issues ([#49684](https://github.com/pytorch/pytorch/pull/49684))\r\n* Improve `torch.flatten` doc ([#49501](https://github.com/pytorch/pytorch/pull/49501))\r\n* Add note about `torch.flip` returning new tensor and not view. ([#50041](https://github.com/pytorch/pytorch/pull/50041))\r\n* Add instructional error message for cudnn RNN double backward workaround ([#33884](https://github.com/pytorch/pytorch/pull/33884))\r\n* Add centered FFT example to `torch.fft.fftshift` doc ([#51223](https://github.com/pytorch/pytorch/pull/51223))\r\n* Add `torch.sgn` to doc ([#51479](https://github.com/pytorch/pytorch/pull/51479))\r\n\r\n### Autograd\r\n\r\n* Fix many typos and rendering issues in `torch.autograd` doc ([#48765](https://github.com/pytorch/pytorch/pull/48765), [#45849](https://github.com/pytorch/pytorch/pull/45849), [#50166](https://github.com/pytorch/pytorch/pull/50166), [#51035](https://github.com/pytorch/pytorch/pull/51035), [#51335](https://github.com/pytorch/pytorch/pull/51335))\r\n* Update the error message explaining when to use the `retain_grad` flag ([#47084](https://github.com/pytorch/pytorch/pull/47084))\r\n\r\n### Complex Number\r\n\r\n* Fix typo in complex autograd docs ([#49755](https://github.com/pytorch/pytorch/pull/49755))\r\n* Doc update for complex numbers ([#51129](https://github.com/pytorch/pytorch/pull/51129), [#51661](https://github.com/pytorch/pytorch/pull/51661))\r\n* Document that `torch.remainder` does not support complex inputs ([#48024](https://github.com/pytorch/pytorch/pull/48024))\r\n\r\n### CUDA\r\n\r\n* Add a Note on CUDA Stream ([#45754](https://github.com/pytorch/pytorch/pull/45754%20(http:/#45754))), [#45754](https://github.com/pytorch/pytorch/pull/45754))\r\n* Add docs on how to toggle TF32 flags on C++ ([#47331](https://github.com/pytorch/pytorch/pull/47331))\r\n* Fix syntax issue in C++ cuda api note ([#48434](https://github.com/pytorch/pytorch/pull/48434))\r\n* Change \u201ctruncating\u201d to \u201crounding\u201c in TF32 docs ([#49625](https://github.com/pytorch/pytorch/pull/49625))\r\n* Add docstring to `torch.cuda.get_device_properties` ([#49792](https://github.com/pytorch/pytorch/pull/49792))\r\n* Add doc for `cuda.memory_fraction` and `cuda.gpu_process` ([#51372](https://github.com/pytorch/pytorch/pull/51372))\r\n\r\n### C++ API\r\n\r\n* Add guide for choosing dispatch keys in `native_functions.yaml` ([#46126](https://github.com/pytorch/pytorch/pull/46126))\r\n* Add a few more comments on dispatch key computation methods ([#46128](https://github.com/pytorch/pytorch/pull/46128))\r\n* Improve error messages for operator registration API ([#47636](https://github.com/pytorch/pytorch/pull/47636))\r\n* Add Math/DefaultBackend to dispatch key guide, introduce `PythonDispatcher` ([#50854](https://github.com/pytorch/pytorch/pull/50854))\r\n\r\n### Distributed\r\n\r\n* Clarify callback behavior when future is completed ([#50978](https://github.com/pytorch/pytorch/pull/50978))\r\n* Enhance `new_group` doc to mention using NCCL concurrently. ([#48872](https://github.com/pytorch/pytorch/pull/48872))\r\n* Adding c10d Store API Docs ([#45543](https://github.com/pytorch/pytorch/pull/45543))\r\n* Fix distributed documentation for asynchronous collective Work objects ([#45709](https://github.com/pytorch/pytorch/pull/45709))\r\n* Fix DDP documentation ([#46861](https://github.com/pytorch/pytorch/pull/46861))\r\n* Fix inaccurate note in `DistributedDataParallel` ([#47156](https://github.com/pytorch/pytorch/pull/47156))\r\n* Minor doc fixes for `init_process_group` ([#47644](https://github.com/pytorch/pytorch/pull/47644))\r\n* Docs fixes for `HashStore` API ([#47643](https://github.com/pytorch/pytorch/pull/47643))\r\n* Update links in DDP note ([#47663](https://github.com/pytorch/pytorch/pull/47663))\r\n* Small documentation changes for `RRef` and Dist Autograd ([#48123](https://github.com/pytorch/pytorch/pull/48123))\r\n* Add examples for new object-based c10d APIs ([#43932](https://github.com/pytorch/pytorch/pull/43932))\r\n* Minor update of the comments on PowerSGD. ([#49246](https://github.com/pytorch/pytorch/pull/49246))\r\n* Updating `init_process_group` docs to indicate correct rank range ([#49131](https://github.com/pytorch/pytorch/pull/49131))\r\n* Store Python API Docs Fixes ([#49130](https://github.com/pytorch/pytorch/pull/49130))\r\n* Fix link in distributed contributing doc and add link ([#49141](https://github.com/pytorch/pytorch/pull/49141))\r\n* Updating Docs to Reflect `FileStore` changes ([#49557](https://github.com/pytorch/pytorch/pull/49557))\r\n* Improve documentation for pipeline parallelism. ([#48638](https://github.com/pytorch/pytorch/pull/48638))\r\n* Reorder `torch.distributed.rpc.init_rpc` docstring arguments ([#50419](https://github.com/pytorch/pytorch/pull/50419))\r\n* Add documentation page for pipeline parallelism. ([#50791](https://github.com/pytorch/pytorch/pull/50791))\r\n* Update the doc of `DistributedOptimizer` ([#51314](https://github.com/pytorch/pytorch/pull/51314))\r\n* Fix doc inconsistency about callback args in `torch.futures.Future` ([#50979](https://github.com/pytorch/pytorch/pull/50979))\r\n\r\n### TorchScript\r\n\r\n* Added a developer tutorial for tensor expressions - the core technology used in CUDA fuser ([#45527](https://github.com/pytorch/pytorch/pull/45527))\r\n* Fix jit model loading example ([#48104](https://github.com/pytorch/pytorch/pull/48104))\r\n* Fix archive file extension in examples and docs ([#50649](https://github.com/pytorch/pytorch/pull/50649))\r\n* Fix `ScriptModule` docstring ([#48608](https://github.com/pytorch/pytorch/pull/48608))\r\n* Clarify logic in `ir_emitter` ([#51299](https://github.com/pytorch/pytorch/pull/51299))\r\n\r\n### torch.fx\r\n\r\n* Add `torch.fx` section to doc ([#48814](https://github.com/pytorch/pytorch/pull/48814), [#50291](https://github.com/pytorch/pytorch/pull/50291), [#50562](https://github.com/pytorch/pytorch/pull/50562), [#50896](https://github.com/pytorch/pytorch/pull/50896), [#50966](https://github.com/pytorch/pytorch/pull/50966), [#51728](https://github.com/pytorch/pytorch/pull/51728))\r\n* Add example on how to split up an FX graph into smaller subgraphs with own submodules ([#45404](https://github.com/pytorch/pytorch/pull/45404))\r\n* Shape propagation example ([#45637](https://github.com/pytorch/pytorch/pull/45637))\r\n* Add many docstrings and improve their rendering ([#47719](https://github.com/pytorch/pytorch/pull/47719), [#48100](https://github.com/pytorch/pytorch/pull/48100), [#48738](https://github.com/pytorch/pytorch/pull/48738), [#48871](https://github.com/pytorch/pytorch/pull/48871), [#50145](https://github.com/pytorch/pytorch/pull/50145), [#50396](https://github.com/pytorch/pytorch/pull/50396), [#50555](https://github.com/pytorch/pytorch/pull/50555))\r\n* Document single op replacement ([#50116](https://github.com/pytorch/pytorch/pull/50116), [#50377](https://github.com/pytorch/pytorch/pull/50377))\r\n* Document example of Proxy use ([#50583](https://github.com/pytorch/pytorch/pull/50583))\r\n* Add limitations of symbolic tracing ([#50638](https://github.com/pytorch/pytorch/pull/50638))\r\n* Added how to write transformations section ([#51278](https://github.com/pytorch/pytorch/pull/51278))\r\n* Added invert example ([#51478](https://github.com/pytorch/pytorch/pull/51478))\r\n* Document FX debugging ([#51530](https://github.com/pytorch/pytorch/pull/51530))\r\n* Write FX Subgraph Rewriter tutorial ([#51531](https://github.com/pytorch/pytorch/pull/51531))\r\n* Add note about more use cases of FX ([#51576](https://github.com/pytorch/pytorch/pull/51576))\r\n\r\n### Quantization\r\n\r\n* Add API summary section in quantization docs ([#45848](https://github.com/pytorch/pytorch/pull/45848), [#50681](https://github.com/pytorch/pytorch/pull/50681), [#50187](https://github.com/pytorch/pytorch/pull/50187))\r\n* Fix misleading doc string in quint8.h ([#48418](https://github.com/pytorch/pytorch/pull/48418))\r\n* Add fx graph mode quantization to quantization docs ([#49515](https://github.com/pytorch/pytorch/pull/49515))\r\n* Add common errors section ([#49902](https://github.com/pytorch/pytorch/pull/49902))\r\n* Adding a table comparing eager and fx graph mode ([#50413](https://github.com/pytorch/pytorch/pull/50413))\r\n* Add docs for embedding/embedding_bag ([#51770](https://github.com/pytorch/pytorch/pull/51770))\r\n* Add fake_quantize functions documentation ([#51748](https://github.com/pytorch/pytorch/pull/51748))\r\n\r\n### ONNX\r\n\r\n* Update ONNX doc for indexing export ([#46349](https://github.com/pytorch/pytorch/pull/46349))\r\n* Update ONNX doc for writing pytorch model ([#46961](https://github.com/pytorch/pytorch/pull/46961))\r\n\r\n### Misc\r\n\r\n* Add `docs/README.md` to make existing doc build info more discoverable ([#49286](https://github.com/pytorch/pytorch/pull/49286))\r\n* Update CONTRIBUTING for doc build ([#47539](https://github.com/pytorch/pytorch/pull/47539))\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.8.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.8.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.8.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/39280362", "dateCreated": "2021-02-26T22:13:54Z", "datePublished": "2021-03-04T20:44:39Z"}, {"tagName": "v1.7.1", "name": "Bug fix release with updated binaries for Python 3.9 and cuDNN 8.0.5", "authorName": "albanD", "authorType": "User", "body": "# PyTorch 1.7.1 Release Notes\r\n\r\n* New Features\r\n* Critical Fixes\r\n* Other Fixes\r\n\r\n# New Features\r\n\r\n### Add Python 3.9 binaries for linux and macOS ([#48133](https://github.com/pytorch/pytorch/pull/48133)) and Windows ([#48218](https://github.com/pytorch/pytorch/pull/48218))\r\n\r\n*NOTE*: Conda installs for Python 3.9 will require the `conda-forge` channel, example:\r\n`conda install -y -c pytorch -c conda-forge pytorch`.\r\n\r\n### Upgrade CUDA binaries to use cuDNN 8.0.5 (builder repo [#571](https://github.com/pytorch/builder/pull/571))\r\n\r\nThis upgrade fix regressions on Ampere cards introduced in cuDNN 8.0.4.\r\nIt will improve performance for 3090 RTX cards, and may improve performance in other RTX-30 series card.\r\n\r\n# Critical Fixes\r\n\r\n### Python 3.9\r\n\r\n- Use custom version of pybind11 to work around Python 3.9 issues ([#48312](https://github.com/pytorch/pytorch/pull/48312))\r\n- Fix jit Python 3.9 parsing ([#48744](https://github.com/pytorch/pytorch/pull/48744))\r\n- Fix cpp_extension to work with Python 3.9 ([#48768](https://github.com/pytorch/pytorch/pull/48768))\r\n\r\n### Build\r\n\r\n- Fix cpp_extension to properly handle env variable on Windows ([#48937](https://github.com/pytorch/pytorch/pull/48937))\r\n- Properly package libomp.dylib for macOS binaries ([#48337](https://github.com/pytorch/pytorch/pull/48337))\r\n- Fix build for statically linked OpenBLAS on aarch64 ([#48819](https://github.com/pytorch/pytorch/pull/48819))\r\n\r\n### Misc\r\n\r\n- `torch.sqrt`: fix wrong output values for very large complex input ([#48216](https://github.com/pytorch/pytorch/pull/48216))\r\n- `max_pool1d`: fix for discontiguous inputs ([#48219](https://github.com/pytorch/pytorch/pull/48219))\r\n- `collect_env`: fix detection of DEBUG flag ([#48319](https://github.com/pytorch/pytorch/pull/48319))\r\n- `collect_env`: Fix to work when PyTorch is not installed ([#48311](https://github.com/pytorch/pytorch/pull/48311))\r\n- Fix `amp` memory usage when running in `no_grad()` mode ([#48936](https://github.com/pytorch/pytorch/pull/48936))\r\n- `nn.ParameterList` and `nn.ParameterDict`: Remove spurious warnings ([#48215](https://github.com/pytorch/pytorch/pull/48215))\r\n- Tensor Expression fuser bugfixes ([#48137](https://github.com/pytorch/pytorch/pull/48137))\r\n\r\n# Other Fixes\r\n\r\n- Tensor Expression fix for CUDA 11.0 ([#48309](https://github.com/pytorch/pytorch/pull/48309))\r\n- `torch.overrides`: doc fix ([#47843](https://github.com/pytorch/pytorch/pull/47843))\r\n- `torch.max`: Fix output type for Tensor subclasses ([#47735](https://github.com/pytorch/pytorch/pull/47735))\r\n- `torch.mul`: Add support for boolean Tensors ([#48310](https://github.com/pytorch/pytorch/pull/48310))\r\n- Add user friendly error when trying to compile from source with Python 2 ([#48317](https://github.com/pytorch/pytorch/pull/48317))\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.7.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.7.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.7.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/35104340", "dateCreated": "2020-12-07T19:28:38Z", "datePublished": "2020-12-10T17:19:58Z"}, {"tagName": "v1.7.0", "name": "PyTorch 1.7 released w/ CUDA 11, New APIs for FFTs, Windows support for Distributed training and more", "authorName": "albanD", "authorType": "User", "body": "# PyTorch 1.7.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Change\r\n* New Features\r\n* Improvements\r\n* Performance\r\n* Documentation\r\n\r\n# Highlights\r\n\r\nThe PyTorch 1.7 release includes a number of new APIs including support for NumPy-Compatible FFT operations, profiling tools and major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. In addition, several features moved to [stable](https://pytorch.org/docs/stable/index.html#pytorch-documentation) including custom C++ Classes, the memory profiler, the creation of custom tensor-like objects, user async functions in RPC and a number of other features in torch.distributed such as Per-RPC timeout, DDP dynamic bucketing and RRef helper. \r\n\r\nA few of the highlights include: \r\n\r\n* CUDA 11 is now officially supported with binaries available at [PyTorch.org](http://pytorch.org/)\r\n* Updates and additions to profiling and performance for RPC, TorchScript and Stack traces in the autograd profiler\r\n* (Beta) Support for NumPy compatible Fast Fourier transforms (FFT) via torch.fft\r\n* (Prototype) Support for Nvidia A100 generation GPUs and native TF32 format \r\n* (Prototype) Distributed training on Windows now supported\r\n\r\nTo reiterate, starting [PyTorch 1.6](https://pytorch.org/blog/pytorch-feature-classification-changes/), features are now classified as stable, beta and prototype. You can see the detailed announcement [here](https://pytorch.org/blog/pytorch-feature-classification-changes/). Note that the prototype features listed in this blog are available as part of this release. \r\n\r\n## Front End APIs\r\n\r\n### [Beta] NumPy Compatible torch.fft module\r\n\r\nFFT-related functionality is commonly used in a variety of scientific fields like signal processing. While PyTorch has historically supported a few FFT-related functions, the 1.7 release adds a new torch.fft module that implements FFT-related functions with the same API as NumPy.  \r\n\r\nThis new module must be imported to be used in the 1.7 release, since its name conflicts with the historic (and now deprecated) torch.fft function.\r\n\r\n**Example usage:**\r\n\r\n```python\r\n>>> import torch.fft\r\n>>> t = torch.arange(4)\r\n>>> t\r\ntensor([0, 1, 2, 3])\r\n\r\n>>> torch.fft.fft(t)\r\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\r\n\r\n>>> t = tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\r\n>>> torch.fft.fft(t)\r\ntensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\r\n```\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/fft.html#torch-fft)\r\n\r\n### [Beta] C++ Support for Transformer NN Modules\r\n\r\nSince [PyTorch 1.5](https://pytorch.org/blog/pytorch-1-dot-5-released-with-new-and-updated-apis/), we\u2019ve continued to maintain parity between the python and C++ frontend APIs. This update allows developers to use the nn.transformer module abstraction from the C++ Frontend. And moreover, developers no longer need to save a module from python/JIT and load into C++ as it can now be used it in C++ directly. \r\n\r\n* Documentation | [Link](https://pytorch.org/cppdocs/api/classtorch_1_1nn_1_1_transformer_impl.html#_CPPv4N5torch2nn15TransformerImplE)\r\n\r\n### [Beta] torch.set_deterministic \r\n\r\nReproducibility (bit-for-bit determinism) may help identify errors when debugging or testing a program. To facilitate reproducibility, PyTorch 1.7 adds the  `torch.set_deterministic(bool)` function that can direct PyTorch operators to select deterministic algorithms when available, and to throw a runtime error if an operation may result in nondeterministic behavior. By default, the flag this function controls is false and there is no change in behavior, meaning PyTorch may implement its operations nondeterministically by default. \r\n\r\nMore precisely, when this flag is true:\r\n\r\n* Operations known to not have a deterministic implementation throw a runtime error;\r\n* Operations with deterministic variants use those variants (usually with a performance penalty versus the non-deterministic version); and\r\n* `torch.backends.cudnn.deterministic = True` is set.\r\n\r\nNote that this is necessary, **but not sufficient**, for determinism **within a single run of a PyTorch program**. Other sources of randomness like random number generators, unknown operations, or asynchronous or distributed computation may still cause nondeterministic behavior.\r\n\r\nSee the documentation for `torch.set_deterministic(bool)` for the list of affected operations.\r\n\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/15359)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/generated/torch.set_deterministic.html)\r\n\r\n## Performance & Profiling\r\n\r\n### [Beta] Stack traces added to profiler\r\n\r\nUsers can now see not only operator name/inputs in the profiler output table but also where the operator is in the code. The workflow requires very little change to take advantage of this capability. The user uses the [autograd profiler](https://pytorch.org/docs/stable/autograd.html#profiler) as before but with optional new parameters: `with_stack` and `group_by_stack_n`. Caution: regular profiling runs should not use this feature as it adds significant overhead. \r\n\r\n* Details | [Link](https://github.com/pytorch/pytorch/pull/43898/)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/autograd.html)\r\n\r\n## Distributed Training & RPC \r\n\r\n### [Stable] TorchElastic now bundled into PyTorch docker image\r\n\r\nTorchelastic offers a strict superset of the current `torch.distributed.launch` CLI with the added features for fault-tolerance and elasticity. If the user is not be interested in fault-tolerance, they can get the exact functionality/behavior parity by setting `max_restarts=0` with the added convenience of auto-assigned `RANK` and `MASTER_ADDR|PORT` (versus manually specified in `torch.distributed.launch)`.\r\n\r\nBy bundling `torchelastic` in the same docker image as PyTorch, users can start experimenting with torchelastic right-away without having to separately install `torchelastic`. In addition to convenience, this work is a nice-to-have when adding support for elastic parameters in the existing Kubeflow\u2019s distributed PyTorch operators.\r\n\r\n* Usage examples and how to get started | [Link](https://pytorch.org/elastic/0.2.0/examples.html)\r\n\r\n### [Beta] Support for uneven dataset inputs in DDP\r\n\r\nPyTorch 1.7 introduces a new context manager to be used in conjunction with models trained using `torch.nn.parallel.DistributedDataParallel` to enable training with uneven dataset size across different processes. This feature enables greater flexibility when using DDP and prevents the user from having to manually ensure dataset sizes are the same across different process. With this context manager, DDP will handle uneven dataset sizes automatically, which can prevent errors or hangs at the end of training.\r\n\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/38174)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.join)\r\n\r\n### [Beta] NCCL Reliability - Async Error/Timeout Handling\r\n\r\nIn the past, NCCL training runs would hang indefinitely due to stuck collectives, leading to a very unpleasant experience for users. This feature will abort stuck collectives and throw an exception/crash the process if a potential hang is detected. When used with something like torchelastic (which can recover the training process from the last checkpoint), users can have much greater reliability for distributed training. This feature is completely opt-in and sits behind an environment variable that needs to be explicitly set in order to enable this functionality (otherwise users will see the same behavior as before).\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group)\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/46874)\r\n\r\n### [Beta] TorchScript `remote` and `rpc_sync`\r\n\r\n`torch.distributed.rpc.rpc_async` has been available in TorchScript in prior releases. For PyTorch 1.7, this functionality will be extended the remaining two core RPC APIs, `torch.distributed.rpc.rpc_sync` and `torch.distributed.rpc.remote`. This will complete the major RPC APIs targeted for support in TorchScript, it allows users to use the existing python RPC APIs within TorchScript (in a script function or script method, which releases the python Global Interpreter Lock) and could possibly improve application performance in multithreaded environment.\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc.html#rpc)\r\n* Usage examples | [Link](https://github.com/pytorch/pytorch/blob/58ed60c259834e324e86f3e3118e4fcbbfea8dd1/torch/testing/_internal/distributed/rpc/jit/rpc_test.py#L505-L525)\r\n\r\n### [Beta] Distributed optimizer with TorchScript support\r\n\r\nPyTorch provides a broad set of optimizers for training algorithms, and these have been used repeatedly as part of the python API. However, users often want to use multithreaded training instead of multiprocess training as it provides better resource utilization and efficiency in the context of large scale distributed training (e.g. Distributed Model Parallel) or any RPC-based training application). Users couldn\u2019t do this with with distributed optimizer before because we need to get rid of the python Global Interpreter Lock (GIL) limitation to achieve this.\r\n\r\nIn PyTorch 1.7, we are enabling the TorchScript support in distributed optimizer to remove the GIL, and make it possible to run optimizer in multithreaded applications. The new distributed optimizer has the exact same interface as before but it automatically converts optimizers within each worker into TorchScript to make each GIL free. This is done by leveraging a functional optimizer concept and allowing the distributed optimizer to convert the computational portion of the optimizer into TorchScript. This will help use cases like distributed model parallel training and improve performance using multithreading. \r\n\r\nCurrently, the only optimizer that supports automatic conversion with TorchScript is `Adagrad` and all other optimizers will still work as before without TorchScript support. We are working on expanding the coverage to all PyTorch optimizers and expect more to come in future releases. The usage to enable TorchScript support is automatic and exactly the same with existing python APIs, here is an example of how to use this:\r\n\r\n```python\r\nimport torch.distributed.autograd as dist_autograd\r\nimport torch.distributed.rpc as rpc\r\nfrom torch import optim\r\nfrom torch.distributed.optim import DistributedOptimizer\r\n\r\nwith dist_autograd.context() as context_id:\r\n  # Forward pass.\r\n  rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n  rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\r\n  loss = rref1.to_here() + rref2.to_here()\r\n\r\n  # Backward pass.\r\n  dist_autograd.backward(context_id, [loss.sum()])\r\n\r\n  # Optimizer, pass in optim.Adagrad, DistributedOptimizer will\r\n  # automatically convert/compile it to TorchScript (GIL-free)\r\n  dist_optim = DistributedOptimizer(\r\n     optim.Adagrad,\r\n     [rref1, rref2],\r\n     lr=0.05,\r\n  )\r\n  dist_optim.step(context_id)\r\n```\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc.html#module-torch.distributed.optim)\r\n* RFC | [Link](https://github.com/pytorch/pytorch/issues/46883)\r\n\r\n### [Beta] Enhancements to RPC-based Profiling\r\n\r\nSupport for using the PyTorch profiler in conjunction with the RPC framework was first introduced in PyTorch 1.6. In PyTorch 1.7, the following enhancements have been made:\r\n\r\n* Implemented better support for profiling TorchScript functions over RPC\r\n* Achieved parity in terms of profiler features that work with RPC\r\n* Added support for asynchronous RPC functions on the server-side (functions decorated with `rpc.functions.async_execution)`.\r\n\r\nUser are now able to use familiar profiling tools such as `with torch.autograd.profiler.profile()` and `with torch.autograd.profiler.record_function,` and this works transparently with the RPC framework with full feature support, profiles asynchronous functions, and TorchScript functions.\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/39675)\r\n* Usage examples | [Link](https://pytorch.org/tutorials/recipes/distributed_rpc_profiling.html)\r\n\r\n### [Prototype] Windows support for Distributed Training\r\n\r\nPyTorch 1.7 brings prototype support for `DistributedDataParallel` and collective communications on the Windows platform. In this release, the support only covers Gloo-based `ProcessGroup` and `FileStore`.\r\nTo use this feature across multiple machines, please provide a file from a shared file system in `init_process_group`. \r\n\r\n```python\r\n# initialize the process group\r\ndist.init_process_group(\r\n    \"gloo\",\r\n    # multi-machine example:\r\n    # Shared files need six \"/\"\r\n    # init_method = `\"file://////{machine}/{share_folder}/file\"`\r\n    # Local file need three \"/\"\r\n    init_method=\"file:///{your local file path}\",\r\n    rank=rank,\r\n    world_size=world_size\r\n)\r\n\r\nmodel = DistributedDataParallel(local_model, device_ids=[rank])\r\n```\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/42095)\r\n* Documentation | [Link](https://pytorch.org/docs/master/distributed.html#backends-that-come-with-pytorch)\r\n* Acknowledgement | [gunandrose4u](https://github.com/gunandrose4u)\r\n\r\n## Mobile\r\n\r\nPyTorch Mobile supports both [iOS](https://pytorch.org/mobile/ios) and [Android](https://pytorch.org/mobile/android/) with binary packages available in [Cocoapods](https://cocoapods.org/) and J[Center](https://mvnrepository.com/repos/jcenter) respectively. You can learn more about PyTorch-Mobile [here](https://pytorch.org/mobile/home/). \r\n\r\n### [Beta] PyTorch Mobile Caching allocator for performance improvements\r\n\r\nOn some mobile platforms, such as Pixel, we observed that memory is returned to the system more aggressively. This results in frequent page faults as PyTorch being a functional framework does not maintain state for the operators. Thus outputs are allocated dynamically on each execution of the op, for the most ops. To ameliorate performance penalties due to this, PyTorch 1.7 provides a simple caching allocator for CPU. The allocator caches allocations by tensor sizes and, is currently, available only via the PyTorch C++ API. The caching allocator itself is owned by client and thus the lifetime of the allocator is also maintained by client code. Such a client owned caching allocator can then be used with scoped guard, `c10::WithCPUCachingAllocatorGuard`, to enable the use of cached allocation within that scope.\r\n\r\n**Example usage:**\r\n\r\n```cpp\r\n#include <c10/mobile/CPUCachingAllocator.h>\r\n.....\r\nc10::CPUCachingAllocator caching_allocator;\r\n  // Owned by client code. Can be a member of some client class so as to tie the\r\n  // the lifetime of caching allocator to that of the class.\r\n.....\r\n{\r\n  c10::optional<c10::WithCPUCachingAllocatorGuard> caching_allocator_guard;\r\n  if (FLAGS_use_caching_allocator) {\r\n    caching_allocator_guard.emplace(&caching_allocator);\r\n  }\r\n  ....\r\n  model.forward(..);\r\n}\r\n.....\r\n```\r\n\r\n**NOTE**: Caching allocator is only available on mobile builds, thus the use of caching allocator outside of mobile builds won\u2019t be effective.\r\n\r\n* Documentation | [Link](https://github.com/pytorch/pytorch/blob/master/c10/mobile/CPUCachingAllocator.h#L13-L43)\r\n* Usage examples | [Link](https://github.com/pytorch/pytorch/blob/master/binaries/speed_benchmark_torch.cc#L207)\r\n\r\n\r\n\r\n# Backwards Incompatible changes\r\n\r\n## Python API\r\n\r\n### `torch.conj` now returns the input as-is for real Tensors ([#43270](https://github.com/pytorch/pytorch/pull/43270))\r\n\r\nPreviously, `torch.conj` and `Tensor.conj` were making a clone for Tensors of real dtype. It now returns the Tensor as-is to improve performance.\r\nYou can recover the original behavior by adding a `.clone()` for real Tensors.\r\nNote that this behavior is different from `numpy` for which `np.conj` returns a new ndarray and `ndarray.conj` returns the ndarray as-is.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.is_complex()\r\nFalse\r\n>>> t.conj() is t\r\nFalse   \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.is_complex()\r\nFalse\r\n>>> t.conj() is t\r\nTrue\r\n>>>t.conj().clone() is t\r\nFalse   \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.tensor`, `torch.as_tensor`, and `torch.sparse_coo_tensor` now use the input Tensor\u2019s device when it is not specified ([#41984](https://github.com/pytorch/pytorch/pull/41984))\r\n\r\nThis will change the device on which the Tensor is created and so the user can start seeing device mismatch errors.\r\nIt also means for sparse Tensors that both of the provided Tensors must be on the same device if the device is not specified.\r\nYou can recover the original behavior by passing the `device` argument.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.device\r\ndevice(type=\u2018cuda:0\u2019)\r\n>>> # tensor constructor\r\n>>> torch.tensor(t, dtype=torch.float32).device\r\ndevice(type=\u2018cpu\u2019)\r\n>>> # sparse constructor\r\n>>> torch.sparse_coo_tensor(\r\n            torch.tensor(([0], [2]), device=\"cpu\"),\r\n            torch.tensor(([1.],), device=\"cuda\"),\r\n            size=(3, 3, 1)).device\r\ndevice(type='cuda', index=0)    \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.device\r\ndevice(type=\u2018cuda:0\u2019)\r\n>>> # tensor constructor\r\n>>> torch.tensor(t, dtype=torch.float32).device\r\ndevice(type=\u2018cuda:0\u2019)\r\n>>> # Specify the device to get the same behavior as 1.6\r\n>>> torch.tensor(t, dtype=torch.float32, device='cpu').device\r\ndevice(type=\u2018cpu\u2019)\r\n>>> # sparse constructor\r\n>>> torch.sparse_coo_tensor(\r\n            torch.tensor(([0], [2]), device=\"cpu\"),\r\n            torch.tensor(([1.],), device=\"cuda\"),\r\n            size=(3, 3, 1)).device\r\nRuntimeError: backend of indices (CPU) must match backend\r\nof values (CUDA)\r\n>>> # Specify the device to get the same behavior as 1.6\r\n>>> torch.sparse_coo_tensor(\r\n            torch.tensor(([0], [2]), device=\"cpu\"),\r\n            torch.tensor(([1.],), device=\"cuda\"),\r\n            size=(3, 3, 1),\r\n            device=\"cuda:0\").device\r\ndevice(type='cuda', index=0)    \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.nn.utils.pack_padded_sequence`: remove hidden cross-device copy for `lengths` ([#41984](https://github.com/pytorch/pytorch/pull/41984))\r\n\r\nIn previous versions, when the lengths argument was a CUDA tensor, it would incorrectly be moved to the CPU silently.\r\nThis can lead to surprising performances and CPU/GPU sync when using CUDA so this has been removed.\r\nYou need to make sure that the provided `lenghts` is a CPU Tensor when it is provided as a Tensor.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> inp = torch.rand(10, 2, 3, device=\"cuda\")\r\n>>> lengths = torch.tensor([10, 7], device=\"cuda\")\r\n>>> torch.nn.utils.rnn.pack_padded_sequence(inp, lengths)\r\n>>> # Implicitly move lengths to the CPU and runs fine\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> inp = torch.rand(10, 2, 3, device=\"cuda\")\r\n>>> lengths = torch.tensor([10, 7], device=\"cuda\")\r\n>>> torch.nn.utils.rnn.pack_padded_sequence(inp, lengths)\r\nRuntimeError: 'lengths' argument should be a 1D CPU int64 tensor,\r\nbut got 1D cuda:0 Long tensor\r\n>>> # Ensure the lenghts is already on the right device\r\n>>> lengths = lengths.cpu()\r\n>>> torch.nn.utils.rnn.pack_padded_sequence(inp, lengths)\r\n>>> # Runs fine with no implicit move across device\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Improve `torch.norm` handling of `keepdim=True` ([#41956](https://github.com/pytorch/pytorch/pull/41956))\r\n\r\nBefore this change, when calling `torch.norm` with `keepdim=True` and `p='fro'` or `p=number`, leaving all other optional arguments as their default values, the keepdim argument would be ignored. It is now properly respected.\r\nAlso, any time `torch.norm` was called with `p='nuc'` and `keepdim=True`, the result would have one fewer dimension than the input, and the dimensions could be out of order depending on which dimensions were being reduced. It is now properly keeping all the dimensions.\r\nYou can recover the original behavior by setting `keepdim=False`.\r\n**NOTE: this function is now deprecated (see below) and we recommend you use `torch.linalg.norm`, which follows NumPy\u2019s conventions.**\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.size()\r\ntorch.Size([4, 4])\r\n>>> t.norm(p=\u2018fro\u2019, keepdim=True).size()\r\ntorch.size([])\r\n>>> t.norm(p=3, keepdim=True).size()\r\ntorch.size([])\r\n>>> t.norm(p=\u2018nuc\u2019, keepdim=True).size()\r\ntorch.size([1]) \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> t.size()\r\ntorch.Size([4, 4])\r\n>>> t.norm(p=\u2018fro\u2019, keepdim=True).size()\r\ntorch.size([1, 1])\r\n>>> t.norm(p=3, keepdim=True).size()\r\ntorch.size([1, 1])\r\n>>> t.norm(p=\u2018nuc\u2019, keepdim=True).size()\r\ntorch.size([1, 1])  \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.split` and `torch.chunk`: Fix view tracking for the autograd ([#41567](https://github.com/pytorch/pytorch/pull/41567))\r\n\r\nThe autograd system is able to correctly handle modifications through views of Tensors by explicitly tracking known view operations. In prior releases, `torch.split` and `torch.chunk` were not marked as known view operations, which could lead to silently wrong gradients.\r\n\r\nNote that since v1.5, inplace modification of views created by functions that return multiple views is deprecated. Such case is not properly handled by the autograd and can lead to internal errors or wrong gradients. So, as a side effect of this view fix, inplace modifications of the outputs of `torch.split` and `torch.chunk` will now raise a warning and can lead to internal errors or wrong gradients while they were previously silently computing wrong gradients.\r\nIf you see such a warning, you should replace the inplace operation with an out of place one.\r\nYou can recover the original behavior by using the new `torch.unsafe_split` and `torch.unsafe_chunk`. Note that these functions are only here to ease the transition and will also be removed in a future version.\r\n\r\n### `torch.{argmin,argmax}` now always return the first min/max index ([#42004](https://github.com/pytorch/pytorch/pull/42004))\r\n\r\n`torch.argmin` (`torch.argmax`) now always returns the index of the first minimum (maximum) element. This choice is consistent with NumPy. Previously if there were multiple minima (maxima) the index returned could be the index of any of them.\r\nYou cannot recover the original behavior as it was platform dependent and not guaranteed. If your code was relying on a specific index for your specific platform, you should update it to work with the first index and this new code will work on all platforms.\r\n\r\n### `torch.{min,max,median}`: Update backward formula when doing full reduction (`dim` argument not provided) ([#43519](https://github.com/pytorch/pytorch/pull/43519))\r\n\r\nWhen no dimension is specified, full reduction is performed and the gradient will now flow back evenly towards all the input that realized the output value. The old behavior was to propagate the gradient only for one of such input selected arbitrarily.\r\nThis should improve stability of training by gradient descent.\r\nTo recover the previous behavior, you can perform the reduction with the `dim=` argument. It will ensure that the gradient only flows back for the input whose index was returned.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a\r\ntensor([3, 2, 3])\r\n>>> a.max().backward()\r\n>>> a.grad\r\ntensor([0, 0, 1])   \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a\r\ntensor([3, 2, 3])\r\n>>> a.max().backward()\r\n>>> a.grad\r\ntensor([0.5, 0, 0.5])\r\n>>> a.max(dim=0).max(dim=0).max(dim=0).backward()\r\n>>> a.grad\r\ntensor([0, 0, 1])   \r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `nn.BCELoss` size mismatch warning is now an error ([#41426](https://github.com/pytorch/pytorch/pull/41426))\r\n\r\nThis is the end of the deprecation cycle for this op to make sure it does not have different broadcasting semantic compared to numpy\u2019s broadcasting semantic used everywhere else in PyTorch\u2019s codebase.\r\nYou need to make sure all inputs are the same size to avoid the error.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> bceloss = nn.BCELoss()\r\n>>> a = torch.rand(25)\r\n>>> b = torch.rand(25, 1)\r\n>>> bceloss(a, b)\r\nUserWarning: Using a target size (torch.Size([25, 1]))\r\nthat is different to the input size (torch.Size([25]))\r\nis deprecated. Please ensure they have the same size.\r\ntensor(1.0604)  \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> bceloss = nn.BCELoss()\r\n>>> a = torch.rand(25)\r\n>>> b = torch.rand(25, 1)\r\n>>> bceloss(a, b)\r\nValueError: Using a target size (torch.Size([25, 1]))\r\nthat is different to the input size (torch.Size([25]))\r\nis deprecated. Please ensure they have the same size.\r\n>>> b = b.reshape(25)\r\n>>> bceloss(a, b)\r\ntensor(1.0604)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Custom `autograd.Function` stop materializing `None` output Tensors ([#41490](https://github.com/pytorch/pytorch/pull/41490))\r\n\r\nTo improve performance, the custom `autograd.Function` will not create a Tensor full of zeros when an input is differentiable but the user\u2019s `backward` function returns `None` for it. This means that code for which the `.backward()` or `autograd.grad()` final result will now be `None` while it used to be a Tensor full of zeros.\r\nYou can recover the previous behavior by having your custom `autograd.Function` materialize the zero Tensor with `torch.zeros_like(input)` to replace the `None` output for the `backward` method.\r\n\r\n```python\r\nimport torch\r\n\r\n# Custom Function that returns None for the gradient\r\nclass GetTwos(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, inp):\r\n        return inp.clone().fill_(2)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_out):\r\n        # To recover the 1.6 behavior, replace the line below with `return torch.zeros_like(grad_out)`\r\n        return None\r\n\r\na = torch.rand(10, requires_grad=True)\r\nb = GetTwos.apply(a)\r\nb.sum().backward()\r\n\r\nprint(a.grad)\r\n# In PyTorch 1.6 this will print\r\n# tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\r\n# In PyTorch 1.7 this will print\r\n# None\r\n```\r\n\r\n### Fix inplace detection for non-differentiable outputs ([#41269](https://github.com/pytorch/pytorch/pull/41269))\r\n\r\nWe fixed a bug in the inplace detection code that was preventing the detection of some inplace operations for output that are not differentiable (like integer type Tensors).\r\nThis can lead to code that used to run fine to throw the error \u201ca Tensor that was needed for backward was modified in an inplace operation\u201d.\r\nSuch failure is true and the user code must be fixed to compute proper gradients. In general, this involves cloning the Tensor before modifying it inplace to make sure the backward pass can happen safely.\r\n\r\n\r\n```python\r\nimport torch\r\n\r\na = torch.rand(10, requires_grad=True)\r\nwith torch.no_grad():\r\n    a[2] = 10\r\n\r\nb, ind = a.max(dim=0)\r\n# ind is 2 here\r\n\r\nwith torch.no_grad():\r\n    t = torch.rand(10)\r\n    t[4] = 10\r\n    res = torch.max(t, dim=0, out=(torch.Tensor(), ind))\r\n    # ind becomes 4 here\r\n\r\n# This backward runs in 1.6 but will fail in 1.7\r\nb.sum().backward()\r\nprint(a.grad)\r\n# tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\r\n# The value is wrong is at index 4 while it should be at index 2\r\n\r\n# The issue is avoided by not modifying ind inplace by replacing the line\r\n# above with:\r\n# res = torch.max(t, dim=0, out=(torch.Tensor(), ind.clone()))\r\n```\r\n\r\n### Add `__torch_functions__` for methods ([#37091](https://github.com/pytorch/pytorch/pull/37091))\r\n\r\nFunctions, slicing and Tensor methods will now properly preserve the subclass type when possible.\r\n\r\n```python\r\n>>> class SubTensor(torch.Tensor):\r\n...     pass\r\n>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__\r\n'SubTensor'\r\n>>> type(torch.add(SubTensor([0]), torch.Tensor([1]))).__name__\r\n'SubTensor'\r\n```\r\n\r\nThe old behavior of \u201cany operations on your subclass produces a torch.Tensor instead of the subclass\u201d can be recovered by doing:\r\n\r\n```python\r\nfrom torch._C import _disabled_torch_function_impl\r\n    \r\nclass SubTensor(torch.Tensor):\r\n    __torch_function__ = _disabled_torch_function_impl\r\n```\r\n\r\nFor all details on how to use this feature, please refer to the [doc](https://pytorch.org/docs/stable/notes/extending.html#extending-torch) page for it.\r\n\r\n### `tensor.__iter__`: Use `torch.unbind` instead of a for loop ([#40884](https://github.com/pytorch/pytorch/pull/40884))\r\n\r\nThis improves performances significantly but it changes the behavior of in-place operations on the value returned by the iterator. This happens only if either the input Tensor or any argument of the in-place operation is a Tensor that requires gradients. And it will fail with \"Output X of UnbindBackward is a view and is being modified inplace\".\r\nYou can recover the previous behavior by manually slicing the Tensor: `[t[i] for i in range(t.size(0))]` as shown in the example below.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(5, 10, requires_grad=True)\r\n>>> for i, v in enumerate(x):\r\n>>>     v.fill_(i)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(5, 10, requires_grad=True)\r\n>>> for i, v in enumerate([x[j] for j in range(x.size(0))]):\r\n>>>   v.fill_(i)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Updated most function that take zero, one or two Tensor arguments and indexing op to check for memory overlap in the Tensor being worked on ([#43418](https://github.com/pytorch/pytorch/pull/43418), [#43419](https://github.com/pytorch/pytorch/pull/43419), [#43](https://github.com/pytorch/pytorch/pull/43420)[420](https://github.com/pytorch/pytorch/pull/43420), [#43421](https://github.com/pytorch/pytorch/pull/43421), [#43423](https://github.com/pytorch/pytorch/pull/43423), [#43422](https://github.com/pytorch/pytorch/pull/43422))\r\n\r\nIt fixes silent correctness errors: something that used to be silently incorrect now errors out. Code that raises this error must be updated to avoid doing such op that was returning wrong results as shown in the example below:\r\n\r\n```python\r\n>>> x = torch.randn(1, 3)\r\n>>> # Create a tensor that has internal memory overlap\r\n>>> y = x.expand(2, 3)\r\n\r\n# In 1.6, this would not error out, but in 1.7, this errors out\r\n>>> torch.nn.functional.elu(y, inplace=True)\r\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single m\r\nemory location. Please clone() the tensor before performing the operation.\r\n\r\n# Here is the fix in 1.7\r\n>>> torch.nn.functional.elu(y, inplace=False)\r\n```\r\n\r\nc++ API: Any external users of `TensorIterator` now always get the memory overlap check. The previous behavior can be recovered by setting `set_check_mem_overlap(false)` when creating the iterator.\r\n\r\n## TorchScript\r\n\r\n### TorchScript now correctly supports various exception type and custom exception message ([#41907](https://github.com/pytorch/pytorch/pull/41907))\r\n\r\n* Exceptions raised in TorchScript was traditionally replaced with a generic runtime error that doesn\u2019t carry exception type or message, leading to crashes that are difficult to pin-point and debug. We improved TorchScript to correctly parse exception types and messages and surface them to users. \r\n* This change is backward incompatible because TorchScript now attempts to compile user code that creates custom exception messages instead of ignoring them. Any TorchScript-incompatible Python features used in those code snippets would lead to failures.\r\n* There is no fixed formula to fix this backward incompatibility failure other than updating code that generates exceptions to be TorchScript-able.\r\n\r\n### TorchScript now supports properties of TorchScript classes and ScriptModules ([#42389](https://github.com/pytorch/pytorch/pull/42389), [#42390](https://github.com/pytorch/pytorch/pull/42390))\r\n\r\n* TorchScript added support for `@property` of TorchScript classes and ScriptModules. Custom setters and getters are also supported. Custom deleters are not supported.\r\n* This improvement is backward incompatible because TorchScript now attempts to script properties of existing classes and `Modules`. If these properties use Python or Pytorch features that are not supported in Torchscript, scripting will fail.\r\n* There are two ways of fixing backward incompatibility failures introduced by this change. One is using `@torch.jit.unused` to annotate problematic properties, the other is to update the implementation of the property so that the getter and setter are scriptable.\r\n\r\n## Quantization\r\n\r\n### The convolution parameters now support versioning.\r\n\r\n* This change means that any quantized convolution module **saved** using PyTorch 1.7+ cannot be loaded in v1.6 and lower.\r\n* But this change is backward compatible: if the model (with conv layers) is saved in version 1.6, it can be safely loaded in version 1.7.\r\n\r\n## Some undocumented functions that were mistakenly made public have been removed\r\n\r\n* `torch.absolute_` has been removed, the Tensor method (`Tensor.absolute_`) should be used instead just like all other inplace ops.\r\n* `torch.ExtraFilesMap` is an internal jit construct and should not be used.\r\n\r\n## TorchScript Compiler Update\r\n\r\nIn 1.7, we are enabling a Profiling Executor and a new Tensor-Expressions-based (TE) Fuser. All compilations will now go through one (an adjustable setting) profiling run and one optimization run. For the profiling run, complete tensor shapes are recorded and used by the new Fuser. For the optimization run, the focus is on finding (in `torch.jit.ScriptModule`s) and fusing element-wise operations over CUDA tensors into a single CUDA kernel.\r\n\r\nThe TE fuser is expected to deliver performance similar to the old fuser used in 1.6. It however unlocks more opportunities for performance improvements in future releases. In rare cases, performance of some models may degrade 5-10%. If you experience any regressions please report it on Github, so we can address them as soon as possible! For 1.7, we are providing an option for our users to revert back to the old fuser by calling `torch._C._jit_set_profiling_executor(False)` in Python and `torch::jit::getExecutorMode()`` = false;` in C++. For more information, please see [\u201cGraph Executor\u201d section](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md#graph-executor) in our documentation.\r\n\r\n# Deprecations\r\n\r\n## Python API\r\n\r\n### `torch.norm` and `torch.functional.norm` are deprecated in favor of `torch.linalg.norm` ([#44321](https://github.com/pytorch/pytorch/pull/44321))\r\n\r\nThe new `torch.linalg.norm` has the same behavior as `numpy.linalg.norm`\r\nBoth deprecated functions had odd behaviors for matrix and vector norms. You should refer to the doc [here](https://pytorch.org/docs/stable/generated/torch.norm.html?highlight=norm#torch.norm) to find the exact behavior they had and how to replicate it with the new API.\r\n\r\n### Deprecate fft functions in `torch.` namespace in favor of `torch.fft.` namespace ([#44876](https://github.com/pytorch/pytorch/pull/44876))\r\n\r\nPlease use `torch.fft.foo` as a drop-in replacement for `torch.foo` for the following functions: `fft`, `ifft`, `rfft` and `irfft`.\r\n\r\n### Warns when some `out=` functions need to resize an output which is not 0-size ([#42079](https://github.com/pytorch/pytorch/pull/42079))\r\n\r\nThis behavior is dangerous and leads to an API that is hard to use. It is being deprecated to be able to fix that API in future versions.\r\nYou should resize the output before-hand to avoid any issue in the future:\r\n\r\n```python\r\na = torch.rand(5)\r\nb = torch.rand(25)\r\n\r\n# This is deprecated\r\ntorch.add(a, a, out=b)\r\n\r\n# This has the same behavior but will work in future versions\r\ntorch.add(a, a, out=b.resize_(0))\r\n```\r\n\r\n### `torch.optim`: Warn for duplicate params in param group ([#41597](https://github.com/pytorch/pytorch/pull/41597))\r\n\r\nProviding multiple times the same Parameter in a single param group is most likely due to user error and is being deprecated.\r\nPlease open an issue if you have a valid use case that require this feature.\r\n\r\n### `torch.linspace` and `torch.logspace`: Not giving the step argument is deprecated ([#43860](https://github.com/pytorch/pytorch/pull/43860))\r\n\r\nThe default `steps` argument that has been used historically in PyTorch is not consistent with other libraries and so is being removed to avoid confusion.\r\nFor both functions, passing `steps=100` keyword argument can be used to recover the original behavior.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.6.0</th><th>1.7.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.linspace(0, 10).size()\r\ntorch.Size([100])   \r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.linspace(0, 10).size()\r\nUserWarning: Not providing a value for linspace's\r\nsteps is deprecated and will throw a runtime error\r\nin a future release.\r\ntorch.Size([100])\r\n>>> torch.linspace(0, 10, steps=100).size()\r\ntorch.Size([100])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## Distributed\r\n\r\n* Make TensorPipe the default backend for RPC ([#43246](https://github.com/pytorch/pytorch/pull/43246))\r\n* Infer RPC backend type to preserve backward compatibility as we make TensorPipe the default ([#45065](https://github.com/pytorch/pytorch/pull/45065))\r\n* Add deprecation warning to ProcessGroup backend and make TensorPipe backend stable. ([#45356](https://github.com/pytorch/pytorch/pull/45356))\r\n* Add warnings on `ProcessGroup` and `ProcessGroup::Work` APIs which will be retired soon. ([#46366](https://github.com/pytorch/pytorch/pull/46366))\r\n\r\n# New features\r\n\r\n### Python API\r\n\r\nNew namespaces:\r\n\r\n* `torch.fft` added ([#41911](https://github.com/pytorch/pytorch/pull/41911))\r\n* `torch.linalg` added ([#42664](https://github.com/pytorch/pytorch/pull/42664))\r\n* `torch.optim.functional` added ([#44715](https://github.com/pytorch/pytorch/pull/44715))\r\n\r\nNew operators:\r\n\r\n* `torch.count_nonzero` added ([#39992](https://github.com/pytorch/pytorch/pull/39992))\r\n* `nn.SiLU` activation added ([#41034](https://github.com/pytorch/pytorch/pull/41034))\r\n* `torch.logit` added ([#41062](https://github.com/pytorch/pytorch/pull/41062))\r\n* `torch.gcd`, `torch.lcm` added ([#40651](https://github.com/pytorch/pytorch/pull/40651), [#41552](https://github.com/pytorch/pytorch/pull/41552), [#42254](https://github.com/pytorch/pytorch/pull/42254))\r\n* `torch.functional.atleast_{1d/2d/3d}` added ([#41317](https://github.com/pytorch/pytorch/pull/41317))\r\n* `torch.isreal` added ([#41298](https://github.com/pytorch/pytorch/pull/41298))\r\n* `nn.Unflatten` added ([#41564](https://github.com/pytorch/pytorch/pull/41564))\r\n* `torch.movedim` added ([#41480](https://github.com/pytorch/pytorch/pull/41480))\r\n* `torch.isposinf`, `torch.isneginf` added ([#41588](https://github.com/pytorch/pytorch/pull/41588))\r\n* `torch.signbit` added ([#41589](https://github.com/pytorch/pytorch/pull/41589))\r\n* `torch.absolute` added ([#42586](https://github.com/pytorch/pytorch/pull/42586))\r\n* `torch.clip` alias added ([#42770](https://github.com/pytorch/pytorch/pull/42770))\r\n* `torch.quantile` added ([#42755](https://github.com/pytorch/pytorch/pull/42755))\r\n* `torch.linalg.det` and `torch.outer` alias added ([#42802](https://github.com/pytorch/pytorch/pull/42802))\r\n* `torch.nansum` added ([#38628](https://github.com/pytorch/pytorch/pull/38628))\r\n* `torch.hypot` added ([#42291](https://github.com/pytorch/pytorch/pull/42291))\r\n* `torch.nextafter` added ([#42580](https://github.com/pytorch/pytorch/pull/42580))\r\n* `torch.hstack`, `torch.vstack`, `torch.dstack` added ([#42799](https://github.com/pytorch/pytorch/pull/42799))\r\n* `torch.arccosh` alias added ([#43107](https://github.com/pytorch/pytorch/pull/43107))\r\n* `Tensor.movedim` as a method added ([#43122](https://github.com/pytorch/pytorch/pull/43122))\r\n* `torch.matrix_exp` added ([#40161](https://github.com/pytorch/pytorch/pull/40161))\r\n* `torch.fix` alias added ([#43326](https://github.com/pytorch/pytorch/pull/43326))\r\n* `torch.arccos`, `torch.arcsin`, `torch.arctan` aliases added ([#43319](https://github.com/pytorch/pytorch/pull/43319))\r\n* `torch.negative` alias added ([#43400](https://github.com/pytorch/pytorch/pull/43400))\r\n* `torch.maximum`, `torch.minimum` added ([#42579](https://github.com/pytorch/pytorch/pull/42579))\r\n* `torch.arctanh`, `torch.arcsinh` aliases added ([#43762](https://github.com/pytorch/pytorch/pull/43762))\r\n* `torch.linalg.norm` added ([#42749](https://github.com/pytorch/pytorch/pull/42749), [#43907](https://github.com/pytorch/pytorch/pull/43907))\r\n* `torch.amax`, `torch.amin` added ([#43819](https://github.com/pytorch/pytorch/pull/43819))\r\n* `torch.heaviside` added ([#42523](https://github.com/pytorch/pytorch/pull/42523))\r\n* `torch.i0` added ([#43132](https://github.com/pytorch/pytorch/pull/43132))\r\n* `torch.not_equal`, `torch.greater`, `torch.greater_equal`, `torch.less`, `torch.less_equal` aliases added ([#43870](https://github.com/pytorch/pytorch/pull/43870))\r\n* `torch.exp2` added ([#44184](https://github.com/pytorch/pytorch/pull/44184))\r\n* `torch.kaiser_window` added ([#44271](https://github.com/pytorch/pytorch/pull/44271))\r\n* `torch.nanquantile` added ([#44393](https://github.com/pytorch/pytorch/pull/44393))\r\n* `torch.multiply`, `torch.divide` aliases added ([#44463](https://github.com/pytorch/pytorch/pull/44463))\r\n* `nn.TripletMarginWithDistanceLoss` added ([#43680](https://github.com/pytorch/pytorch/pull/43680))\r\n* `torch.fft.fft`, `torch.fft.ifft`, `torch.fft.rfft`, `torch.fft.irfft`, `torch.fft.hfft`, `torch.fft.ihfft` added ([#43011](https://github.com/pytorch/pytorch/pull/43011))\r\n* `torch.fft.fftn`, `torch.fft.ifftn`, `torch.fft.rfftn`, `torch.fft.irfftn` added ([#44550](https://github.com/pytorch/pytorch/pull/44550))\r\n* `optim.functional.adagrad` added ([#44715](https://github.com/pytorch/pytorch/pull/44715))\r\n* `optim.functional.adam` added ([#44791](https://github.com/pytorch/pytorch/pull/44791))\r\n* `torch.complex`,  `torch.polar` added ([#39617](https://github.com/pytorch/pytorch/pull/39617))\r\n* `Tensor.__complex__` added ([#43844](https://github.com/pytorch/pytorch/pull/43844))\r\n* `torch.vdot` added ([#43004](https://github.com/pytorch/pytorch/pull/43004))\r\n\r\nAPI extension:\r\n\r\n* `torch.full` added support for bool and integer dtypes ([#41912](https://github.com/pytorch/pytorch/pull/41912))\r\n* `torch.lt` and `torch.masked_select` added support for half dtype ([#43704](https://github.com/pytorch/pytorch/pull/43704))\r\n* `torch.div`, `torch.true_divide`, `torch.atan2` added support for integer to float type promotion in ([#42359](https://github.com/pytorch/pytorch/pull/42359))\r\n* `unflatten`  added support for non-named dimensions ([#42563](https://github.com/pytorch/pytorch/pull/42563))\r\n* `torch.polygamma`  added support for n >= 2 ([#42499](https://github.com/pytorch/pytorch/pull/42499))\r\n* `torch.qr` added backward support for wide input matrices ([#42216](https://github.com/pytorch/pytorch/pull/42216))\r\n* `nn.Linear`  for MKLDNN added support for no-bias ([#43703](https://github.com/pytorch/pytorch/pull/43703))\r\n* `torch.lerp` added support for half dtype ([#43541](https://github.com/pytorch/pytorch/pull/43541))\r\n* Updates `torch.div` to perform true division (end of deprecation cycle) ([#42907](https://github.com/pytorch/pytorch/pull/42907))\r\n* `torch.scatter` added support for reductions on CUDA ([#41977](https://github.com/pytorch/pytorch/pull/41977))\r\n* BFloat16 support type promotion ([#41698](https://github.com/pytorch/pytorch/pull/41698), [#43324](https://github.com/pytorch/pytorch/pull/43324))\r\n* BFloat16 support on CUDA for `torch.pow` ([#44760](https://github.com/pytorch/pytorch/pull/44760)), unary ops and activations ([#44813](https://github.com/pytorch/pytorch/pull/44813), [#44824](https://github.com/pytorch/pytorch/pull/44824), [#44834](https://github.com/pytorch/pytorch/pull/44834)), `torch.i0` ([#44750](https://github.com/pytorch/pytorch/pull/44750)), `softmax` ([#44837](https://github.com/pytorch/pytorch/pull/44837)), `div`, `addcdiv`, `addcmul`, `mean`, `var` ([#44758](https://github.com/pytorch/pytorch/pull/44758)), `layernorm` ([#45002](https://github.com/pytorch/pytorch/pull/45002)),all pooling layers ([#44836](https://github.com/pytorch/pytorch/pull/44836), [#45151](https://github.com/pytorch/pytorch/pull/45151))), `torch.logspace` (CPU and CUDA) ([#44675](https://github.com/pytorch/pytorch/pull/44675)), random kernels on Windows ([#44918](https://github.com/pytorch/pytorch/pull/44918)), `torch.addmm`, `torch.addmv` ([#44986](https://github.com/pytorch/pytorch/pull/44986)), loss functions ([#45011](https://github.com/pytorch/pytorch/pull/45011)), batched gemm ([#45167](https://github.com/pytorch/pytorch/pull/45167)), nccl path ([#38515](https://github.com/pytorch/pytorch/pull/38515)), binary logical operators ([#42485](https://github.com/pytorch/pytorch/pull/42485)), `torch.neg` ([#45240](https://github.com/pytorch/pytorch/pull/45240)), Conv (non-cuDNN) ([#45007](https://github.com/pytorch/pytorch/pull/45007)), `torch.abs` ([#44804](https://github.com/pytorch/pytorch/pull/44804)), `torch.erfinv` ([#43399](https://github.com/pytorch/pytorch/pull/43399)), comparison ops ([#44748](https://github.com/pytorch/pytorch/pull/44748))\r\n* `torch.asin`, `torch.neg` added support for sparse Tensors ([#44028](https://github.com/pytorch/pytorch/pull/44028))\r\n* `torch.softmax` added support for CUDA ([#42307](https://github.com/pytorch/pytorch/pull/42307))\r\n* `Tensor.{real,imag}` added setter for these attributes ([#39860](https://github.com/pytorch/pytorch/pull/39860))\r\n* `torch.{addmm,addmv}` added support for complex on CUDA ([#40431](https://github.com/pytorch/pytorch/pull/40431), [#43827](https://github.com/pytorch/pytorch/pull/43827))\r\n* `torch.bmm` added support for complex on CPU [#42383](https://github.com/pytorch/pytorch/pull/42383),\r\n* `torch.{dot, vdot}` added support for complex ([#42745](https://github.com/pytorch/pytorch/pull/42745))\r\n* `torch.stft`, `torch.istft` added support for complex ([#43886](https://github.com/pytorch/pytorch/pull/43886))\r\n* `torch.cholesky` added support for complex ([#44895](https://github.com/pytorch/pytorch/pull/44895), [#45267](https://github.com/pytorch/pytorch/pull/45267))\r\n* `torch.sgn` added (to support complex) ([#39955](https://github.com/pytorch/pytorch/pull/39955))\r\n* Binary ops added support for complex ([#43174](https://github.com/pytorch/pytorch/pull/43174))\r\n* Add allowlist for complex backward ([#45461](https://github.com/pytorch/pytorch/pull/45461))\r\n\r\n### Autograd\r\n\r\n* Don't automatically materialize output grads with zeros for `autograd.Function` ([#41821](https://github.com/pytorch/pytorch/pull/41821))\r\n* Benchmark tool for `autograd.functional` API ([#43428](https://github.com/pytorch/pytorch/pull/43428))\r\n* Added `reset_grad` API to remove gradient instead of setting them to zero ([#44423](https://github.com/pytorch/pytorch/pull/44423))\r\n* Allow Tensor-like objects in `torch.autograd.gradcheck` ([#43877](https://github.com/pytorch/pytorch/pull/43877))\r\n* Added support for nested call for `@torch.no_grad()` decorator ([#44633](https://github.com/pytorch/pytorch/pull/44633))\r\n* Added support for `torch.lobpcg` backward ([#43002](https://github.com/pytorch/pytorch/pull/43002))\r\n\r\n### CUDA\r\n\r\n* Added TF32 support ([#41498](https://github.com/pytorch/pytorch/pull/41498))\r\n* CUDA RTX30 series support ([#45489](https://github.com/pytorch/pytorch/pull/45489), [#45130](https://github.com/pytorch/pytorch/pull/45130))\r\n    * **Note: **At the time of the 1.7 release, the currently available and stable Nvidia CUDA libraries are not fully tuned for the RTX 3080 and 3090 so users might see performance regressions.\r\n* `torch.cuda.amp.GradScaler` now supports sparse gradients ([#36786](https://github.com/pytorch/pytorch/pull/36786))\r\n* Autocast support for cudnn RNNs ([#42385](https://github.com/pytorch/pytorch/pull/42385))\r\n* Support AMP in nn.parallel ([#43102](https://github.com/pytorch/pytorch/pull/43102))\r\n* Support for tf32 in cudnn and `backends.cudnn.allow_tf32` flag to control it ([#40737](https://github.com/pytorch/pytorch/pull/40737))\r\n* Added `torch.cuda.memory.list_gpu_processes` to list running processes on a give GPU ([#44616](https://github.com/pytorch/pytorch/pull/44616))\r\n* Add env variable to bypass CUDACachingAllocator for debugging ([#45294](https://github.com/pytorch/pytorch/pull/45294))\r\n* Add non-deterministic alert to CUDA operations that use `atomicAdd()` ([#41538](https://github.com/pytorch/pytorch/pull/41538))\r\n\r\n### C++ API\r\n\r\n* `nn::TransformerEncoderLayer` added ([#42633](https://github.com/pytorch/pytorch/pull/42633))\r\n* `nn::TransformerDecoderLayer` added ([#42717](https://github.com/pytorch/pytorch/pull/42717))\r\n* `nn::TransformerEncoder` added ([#43187](https://github.com/pytorch/pytorch/pull/43187))\r\n* `nn::TransformerDecoder` added ([#42886](https://github.com/pytorch/pytorch/pull/42886))\r\n* `nn::Transformer` added ([#44333](https://github.com/pytorch/pytorch/pull/44333))\r\n* `nn::Unflatten` added ([#42613](https://github.com/pytorch/pytorch/pull/42613))\r\n* `nn.ParameterList` added ([#41259](https://github.com/pytorch/pytorch/pull/41259))\r\n* `torch::cuda::manual_seed` and `torch::cuda::manual_seed_all` added ([#42638](https://github.com/pytorch/pytorch/pull/42638))\r\n\r\n### Mobile\r\n\r\n* Support Tensor MemoryFormat in java wrappers ([#40785](https://github.com/pytorch/pytorch/pull/40785))\r\n* Add `mobile_optimized` boolean flag to optimized model. ([#45479](https://github.com/pytorch/pytorch/pull/45479))\r\n\r\n### Vulkan\r\n\r\n* Backend added ([#36491](https://github.com/pytorch/pytorch/pull/36491), [#43076](https://github.com/pytorch/pytorch/pull/43076))\r\n* Add many operators `adaptive_avg_pool2d` ([#41220](https://github.com/pytorch/pytorch/pull/41220)), `mm` ([#41221](https://github.com/pytorch/pytorch/pull/41221)), `reshape` ([#41223](https://github.com/pytorch/pytorch/pull/41223)), `max_pool2d` ([#41379](https://github.com/pytorch/pytorch/pull/41379)), `add_` and `relu_` ([#41380](https://github.com/pytorch/pytorch/pull/41380)), `cat` ([#41434](https://github.com/pytorch/pytorch/pull/41434)), `add` and `mul` ([#42674](https://github.com/pytorch/pytorch/pull/42674)) and `avg_pool2d` ([#42675](https://github.com/pytorch/pytorch/pull/42675)).\r\n* Model preparation via `torch.utils.optimize_for_vulkan` ([#44903](https://github.com/pytorch/pytorch/pull/44903))\r\n* Add to Java API option to load on Vulkan and test app ([#44896](https://github.com/pytorch/pytorch/pull/44896), [#44897](https://github.com/pytorch/pytorch/pull/44897))\r\n\r\n### Distributed\r\n\r\n* Support alltoall collective in ProcessGroupGloo ([#41424](https://github.com/pytorch/pytorch/pull/41424), [#41690](https://github.com/pytorch/pytorch/pull/41690))\r\n* Add a DDP Communication Hook providing the flexibility to completely override DDP gradient communication ([#40848](https://github.com/pytorch/pytorch/pull/40848))\r\n* Examples on how to use the DDP communication hook ([#43310](https://github.com/pytorch/pytorch/pull/43310))\r\n* Add NCCL Alltoall to NCCL process group ([#42514](https://github.com/pytorch/pytorch/pull/42514))\r\n* Support allgather and gather APIs for Python Objects ([#42189](https://github.com/pytorch/pytorch/pull/42189))\r\n* Join-based API to support uneven inputs in DDP ([#42577](https://github.com/pytorch/pytorch/pull/42577))\r\n* broadcast_object API for c10d ([#43887](https://github.com/pytorch/pytorch/pull/43887))\r\n* Async Error Handling support for ProcessGroupNCCL ([#41050](https://github.com/pytorch/pytorch/pull/41050), [#41051](https://github.com/pytorch/pytorch/pull/41051), [#41052](https://github.com/pytorch/pytorch/pull/41052), [#41053](https://github.com/pytorch/pytorch/pull/41053), [#41054](https://github.com/pytorch/pytorch/pull/41054), [#44163](https://github.com/pytorch/pytorch/pull/44163))\r\n* Add a \u201cgradient_as_bucket_view\" parameter to DDP to reduce memory overhead ([#44344](https://github.com/pytorch/pytorch/pull/44344))\r\n* Add getNumKeys API to c10d TCPStore ([#43962](https://github.com/pytorch/pytorch/pull/43962))\r\n* Add DeleteKey API for c10d TCP Store ([#45401](https://github.com/pytorch/pytorch/pull/45401))\r\n\r\n### Quantization\r\n\r\n* New quantized ops\r\n    * Adaptive average pooling ([#40271](https://github.com/pytorch/pytorch/pull/40271))\r\n    * Max pooling ([#45152](https://github.com/pytorch/pytorch/pull/45152))\r\n    * Embedding and EmbeddingBag quantization (8-bit + partial support for 4-bit): ([#40076](https://github.com/pytorch/pytorch/pull/40076), [#41293](https://github.com/pytorch/pytorch/pull/41293), [#41612](https://github.com/pytorch/pytorch/pull/41612), [#42924](https://github.com/pytorch/pytorch/pull/42924), [#42762](https://github.com/pytorch/pytorch/pull/42762), [#42881](https://github.com/pytorch/pytorch/pull/42881), [#43077](https://github.com/pytorch/pytorch/pull/43077), [#43088](https://github.com/pytorch/pytorch/pull/43088), [#43090](https://github.com/pytorch/pytorch/pull/43090), [#43176](https://github.com/pytorch/pytorch/pull/43176), [#43296](https://github.com/pytorch/pytorch/pull/43296), [#43433](https://github.com/pytorch/pytorch/pull/43433), [#43989](https://github.com/pytorch/pytorch/pull/43989), [#44008](https://github.com/pytorch/pytorch/pull/44008), [#44207](https://github.com/pytorch/pytorch/pull/44207), [#44208](https://github.com/pytorch/pytorch/pull/44208), [#44217](https://github.com/pytorch/pytorch/pull/44217), [#45149](https://github.com/pytorch/pytorch/pull/45149), [#44845](https://github.com/pytorch/pytorch/pull/44845), [#44048](https://github.com/pytorch/pytorch/pull/44048), [#42690](https://github.com/pytorch/pytorch/pull/42690), [#42612](https://github.com/pytorch/pytorch/pull/42612))\r\n    * QNNPACK Transposed convolution2D and 3D ([#39714](https://github.com/pytorch/pytorch/pull/39714), [#40351](https://github.com/pytorch/pytorch/pull/40351), [#40360](https://github.com/pytorch/pytorch/pull/40360), [#40370](https://github.com/pytorch/pytorch/pull/40370), [#40371](https://github.com/pytorch/pytorch/pull/40371), [#44844](https://github.com/pytorch/pytorch/pull/44844), [#45078](https://github.com/pytorch/pytorch/pull/45078), [#45081](https://github.com/pytorch/pytorch/pull/45081))\r\n    * Operations on quantized tensors\r\n        * `aten::repeat` ([#40644](https://github.com/pytorch/pytorch/pull/40644))\r\n        * `aten::apend` ([#40743](https://github.com/pytorch/pytorch/pull/40743))\r\n        * `stack` ([#42187](https://github.com/pytorch/pytorch/pull/42187))\r\n        * `fill_` ([#43303](https://github.com/pytorch/pytorch/pull/43303))\r\n        * `clone` for per channel affine quantized tensor ([#44573](https://github.com/pytorch/pytorch/pull/44573))\r\n        * `append` (graphmode) ([#44641](https://github.com/pytorch/pytorch/pull/44641))\r\n    * 1D batch normalization support ([#42491](https://github.com/pytorch/pytorch/pull/42491))\r\n    * N-Dimensional constant padding ([#43304](https://github.com/pytorch/pytorch/pull/43304))\r\n    * CELU operator ([#39199](https://github.com/pytorch/pytorch/pull/39199))\r\n* Support for FP16 quantization ([#40708](https://github.com/pytorch/pytorch/pull/40708), [#40709](https://github.com/pytorch/pytorch/pull/40709), [#40710](https://github.com/pytorch/pytorch/pull/40710), [#42147](https://github.com/pytorch/pytorch/pull/42147), [#42221](https://github.com/pytorch/pytorch/pull/42221), [#42222](https://github.com/pytorch/pytorch/pull/42222), [#42348](https://github.com/pytorch/pytorch/pull/42348), [#41049](https://github.com/pytorch/pytorch/pull/41049))\r\n* Add Quantizer support to IValue ([#42438](https://github.com/pytorch/pytorch/pull/42438))\r\n* Custom module support ([#44835](https://github.com/pytorch/pytorch/pull/44835))\r\n* Preserving pre and post forward hooks ([#37233](https://github.com/pytorch/pytorch/pull/37233))\r\n\r\n### Misc\r\n\r\n* `torch.set_deterministic` and `torch.is_deterministic`: Raise error when the flag is set and a non-deterministic operation is used ([#15359](https://github.com/pytorch/pytorch/issues/15359), [#41377](https://github.com/pytorch/pytorch/issues/41377))\r\n* Add CUDA 11 to nightly binaries ([#44086](https://github.com/pytorch/pytorch/pull/44086), [#43366](https://github.com/pytorch/pytorch/pull/43366))\r\n* Dev Tool: Nightly checkout tool and doc in `CONTRIBUTING.md` ([#42635](https://github.com/pytorch/pytorch/pull/42635),  [#43294](https://github.com/pytorch/pytorch/pull/43294))\r\n* Website: Add docs for tagged version (include rc) on the general website ([#45204](https://github.com/pytorch/pytorch/pull/45204))\r\n* Build: Added BUILD_CAFFE2 flag to be able to disable caffe2 compilation ([#43673](https://github.com/pytorch/pytorch/pull/43673))\r\n* Dataloader: Add `prefetch_factor` argument to control the number of batch loaded ahead of time([#41130](https://github.com/pytorch/pytorch/pull/41130))\r\n* Dataloader: Allow handling of `np.memmap` objects ([#39847](https://github.com/pytorch/pytorch/pull/39847))\r\n* ROCm: Add support torch `utils.cpp_extension` ([#41257](https://github.com/pytorch/pytorch/pull/41257), [#43528](https://github.com/pytorch/pytorch/pull/43528))\r\n* ROCm: Enable complex BLAS ([#43744](https://github.com/pytorch/pytorch/pull/43744))\r\n* docker: Add torchelastic to docker image ([#45438](https://github.com/pytorch/pytorch/pull/45438))\r\n* docker: Add CUDA 11 support ([#45071](https://github.com/pytorch/pytorch/pull/45071))\r\n* docker: Use python 3.8 in pytorch docker image ([#45466](https://github.com/pytorch/pytorch/pull/45466))\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Use tree-based sum for floats to avoid numerical instability ([#39516](https://github.com/pytorch/pytorch/pull/39516))\r\n* `nn.ReflectionPad`: Add support for 0-dim batch sizes. ([#39231](https://github.com/pytorch/pytorch/pull/39231))\r\n* `torch.scatter`: Add reductions for CPU ([#36447](https://github.com/pytorch/pytorch/pull/36447))\r\n* Allow any valid ASCII python identifiers as dimnames ([#40871](https://github.com/pytorch/pytorch/pull/40871))\r\n* Improve Python warning prints when there is also an error ([#41116](https://github.com/pytorch/pytorch/pull/41116))\r\n* `torch.iinfo`, `torch.finfo`: Improve printing ([#40488](https://github.com/pytorch/pytorch/pull/40488))\r\n* `torch.where`: Add support for scalar input ([#40336](https://github.com/pytorch/pytorch/pull/40336))\r\n* `torch.nonzero`: Remove deprecation warning for `as_tuple` argument ([#45413](https://github.com/pytorch/pytorch/pull/45413))\r\n* `torch.distributions.Categorical`: Clamp logit to avoid `-inf` when calculating entropy ([#41002](https://github.com/pytorch/pytorch/pull/41002))\r\n* `torch.futures.Future`: Add `done` function to query the status of the future ([#42013](https://github.com/pytorch/pytorch/pull/42013))\r\n\r\n### torch.nn\r\n\r\n* `nn.EmbeddingBag`: Add support for `incude_last_offset=True` when reduction is mean or max ([#42215](https://github.com/pytorch/pytorch/pull/42215))\r\n* `nn.AvgPooling{1,2,3}d`: Ensure all cells are valid in ceil mode to avoid division by 0 ([#41368](https://github.com/pytorch/pytorch/pull/41368))\r\n* `nn,[Adaptive]MaxPool{1,2,3}d`: Handle edge case when input is filled with -inf ([#40665](https://github.com/pytorch/pytorch/pull/40665))\r\n* `nn.Hardsigmoid`, `nn.Hardswish`: Add inplace option ([#42346](https://github.com/pytorch/pytorch/pull/42346))\r\n* `nn.MSELoss`, `nn.L1Loss`, `nn.SmoothL1Loss`: Add support for target that requires gradients. ([#44437](https://github.com/pytorch/pytorch/pull/44437), [#44471](https://github.com/pytorch/pytorch/pull/44471), [#44486](https://github.com/pytorch/pytorch/pull/44486))\r\n* `nn.Parameter{List,Dict}`: Add warning when improperly used (with DataParallel or weight_norm) ([#44405](https://github.com/pytorch/pytorch/pull/44405))\r\n* `nn.functional.smooth_l1`: Add beta parameter ([#44433](https://github.com/pytorch/pytorch/pull/44433))\r\n\r\n### Build\r\n\r\n* Report error when ATEN_THEADING is OMP and USE_OPENMP is turned off. ([#40146](https://github.com/pytorch/pytorch/pull/40146))\r\n* Raise nice error when trying to build PyTorch on 32-bit Windows system ([#40321](https://github.com/pytorch/pytorch/pull/40321))\r\n* Make setup.py Python-2 syntactically correct and work for version >= 3.9 ([#41960](https://github.com/pytorch/pytorch/pull/41960), [#46388](https://github.com/pytorch/pytorch/pull/46388))\r\n* Don't proceed into setup.py too far if Python version is unsupported ([#42870](https://github.com/pytorch/pytorch/pull/42870))\r\n\r\n### Distributed\r\n\r\n* Support profiling rpc_async in TorchScript ([#40652](https://github.com/pytorch/pytorch/pull/40652))\r\n* Allow RPC to be initialized again after shutdown. ([#42723](https://github.com/pytorch/pytorch/pull/42723))\r\n* Support rpc_sync, rpc.remote in TorchScript ([#43043](https://github.com/pytorch/pytorch/pull/43043), [#43046](https://github.com/pytorch/pytorch/pull/43046))\r\n* Make async_execution compatible with RRef helpers ([#44666](https://github.com/pytorch/pytorch/pull/44666))\r\n* Extend RPC profiling to support async function execution over RPC. ([#44664](https://github.com/pytorch/pytorch/pull/44664))\r\n* Support record_shapes in RPC profiling ([#44419](https://github.com/pytorch/pytorch/pull/44419))\r\n* Add variants for cuda.comm.broadcast/gather/scatter which store the result in a provided \u201cout\u201d parameter ([#39681](https://github.com/pytorch/pytorch/pull/39681))\r\n* Explicitly abort NCCL Communicators on ProcessGroupNCCL Destruction ([#40585](https://github.com/pytorch/pytorch/pull/40585))\r\n* Helper function to print out all DDP-relevant env vars ([#41297](https://github.com/pytorch/pytorch/pull/41297))\r\n* Add timeout to ProcessGroup Work Wait ([#40944](https://github.com/pytorch/pytorch/pull/40944))\r\n* Support Wait Timeout in ProcessGroupNCCL ([#40946](https://github.com/pytorch/pytorch/pull/40946))\r\n* Support work-level timeouts in ProcessGroupGloo ([#40948](https://github.com/pytorch/pytorch/pull/40948))\r\n* Support for torch.bool in ProcessGroupNCCL ([#41959](https://github.com/pytorch/pytorch/pull/41959))\r\n* DDP.train() returns self to stay consistent with nn.Module ([#42131](https://github.com/pytorch/pytorch/pull/42131))\r\n* Add a drop_last option in DistributedSampler to drop tail of the data to ensure data is even across ranks ([#41171](https://github.com/pytorch/pytorch/pull/41171))\r\n* Additional error checking for `torch.cuda.nccl` APIs. ([#43247](https://github.com/pytorch/pytorch/pull/43247))\r\n* Support work.result() to get result tensors for allreduce for Gloo, NCCL backends ([#43970](https://github.com/pytorch/pytorch/pull/43970))\r\n* Add a device parameter to RemoteModule ([#44254](https://github.com/pytorch/pytorch/pull/44254))\r\n* Add remote_parameters() API for RemoteModule. ([#43906](https://github.com/pytorch/pytorch/pull/43906))\r\n* Add a warning log when there is high skew of uneven inputs in DDP training ([#45238](https://github.com/pytorch/pytorch/pull/45238))\r\n\r\n### TorchScript\r\n\r\n* Support string concatenation (cc29c192a6) \r\n* Support using Python Enum in TorchScript ([#41390](https://github.com/pytorch/pytorch/pull/41390),[#41965,](https://github.com/pytorch/pytorch/pull/41965)[#42085,](https://github.com/pytorch/pytorch/pull/42085)[#42623,](https://github.com/pytorch/pytorch/pull/42623)[#42661,](https://github.com/pytorch/pytorch/pull/42661)[#42661,](https://github.com/pytorch/pytorch/pull/42661)[#42874,](https://github.com/pytorch/pytorch/pull/42874)[#43460,](https://github.com/pytorch/pytorch/pull/43460)[#43188,](https://github.com/pytorch/pytorch/pull/43188)[#44243,](https://github.com/pytorch/pytorch/pull/44243)[#44891](https://github.com/pytorch/pytorch/pull/44891))\r\n* Support sorting list of strings ([#42398](https://github.com/pytorch/pytorch/pull/42398))\r\n* Support boolean key in dictionary ([#42833](https://github.com/pytorch/pytorch/pull/42833))\r\n* Support `@torch.no_grad` ([#41371](https://github.com/pytorch/pytorch/pull/41371))\r\n* Support `del` to TorchScript classes ([#44352](https://github.com/pytorch/pytorch/pull/44352))\r\n* Speed up saving modules in case of having many classes ([#44589](https://github.com/pytorch/pytorch/pull/44589))\r\n* Support Python Slice class in TorchScript ([#44335](https://github.com/pytorch/pytorch/pull/44335))\r\n* Support sorting a list of tuples ([#43448](https://github.com/pytorch/pytorch/pull/43448))\r\n* Enable `@torch.jit.unused` syntax for ignoring properties ([#45261](https://github.com/pytorch/pytorch/pull/45261))\r\n* Enable ProfilingExecutor + TensorExpression (#45546) ([#45546](https://github.com/pytorch/pytorch/pull/45546))\r\n* Support `@torch.jit.unused` on a `@torch.no_grad` decorated function ([#41496](https://github.com/pytorch/pytorch/pull/41496))\r\n* Improve ModuleList indexing error msg ([#43361](https://github.com/pytorch/pytorch/pull/43361))\r\n* Better match behavior of loaded `ScriptModule``s vs. freshly created ones ([#43298](https://github.com/pytorch/pytorch/pull/43298))\r\n* Support backend-lowered submodules ([#41146](https://github.com/pytorch/pytorch/pull/41146))\r\n* Allow freezing of modules containing interface attribute ([#41860](https://github.com/pytorch/pytorch/pull/41860))\r\n* `to_backend` API now accepts wrapped modules ([#43612](https://github.com/pytorch/pytorch/pull/43612))\r\n* Allow submodule methods inference rules to be different ([#43872](https://github.com/pytorch/pytorch/pull/43872))\r\n* Support default values for arguments of class type methods ([#45098](https://github.com/pytorch/pytorch/pull/45098))\r\n* Improve sugared value's error message when closing over global variables ([#42889](https://github.com/pytorch/pytorch/pull/42889))\r\n* Support backend-lowered submodules ([#40841](https://github.com/pytorch/pytorch/pull/40841))\r\n* Turn on non-ASCII string literals serialization ([#40719](https://github.com/pytorch/pytorch/pull/40719))\r\n* Better printing of Tensor stride information (#[45156](https://github.com/pytorch/pytorch/pull/45156))\r\n\r\n### Mobile\r\n\r\n* Allow specifying PYTHON executable to build_android ([#41927](https://github.com/pytorch/pytorch/pull/41927))\r\n* Include all overloads for OSS custom build (a01e91e6b2)\r\n\r\n### Quantization\r\n\r\n* Change the `whitelist` to `allowlist` ([#41771](https://github.com/pytorch/pytorch/pull/41771), [#41802](https://github.com/pytorch/pytorch/pull/41802))\r\n* `dequantize` now supports list and tuple of tensors ([#41079](https://github.com/pytorch/pytorch/pull/41079))\r\n* User now has a way to add a activation post process hook using `register_activation_post_process_hook` function ([#42342](https://github.com/pytorch/pytorch/pull/42342))\r\n* `add`/`mul` now support different variants ([#42769](https://github.com/pytorch/pytorch/pull/42769))\r\n* Fake quantizer now has more info when printed ([#43031](https://github.com/pytorch/pytorch/pull/43031))\r\n* `OP_LIST_TO_FUSER_METHOD` is exposed to the user ([#43286](https://github.com/pytorch/pytorch/pull/43286))\r\n* `quantize_jit`  can handle new upsample overloads ([#43407](https://github.com/pytorch/pytorch/pull/43407))\r\n* Setter/getter method for quantization and fusion mappings ([#43990](https://github.com/pytorch/pytorch/pull/43990))\r\n* fake_quant and observer can be disabled in scriptmodule ([#44773](https://github.com/pytorch/pytorch/pull/44773))\r\n* `convert_jit` can now take `preserved_attrs` argument ([#44490](https://github.com/pytorch/pytorch/pull/44490))\r\n* `SyncBN`: preserve qconfig if it exists ([#45317](https://github.com/pytorch/pytorch/pull/45317))\r\n* Add quant APIs to save/load observer `state_dict` ([#44846](https://github.com/pytorch/pytorch/pull/44846))\r\n* Add version support for the `conv` parameters ([#43524](https://github.com/pytorch/pytorch/pull/43524), [#43086](https://github.com/pytorch/pytorch/pull/43086), [#43651](https://github.com/pytorch/pytorch/pull/43651), [#44671](https://github.com/pytorch/pytorch/pull/44671))\r\n\r\n### ONNX\r\n\r\nIn PyTorch 1.7, we have continued to add and improve PyTorch operator export to ONNX. We have enabled export of 10 new operators, and further enhanced and optimized export of 10+ torch operators to ONNX. We have also focused on improving export of TorchScript modules, in particular laying some groundwork required for better support in near future. We have also created an API  (torch.onnx.utils._find_missing_ops_onnx_export) as a diagnostic tool (preview only) to get a list of operators in a model that are not supported or implemented by ONNX exporter. Support for export of torch.quantization.FakeQuantize has also been added to help enable some QAT workflows. \r\n\r\n* Add support to export more torch ops `torch.view_as` ([#40496](https://github.com/pytorch/pytorch/pull/40496)), fake quantize functions ([#39738](https://github.com/pytorch/pytorch/pull/39738)), embedding_bag ([#41234](https://github.com/pytorch/pytorch/pull/41234), [#44693](https://github.com/pytorch/pytorch/pull/44693)), `torch.eye` ([#41357](https://github.com/pytorch/pytorch/pull/41357)), `Tensor.as_strided` ([#41569](https://github.com/pytorch/pytorch/pull/41569)), `torch.tensor` ([#41872](https://github.com/pytorch/pytorch/pull/41872)), addition between list of tensors ([#41888](https://github.com/pytorch/pytorch/pull/41888)), `Tensor.__floordiv__` ([#43022](https://github.com/pytorch/pytorch/pull/43022)), `torch.nn.KLDivLoss` ([#41858](https://github.com/pytorch/pytorch/pull/41858)), `Tensor.new_empty` and `Tensor.new_zeros` ([#43506](https://github.com/pytorch/pytorch/pull/43506))\r\n* Improves existing export logic and optimizing exported ONNX graph\r\n    * Add warning in ONNX export when constant folding is on in training-amenable mode ([#40546](https://github.com/pytorch/pytorch/pull/40546))\r\n    * Fix export of `torch.full_like` ([#40063](https://github.com/pytorch/pytorch/pull/40063))\r\n    * Add pass that fuses Conv and BatchNormalization ([#40547](https://github.com/pytorch/pytorch/pull/40547))\r\n    *  `torch.where` export, add support for ByteTensor ([#42264](https://github.com/pytorch/pytorch/pull/42264))\r\n    * Fix scalar type cast for comparison ops ([#37787](https://github.com/pytorch/pytorch/pull/37787))\r\n    * `torch.scatter` export, add support for src being scalar or different dtype ([#42765](https://github.com/pytorch/pytorch/pull/42765), [#43440](https://github.com/pytorch/pytorch/pull/43440))\r\n    * Fix Squeeze operator when applied to a dimension with shape > 1 ([#38476](https://github.com/pytorch/pytorch/pull/38476))\r\n    *  Extend support for `torch.where` ([#41544](https://github.com/pytorch/pytorch/pull/41544))\r\n    * Update ops `torch.slice` ([#42935](https://github.com/pytorch/pytorch/pull/42935)), `torch.split` ([#43670](https://github.com/pytorch/pytorch/pull/43670)), `torch.repeat` ([#43430](https://github.com/pytorch/pytorch/pull/43430)), `torch.arange` ([#43777](https://github.com/pytorch/pytorch/pull/43777)), `len` ([#43824](https://github.com/pytorch/pytorch/pull/43824)), `torch.narrow` ([#44039](https://github.com/pytorch/pytorch/pull/44039)), flatten ([#40418](https://github.com/pytorch/pytorch/pull/40418)), adaptive_pool ([#46100](https://github.com/pytorch/pytorch/pull/46100))\r\n* Update export to follow pytorch changes\r\n\r\n    * Update div export to perform true divide ([#44831](https://github.com/pytorch/pytorch/pull/))\r\n    * Enable true_divide scripting export with ONNX  shape inference ([#43991](https://github.com/pytorch/pytorch/pull/43911))\r\n\r\n### Misc\r\n\r\n* `torch.utils.collect_env`: Collect more informations (python 32/64bit, clang version, CPU architecture, ROCm version) ([#42887](https://github.com/pytorch/pytorch/pull/42887), [#42961](https://github.com/pytorch/pytorch/pull/42961), [#44106](https://github.com/pytorch/pytorch/pull/44106))\r\n* `torch.hub.load_local`: Allow to load models from any local directory ([#44204](https://github.com/pytorch/pytorch/pull/44204))\r\n* Add warning if `import torch` is called from the source root ([#39995](https://github.com/pytorch/pytorch/pull/39995))\r\n* Improve Dynamic Library loading for Windows ([#40365](https://github.com/pytorch/pytorch/pull/40365))\r\n* serialization: validate sparse tensors after loading ([#34059](https://github.com/pytorch/pytorch/pull/34059))\r\n* Add `--continue-through-error` option to run_test.sh script ([#41136](https://github.com/pytorch/pytorch/pull/41136))\r\n* Tensorboard: Support custom `run_name` and ``hparam_domain_discrete` in `add_hparams` ([#40660](https://github.com/pytorch/pytorch/pull/40660), [#40720](https://github.com/pytorch/pytorch/pull/40720))\r\n* MKLDNN: Enable conv3d, batchnorm3d, max_pool3d and avg_pool3d ([#40691](https://github.com/pytorch/pytorch/pull/40691), [#40995](https://github.com/pytorch/pytorch/pull/40995), [#40996](https://github.com/pytorch/pytorch/pull/40996))\r\n* Profiler: Do not record zero duration kernel events ([#41540](https://github.com/pytorch/pytorch/pull/41540))\r\n* Profiler: Improve cuda time counting ([#45209](https://github.com/pytorch/pytorch/pull/45209))\r\n* Profiler: Adding `with_source` parameter to enable tracking source code ([#43898](https://github.com/pytorch/pytorch/pull/43898))\r\n* Optim: Add verbose param for all schedulers ([#41580](https://github.com/pytorch/pytorch/pull/41580))\r\n* Pruning: check attributes before deleting ([#41913](https://github.com/pytorch/pytorch/pull/41913))\r\n* Autograd: In `zero_grad`, avoid using inpalce `detach` when it is not required ([#41283](https://github.com/pytorch/pytorch/pull/41283))\r\n* Autograd: Update the `torch.div` backward formula to improve numerical stability ([#43627](https://github.com/pytorch/pytorch/pull/43627))\r\n* Autograd: Print all traceback for higher order backwards in detect_anomaly ([#43626](https://github.com/pytorch/pytorch/pull/43626))\r\n* Autograd: Stop saving input of `torch.repeat` as only `input.dim()` is needed in backward ([#40766](https://github.com/pytorch/pytorch/pull/40766))\r\n* CUDA: Improve cuDNN error messages to include call parameters ([#45023](https://github.com/pytorch/pytorch/pull/45023))\r\n* CUDA: Improve `device_count` and cuda init error detection and messages ([#42249](https://github.com/pytorch/pytorch/pull/42249))\r\n* Improve Tensor layout propagation for pointwise ops to follow input layout more closely ([#42922](https://github.com/pytorch/pytorch/pull/42922))\r\n* Remove blacklist/whitelist references ([#41447](https://github.com/pytorch/pytorch/pull/41447), [#41644](https://github.com/pytorch/pytorch/pull/41644), [#41636](https://github.com/pytorch/pytorch/pull/41636), [#41777](https://github.com/pytorch/pytorch/pull/41777), [#41822](https://github.com/pytorch/pytorch/pull/41822), [#41691](https://github.com/pytorch/pytorch/pull/41691), [#41789](https://github.com/pytorch/pytorch/pull/41789), [#41979](https://github.com/pytorch/pytorch/pull/41979), [#41627](https://github.com/pytorch/pytorch/pull/41627), [#42011](https://github.com/pytorch/pytorch/pull/42011), [#41796](https://github.com/pytorch/pytorch/pull/41796), [#42067](https://github.com/pytorch/pytorch/pull/42067), [#42091](https://github.com/pytorch/pytorch/pull/42091), [#42097](https://github.com/pytorch/pytorch/pull/42097), [#42071](https://github.com/pytorch/pytorch/pull/42071), [#42089](https://github.com/pytorch/pytorch/pull/42089), [#42279](https://github.com/pytorch/pytorch/pull/42279), [#42047](https://github.com/pytorch/pytorch/pull/42047), [#42088](https://github.com/pytorch/pytorch/pull/42088), [#45260](https://github.com/pytorch/pytorch/pull/45260))\r\n\r\n### Python Type Annotations\r\n\r\n* Update some types in top level `torch/*.py` ([#40235](https://github.com/pytorch/pytorch/pull/40235), [#40873](https://github.com/pytorch/pytorch/pull/40873))\r\n* Added typing for `Tensor` attributes and methods: `T` and `grad_fn` ([#40879](https://github.com/pytorch/pytorch/pull/40879)),  `Tensor._version` ([#41125](https://github.com/pytorch/pytorch/pull/41125)), `ndim` ([#42909](https://github.com/pytorch/pytorch/pull/42909)), `nonzero`  ([#43053](https://github.com/pytorch/pytorch/pull/43053)), [#40499](https://github.com/pytorch/pytorch/pull/40499))\r\n* Added typing for `torch.serialization` ([#40862](https://github.com/pytorch/pytorch/pull/40862))\r\n* Added typing for `torch.tensor` ([#45077](https://github.com/pytorch/pytorch/pull/45077))\r\n* Added typing for  `torch.Size` ([#40879](https://github.com/pytorch/pytorch/pull/40879))\r\n* Added typing for `torch.futures` ([#41675](https://github.com/pytorch/pytorch/pull/41675))\r\n* Added typing for `torch.random` ([#42234](https://github.com/pytorch/pytorch/pull/42234))\r\n* Added typing for `torch.hub` ([#42252](https://github.com/pytorch/pytorch/pull/42252))\r\n* Added typing for `collect_env.py` ([#43062](https://github.com/pytorch/pytorch/pull/43062))\r\n* Added typing for `torch.utils` ([#39392](https://github.com/pytorch/pytorch/pull/39392), [#42647](https://github.com/pytorch/pytorch/pull/42647), [#42711](https://github.com/pytorch/pytorch/pull/42711), [#42960](https://github.com/pytorch/pytorch/pull/42960), [#43806](https://github.com/pytorch/pytorch/pull/43806), [#44136](https://github.com/pytorch/pytorch/pull/44136), [#44216](https://github.com/pytorch/pytorch/pull/44216))\r\n* Added typing for `torch.nn` ([#43044](https://github.com/pytorch/pytorch/pull/43044), [#44093](https://github.com/pytorch/pytorch/pull/44093), [#43080](https://github.com/pytorch/pytorch/pull/43080), [#42231](https://github.com/pytorch/pytorch/pull/42231), [#40669](https://github.com/pytorch/pytorch/pull/40669))\r\n* Added typing for `torch.sparse` ([#43108](https://github.com/pytorch/pytorch/pull/43108))\r\n* Added typing for `torch.cuda.nvtx` ([#43443](https://github.com/pytorch/pytorch/pull/43443))\r\n* Added typing for `torch.cuda.memory` ([#43444](https://github.com/pytorch/pytorch/pull/43444))\r\n* Added typing for `torch.functional` ([#43446](https://github.com/pytorch/pytorch/pull/43446))\r\n* Added typing for `torch.autograd` ([#44451](https://github.com/pytorch/pytorch/pull/44451), [#46206](https://github.com/pytorch/pytorch/pull/46206))\r\n* Added typing for `torch.quantization.fuse_modules` ([#43786](https://github.com/pytorch/pytorch/pull/43786))\r\n* Added typing for `torch.nn.quantized` ([#43186](https://github.com/pytorch/pytorch/pull/43186), [#44154](https://github.com/pytorch/pytorch/pull/44154), [#43110](https://github.com/pytorch/pytorch/pull/43110))\r\n* Added typing for `torch.testing._internal` submodules ([#44575](https://github.com/pytorch/pytorch/pull/44575), [#44805](https://github.com/pytorch/pytorch/pull/44805), [#44832](https://github.com/pytorch/pytorch/pull/44832), [#44911](https://github.com/pytorch/pytorch/pull/44911), [#44927](https://github.com/pytorch/pytorch/pull/44927), [#44985](https://github.com/pytorch/pytorch/pull/44985), [#44971](https://github.com/pytorch/pytorch/pull/44971), [#45107](https://github.com/pytorch/pytorch/pull/45107), [#45368](https://github.com/pytorch/pytorch/pull/45368), [#45375](https://github.com/pytorch/pytorch/pull/45375))\r\n* Added typing for `torch.backends.quantized` ([#44794](https://github.com/pytorch/pytorch/pull/44794))\r\n* Added typing for `torch.backends.cuda` ([#44916](https://github.com/pytorch/pytorch/pull/44916))\r\n* Added typing for `torch.cuda.{comm,nccl,amp}` ([#45350](https://github.com/pytorch/pytorch/pull/45350), [#45344](https://github.com/pytorch/pytorch/pull/45344), [#45480](https://github.com/pytorch/pytorch/pull/45480))\r\n* Added typing for `torch.quasirandom` ([#45434](https://github.com/pytorch/pytorch/pull/45434))\r\n* Fix typing for `jit.trace` and `onnx.export` ([#41093](https://github.com/pytorch/pytorch/pull/41093))\r\n* Fix typing for `torch/optim/lr_scheduler.pyi` ([#41775](https://github.com/pytorch/pytorch/pull/41775), [#41866](https://github.com/pytorch/pytorch/pull/41866))\r\n\r\n# Bug fixes\r\n\r\n### Python API\r\n\r\n* `torch.linspace`: Fix step computation for large integral types ([#40132](https://github.com/pytorch/pytorch/pull/40132))\r\n* `torch.pca_lowrank`: Fix un-expected memory consumption ([#40853](https://github.com/pytorch/pytorch/pull/40853))\r\n* `torch.linspace`: Fix behavior for non-contiguous inputs on CPU ([#41286](https://github.com/pytorch/pytorch/pull/41286))\r\n* `torch.div`: Fix division by low precision scalar ([#41446](https://github.com/pytorch/pytorch/pull/41446))\r\n* `torch.expm1`: disable mkl as it produces wrong values in some cases ([#41654](https://github.com/pytorch/pytorch/pull/41654))\r\n* `torch.utils.data.RandomSampler`: Stop generating samples one at a time when replacement=True ([#41682](https://github.com/pytorch/pytorch/pull/41682))\r\n* `torch.nn.functional.grid_sample`: Fix 64-bit indexing ([#41923](https://github.com/pytorch/pytorch/pull/41923))\r\n* `torch.nn.functional.grid_sample`: Fix crash when `grid` has NaNs ([#42703](https://github.com/pytorch/pytorch/pull/42703))\r\n* `torch.det`: Fix on CPU ([#35136](https://github.com/pytorch/pytorch/pull/35136))\r\n* `torch.interpolate`: Avoid zero division in cubic mode ([#42093](https://github.com/pytorch/pytorch/pull/42093))\r\n* `torch.fmod`: Fix to work with zero divisors consistently ([#41948](https://github.com/pytorch/pytorch/pull/41948))\r\n* `torch.masked_select`: Fix for discontiguous outputs ([#41841](https://github.com/pytorch/pytorch/pull/41841))\r\n* `torch.cummin`, `torch.cummax`: Fix for discontiguous inputs/outputs ([#42507](https://github.com/pytorch/pytorch/pull/42507))\r\n* `torch.einsum`: Fix for discontiguous inputs ([#42425](https://github.com/pytorch/pytorch/pull/42425))\r\n* `torch.orgqr`: Fix input size conditions ([#42825](https://github.com/pytorch/pytorch/pull/42825))\r\n* `torch.manual_seed`: Fix argument unpacking ([#42206](https://github.com/pytorch/pytorch/pull/42206))\r\n* `torch.searchsorted`: Properly mark output as non differentiable ([#42933](https://github.com/pytorch/pytorch/pull/42933))\r\n* `torch.bucketize`: Properly mark output as non differentiable ([#44102](https://github.com/pytorch/pytorch/pull/44102))\r\n* `torch.addmm`: Properly raise error on device mismatch  ([#43505](https://github.com/pytorch/pytorch/pull/43505))\r\n* `torch.chain_matmul`: Properly handle empty args ([#43553](https://github.com/pytorch/pytorch/pull/43553))\r\n* `torch.multinomial`: Properly handle 0 size dim ([#43775](https://github.com/pytorch/pytorch/pull/43775))\r\n* `torch.cholesky_solve`: Fix broadcast and error checking ([#43137](https://github.com/pytorch/pytorch/pull/43137))\r\n* `torch.movedim`: Fix uniqueness check  ([#44307](https://github.com/pytorch/pytorch/pull/44307))\r\n* `torch.min`, ` torch.max`, `torch.mean`: Properly throw error if dim is repeated ([#44281](https://github.com/pytorch/pytorch/pull/44281))\r\n* `torch.lerp`: Fix for discontiguous outputs on CUDA ([#44559](https://github.com/pytorch/pytorch/pull/44559))\r\n* `torch.addmv`, `torch.mv`: Fix beta=0 case in slow path ([#44681](https://github.com/pytorch/pytorch/pull/44681))\r\n* `torch.triangular_solve`: Fix error check on CPU ([#44720](https://github.com/pytorch/pytorch/pull/44720))\r\n* `torch.empty_like`, `torch.zeros_like`: Properly raise error if any memory format is provided with sparse input ([#44058](https://github.com/pytorch/pytorch/pull/44058))\r\n* `torch.atan2`: Fix type promotion ([#43466](https://github.com/pytorch/pytorch/pull/43466))\r\n* `torch.repeat`: Fix backward for 0 size repeats ([#45212](https://github.com/pytorch/pytorch/pull/45212))\r\n* `torch.min`, ` torch.max`, `torch.median`: Fix handling of nan in backward ([#45280](https://github.com/pytorch/pytorch/pull/45280))\r\n* `torch.rdiv`: Properly make it consistent with div ([#45407](https://github.com/pytorch/pytorch/pull/45407))\r\n* `torch.std`: Fix hanling of nan in backward ([#45468](https://github.com/pytorch/pytorch/pull/45468))\r\n* `torch.distributions.Binomial`: Fix CUDA sampling at extreme points ([#42702](https://github.com/pytorch/pytorch/pull/42702))\r\n* `torch.dot`, `torch.vdot`: Add complex support ([#45074](https://github.com/pytorch/pytorch/pull/45074))\r\n* `torch.pow`: Fix when scalar base is complex ([#45259](https://github.com/pytorch/pytorch/pull/45259))\r\n* `torch.round`, `torch.abs_`: Disable complex inputs ([#45330](https://github.com/pytorch/pytorch/pull/45330))\r\n* `torch.svd`: Fix memory corruption for complex inputs ([#45486](https://github.com/pytorch/pytorch/pull/45486))\r\n* `torch.view_as_complex`: Fix zero dimensional input ([#44175](https://github.com/pytorch/pytorch/pull/44175))\r\n* `torch.kthvalue`: Fix for non-contiguous input ([#46177](https://github.com/pytorch/pytorch/pull/46177))\r\n* `torch.save`: Fix python binding that could lead to out of bound read ([#46207](https://github.com/pytorch/pytorch/pull/46207))\r\n\r\n### Torch.nn\r\n\r\n* `nn.ModuleDict`: Fix input dict key ordering ([#40905](https://github.com/pytorch/pytorch/pull/40905))\r\n* `nn.LayerNorm`: Fix handling of `gamma` in the backward when `create_graph=True`  ([#41595](https://github.com/pytorch/pytorch/pull/41595))\r\n* `nn.functional.{max,avg}_pool{1,2,3}d`: Raise RuntimeError for zero stride ([#41819](https://github.com/pytorch/pytorch/pull/41819))\r\n* `nn.Module`: Fix missing attribute when loading model from older version ([#42290](https://github.com/pytorch/pytorch/pull/42290))\r\n* `nn.Embedding`: Raise proper error for 0-D weight ([#42550](https://github.com/pytorch/pytorch/pull/42550))\r\n* `nn.SyncBatchNorm`: Fix forward pass for non-default process group ([#43861](https://github.com/pytorch/pytorch/pull/43861))\r\n* `nn.functional.embedding_bag`: Fix for non-contiguous weight ([#44032](https://github.com/pytorch/pytorch/pull/44032))\r\n* `nn.functional.upsample`: Add nondeterministic checks (df6ea62526)\r\n* `nn.GroupNorm`: Fix bug when input does not require_grad on CUDA ([#44863](https://github.com/pytorch/pytorch/pull/44863))\r\n* `functional.{l1_loss,smoothl1_loss,mse_loss}`: Properly check that reduction strings are valid ([#43527](https://github.com/pytorch/pytorch/pull/43527))\r\n* `functional.smoothl1_loss`: Properly raise error for negative `beta` values ([#45759](https://github.com/pytorch/pytorch/pull/45759))\r\n* `functional.pad`: Fix extra memory allocation and invalid result for negative or zero pad when using circular padding ([#39273](https://github.com/pytorch/pytorch/pull/39273))\r\n\r\n### C++ API\r\n\r\n* `nn::MultiheadAttention`: Ensure all parameters are properly registered ([#42037](https://github.com/pytorch/pytorch/pull/42037))\r\n* `Tensor::grad`: Fix the thread safety issues ([#40887](https://github.com/pytorch/pytorch/pull/40887))\r\n* `Tensor::var`: Ensure that `var(0)` does not call the `var(bool keepdim)` overload but `var(int dim)` ([#40451](https://github.com/pytorch/pytorch/pull/40451))\r\n\r\n### Distributed\r\n\r\n* Fix RPC and ProcessGroup GIL deadlock ([#45088](https://github.com/pytorch/pytorch/pull/45088))\r\n* Relax size check in flatten_for_scatter_gather ([#40573](https://github.com/pytorch/pytorch/pull/40573))\r\n* BAND, BOR and BXOR for NCCL all_reduce should throw runtime errors ([#42669](https://github.com/pytorch/pytorch/pull/42669))\r\n* Disallow creation of ProcessGroupNCCL without GPUs ([#45642](https://github.com/pytorch/pytorch/pull/45642))\r\n* Fix read/write of bulk data ([#42504](https://github.com/pytorch/pytorch/pull/42504))\r\n* Fix thread safety issue with distributed optimizers and TorchScript ([#46071](https://github.com/pytorch/pytorch/pull/46071))\r\n\r\n### TorchScript\r\n\r\n* Fix type annotations in select assignments ([#40528](https://github.com/pytorch/pytorch/pull/40528))\r\n* Fix compilation issues with GCC-5.4 ([#41055](https://github.com/pytorch/pytorch/pull/41055), [#41063](https://github.com/pytorch/pytorch/pull/41063), [#43223](https://github.com/pytorch/pytorch/pull/43223))\r\n* Fix JIT not round to even if constant is folded ([#40897](https://github.com/pytorch/pytorch/pull/40897))\r\n* Fix `torch.jit.freeze` import ([#42319](https://github.com/pytorch/pytorch/pull/42319))\r\n* Fix `List[str].index` ([#40348](https://github.com/pytorch/pytorch/pull/40348))\r\n* Fix `torch.jit.is_tracing()` so that it is correctly called rather than returning the method itself ([#42486](https://github.com/pytorch/pytorch/pull/42486))\r\n* Fix Str -> Device implicit conversions ([#43213](https://github.com/pytorch/pytorch/pull/43213))\r\n* Fix `NaN` propagation in fuser's min/max implementation ([#43590](https://github.com/pytorch/pytorch/pull/43590))\r\n*  Cast return values of functions returning Any ([#42259](https://github.com/pytorch/pytorch/pull/42259))\r\n* Fix `NaN` propagation in TensorExpression fuser's min/max implementation ([#43609](https://github.com/pytorch/pytorch/pull/43609))\r\n* Fix segfault in attribute lookup on loaded `ScriptModules` ([#43284](https://github.com/pytorch/pytorch/pull/43284))\r\n* Fix casting of `unsigned char`, and `abs(int)` ([#44157](https://github.com/pytorch/pytorch/pull/44157))\r\n* Fix frac in CUDA fuser ([#44152](https://github.com/pytorch/pytorch/pull/44152))\r\n* Fix model_name not logged properly issue. ([#45488](https://github.com/pytorch/pytorch/pull/45488))\r\n* Fix `len`, `contains`, `getitem` inherited from interface class derived from nn container ([#40789](https://github.com/pytorch/pytorch/pull/40789))\r\n* Fix support for FP16 in CudaCodgen ([#44209](https://github.com/pytorch/pytorch/pull/44209))\r\n* Fix `torch.tensor` for empty multidimensional-typed lists ([#44652](https://github.com/pytorch/pytorch/pull/44652))\r\n* Fix freeze_module pass for sharedtype ([#42457](https://github.com/pytorch/pytorch/pull/42457))\r\n* Correctly clone schema in `insert_observers` ([#40624](https://github.com/pytorch/pytorch/pull/40624))\r\n*  Fix value association with dictionaries in the tracer ([#40885](https://github.com/pytorch/pytorch/pull/40885))\r\n* Fix preserve submodule attribute in freezing ([#45143](https://github.com/pytorch/pytorch/pull/45143))\r\n* Fix Half conversion of immediates in NNC Cuda backend ([#45213](https://github.com/pytorch/pytorch/pull/45213))\r\n* Fix a bug in `SplitWithMask` when splitting multiple times ([#45141](https://github.com/pytorch/pytorch/pull/45141))\r\n* Fix inlining interface call in fork subgraph ([#43790](https://github.com/pytorch/pytorch/pull/43790))\r\n* Fix operator order in combineMultilane in TensorExpr fuser([#45157](https://github.com/pytorch/pytorch/pull/45157))\r\n* Correctly mark Tensor types inferred from empty annotation as `inferred=True` (#[45360](https://github.com/pytorch/pytorch/pull/45360))\r\n* Fix some bugs in Round+Mod simplification in NNC ([#42934](https://github.com/pytorch/pytorch/pull/42934))\r\n* Fix `set_grad_enabled` scripted version ([#46060](https://github.com/pytorch/pytorch/pull/46060))\r\n* Fix for `dict.update()` scripted version ([#46105](https://github.com/pytorch/pytorch/pull/46105))\r\n* Fix segfault when scripting nested classes ([#46422](https://github.com/pytorch/pytorch/pull/46422))\r\n* Fix memory leak in Profiling Mode ([#46621](https://github.com/pytorch/pytorch/pull/46621))\r\n\r\n### Quantization\r\n\r\n* Resolved namespace conflict in qnnpack for init_win symbol (a7e09b8727)\r\n* Fix linking of qnnpack params on windows. ([#40920](https://github.com/pytorch/pytorch/pull/40920))\r\n* Adding zero point type check for per channel quantization ([#40811](https://github.com/pytorch/pytorch/pull/40811))\r\n* Remove activation_post_process in qat modules (#42343) ([#43015](https://github.com/pytorch/pytorch/pull/43015))\r\n* `qlinear_dynamic`: Fix ASAN error in QNNPACK's integration. ([#41967](https://github.com/pytorch/pytorch/pull/41967))\r\n* Change quantizer to account for input tensor's memory format. ([#42178](https://github.com/pytorch/pytorch/pull/42178))\r\n* Fixing the output shape for the linear ([#44513](https://github.com/pytorch/pytorch/pull/44513))\r\n* Ensure observers and fq modules are scriptable ([#44749](https://github.com/pytorch/pytorch/pull/44749))\r\n* histogram observer: ensure buffer shape consistency ([#44956](https://github.com/pytorch/pytorch/pull/44956))\r\n* Attach qconfig to all modules ([#42576](https://github.com/pytorch/pytorch/pull/42576))\r\n* Fix qnnpack quantized activations for NHWC memory format ([#46217](https://github.com/pytorch/pytorch/pull/46217))\r\n\r\n### ONNX\r\n\r\n* Fix crash  when exporting a model with `nn.Sequential` ([#19227](https://github.com/pytorch/pytorch/pull/19227))\r\n* Fix default `ignore_index` for nll loss ([#44816](https://github.com/pytorch/pytorch/pull/44816))\r\n* Rename Black to Block  for various files ([#42913](https://github.com/pytorch/pytorch/pull/42913))\r\n* Fix bug in `onnx::SsaRewrite` ([#42148](https://github.com/pytorch/pytorch/pull/42148))\r\n\r\n### Misc\r\n\r\n* Fix `torch.hub` for new zipfile format. ([#42333](https://github.com/pytorch/pytorch/pull/42333))\r\n* Preserve python backtrace in autograd engine errors. ([#43684](https://github.com/pytorch/pytorch/pull/43684))\r\n* `optim.SparseAdam`: Fix check that params are dense on init ([#43668](https://github.com/pytorch/pytorch/pull/43668))\r\n* Fix clang build ([#44934](https://github.com/pytorch/pytorch/pull/44934))\r\n* `nn::MultiheadAttention:` Fix parameter registration ([#42037](https://github.com/pytorch/pytorch/pull/42037))\r\n* MaxPool2D: Fix memory leak for XNNPACK ([#41874](https://github.com/pytorch/pytorch/pull/41874))\r\n* Numpy scalar detection for bool and complex types fixed ([#43644](https://github.com/pytorch/pytorch/pull/43644))\r\n* Add missing file to `BUILD.bazel` ([#40536](https://github.com/pytorch/pytorch/pull/40536))\r\n* `autograd.gradcheck`: Add support for complex ([#43208](https://github.com/pytorch/pytorch/pull/43208))\r\n* Fix bug in mobile-specific CPU caching allocator ([#43719](https://github.com/pytorch/pytorch/pull/43719))\r\n\r\n# Performance\r\n\r\n### Python API\r\n\r\n* `torch.{view_as_complex,view_as_real}`: Remove unnecessary temporary Tensor ([#44908](https://github.com/pytorch/pytorch/pull/44908))\r\n* `tensorboard.SummaryWriter.add_audio`: Remove unnecessary for loops ([#44201](https://github.com/pytorch/pytorch/pull/44201))\r\n* `Conv2d` and `Conv3d`:  bypass the im2col for 1x1 conv ([#40324](https://github.com/pytorch/pytorch/pull/40324))\r\n* Fix `max_pool2d` perf regression ([#41174](https://github.com/pytorch/pytorch/pull/41174))\r\n* Disable the mkldnn for `conv2d` in some special cases ([#40610](https://github.com/pytorch/pytorch/pull/40610))\r\n* `addmm`: Reduce constant time overhead ([#41374](https://github.com/pytorch/pytorch/pull/41374))\r\n* `cumsum, cumprod`: Enable non-synchronizing cub scan for cum* operations ([#42036](https://github.com/pytorch/pytorch/pull/42036))\r\n* `max_pool2d`: CUDA NCHW performance improvement ([#42182](https://github.com/pytorch/pytorch/pull/42182))\r\n* `arenge`: Vectorize CPU implementation ([#38697](https://github.com/pytorch/pytorch/pull/38697))\r\n* `istft`: optimize by using col2im ([#42826](https://github.com/pytorch/pytorch/pull/42826))\r\n* `LayerNorm`:  improved performance on CPU both forward and backward ([#35750](https://github.com/pytorch/pytorch/pull/35750))\r\n* `silu`: improved performance ([#42976](https://github.com/pytorch/pytorch/pull/42976))\r\n* `addmv`: improved performance for zero sized input cases ([#41824](https://github.com/pytorch/pytorch/pull/41824))\r\n* Mobile: Simple caching allocator for CPU ([#42006](https://github.com/pytorch/pytorch/pull/42006))\r\n* `MaxPool1d`: improved performance for cases without indices ([#43745](https://github.com/pytorch/pytorch/pull/43745))\r\n* `adaptive_avg_pool2d:` optimized code path for cases when output size is (1, 1) ([#44211](https://github.com/pytorch/pytorch/pull/44211))\r\n* Vectorized complex copy ([#44722](https://github.com/pytorch/pytorch/pull/44722))\r\n* `cat`: optimized cuda kernel ([#44833](https://github.com/pytorch/pytorch/pull/44833))\r\n* Vectorized int8_t on CPU ([#44759](https://github.com/pytorch/pytorch/pull/44759))\r\n* Vectorized `bitwise_not` ([#45103](https://github.com/pytorch/pytorch/pull/45103))\r\n* Added stateful XNNPack deconvolution2d operator to torch ([#43233](https://github.com/pytorch/pytorch/pull/43233))\r\n* Enabled mkldnn dilation convolution ([#40483](https://github.com/pytorch/pytorch/pull/40483))\r\n\r\n### Distributed\r\n\r\n* Skip allreducing `local_used_maps_dev_` when `find_unused_param=False` in DDP to improve performance ([#40407](https://github.com/pytorch/pytorch/pull/40407))\r\n* Remove unnecessary copies in ProcessGroupGloo for multiple inputs allreduce ([#43543](https://github.com/pytorch/pytorch/pull/43543))\r\n* Add option to run NCCL operations on high priority cuda stream ([#43796](https://github.com/pytorch/pytorch/pull/43796))\r\n* Enhance DistributedOptimizer to be functional and torchscriptable to avoid GIL and global lock ([#45221](https://github.com/pytorch/pytorch/pull/45221))\r\n\r\n### TorchScript\r\n\r\n* JIT pass for add relu fusion. ([#39343](https://github.com/pytorch/pytorch/pull/39343))\r\n* Optimize autodiff subgraph slicing ([#41437](https://github.com/pytorch/pytorch/pull/41437))\r\n* Don't re-run CSE on every block ([#41479](https://github.com/pytorch/pytorch/pull/41479))\r\n* Add loop unroll optimization in NNC ([#42465](https://github.com/pytorch/pytorch/pull/42465))\r\n* Speed up CUDA kernel launch when block/thread extents are statically known ([#42899](https://github.com/pytorch/pytorch/pull/42899))\r\n* Support merging adjacent fusion groups in TensorExpression Fuser. ([#43671](https://github.com/pytorch/pytorch/pull/43671))\r\n* Add passes to profiling executor pipeline ([#43636](https://github.com/pytorch/pytorch/pull/43636))\r\n* Improve performance of `KernelSumMultipleAxes` ([#43905](https://github.com/pytorch/pytorch/pull/43905))\r\n* Latency improvements for pointwise + reduction fusion ([#45218](https://github.com/pytorch/pytorch/pull/45218))\r\n* Add simplification of Loop + Condition patterns in NNC ([#44764](https://github.com/pytorch/pytorch/pull/44764))\r\n* Fix fallback graph in specialize autogradzero ([#44654](https://github.com/pytorch/pytorch/pull/44654))\r\n* Fix masking for all block and thread dimensions in CudaCodeGen ([#44733](https://github.com/pytorch/pytorch/pull/44733))\r\n* Improve performance of simple reduction and softmax in nvFuser ([#40864](https://github.com/pytorch/pytorch/pull/40864))\r\n* Add a new optimization pass, the Registerizer, which looks for common Stores and Loads to a single item in a buffer and replaces them with a local temporary scalar which is cheaper to write. ([#42606](https://github.com/pytorch/pytorch/pull/42606))\r\n* Fuse identical conditions in NNC simplifier ([#44886](https://github.com/pytorch/pytorch/pull/44886))\r\n* Add _out variants and reuse memory in static runtime([#44128](https://github.com/pytorch/pytorch/pull/44128))\r\n\r\n### Mobile\r\n\r\n* Add add_relu fusion pass to optimize_for_mobile. ([#40252](https://github.com/pytorch/pytorch/pull/40252))\r\n* optimize_for_mobile: bring packed params to root module ([#42740](https://github.com/pytorch/pytorch/pull/42740))\r\n* Apply selective build on RNN operators ([#44132](https://github.com/pytorch/pytorch/pull/44132))\r\n* Add neon backend for vectorization ([#39341](https://github.com/pytorch/pytorch/pull/39341))\r\n\r\n### Quantization\r\n\r\n* Use the _min_max function instead of two separate calls for min and max([#41570](https://github.com/pytorch/pytorch/pull/41570), [#42957](https://github.com/pytorch/pytorch/pull/42957), [#44537](https://github.com/pytorch/pytorch/pull/44537))\r\n* Improve performance of the QNNPACK kernels ([#41342](https://github.com/pytorch/pytorch/pull/41342), [#42007](https://github.com/pytorch/pytorch/pull/42007), [#42008](https://github.com/pytorch/pytorch/pull/42008))\r\n* Speed up HistogramObserver by vectorizing critical path ([#41041](https://github.com/pytorch/pytorch/pull/41041))\r\n* Speed up AdaptivePool3d by checking if input is ChannelsLast or ChannelsLast3d ([#42780](https://github.com/pytorch/pytorch/pull/42780))\r\n* observers: use clamp instead of min/max in calculate_qparams ([#43150](https://github.com/pytorch/pytorch/pull/43150))\r\n* observers: use torch.all to check for valid min and max values ([#43151](https://github.com/pytorch/pytorch/pull/43151))\r\n* Avoid resizing in MinMaxObserver ([#43789](https://github.com/pytorch/pytorch/pull/43789))\r\n* observers: make eps a buffer ([#43149](https://github.com/pytorch/pytorch/pull/43149))\r\n\r\n### Misc\r\n\r\n* ROCm: Fix performance issues with `torch.cat` ([#46323](https://github.com/pytorch/pytorch/pull/46323))\r\n\r\n# Documentation\r\n\r\n### Python API\r\n\r\n* Numerous typo and grammatical improvements ([#39854](https://github.com/pytorch/pytorch/pull/39854), [#40217](https://github.com/pytorch/pytorch/pull/40217), [#40285](https://github.com/pytorch/pytorch/pull/40285), [#40544](https://github.com/pytorch/pytorch/pull/40544), [#40692](https://github.com/pytorch/pytorch/pull/40692), [#40617](https://github.com/pytorch/pytorch/pull/40617), [#41025](https://github.com/pytorch/pytorch/pull/41025), [#41031](https://github.com/pytorch/pytorch/pull/41031), [#40984](https://github.com/pytorch/pytorch/pull/40984), [#41066](https://github.com/pytorch/pytorch/pull/41066), [#41203](https://github.com/pytorch/pytorch/pull/41203), [#41263](https://github.com/pytorch/pytorch/pull/41263), [#41384](https://github.com/pytorch/pytorch/pull/41384), [#41526](https://github.com/pytorch/pytorch/pull/41526), [#41563](https://github.com/pytorch/pytorch/pull/41563), [#41632](https://github.com/pytorch/pytorch/pull/41632), [#41643](https://github.com/pytorch/pytorch/pull/41643), [#41599](https://github.com/pytorch/pytorch/pull/41599), [#41799](https://github.com/pytorch/pytorch/pull/41799), [#41679](https://github.com/pytorch/pytorch/pull/41679), [#41835](https://github.com/pytorch/pytorch/pull/41835), [#41851](https://github.com/pytorch/pytorch/pull/41851), [#41963](https://github.com/pytorch/pytorch/pull/41963), [#42016](https://github.com/pytorch/pytorch/pull/42016), [#42076](https://github.com/pytorch/pytorch/pull/42076), [#41946](https://github.com/pytorch/pytorch/pull/41946), [#42046](https://github.com/pytorch/pytorch/pull/42046), [#42065](https://github.com/pytorch/pytorch/pull/42065), [#42236](https://github.com/pytorch/pytorch/pull/42236), [#42184](https://github.com/pytorch/pytorch/pull/42184), [#42734](https://github.com/pytorch/pytorch/pull/42734), [#42923](https://github.com/pytorch/pytorch/pull/42923), [#42891](https://github.com/pytorch/pytorch/pull/42891), [#43063](https://github.com/pytorch/pytorch/pull/43063), [#43131](https://github.com/pytorch/pytorch/pull/43131), [#43395](https://github.com/pytorch/pytorch/pull/43395), [#43588](https://github.com/pytorch/pytorch/pull/43588), [#43583](https://github.com/pytorch/pytorch/pull/43583), [#43697](https://github.com/pytorch/pytorch/pull/43697), [#43779](https://github.com/pytorch/pytorch/pull/43779), [#43569](https://github.com/pytorch/pytorch/pull/43569), [#43893](https://github.com/pytorch/pytorch/pull/43893), [#43695](https://github.com/pytorch/pytorch/pull/43695), [#43973](https://github.com/pytorch/pytorch/pull/43973), [#44667](https://github.com/pytorch/pytorch/pull/44667), [#44753](https://github.com/pytorch/pytorch/pull/44753), [#44740](https://github.com/pytorch/pytorch/pull/44740), [#45045](https://github.com/pytorch/pytorch/pull/45045), [#45192](https://github.com/pytorch/pytorch/pull/45192), [#43308](https://github.com/pytorch/pytorch/pull/43308), [#40334](https://github.com/pytorch/pytorch/pull/40334))\r\n* Remove use of term \u201cblacklist\u201d ([#41450](https://github.com/pytorch/pytorch/pull/41450))\r\n* Add overflow notice for cuFFT on half precision ([#40551](https://github.com/pytorch/pytorch/pull/40551))\r\n* Add complex Note ([#41012](https://github.com/pytorch/pytorch/pull/41012), [#41252](https://github.com/pytorch/pytorch/pull/41252), [#40450](https://github.com/pytorch/pytorch/pull/40450))\r\n* Add documentation about data sharing for Tensors during serialization ([#40412](https://github.com/pytorch/pytorch/pull/40412))\r\n* Add `nn.Module.training` to docs ([#40923](https://github.com/pytorch/pytorch/pull/40923))\r\n*  `nn.CrossEntropyLoss`: Clarify that the mean argument is weighted ([#40991](https://github.com/pytorch/pytorch/pull/40991))\r\n* `torch.scatter_`: Update doc with support for reduction methods. ([#40962](https://github.com/pytorch/pytorch/pull/40962))\r\n* Fix HTTP links in documentation to HTTPS ([#40878](https://github.com/pytorch/pytorch/pull/40878))\r\n* Fix warnings when building docs ([#41068](https://github.com/pytorch/pytorch/pull/41068), [#41334](https://github.com/pytorch/pytorch/pull/41334), [#41335](https://github.com/pytorch/pytorch/pull/41335), [#44686](https://github.com/pytorch/pytorch/pull/44686))\r\n* Add PyTorch Glossary ([#40639](https://github.com/pytorch/pytorch/pull/40639))\r\n* Fix documentation references following page split ([#39086](https://github.com/pytorch/pytorch/pull/39086))\r\n* Update serialization note to explain versioned symbols and dynamic versioning ([#41395](https://github.com/pytorch/pytorch/pull/41395))\r\n* Make elementwise comparison docs more consistent ([#41626](https://github.com/pytorch/pytorch/pull/41626))\r\n* Update CONTRIBUTING.md to explain how to use ccache ([#41619](https://github.com/pytorch/pytorch/pull/41619))\r\n* Add doc warning for LSTM non-deterministic behavior ([#40893](https://github.com/pytorch/pytorch/pull/40893))\r\n* Document default dim for cross being None ([#41850](https://github.com/pytorch/pytorch/pull/41850))\r\n* Clarify Python 3.6 is the minimum supported version in the installation section. ([#41937](https://github.com/pytorch/pytorch/pull/41937))\r\n* Split quantization subsection into smaller pages ([#41321](https://github.com/pytorch/pytorch/pull/41321))\r\n* Documentation for `torch.optim.swa_utils` ([#41228](https://github.com/pytorch/pytorch/pull/41228))\r\n* Improve the documentation of DistributedDataParallel ([#42471](https://github.com/pytorch/pytorch/pull/42471))\r\n* Update docs about CUDA stream priority ([#41364](https://github.com/pytorch/pytorch/pull/41364))\r\n* Update the documentation for `torch.scatter` to include streams parameter. ([#42814](https://github.com/pytorch/pytorch/pull/42814))\r\n* Update `Tensor.clone` doc ([#42931](https://github.com/pytorch/pytorch/pull/42931), [#43098](https://github.com/pytorch/pytorch/pull/43098))\r\n* Update external links in the README.md ([#43100](https://github.com/pytorch/pytorch/pull/43100))\r\n* Update `torch.Tensor.is_set_to` documentation ([#43052](https://github.com/pytorch/pytorch/pull/43052))\r\n* Polish the nightly pull docs in CONTRIBUTING ([#43494](https://github.com/pytorch/pytorch/pull/43494))\r\n* Update the `torch.qr` documentation to include a warning about when the QR.backward is well-defined. ([#43547](https://github.com/pytorch/pytorch/pull/43547))\r\n* Update the instructions to build from source on windows ([#43479](https://github.com/pytorch/pytorch/pull/43479), [#45553](https://github.com/pytorch/pytorch/pull/45553))\r\n* Document the beta=0 behavior of BLAS functions ([#43823](https://github.com/pytorch/pytorch/pull/43823))\r\n* Fix docs for kwargs-only functions ([#43586](https://github.com/pytorch/pytorch/pull/43586), [#43589](https://github.com/pytorch/pytorch/pull/43589))\r\n* Documents `torch.sub` properly, adds `torch.subtract` alias ([#43850](https://github.com/pytorch/pytorch/pull/43850))\r\n* Update determinism documentation ([#41692](https://github.com/pytorch/pytorch/pull/41692))\r\n* Update instructions to build ([#42850](https://github.com/pytorch/pytorch/pull/42850))\r\n* Clarify `nn.Batchnorm` `track_running_stats` docs ([#44445](https://github.com/pytorch/pytorch/pull/44445))\r\n* Fix latex error in `torch.heaviside` docs ([#44481](https://github.com/pytorch/pytorch/pull/44481))\r\n* Update `torch.median` doc to explain returned value for even-sized input ([#44562](https://github.com/pytorch/pytorch/pull/44562))\r\n* Fix the `nn.ELU` formula in the docs ([#43764](https://github.com/pytorch/pytorch/pull/43764))\r\n* `torch.min`, `torch.max`: remove incorrect warning from docs ([#44615](https://github.com/pytorch/pytorch/pull/44615))\r\n* Reference `torch.cuda.amp` tutorial from core amp docs ([#44725](https://github.com/pytorch/pytorch/pull/44725))\r\n* Mention TF32 on related docs ([#44690](https://github.com/pytorch/pytorch/pull/44690))\r\n* Clarify that 5-D 'bilinear' grid_sample is actually trilinear ([#45090](https://github.com/pytorch/pytorch/pull/45090))\r\n* Update linalg warning + docs ([#45415](https://github.com/pytorch/pytorch/pull/45415))\r\n* Update `torch.floor_divide` documentation to clarify it's actually `torch.trunc_divide` ([#45411](https://github.com/pytorch/pytorch/pull/45411))\r\n* Update `torch.fft` doc and make warning clearer ([#45409](https://github.com/pytorch/pytorch/pull/45409))\r\n* Update for complex autograd ([#45270](https://github.com/pytorch/pytorch/pull/45270), [#46281](https://github.com/pytorch/pytorch/pull/46281))\r\n* Update `nn.Flatten` docs ([#42084](https://github.com/pytorch/pytorch/pull/42084))\r\n\r\n### Distributed\r\n\r\n* Add a CONTRIBUTING.md for the distributed package. ([#44224](https://github.com/pytorch/pytorch/pull/44224))\r\n* Added docs for Store API ([#45543](https://github.com/pytorch/pytorch/pull/45543))\r\n* Add `all_gather_object` and `gather_object` documentation ([#43772](https://github.com/pytorch/pytorch/pull/43772))\r\n\r\n### TorchScript\r\n\r\n* Fix `torch.jit.trace_module` documentation ([#40248](https://github.com/pytorch/pytorch/pull/40248))\r\n* Fix the docs for the inputs arg of `torch.jit.trace_module` ([#41586](https://github.com/pytorch/pytorch/pull/41586))\r\n* Add documentation for `PYTORCH_JIT_TYPE_VERBOSITY` ([#42241](https://github.com/pytorch/pytorch/pull/42241))\r\n* Grammatical corrections in JIT overview ([#43473](https://github.com/pytorch/pytorch/pull/43473))\r\n* Update docs for recently added JIT features, including Enum Support, `torch.no_grad` etc. ([#45232](https://github.com/pytorch/pytorch/pull/45232))\r\n* Add function signature for `pixel_shuffle` (#[45661](https://github.com/pytorch/pytorch/pull/45661))\r\n* Fix signature for `torch.poisson` in documentation (#[45656](https://github.com/pytorch/pytorch/pull/45656))\r\n\r\n### Mobile\r\n\r\n* Aar native linking add fbjni ([#40578](https://github.com/pytorch/pytorch/pull/40578))\r\n* fix scripts ([#44464](https://github.com/pytorch/pytorch/pull/44464))\r\n* [PyTorch Mobile] Move some string ops to register_prim_ops.cpp and make them selective ([#44500](https://github.com/pytorch/pytorch/pull/44500))\r\n\r\n### Quantization\r\n\r\n* Fix several quantization documentation typos ([#40567](https://github.com/pytorch/pytorch/pull/40567), [#43693](https://github.com/pytorch/pytorch/pull/43693))\r\n* API summary section ([#45848](https://github.com/pytorch/pytorch/pull/45848))\r\n* Documentation for dynamically quantized RNN cells ([#40896](https://github.com/pytorch/pytorch/pull/40896))\r\n\r\n### Misc\r\n\r\n* Update  ONNX docs for release ([#45086](https://github.com/pytorch/pytorch/pull/45086))", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.7.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.7.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.7.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/33111309", "dateCreated": "2020-10-23T19:31:23Z", "datePublished": "2020-10-27T16:35:58Z"}, {"tagName": "v1.6.0", "name": "Stable release of automatic mixed precision (AMP). New Beta features include a TensorPipe backend for RPC, memory profiler, and several improvements to distributed training for both RPC and DDP.", "authorName": "zou3519", "authorType": "User", "body": "# PyTorch 1.6.0 Release Notes\r\n\r\n* Highlights\r\n* Backwards Incompatible Changes\r\n* Deprecations\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Performance\r\n* Documentation\r\n\r\n\r\n# Highlights\r\n\r\nThe PyTorch 1.6 release includes a number of new APIs, tools for performance improvement and profiling, as well as major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. \r\n \r\nA few of the highlights include: \r\n\r\n1. Automatic mixed precision (AMP) training is now natively supported and a stable feature - thanks to NVIDIA\u2019s contributions; \r\n2. Native TensorPipe support now added for tensor-aware, point-to-point communication primitives built specifically for machine learning; \r\n3. New profiling tools providing tensor-level memory consumption information; and \r\n4. Numerous improvements and new features for both distributed data parallel (DDP) training and the remote procedural call (RPC) packages.\r\n\r\n Additionally, from this release onward, features will be classified as Stable, Beta and Prototype. Prototype features are not included as part of the binary distribution and are instead available through either building from source, using nightlies or via compiler flag. You can learn more about what this change means in the post [here](https://pytorch.org/blog/pytorch-feature-classification-changes/).\r\n\r\n## [Stable] Automatic Mixed Precision (AMP) Training \r\n\r\nAMP allows users to easily enable automatic mixed precision training enabling higher performance and memory savings of up to 50% on Tensor Core GPUs. Using the natively supported `torch.cuda.amp` API, AMP provides convenience methods for mixed precision, where some operations use the `torch.float32` (`float`) datatype and other operations use `torch.float16` (`half`). Some ops, like linear layers and convolutions, are much faster in `float16`. Other ops, like reductions, often require the dynamic range of `float32`. Mixed precision tries to match each op to its appropriate datatype.\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/25081)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/amp.html)\r\n* Usage examples | [Link](https://pytorch.org/docs/stable/notes/amp_examples.html)\r\n\r\n## [Beta] TensorPipe backend for RPC\r\n\r\nPyTorch 1.6 introduces a new backend for the RPC module which leverages the TensorPipe library, a tensor-aware point-to-point communication primitive targeted at machine learning, intended to complement the current primitives for distributed training in PyTorch (Gloo, MPI, ...) which are collective and blocking. The pairwise and asynchronous nature of TensorPipe lends itself to new networking paradigms that go beyond data parallel: client-server approaches (e.g., parameter server for embeddings, actor-learner separation in Impala-style RL, ...) and model and pipeline parallel training (think GPipe), gossip SGD, etc.\r\n\r\n```python\r\n# One-line change needed to opt in\r\ntorch.distributed.rpc.init_rpc(\r\n    ...\r\n    backend=torch.distributed.rpc.BackendType.TENSORPIPE,\r\n)\r\n\r\n# No changes to the rest of the RPC API\r\ntorch.distributed.rpc.rpc_sync(...)\r\n```\r\n\r\n* Design doc | [Link](https://github.com/pytorch/pytorch/issues/35251)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc/index.html)\r\n\r\n## [Beta] Memory Profiler \r\n\r\nThe `torch.autograd.profiler` API now includes a memory profiler that lets you inspect the tensor memory cost of different operators inside your CPU and GPU models.\r\n\r\nHere is an example usage of the API:\r\n\r\n```python\r\nimport torch\r\nimport torchvision.models as models\r\nimport torch.autograd.profiler as profiler\r\n\r\nmodel = models.resnet18()\r\ninputs = torch.randn(5, 3, 224, 224)\r\nwith profiler.profile(profile_memory=True, record_shapes=True) as prof:\r\n    model(inputs)\r\n\r\n# NOTE: some columns were removed for brevity\r\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\r\n# ---------------------------  ---------------  ---------------  ---------------\r\n# Name                         CPU Mem          Self CPU Mem     Number of Calls\r\n# ---------------------------  ---------------  ---------------  ---------------\r\n# empty                        94.79 Mb         94.79 Mb         123\r\n# resize_                      11.48 Mb         11.48 Mb         2\r\n# addmm                        19.53 Kb         19.53 Kb         1\r\n# empty_strided                4 b              4 b              1\r\n# conv2d                       47.37 Mb         0 b              20\r\n# ---------------------------  ---------------  ---------------  ---------------\r\n```\r\n\r\n* PR | [Link](https://github.com/pytorch/pytorch/pull/37775) \r\n* Documentation | [Link](https://pytorch.org/docs/stable/autograd.html#profiler)\r\n\r\n## Distributed and RPC Features and Improvements\r\n\r\n### [Beta] DDP+RPC \r\n\r\nPyTorch Distributed supports two powerful paradigms: DDP for full sync data parallel training of models and the RPC framework which allows for distributed model parallelism. Currently, these two features work independently and users can\u2019t mix and match these to try out hybrid parallelism paradigms.\r\n\r\nStarting PyTorch 1.6, we\u2019ve enabled DDP and RPC to work together seamlessly so that users can combine these two techniques to achieve both data parallelism and model parallelism. An example is where users would like to place large embedding tables on parameter servers and use the RPC framework for embedding lookups, but store smaller dense parameters on trainers and use DDP to synchronize the dense parameters. Below is a simple code snippet. \r\n\r\n```python\r\n// On each trainer\r\n\r\nremote_emb = create_emb(on=\"ps\", ...)\r\nddp_model = DDP(dense_model)\r\n\r\nfor data in batch:\r\n   with torch.distributed.autograd.context():\r\n      res = remote_emb(data)\r\n      loss = ddp_model(res)\r\n      torch.distributed.autograd.backward([loss])\r\n```\r\n\r\n* DDP+RPC Tutorial | [Link](https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc/index.html)\r\n* Usage Examples | [Link](https://github.com/pytorch/examples/tree/master/distributed/rpc/ddp_rpc)\r\n\r\n### **[Beta] RPC - Asynchronous User Functions**\r\n\r\nRPC Asynchronous User Functions supports the ability to yield and resume on the server side when executing a user-defined function. Prior to this feature, when an callee processes a request, one RPC thread waits until the user function returns. If the user function contains IO (e.g., nested RPC) or signaling (e.g., waiting for another request to unblock), the corresponding RPC thread would sit idle waiting for these events. As a result, some applications have to use a very large number of threads and send additional RPC requests, which can potentially lead to performance degradation. To make a user function yield on such events, applications need to: 1) Decorate the function with the `@rpc.functions.async_execution` decorator; and 2) Let the function return a `torch.futures.Future` and install the resume logic as callbacks on the `Future` object. See below for an example:\r\n\r\n```python\r\n@rpc.functions.async_execution\r\ndef async_add_chained(to, x, y, z):\r\n    return rpc.rpc_async(to, torch.add, args=(x, y)).then(\r\n        lambda fut: fut.wait() + z\r\n    )\r\n\r\nret = rpc.rpc_sync(\r\n    \"worker1\", \r\n    async_add_chained, \r\n    args=(\"worker2\", torch.ones(2), 1, 1)\r\n)\r\n        \r\nprint(ret)  # prints tensor([3., 3.])\r\n```\r\n\r\n* Tutorial for performant batch RPC using Asynchronous User Functions| [Link](https://pytorch.org/tutorials/intermediate/rpc_async_execution.html)\r\n* Documentation | [Link](https://pytorch.org/docs/stable/rpc.html#torch.distributed.rpc.functions.async_execution)\r\n* Usage examples | [Link](https://github.com/pytorch/examples/tree/master/distributed/rpc/batch)\r\n\r\n## [Beta] Fork/Join Parallelism \r\n\r\nThis release adds support for a language-level construct as well as runtime support for coarse-grained parallelism in TorchScript code. This support is useful for situations such as running models in an ensemble in parallel, or running bidirectional components of recurrent nets in parallel, and allows the ability to unlock the computational power of parallel architectures (e.g. many-core CPUs) for task level parallelism.\r\n\r\n Parallel execution of TorchScript programs is enabled through two primitives: `torch.jit.fork` and `torch.jit.wait`. In the below example, we parallelize execution of `foo:`\r\n\r\n```python\r\nimport torch\r\nfrom typing import List\r\n\r\ndef foo(x):\r\n    return torch.neg(x)\r\n\r\n@torch.jit.script\r\ndef example(x):\r\n    futures = [torch.jit.fork(foo, x) for _ in range(100)]\r\n    results = [torch.jit.wait(future) for future in futures]\r\n    return torch.sum(torch.stack(results))\r\n\r\nprint(example(torch.ones([])))\r\n```\r\n\r\n* Documentation | [Link](https://pytorch.org/docs/stable/jit.html)\r\n\r\n# Backwards Incompatible Changes\r\n\r\n### Dropped support for Python <= 3.5 ([#39879](https://github.com/pytorch/pytorch/pull/39879))\r\n\r\nThe minimum version of Python we support now is 3.6. Please upgrade your Python to match. If you use conda, instructions for setting up a new environment with Python >= 3.6 can be [found here](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-python.html#installing-a-different-version-of-python).\r\n\r\n\r\n### Throw a RuntimeError for deprecated `torch.div` and `torch.addcdiv` integer floor division behavior ([#38762](https://github.com/pytorch/pytorch/pull/38762), [#38620](https://github.com/pytorch/pytorch/pull/38620))\r\n\r\nIn 1.5.1 and older PyTorch releases `torch.div` , `torch.addcdiv`, and the `/` operator perform integer floor division. In 1.6 attempting to perform integer division throw a RuntimeError, and in 1.7 the behavior will change so that these operations always perform true division (consistent with Python and NumPy division).\r\n\r\nTo floor divide integer tensors, please use `torch.floor_divide` instead.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.5.1</th><th>1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(3) / torch.tensor(2)\r\n../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer\r\ndivision of tensors using div or / is deprecated, and in a future\r\nrelease div will perform true division as in Python 3. Use true_divide\r\nor floor_divide (// in Python) instead.\r\ntensor(1)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> # NB: the following is equivalent to \r\n>>> # torch.floor_divide(torch.tensor(3), torch.tensor(2))\r\n>>> torch.tensor(3) // torch.tensor(2)\r\ntensor(1)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThe fix for `torch.addcdiv` is similar.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>1.5.1</th><th>1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> torch.addcdiv(input, tensor, other, value=value)\r\n../aten/src/ATen/native/PointwiseOps.cpp:81: UserWarning:\r\nInteger division with addcdiv is deprecated, and in a future \r\nrelease addcdiv will perform a true division of tensor1 and\r\ntensor2. The current addcdiv behavior can be replicated using\r\nfloor_divide for integral inputs (self + value * tensor1 // tensor2)\r\nand division for float inputs (self + value * tensor1 / tensor2).\r\nThe new addcdiv behavior can be implemented with\r\ntrue_divide (self + value * torch.true_divide(tensor1, tensor2).\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> (input + torch.floor_divide(value * tensor, other))\r\ntensor(0)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Prevent cross-device data movement for zero-dimension CUDA tensors in binary pointwise PyTorch operators ([#38998](https://github.com/pytorch/pytorch/pull/38998))\r\n\r\nIn previous versions of PyTorch, zero dimensional CUDA tensors could be moved across devices implicitly while performing binary pointwise operations (e.g. addition, subtraction, multiplication, division, and others). For example,\r\n\r\n```python\r\ntorch.tensor(5, device='cuda:0') + torch.tensor((1, 1), device='cuda:1')\r\n```\r\n\r\nwould work, even though the tensors are on different CUDA devices. This is a frequent source of user confusion, however, and PyTorch generally does not move data across devices without it being explicit. This functionality is removed in PyTorch 1.6.\r\n\r\nTo perform binary pointwise operations on data of different devices, please cast the tensors to the correct device by using `Tensor.to`: \r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5, device='cuda:0') + torch.tensor((1, 1), device='cuda:1')\r\ntorch.tensor([6, 6], device='cuda:1')\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5, device='cuda:0').to('cuda:1') + torch.tensor((1, 1), device='cuda:1')\r\ntorch.tensor([6, 6], device='cuda:1')\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### Dropped support for CUDA 9.2 on Windows\r\n\r\nIn previous versions of PyTorch, we provided an installation option for Windows environments running CUDA 9.2. Starting from PyTorch 1.6.0, we are no longer providing those binaries. Please upgrade your CUDA version to 10.1 or 10.2 and install a PyTorch binary for one of those CUDA versions instead.\r\n\r\n\r\n### PyTorch release binaries dropped dedicated bytecode for CUDA compute capability 6.1; removed PTX for CUDA compute capability 3.7\r\n\r\nTo check whether you are affected, please find your GPU in a table in[this link](https://developer.nvidia.com/cuda-gpus).\r\n\r\nIf you are using a Nvidia GPU with compute capability 6.1, you may notice a performance hit when using the release binaries (installed via pip or conda). We stopped building for CUDA compute capability 6.1 but PyTorch programs should still continue to work with those devices. If you do notice a performance hit, a workaround is to [compile PyTorch from source](https://github.com/pytorch/pytorch#from-source).\r\n\r\nIf you are using a Nvidia GPU with compute capability 3.7 and relied on PTX, we have dropped support for that in our release binaries (installed via pip or conda). Potential workarounds are: install a previous version of PyTorch or to [compile PyTorch from source](https://github.com/pytorch/pytorch#from-source).\r\n\r\n\r\n### Changed how bool tensors are constructed from non-bool values to match Python, C++, and NumPy ([#38392](https://github.com/pytorch/pytorch/pull/38392))\r\n\r\nIn previous versions of PyTorch, when a bool tensor is constructed from a floating-point tensor, we would first convert the tensor to a long tensor, then to float tensor. This is not consistent with how bools are interpreted in Python, C++, and NumPy (just to name a few), which interpret 0 floating-point values as False and everything else as True.\r\n\r\nIf you were relying on the previous behavior, the following code will achieve the same effect.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor([-2, -1, -0.9, 0, 0.9, 1, 2], dtype=torch.bool)\r\ntensor([ True,  True, False, False, False,  True,  True])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor([-2, -1, -0.9, 0, 0.9, 1, 2]).long().bool()\r\ntensor([ True,  True, False, False, False,  True,  True])\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Throw RuntimeError when torch.full would infer a float dtype from a bool or integral fill value (#40364)\r\n\r\nIn PyTorch 1.6 bool and integral fill values given to torch.full must set the dtype our out keyword arguments. In prior versions of PyTorch these fill values would return float tensors by default, but in PyTorch 1.7 they will return a bool or long tensor, respectively. The documentation for torch.full has been updated to reflect this.\r\n\r\n\r\n### Enabled thread parallelism for autograd on CPU ([#33157](https://github.com/pytorch/pytorch/pull/33157))\r\n\r\nIn previous versions of PyTorch, running `.backward()` in multiple threads causes them to be serialized in a specific order, resulting in no parallelism on CPU. In PyTorch 1.6.0, running `.backward()` in multiple threads no longer serializes the execution and instead autograd will run those in parallel.\r\n\r\nThis is BC-breaking for the following two use cases:\r\n\r\n* If any weights are shared among threads, gradient accumulation that was previously deterministic may become non-deterministic in 1.6 as two different threads will write to the .grad attribute in a non-deterministic order.\r\n* If you use any C++ hooks, those are not guaranteed to be thread-safe. Please change them to be thread-safe.\r\n\r\n\r\nIn more detail, in 1.6.0, when you run `backward()` or `grad()` via python, TorchScript or the C++ API in multiple threads on CPU, you should expect to see extra concurrency. For example, you can manually write multithreaded Hogwild training code like:\r\n\r\n```python\r\n# Define a train function to be used in different threads\r\ndef train_fn(model, input):\r\n    # forward\r\n    y = model(input)\r\n    # backward\r\n    y.sum().backward()\r\n    # potential optimizer update\r\n\r\n# define your model in python or in TorchScript\r\nmodel = Model()\r\n# User write their own threading code to drive the train_fn\r\nthreads = []\r\nfor _ in range(10):\r\n    # define or load the data\r\n    input = torch.ones(5, 5, requires_grad=True)\r\n    p = threading.Thread(target=train_fn, args=(model, input))\r\n    p.start()\r\n    threads.append(p)\r\n\r\nfor p in threads:\r\n    p.join()\r\n```\r\n\r\n\r\nNote when you use the same model and call `backward()` concurrently in multiple threads, model parameters are automatically shared across threads. The gradient accumulation might become non-deterministic as two backward calls might access and try to accumulate the same .grad attribute. Although we do proper locking to avoid data corruption, we don't guarantee the order in which the ops are executed, so non-determinism might arise, but this is an expected pattern in multithread training. You could use the functional API `torch.autograd.grad()` to calculate the gradients instead of backward() to avoid the non-determinism.\r\n\r\nFor thread safety:\r\n\r\n* The custom Python/C++ Autograd Functions (both forward and backward) are properly protected and are guaranteed to be thread safe in 1.6.0.\r\n* For hooks, both Python/C++ hooks will run concurrently. Note that in C++, just like in regular C++ threading, you will need to do proper locking when writing shared objects, so previous custom C++ hooks might not work nicely under a  multithreaded environment in 1.6.0. In Python, just like in regular python threading, you can read/write objects safely but the order (and thus determinism) is not guaranteed.\r\n\r\n### Change autograd gradient accumulation logic to yield `.grad`s that match the weights' memory layout ([#40358](https://github.com/pytorch/pytorch/pull/40358))\r\n\r\nIn previous versions of PyTorch, autograd would yield contiguous gradients. Now, gradients have the same memory layout as their respective weights. This should result in silent performance improvements. Since PyTorch operators generally support non-contiguous tensors, this should have no functional effect on most PyTorch programs.  A known exception is when accessing `param.grad` and performing an operation that requires a contiguous tensor, such as `param.grad.view(-1)`. In this case, you will receive an error as follows:\r\n`RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead`.\r\n\r\nIf a user wants to force accumulation into a grad with a particular layout, they can preset `param.grad` to a zeroed tensor with the desired strides or manually set grad to have the desired strides ( `param.grad = param.grad.contiguous(desired format)`.)\r\n\r\nSee the below section on \u201cNote: BC-breaking memory format changes\u201d for more details.\r\n\r\n\r\n### Change memory format promotion rules of pointwise operators ([#37968](https://github.com/pytorch/pytorch/pull/37968))\r\n\r\nIn previous versions of PyTorch, performing a binary pointwise operation between a Contiguous and a Channels Last tensor produced a Channels Last. In PyTorch 1.6, this now returns a tensor with the layout of the first operand.\r\n\r\nSee the below section on\u201cNote: BC-breaking memory format changes\u201d for more details.\r\n\r\n\r\n### Note: BC-breaking memory format changes\r\n\r\nOperations that now return tensors in a different memory format generally should have no functional effect on most PyTorch programs because PyTorch operators generally support non-contiguous tensors.\r\n\r\nThe most common incompatibility with Python programs is with the `view` operator, which has specific stride requirements. If these requirements are no longer met as a result of this change, you will get an error message indicating that you should use `reshape` instead, i.e. \"RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\"\r\n\r\nAnother possible exception incompatibility is if you have a (usually) C++ operator implementation that works directly on memory (i.e. calls data_ptr and relies on the strides being contiguous).\r\n\r\n\r\n### `nn.functional.interpolate`: `recompute_scale_factor` default behavior changed from `True` to `False` (#39453)\r\n\r\nIn PyTorch 1.5.1 and older versions, `nn.functional.interpolate(input, size, scale_factor, ..., recompute_scale_factor)` has a default of `recompute_scale_factor = True`. In PyTorch 1.6, we\u2019ve changed the default to `recompute_scale_factor = False`. \r\n\r\nDepending on the precision of the `scale_factor`, this may result in an output tensor with different values than before. To retain the old behavior, simply change your code to use `recompute_scale_factor = True`.\r\n\r\nMore concretely, what `recompute_scale_factor = True` means is, if the user passes in a `scale_factor:`\r\n\r\n1. We will first compute the new output size; and\r\n2. Then, we will compute a new `scale_factor` by dividing the output size by the input size and sending it to an internal helper function.\r\n3. The new `scale_factor` is used in the interpolate computation but in some cases is different from the `scale_factor` the user passed in.\r\n\r\nThis behavior resulted in loss of precision so we deprecated it in PyTorch 1.5.0. In PyTorch 1.6 and onward, `recompute_scale_factor` has a default of `False`, which means that we pass it directly to an internal helper function.\r\n\r\n\r\n### `out=` arguments of pointwise and reduction functions no longer participate in type promotion (#39655)\r\n\r\nIn PyTorch 1.5 passing the out= kwarg to some functions, like torch.add, could affect the computation. That is,\r\n\r\n```python\r\nout = torch.add(a, b)\r\n```\r\n\r\ncould produce a different result than\r\n\r\n```python\r\ntorch.add(a, b, out=out)\r\n```\r\n\r\nThis is because previously the out argument participated in the type promotion rules. For greater consistency with NumPy, Python, and C++, in PyTorch 1.6 the out argument no longer participates in type promotion, and has no effect on the computation performed.\r\n\r\n\r\n### Changed `torch.quasirandom.SobolEngine(..., scramble=True, seed=None)` to respect `torch.manual_seed` when a seed has not been provided ([#36427](https://github.com/pytorch/pytorch/pull/36427))\r\n\r\nIn previous versions of PyTorch, `SobolEngine(..., scramble=True, seed=None)` did not respect any calls to `torch.manual_seed`. The expected behavior for random number generation functions is to respect the seed set by `torch.manual_seed`, so we\u2019ve changed `SobolEngine` to match.\r\n\r\nIf you were relying on the old behavior where `SobolEngine` ignores `torch.manual_seed`, please explicitly pass a different seed to `SobolEngine`:\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.manual_seed(1337)\r\n# SobolEngine ignores the manual_seed and instead uses its own.\r\n>>> `x1 = SobolEngine(dimension=1, scramble=True, seed=None).draw(3)`\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> import time\r\n>>> torch.manual_seed(1337)\r\n# To replicate the old behavior of, pass a seed to SobolEngine.\r\n>>> ms_since_epoch = int(round(time.now() * 1000))\r\n>>> x1 = SobolEngine(dimension=1, scramble=True, seed=ms_since_epoch).draw(3)\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### `Tensor.random_(to, from)`: Enforce check that `from` and `to` are within the bounds of the Tensor\u2019s `dtype` ([#37507](https://github.com/pytorch/pytorch/pull/37507))\r\n\r\nIn previous versions of PyTorch, `to` and `from` did not have to be within the bounds of the tensor\u2019s `dtype` (this raised a warning). The behavior of `random_` in that case can be unexpected. We are making this a hard error starting from PyTorch 1.6.0; please modify your code if you run into the error.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.zeros(10, dtype=torch.uint8)\r\n# 256 is the maximum value for `to` for `torch.uint8`\r\n>>> tensor.random_(0, 257)\r\nUserWarning: to - 1 is out of bounds for unsigned char.\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.zeros(10, dtype=torch.uint8)\r\n# 256 is the maximum value for `to` for `torch.uint8`\r\n>>> tensor.random_(0, 256)\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### Dropped support for CUDA < 9.2 from for source builds (#38977, #36846)\r\n\r\nIf you build PyTorch from source, we\u2019ve dropped support for using CUDA < 9.2 (run `nvcc --version` to check your CUDA version). Users who install PyTorch packages via conda and/or pip are unaffected.\r\n\r\n\r\n### `DataLoader`\u2019s `__len__` changed to return number of batches when holding an `IterableDataset` ([#38925](https://github.com/pytorch/pytorch/pull/38925)) \r\n\r\nIn previous versions of PyTorch, `len(<instance of dataloader holding an IterableDataset>)` would return the number of examples in the dataset. We\u2019ve changed it to be the number of batches (e.g., the number of examples divided by the DataLoader\u2019s `batch_size`) to be consistent with the computation of length when the DataLoader has a BatchedSampler.\r\n\r\n\r\n### `torch.backends.cudnn.flags`: deleted unused `verbose` flag ([#39228](https://github.com/pytorch/pytorch/pull/39228))\r\n\r\nThe verbose flag did nothing, so we deleted it. If you were passing a value to `flags` for `verbose`, please remove it.\r\n\r\n\r\n## RPC\r\n\r\n`RpcBackendOptions` takes `float` instead of `timedelta` for `timeout` argument to stay consistent with `timeout` types in other TorchScriptable RPC APIs.\r\n\r\n\r\n```python\r\n# v1.5\r\nrpc.init_rpc(\r\n    \"worker1\",\r\n    rank=0,\r\n    world_size=2,\r\n    rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(\r\n        num_send_recv_threads=16,\r\n        datetime.timedelta(seconds=20)\r\n    )\r\n)\r\n```\r\n\r\n```python\r\n# v1.6\r\nrpc.init_rpc(\r\n    \"worker1\",\r\n    rank=0,\r\n    world_size=2,\r\n    rpc_backend_options=rpc.ProcessGroupRpcBackendOptions(\r\n        num_send_recv_threads=16,\r\n        20 # seconds\r\n    )\r\n)\r\n```\r\n\r\n## TorchScript\r\n\r\n### The Default Executor Is Rolled Back To Legacy ([#41017](https://github.com/pytorch/pytorch/pull/41017))\r\n\r\nWe rolled back to the old fuser and the legacy executor in this release in order to recover some reported performance regressions. In future releases we plan to reach the same or better performance with a new redesigned executor and fuser.\r\n\r\nIn order to switch back to the executor used in the 1.5 release one could use the following API:\r\n\r\n* in Python: call `torch._C._jit_set_profiling_executor(True)` before you call your model for the first time,\r\n* in C++: include `#include <torch/csrc/jit/runtime/graph_executor.h>` and set `getExecutorMode() = true` before you invoke your model for the first time.\r\n\r\n### Added dynamic versioning ([#40279](https://github.com/pytorch/pytorch/pull/40279))  \r\n\r\nNote: this isn\u2019t actually BC-breaking but we are listing it here because it is BC-Improving.\r\n\r\nThe PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch\u2019s release notes, and modules relying on functionality that has changed may need to be updated to continue working properly.\r\n\r\nIn this release, the historic behavior of `torch.div` and `torch.full` is preserved for models saved via `torch.jit.save` in previous versions of PyTorch.  Modules saved with the current version of PyTorch will use the latest `torch.div` and `torch.full` behavior.  See the notes above for the BC changes to those operators.\r\n\r\n## Internals\r\n\r\nThe following are a list of BC-breaking changes to some of PyTorch\u2019s internal components.\r\n\r\n### Dispatcher C++ API has had some spring cleaning. This is still considered an \u201cinternal\u201d API, but it is becoming more public facing as it stabilizes.\r\n\r\n* Renamed callUnboxed() to call() in Dispatcher, OperatorHandle, KernelFunction ([#37999](https://github.com/pytorch/pytorch/pull/37999))\r\n* The TensorId suffix has been removed from most DispatchKey enum entries ([#36240](https://github.com/pytorch/pytorch/pull/36240))\r\n* Removed ::callOp(); use Dispatcher::call instead (renamed in [#37797](https://github.com/pytorch/pytorch/pull/37797), removed in [#38351](https://github.com/pytorch/pytorch/pull/38351), [#38742](https://github.com/pytorch/pytorch/pull/38742))\r\n* Removed `KernelFunction::makeFromUnboxedFunctorFactory`; use makeFromUnboxedFunctor directly instead ([#35488](https://github.com/pytorch/pytorch/pull/35488))\r\n* Renamed boxing/unboxing files and utilities in ATen/core/boxing ([#35411](https://github.com/pytorch/pytorch/pull/35411))\r\n\r\n\r\n\r\n### `autograd.gradcheck` and `autograd.gradgradcheck`: Added a new default-true argument `check_undefined_grad` ([#39400](https://github.com/pytorch/pytorch/pull/39400))\r\n\r\nInternally, in the autograd engine, we use a special undefined Tensor value to represent zero-filled gradients and expect backward functions and user-defined `torch.autograd.Function`s to gracefully handle those values. When `check_undefined_grad` is True (the default for PyTorch 1.6+), `gradcheck/gradgradcheck` test that the operation in question supports undefined output gradients. This may cause a previously succeeding `gradcheck` to fail. \r\n\r\nYou can turn the check off by setting `check_undefined_grad` to False. As long as autograd does not error out due to an undefined gradient in your model, then everything should be fine.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.1</th><th>Version 1.6.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.autograd.gradcheck(my_custom_function, inputs)\r\nTrue\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> # To keep the previous behavior\r\n>>> torch.autograd.gradcheck(my_custom_function, inputs, check_undefined_grad=False)\r\nTrue\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### [C++ API] Changed the TensorIterator API ([#39803](https://github.com/pytorch/pytorch/pull/39803))\r\n\r\n TensorIterator is an implementation detail for writing kernels that is exposed in our C++ API. We\u2019ve modified how developers interact with TensorIterator, please see the Pull Request for more details.\r\n\r\n\r\n### Removed `torch._min` and `torch._max`([#38440](https://github.com/pytorch/pytorch/pull/38440))\r\n\r\n`torch._min` and `torch._max` are undocumented and were intended to be an implementation detail; we expect very few users, if any at all, to be using it. We\u2019ve deleted it in PyTorch 1.6.0. Please use `torch.min/torch.max` instead if you are using `torch._min/torch._max`.\r\n\r\n\r\n# Deprecations\r\n\r\n### Deprecated old `torch.save` serialization format ([#39460](https://github.com/pytorch/pytorch/pull/39460), [#39893](https://github.com/pytorch/pytorch/pull/39893), [#40288](https://github.com/pytorch/pytorch/pull/40288), #40793)\r\n\r\nWe have switched `torch.save` to use a zip file-based format by default rather than the old Pickle-based format. `torch.load` has retained the ability to load the old format, but use of the new format is recommended. The new format is:\r\n\r\n* more friendly for inspection and building tooling for manipulating the save files\r\n* fixes a long-standing issue wherein serialization (`__getstate__`, `__setstate__`) functions on `Modules` that depended on serialized `Tensor` values were getting the wrong data\r\n* the same as the TorchScript serialization format, making serialization more consistent across PyTorch\r\n\r\nUsage is as follows:\r\n\r\n```python\r\nm = MyMod()\r\ntorch.save(m.state_dict(), 'mymod.pt') # Saves a zipfile to mymod.pt\r\n```\r\n\r\nTo use the old format, pass the flag `_use_new_zipfile_serialization=False`\r\n\r\n```python\r\nm = MyMod()\r\ntorch.save(m.state_dict(), 'mymod.pt', _use_new_zipfile_serialization=False) # Saves pickle\r\n```\r\n\r\n### Fixed missing deprecation warning for Tensor.nonzero() ([#40187](https://github.com/pytorch/pytorch/pull/40187))\r\n\r\nCalling `torch.nonzero(tensor, as_tuple=False)` with one argument or `Tensor.nonzero(as_tuple=False)` with no arguments is deprecated and will be removed in a future version of PyTorch. Please specify the `as_tuple` argument.\r\n\r\n# New Features\r\n\r\n### Python API\r\n\r\nNew Utilities\r\n\r\n* Added global hooks to `torch.nn.Module` ([#38972](https://github.com/pytorch/pytorch/pull/38972))\r\n* Added option to enable cpp stack traces with `TORCH_SHOW_CPP_STACKTRACES=1` ([#38127](https://github.com/pytorch/pytorch/pull/38127))\r\n* Added `torch.utils.show_pickle` for showing pickle contents in saved models ([#35168](https://github.com/pytorch/pytorch/pull/35168))\r\n\r\n\r\nNew Operators\r\n\r\n* `torch.logcumsumexp` added ([#36308](https://github.com/pytorch/pytorch/pull/36308))\r\n* `torch.logaddexp` added ([#38384](https://github.com/pytorch/pytorch/pull/38384))\r\n* `torch.rad2deg`, `torch.deg2rad` added ([#38852](https://github.com/pytorch/pytorch/pull/38852))\r\n* `torch.arccosh`, `torch.arcsinh`, `torch.arctanh` added ([#38388](https://github.com/pytorch/pytorch/pull/38388))\r\n* `torch.flip{lr, ud}` added ([#38599](https://github.com/pytorch/pytorch/pull/38599))\r\n* `torch.bucketize`, `torch.searchsorted` added ([#34577](https://github.com/pytorch/pytorch/pull/34577))\r\n* `torch.istft` (Inverse Short Time Fourier Transform) added ([#35569](https://github.com/pytorch/pytorch/pull/35569))\r\n* `torch.vander`: added support for generating Vandermonde matrices ([#36725](https://github.com/pytorch/pytorch/pull/36725))\r\n* `torch.block_diag` added ([#33449](https://github.com/pytorch/pytorch/pull/33449))\r\n* `nn.Hardswish`, `nn.functional.hardswish` added ([#34747](https://github.com/pytorch/pytorch/pull/34747))\r\n* `torch.nn.init.trunc_normal_` (truncated normal initializer) added ([#32397](https://github.com/pytorch/pytorch/pull/32397))\r\n* Added Stochastic Weight Averaging. See `torch.optim.AveragedModel` and `torch.optim.SWALR` for more details.([#35032](https://github.com/pytorch/pytorch/pull/35032))\r\n\r\n### C++ API\r\n\r\n* Added Optimizer `AdamW` to C++ frontend ([#40009](https://github.com/pytorch/pytorch/pull/40009))\r\n* Custom C++ autograd function now supports c10::optional<Tensor> as parameters ([#37700](https://github.com/pytorch/pytorch/pull/37700))\r\n* torch::Tensor now supports bitwise NOT(!), AND(&), OR(|), XOR(^) operators ([#38691](https://github.com/pytorch/pytorch/pull/38691))\r\n* Cpp extension now supports load and `load_inline` under ROCm ([#35897](https://github.com/pytorch/pytorch/pull/35897))\r\n\r\n\r\n\r\n### [Beta] Complex Tensor support\r\n\r\nThe PyTorch 1.6 release brings beta-level support for complex tensors.  The UX is similar to existing PyTorch tensors and the new complex-specific functionality is compatible with NumPy\u2019s complex arrays.  In particular, you\u2019ll be able to create and manipulate complex tensors, interop with previously existing code that represented complex tensors as tensors of size `(..., 2)`, and more.\r\n\r\nWhile this is an early version of this feature, and we expect it to improve over time, the overall goal is provide a NumPy compatible user experience that leverages PyTorch\u2019s ability to run on accelerators and work with autograd to better support the scientific computing and ML communities.\r\n\r\nPlease find the full documentation [here](https://pytorch.org/docs/master/complex_numbers.html?highlight=complex).\r\n\r\n**Python API:**\r\n\r\n* Added `torch.is_signed() `for complex tensors. ([#33773](https://github.com/pytorch/pytorch/pull/33773))\r\n* Added dtype inference for complex tensors. ([#33713](https://github.com/pytorch/pytorch/pull/33713))\r\n* Added `torch.randn` and `torch.normal_` for complex tensors. ([#34037](https://github.com/pytorch/pytorch/pull/34037), [#35056](https://github.com/pytorch/pytorch/pull/35056))\r\n* Added complex type inference for  `torch.full`. ([#34709](https://github.com/pytorch/pytorch/pull/34709))\r\n* Added type promotion logic for complex numbers. ([#34093](https://github.com/pytorch/pytorch/pull/34093))\r\n* Added `is_complex` tensor attribute for complex numbers. ([#34093](https://github.com/pytorch/pytorch/pull/34093))\r\n* Added torch.fill for complex tensors. ([#34973](https://github.com/pytorch/pytorch/pull/34973))\r\n* Added `torch.rand` for complex dtypes. ([#34924](https://github.com/pytorch/pytorch/pull/34924), [#35585](https://github.com/pytorch/pytorch/pull/35585))\r\n* Fixed complex conversions, used in `torch.copy_` , on cuda. ([#35344](https://github.com/pytorch/pytorch/pull/35344))\r\n* Added `torch.from_numpy` for complex dtypes. ([#35531](https://github.com/pytorch/pytorch/pull/35531))\r\n* Added a check to throw error for in place modification of non-complex tensors with complex number values. ([#35883](https://github.com/pytorch/pytorch/pull/35883))\r\n* Fixed `torch.exp` CPU implementation for complex tensors. ([#35715](https://github.com/pytorch/pytorch/pull/35715))\r\n* Added `torch.masked_fill` for complex tensors. ([#36335](https://github.com/pytorch/pytorch/pull/36335))\r\n* Updated `torch.abs` to return float tensors for complex tensors. ([#35871](https://github.com/pytorch/pytorch/pull/35871))\r\n* Added `torch.isfinite` and `torch.isinf` for complex tensors. ([#36648](https://github.com/pytorch/pytorch/pull/36648))\r\n* Added `torch.isclose` for complex tensors. ([#36456](https://github.com/pytorch/pytorch/pull/36456))\r\n* Updated `torch.angle` to return float tensors for complex tensors. ([#36896](https://github.com/pytorch/pytorch/pull/36896))\r\n* Enabled `requires_grad` for complex tensors. ([#36932](https://github.com/pytorch/pytorch/pull/36932))\r\n* Fixed reciprocal divide for complex tensors. ([#37193](https://github.com/pytorch/pytorch/pull/37193))\r\n* Added `torch.reciprocal` for complex tensors on CUDA. ([#36749](https://github.com/pytorch/pytorch/pull/36749))\r\n* Added Python API for `Complex Storage`. ([#35771](https://github.com/pytorch/pytorch/pull/35771))\r\n* Added `torch.addmv` for complex tensors. ([#37924](https://github.com/pytorch/pytorch/pull/37924), [#40238](https://github.com/pytorch/pytorch/pull/40238))\r\n* Updated dtype inference for `torch.tensor` . ([#38030](https://github.com/pytorch/pytorch/pull/38030))\r\n* Added `torch.pow` for complex tensors on CUDA. ([#36793](https://github.com/pytorch/pytorch/pull/36793))\r\n* Added support for complex values as exponents in `torch.pow` .([#36793](https://github.com/pytorch/pytorch/pull/36793), [#39117](https://github.com/pytorch/pytorch/pull/39117))\r\n* Added `torch.roll` for complex tensors on CUDA. ([#38664](https://github.com/pytorch/pytorch/pull/38664))\r\n* Added `torch.gather` for complex tensors on CPU. ([#36430](https://github.com/pytorch/pytorch/pull/36430))\r\n* Added `torch.tanh` for complex tensors on CUDA. ([#38786](https://github.com/pytorch/pytorch/pull/38786))\r\n* Added complex dtypes to list of supported types in autograd. ([#38325](https://github.com/pytorch/pytorch/pull/38325))\r\n* Added `torch.cumsum, torch.cumprod` for complex tensors on CUDA. ([#39063](https://github.com/pytorch/pytorch/pull/39063))\r\n* Added `real` and `imag` views as tensor attributes. ([#39033](https://github.com/pytorch/pytorch/pull/39033))\r\n* Added `torch.flip` and `torch.rot90` for complex tensors. ([#37826](https://github.com/pytorch/pytorch/pull/37826))\r\n* Added `torch.view_as_real`, `torch.view_as_complex` for complex tensors. ([#39099](https://github.com/pytorch/pytorch/pull/39099))\r\n* Added printing logic for complex tensors ([#40513](https://github.com/pytorch/pytorch/pull/40513), [#38031](https://github.com/pytorch/pytorch/pull/38031))\r\n* Add `torch.tan` for complex tensors on CUDA ([#38400](https://github.com/pytorch/pytorch/pull/38400))\r\n* Added support for complex tensors for `torch.tanh` backward function ([#37791](https://github.com/pytorch/pytorch/pull/37791), [#38786](https://github.com/pytorch/pytorch/pull/38786))\r\n\r\n\r\n**C++ API:**\r\n\r\n* Added core of c10::complex. ([#36626](https://github.com/pytorch/pytorch/pull/36626))\r\n* Added overloads of std:: math functions in c10::complex ([#37468](https://github.com/pytorch/pytorch/pull/37468), [#37689](https://github.com/pytorch/pytorch/pull/37689))\r\n* Added c10::complex as the C++ type for complex tensors ([#37421](https://github.com/pytorch/pytorch/pull/37421), [#39306](https://github.com/pytorch/pytorch/pull/39306))\r\n* Added support for operations on c10::complex and integer scalars ([#38418](https://github.com/pytorch/pytorch/pull/38418))\r\n* Added overloads for complex math functions in both :: and std:: to fix ROCm bugs ([#39829](https://github.com/pytorch/pytorch/pull/39829))\r\n* Added `at::tensor()` and `torch::tensor()` for complex numbers ([#39793](https://github.com/pytorch/pytorch/pull/39793))\r\n\r\n### Distributed\r\n\r\n* `torch.distributed`: Add `all_to_all` API to the MPI backend in the distributed module ([#32361](https://github.com/pytorch/pytorch/pull/32361)).\r\n* `torch.distributed`: Add `c10d` dynamic loading mechanism to support 3rd-party `c10d` implementations ([#28068](https://github.com/pytorch/pytorch/pull/28068)).\r\n* `torch.nn.parallel.DistributedDataParallel`: Add distributed data parallel benchmark tool ([#35198](https://github.com/pytorch/pytorch/pull/35198)).\r\n* `torch.nn.parallel.DistributedDataParallel` and `torch.distributed.rpc`: allow DDP to work with RPC ([#37998](https://github.com/pytorch/pytorch/pull/37998), [#39916](https://github.com/pytorch/pytorch/pull/39916), [#40130](https://github.com/pytorch/pytorch/pull/40130), [#40139](https://github.com/pytorch/pytorch/pull/40139), [#40495](https://github.com/pytorch/pytorch/pull/40495)).\r\n\r\n### Mobile\r\n\r\n* Add `torch.utils.mobile_optimizer.optimize_for_mobile` to encapsulate several model optimizations appropriate for mobile models.  (Note: currently broken on Windows.)  ([#35227](https://github.com/pytorch/pytorch/pull/35227)) ([#36357](https://github.com/pytorch/pytorch/pull/36357))\r\n\r\n### New operator registration API\r\n\r\nPyTorch 1.6 has a new, pybind11-based operator registration API which replaces the torch::RegisterOperators() class.\r\n\r\nBefore:\r\n\r\n```cpp\r\nstatic auto registry =\r\n  torch::RegisterOperators(\"my_ops::warp_perspective\", &warp_perspective);\r\n```\r\n\r\n\r\nAfter:\r\n\r\n```cpp\r\nTORCH_LIBRARY(my_ops, m) {\r\n  m.def(\"warp_perspective\", warp_perspective);\r\n}\r\n```\r\n\r\n\r\nYou can read more about this API in the [custom C++ operators tutorial](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html) or the [reference documentation](https://pytorch.org/cppdocs/library.html).\r\n\r\nThe new API was developed in PRs #35061, [#35629](https://github.com/pytorch/pytorch/pull/35629), [#35706](https://github.com/pytorch/pytorch/pull/35706), [#36222](https://github.com/pytorch/pytorch/pull/36222), [#36223](https://github.com/pytorch/pytorch/pull/36223), [#36258](https://github.com/pytorch/pytorch/pull/36258), [#36742](https://github.com/pytorch/pytorch/pull/36742), [#37019](https://github.com/pytorch/pytorch/pull/37019). Internal code was ported to this API in [#36799](https://github.com/pytorch/pytorch/pull/36799), [#36800](https://github.com/pytorch/pytorch/pull/36800), [#36389](https://github.com/pytorch/pytorch/pull/36389), [#37834](https://github.com/pytorch/pytorch/pull/37834), [#38014](https://github.com/pytorch/pytorch/pull/38014); you may find the code examples in these PRs helpful for your ports.\r\n\r\n\r\n### ONNX\r\n\r\nIn PyTorch 1.6, we have added support for ONNX Opset 12. We have also enhanced export of torchvision models, such as FasterRCNN, MaskRCNN, and KeypointRCNN to support dynamic input image size. Export support for several new ops have also been added. A new operator export mode, ONNX_FALLTHROUGH, has been added to the export API that allows exporting the model with non-standard ONNX operators. For large (> 2 GB) model export (using `external_data_format=True` argument), we now support models with large tensor data in attributes (not just model parameters). \r\n\r\nNew ONNX operator support:\r\n\r\n* Update Dropout Export ([#37641](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37641&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607483755&sdata=gqOLWvDUVWCyRgsXR4K3eXjfe3LmQ5sV9%2BR8hIgmlx0%3D&reserved=0))\r\n*  Update Argmin/Argmax ONNX Export ([#38329](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38329&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607493754&sdata=tAsOMeoN%2BCPeHcyzOtbA86HnHxT76czQndM%2BkgT4RGQ%3D&reserved=0))\r\n* Fix pow op export ([#38065](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38065&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607493754&sdata=9kV75mFBBAztmbhXphDIZw0FmfMvFYIuNfuAX8Bouxc%3D&reserved=0))\r\n* Export Support for Celu ([#38243](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38243&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607503748&sdata=wenYmcUvMBgE1Yyc%2FTnvJURrkBS4%2Ff8MQ7a%2Bn5RMHc8%3D&reserved=0))\r\n* Add GreaterOrEqual and LessOrEqual to opset 12 ONNX export ([#38311](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38311&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607503748&sdata=IgTJ8aFhEDYjUnI0SKzqvT4yfMZNh87mmpeFxLTxrOQ%3D&reserved=0))\r\n*  ONNX Export Support for CrossEntropyLoss ([#34830](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F34830&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607513740&sdata=EmaydXdyXfrJ7GjLherR4A6IiLBJH2AMjquYdUJv8G4%3D&reserved=0))\r\n* Adding 'numel' and 'to' export for script module ([#36501](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36501&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607513740&sdata=h7r6%2FofjNaqfQ2k2kL%2FX7E7DdShuw2n%2Fu2tpEnHeexo%3D&reserved=0))\r\n*  Support clamp_min and clamp_max ([#37872](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37872&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607523734&sdata=IpUM4SNqUOr%2Ft5UXALT66FEBkk8qaOzSVLWVtp3FBkc%3D&reserved=0))\r\n* Quantization: Add `aten::max_pool2d` to onnx jit pass ([#34912](https://github.com/pytorch/pytorch/pull/34912))\r\n* Quantization: Mark `upsample_nearest2d`, sigmoid and reshape as no scale in onnx ([#36325](https://github.com/pytorch/pytorch/pull/36325))\r\n* Quantization: export of quantized models with new conv and linear API in onnx ([#38736](https://github.com/pytorch/pytorch/pull/38736))\r\n\r\n### Quantization\r\n\r\nNew quantization operators:\r\n\r\n* quantized Conv1d ([#35093](https://github.com/pytorch/pytorch/pull/35093), [#36352](https://github.com/pytorch/pytorch/pull/36352), [#38248](https://github.com/pytorch/pytorch/pull/38248), [#38283](https://github.com/pytorch/pytorch/pull/38283), [#38438](https://github.com/pytorch/pytorch/pull/38438), [#38449](https://github.com/pytorch/pytorch/pull/38449), [#38749](https://github.com/pytorch/pytorch/pull/38749))\r\n* quantized hardsigmoid ([#34959,](https://github.com/pytorch/pytorch/pull/34959)[#36351](https://github.com/pytorch/pytorch/pull/36351), [#36698](https://github.com/pytorch/pytorch/pull/36698), [#36699](https://github.com/pytorch/pytorch/pull/36699))\r\n* quantized hardswish ([#34820,](https://github.com/pytorch/pytorch/pull/34820)[#36350](https://github.com/pytorch/pytorch/pull/36350), [#36252](https://github.com/pytorch/pytorch/pull/36252), [#36320](https://github.com/pytorch/pytorch/pull/36320), [#36545](https://github.com/pytorch/pytorch/pull/36545))\r\n* quantized layernorm ([#36593](https://github.com/pytorch/pytorch/pull/36593), [#36690](https://github.com/pytorch/pytorch/pull/36690), [#35693](https://github.com/pytorch/pytorch/pull/35693))\r\n* quantized groupnorm ([#36835](https://github.com/pytorch/pytorch/pull/36835), [#39090](https://github.com/pytorch/pytorch/pull/39090))\r\n* quantized instancenorm ([#36847](https://github.com/pytorch/pytorch/pull/36847), [#39091](https://github.com/pytorch/pytorch/pull/39091))\r\n* quantized reflection_pad1d ([#37452](https://github.com/pytorch/pytorch/pull/37452))\r\n* quantized adaptive avgpool. ([#36813](https://github.com/pytorch/pytorch/pull/36813))\r\n* channel shuffle op fp32 + quantized. ([#36815](https://github.com/pytorch/pytorch/pull/36815))\r\n* qnnpack path for hardtanh ([#35779](https://github.com/pytorch/pytorch/pull/35779))\r\n* Quantized Threshold ([#39352](https://github.com/pytorch/pytorch/pull/39352))\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc`: Add TensorPipe RPC backend ([#36197](https://github.com/pytorch/pytorch/pull/36197), [#35483](https://github.com/pytorch/pytorch/pull/35483), [#37839](https://github.com/pytorch/pytorch/pull/37839), [#37918](https://github.com/pytorch/pytorch/pull/37918), [#37919,](https://github.com/pytorch/pytorch/pull/37919)[#37850,](https://github.com/pytorch/pytorch/pull/37850)[#37851](https://github.com/pytorch/pytorch/pull/37851), [#37852,](https://github.com/pytorch/pytorch/pull/37852)[#37980](https://github.com/pytorch/pytorch/pull/37980), [#38052](https://github.com/pytorch/pytorch/pull/38052), [#38265](https://github.com/pytorch/pytorch/pull/38265), [#38266](https://github.com/pytorch/pytorch/pull/38266), [#40162](https://github.com/pytorch/pytorch/pull/40162), [#40389](https://github.com/pytorch/pytorch/pull/40389), [#37910](https://github.com/pytorch/pytorch/pull/37910), [#38448](https://github.com/pytorch/pytorch/pull/38448), [#38818](https://github.com/pytorch/pytorch/pull/38818), [#38819](https://github.com/pytorch/pytorch/pull/38819), [#38926](https://github.com/pytorch/pytorch/pull/38926), [#38931](https://github.com/pytorch/pytorch/pull/38931), [#38930](https://github.com/pytorch/pytorch/pull/38930), [#38933](https://github.com/pytorch/pytorch/pull/38933), [#38934](https://github.com/pytorch/pytorch/pull/38934), [#39010](https://github.com/pytorch/pytorch/pull/39010), [#39011](https://github.com/pytorch/pytorch/pull/39011), [#39397](https://github.com/pytorch/pytorch/pull/39397))\r\n* `torch.distributed.rpc`: Support per-RPC timeouts for `rpc_sync` and `rpc_async` ([#34650](https://github.com/pytorch/pytorch/pull/34650))\r\n* `torch.distributed.rpc.functions.async_execution`: Add an `@async_execution` decorator to allow pause and resume executions in RPC target functions ([#39216](https://github.com/pytorch/pytorch/pull/39216), [#39267](https://github.com/pytorch/pytorch/pull/39267), [#39485](https://github.com/pytorch/pytorch/pull/39485), [#39486](https://github.com/pytorch/pytorch/pull/39486), [#39758](https://github.com/pytorch/pytorch/pull/39758)).\r\n* `torch.futures.Future`:Expose a `Future` type to Python API ([#39008](https://github.com/pytorch/pytorch/pull/39008), [#37311](https://github.com/pytorch/pytorch/pull/37311), [#39119](https://github.com/pytorch/pytorch/pull/39119), [#39597](https://github.com/pytorch/pytorch/pull/39597), [#39964](https://github.com/pytorch/pytorch/pull/39964), [#39950](https://github.com/pytorch/pytorch/pull/39950))\r\n* `torch.distributed.rpc`: Allow profiler to be enabled remotely with RPC ([#38748](https://github.com/pytorch/pytorch/pull/38748), [#40066](https://github.com/pytorch/pytorch/pull/40066))\r\n* `torch.distributed.rpc`: Implement TorchScript-compatible `RemoteModule` API ([#37139](https://github.com/pytorch/pytorch/pull/37139), [#40173](https://github.com/pytorch/pytorch/pull/40173))\r\n* `torch.distributed.rpc.RRef`: enable retrying RRef control messages on communication failures ([#33636](https://github.com/pytorch/pytorch/pull/33636))\r\n* `torch.distributed.rpc`: Let RPC use `torch._C.Future` instead of exposing a dedicated future type. No impact on user side ([#35039](https://github.com/pytorch/pytorch/pull/35039))\r\n* `torch.distributed.autograd`: Add profiler support for `backward` of the distributed autograd engine ([#35261](https://github.com/pytorch/pytorch/pull/35261))\r\n* `torch.distributed.rpc.RRef`: Add TorchScript support for `RRef.local_value()` ([#35433](https://github.com/pytorch/pytorch/pull/35433))\r\n* `torch.distributed.rpc.WorkerInfo`: Add TorchScript support for `WorkerInfo` ([#35447](https://github.com/pytorch/pytorch/pull/35447))\r\n* `torch.distributed.rpc`: Allow profiling RPC with TorchScript target functions ([#36275](https://github.com/pytorch/pytorch/pull/36275))\r\n* `torch.distributed.rpc.RRef`: Add RRef Python Helper to launch function on the remotely referenced object ([#36619](https://github.com/pytorch/pytorch/pull/36619))\r\n* `torch.distributed.rpc`: Add timeout argument to TorchScriptable `rpc_async` ([#37884](https://github.com/pytorch/pytorch/pull/37884))\r\n* `torch.distributed.rpc`: Enable RPC Server Global Profiler ([#38847](https://github.com/pytorch/pytorch/pull/38847))\r\n* `torch.distributed.rpc`: Implement timeout support for `rpc.remote` and `RRef.to_here()` ([#38590](https://github.com/pytorch/pytorch/pull/38590))\r\n* `torch.distributed.rpc`: Enable RRef timeout for TensorPipe ([#39531](https://github.com/pytorch/pytorch/pull/39531))\r\n* `torch.distributed.rpc.WorkerInfo`: Add `WorkerInfo` python `__repr__` magic method ([#40004](https://github.com/pytorch/pytorch/pull/40004))\r\n\r\n### TorchScript\r\n\r\n* Fork / Join Async Parallelism  ([#40438](https://github.com/pytorch/pytorch/pull/40438/files))\r\n* ScriptModule Freezing ([#40409](https://github.com/pytorch/pytorch/pull/40409), [#37044](https://github.com/pytorch/pytorch/pull/37044), [#38830](https://github.com/pytorch/pytorch/pull/38830), [#34786](https://github.com/pytorch/pytorch/pull/34786), [#34787](https://github.com/pytorch/pytorch/pull/34787))\r\n\r\n# Improvements\r\n\r\n### Python API\r\n\r\n* Added long description to wheel packages ([#39676](https://github.com/pytorch/pytorch/pull/39676))\r\n* `torch.add`: Prevent unbounded growth while adding sparse tensors ([#36030](https://github.com/pytorch/pytorch/pull/36030))\r\n* `torch.mv`: enabled for sparse tensors ([#21782](https://github.com/pytorch/pytorch/pull/21782))\r\n* `torch.bmm`: enabled for sparse x dense tensor operations ([#33430](https://github.com/pytorch/pytorch/pull/33430))\r\n* `torch.cat`: improved error message ([#38978](https://github.com/pytorch/pytorch/pull/38978))\r\n* `torch.masked_select`: enabled bfloat16 support  ([#36859](https://github.com/pytorch/pytorch/pull/36859))\r\n* `torch.absolute`: added as an alias for `torch.abs`  ([#36597](https://github.com/pytorch/pytorch/pull/36597))\r\n* `torch.device`: improved error message to include `xla` as an acceptable device ([#36446](https://github.com/pytorch/pytorch/pull/36446))\r\n* `torch.linspace`, `torch.logspace`: improved precision ([#35461](https://github.com/pytorch/pytorch/pull/35461))\r\n* `Tensor.true_divide` method variant added ([#34794](https://github.com/pytorch/pytorch/pull/34794))\r\n* `Tensor.isnan()`, `Tensor.isinf()`, `Tensor.isfinite()` method variants added ([#37942](https://github.com/pytorch/pytorch/pull/37942))\r\n* `Tensor.is_nonzero`: improved error message  ([#38150](https://github.com/pytorch/pytorch/pull/38150))\r\n* `Tensor.cauchy_`, T`ensor.log_normal_`, `Tensor.exponential_`: added support for bfloat16 ([#38427](https://github.com/pytorch/pytorch/pull/38427))\r\n* `Tensor.as_subclass` method added. ([#34369](https://github.com/pytorch/pytorch/pull/34369))\r\n* `collect_env.py`: improved to detect relevant conda-installed numpy and cudatoolkit ([#35646](https://github.com/pytorch/pytorch/pull/35646))\r\n* `collect_env.py`: made it more robust on Windows ([#39136](https://github.com/pytorch/pytorch/pull/39136))\r\n* `torch.utils.data`: Add `generator=` kwarg for DataLoader & random samplers ([#39737](https://github.com/pytorch/pytorch/pull/39737))\r\n* `torch.utils.data.DataLoader`: properly diagnose exceeding file descriptor limit ([#34768](https://github.com/pytorch/pytorch/pull/34768))\r\n* `torch.utils.data.DataLoader`: added repr for WorkerInfo ([#39975](https://github.com/pytorch/pytorch/pull/39975))\r\n* `torch.utils.data.random_split`: added option to pass a generator for determinism ([#34043](https://github.com/pytorch/pytorch/pull/34043))\r\n* `torch.utils.data.IterableDataset`: make the warning for when a DataLoader holds an IterableDataset clearer (#41185)\r\n* `torch.nn`: Added support for non-persistent buffers that do not show up in a Module\u2019s state dict ([#37191](https://github.com/pytorch/pytorch/pull/37191))\r\n* `nn.Fold`, `nn.Unfold`: added double backwards support ([#36379](https://github.com/pytorch/pytorch/pull/36379))\r\n* `nn.MultiheadAttention`: added support for bool/byte `attn_mask` tensor ([#33763](https://github.com/pytorch/pytorch/pull/33763))\r\n* `nn.functional.upsample`: enabled uint8 sampling support ([#35029](https://github.com/pytorch/pytorch/pull/35029))\r\n* `nn.functional.kl_div`: added option to accept target in log space ([#34586](https://github.com/pytorch/pytorch/pull/34586))\r\n* `nn.functional.softmax`: added support for sparse tensors (CPU) ([#36305](https://github.com/pytorch/pytorch/pull/36305))\r\n* `nn.Softmin`, `nn.Softmax`: improved repr ([#39084](https://github.com/pytorch/pytorch/pull/39084))\r\n* warnings: Changed warnings generated in cpp to show point of Python origination ([#36052](https://github.com/pytorch/pytorch/pull/36052))\r\n* warnings: Improve warnings to actually point at user code ([#39143](https://github.com/pytorch/pytorch/pull/39143))\r\n* Extend some of the basic ops to kHalf ([#37121](https://github.com/pytorch/pytorch/pull/37121))\r\n* Added a warning to a known autograd issue on XLA backend. ([#35449](https://github.com/pytorch/pytorch/pull/35449), [#35543](https://github.com/pytorch/pytorch/pull/35543))\r\n* `torch.cuda`: Change DeprecationWarning to FutureWarning ([#32142](https://github.com/pytorch/pytorch/pull/32142))\r\n* Added `torch.utils.cmake_prefix_path` pointing to `share/cmake` folder ([#38559](https://github.com/pytorch/pytorch/pull/38559))\r\n* `torch.hub`: Added `file_name` argument to `load_state_dict_from_url` ([#39749](https://github.com/pytorch/pytorch/pull/39749))\r\n* Disable autograd while preparing Tensor for printing ([#39420](https://github.com/pytorch/pytorch/pull/39420))\r\n* Improved CUDA error message for MSVC ([#39987](https://github.com/pytorch/pytorch/pull/39987))\r\n* Improved reentrant autograd error message ([#38625](https://github.com/pytorch/pytorch/pull/38625))\r\n* Let >> and << support half on CUDA ([#37670](https://github.com/pytorch/pytorch/pull/37670))\r\n* dockerfile: Update miniconda installer download location & remove unnecessary flag ([#37082](https://github.com/pytorch/pytorch/pull/37082))\r\n* `torch.cuda.get_arch_list()` and `torch.cuda.get_gencode_flags()` added. These return the architecture list and gencode flags PyTorch was compiled with.  (#41212)\r\n* `torch.min, torch.max`: significantly improved CUDA performance (#38440, #39029)\r\n* `torch.multinomial` with `replacement=False:` significantly improved performance (#39742)\r\n\r\n### Python Type Annotations\r\n\r\n* `torch.autograd`: add type hints in-line ([#38080](https://github.com/pytorch/pytorch/pull/38080))\r\n* `torch.finfo`, `torch.iinfo` type annotations added ([#38220](https://github.com/pytorch/pytorch/pull/38220))\r\n* Moved `torch.cuda` annotations inline ([#40075](https://github.com/pytorch/pytorch/pull/40075))\r\n* Add typing for `torch.cuda._CudaStreamBase` and `torch.cuda._CudaEventBase` classes ([#40256](https://github.com/pytorch/pytorch/pull/40256))\r\n* Introduced `torch.types.Device` and stubbed all `torch._C` functions comprehensively ([#38173](https://github.com/pytorch/pytorch/pull/38173))\r\n* Move all `torch.nn` modules type annotations inline ([#38211](https://github.com/pytorch/pytorch/pull/38211))\r\n* Fixes type annotations for named tensors ([#36890](https://github.com/pytorch/pytorch/pull/36890))\r\n* Fix minor issue in type stub for Optimizer ([#38067](https://github.com/pytorch/pytorch/pull/38067))\r\n* Fixed some miscellaneous type hints ([#36584](https://github.com/pytorch/pytorch/pull/36584))\r\n* Fix multiple issues with type annotations ([#36358](https://github.com/pytorch/pytorch/pull/36358))\r\n* `torch.autograd.anomaly_mode`: fixed type hints stub ([#39324](https://github.com/pytorch/pytorch/pull/39324))\r\n* `torch.backends.cudnn` added type annotations ([#38947](https://github.com/pytorch/pytorch/pull/38947))\r\n* `torch.channels_last`, `torch.preserve_format`: added annotations ([#39120](https://github.com/pytorch/pytorch/pull/39120))\r\n\r\n### AMD/ROCm\r\n\r\n* `torch.topk`: enabled support for BFloat16 type on ROCm. ([#34849](https://github.com/pytorch/pytorch/pull/34849))\r\n* `torch.dot`: enabled fp16 support on ROCm ([#30431](https://github.com/pytorch/pytorch/pull/30431), [#30432](https://github.com/pytorch/pytorch/pull/30432))\r\n* `torch.add`: enabled support for BFloat16 type on ROCm for sparse tensors([#35978](https://github.com/pytorch/pytorch/pull/35978))\r\n* Enabled bfloat16 for operators in BERT model ([#37634](https://github.com/pytorch/pytorch/pull/37634))\r\n* `torch.log`: improved ROCm support ([#40079](https://github.com/pytorch/pytorch/pull/40079))\r\n* `torch.pow`, `torch.exp`, `torch.erf`: enabled support for BFloat16 type on ROCm ([#40236](https://github.com/pytorch/pytorch/pull/40236))\r\n\r\n### C++ API\r\n\r\n* Eliminate warnings for cpp extensions on Windows ([#37400](https://github.com/pytorch/pytorch/pull/37400))\r\n* Disable C4251 when compiling `cpp_extensions` on Windows ([#35272](https://github.com/pytorch/pytorch/pull/35272)) \r\n    Note: Above two PRs eliminate unnecessary compile warnings for windows build, make build log more readable.\r\n\r\n### Distributed\r\n\r\n* `torch.distributed`: Enhance error message for MPI unavailability. ([#36781](https://github.com/pytorch/pytorch/pull/36781)).\r\n* `torch.distributed`: Expose `torch.distributed.is_available()` API ([#37021](https://github.com/pytorch/pytorch/pull/37021)).\r\n* `torch.utils.data`: Only create `torch.generator` and seed in `DistributedSampler` when shuffling ([#37604](https://github.com/pytorch/pytorch/pull/37604)).\r\n* `ProcessGroup`: Log incorrect device in `ProcessGroupGloo` ([#38844](https://github.com/pytorch/pytorch/pull/38844)).\r\n* `torch.utils.data`: Improve `DistributedSampler` docs and add seed option ([#39628](https://github.com/pytorch/pytorch/pull/39628)).\r\n* `torch.cuda.comm.reduce`: Avoid initializing unnecessary tensors in `nccl.reduce` ([#39688](https://github.com/pytorch/pytorch/pull/39688)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Remove obsolete warning message from DDP ([#40190](https://github.com/pytorch/pytorch/pull/40190)).\r\n\r\n### Distributions\r\n\r\n* `distributions.Cauchy`: Implemented kl divergence ([#36477](https://github.com/pytorch/pytorch/pull/36477))\r\n* `distributions.Transform`: Add a `.with_cache()` method ([#36882](https://github.com/pytorch/pytorch/pull/36882))\r\n* `distributions.Binary`: Implemented BTRS algorithm for fast/efficient binomial sampling ([#36858](https://github.com/pytorch/pytorch/pull/36858))\r\n\r\n### Internals\r\n\r\n* New macro `TORCH_FN` for passing in compile time function pointers as regular function arguments rather than template arguments ([#39823](https://github.com/pytorch/pytorch/pull/39823), [#40110](https://github.com/pytorch/pytorch/pull/40110))\r\n* Improved support for more types in registered custom kernels\r\n    * Allow std::array as kernel argument and return ([#34399](https://github.com/pytorch/pytorch/pull/34399))\r\n    * Allow ArrayRef as kernel argument ([#34335](https://github.com/pytorch/pytorch/pull/34335))\r\n* Added FPGA DispatchKey, DeviceType, Backend for out-of-tree experimentation ([#38938](https://github.com/pytorch/pytorch/pull/38938))\r\n* Better type safety for calling the dispatcher; we now do a runtime test when casting OperatorHandle to TypedOperatorHandle that you\u2019ve provided the correct type for kernels  ([#40251](https://github.com/pytorch/pytorch/pull/40251))\r\n* OperatorHandle::callBoxed now works on all operators, you no longer need to manually go through JIT registry ([#36010](https://github.com/pytorch/pytorch/pull/36010), [#36850](https://github.com/pytorch/pytorch/pull/36850))\r\n* Added Dispatcher::redispatch for performing a dispatch that bypasses the current key and all keys before it ([#35476](https://github.com/pytorch/pytorch/pull/35476), subsequently renamed)\r\n* More operators are fully supported by the dispatcher ([#37273](https://github.com/pytorch/pytorch/pull/37273), [#36564](https://github.com/pytorch/pytorch/pull/36564), [#36398](https://github.com/pytorch/pytorch/pull/36398), [#36666](https://github.com/pytorch/pytorch/pull/36666), [#36838](https://github.com/pytorch/pytorch/pull/36838))\r\n* Tracing is no longer done inside our autograd code; instead it has been factored into a separate Tracing dispatch key ([#39514](https://github.com/pytorch/pytorch/pull/39514), [#38467](https://github.com/pytorch/pytorch/pull/38467))\r\n* DispatchKey computation no longer relies on TensorOptions; instead, factory functions and other functions with special dispatch key computation needs can register a BackendSelect kernel to compute the required key. (#36290, [#36562](https://github.com/pytorch/pytorch/pull/36562), [#37257](https://github.com/pytorch/pytorch/pull/37257))\r\n\r\n### ONNX\r\n\r\n* Enable Constant Folding for ONNX Opset 12 ([#34823](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F34823&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607523734&sdata=mE9sjvn0hbzbW043n95fhXI%2F0%2F0A9LQX2I%2BSw4XlAfQ%3D&reserved=0))\r\n* ONNX Update training ops and training amenable export API ([#35567](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35567&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607533731&sdata=PCPdWbtr4LR658huourTjSPnMurlpHx30py6AZ4sWrw%3D&reserved=0))\r\n* Fix for constant folding: Slice, Added ReduceL1 and ReduceL2 ([#35280](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35280&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607533731&sdata=%2B4yXdlL5yD5ml7eoaB59u8%2FLCoknOMQqitYMupnFL48%3D&reserved=0))\r\n*  Added support for constant folding onnx::Add and onnx::Sub ([#35869](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35869&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607543728&sdata=VEcm1q7SAAPdmYI0EJxWoSJxwL7Ad8Kh5aEzQgYf0KE%3D&reserved=0))\r\n* Enable constant folding for Shape ([#35386](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35386&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607543728&sdata=EEnX%2FvBKmXXEPaFMiA%2BdKgRx1g0xfkOw7%2Fl23svPnYc%3D&reserved=0))\r\n* Improve error checking for large model export ([#37798](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37798&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607553719&sdata=yGaeZExiqA8bUKocrYRhQiZSlWUu3PnbdVDUJuzpNBo%3D&reserved=0))\r\n* Remove Aten ops from ONNX export ([#37239](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37239&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607553719&sdata=4sOYH6Agwm2BpQsX9HvuTYwlPEeQOsaD9NDZoDked9U%3D&reserved=0))\r\n* Update pytoch/onnx doc ([#39480](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39480&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607563711&sdata=39%2FTkmCQd%2BvHt0TvIVKJoenvZPofsR1WhJdkMdzJtRU%3D&reserved=0))\r\n* Update pytorch/onnx docs for new export API args ([#39802](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39802&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607563711&sdata=iSBv23qZ9LyWRcK%2B%2FWNk5PldN%2Bc7Y7WR0PLhhA4NS3w%3D&reserved=0))\r\n* Support large attribute and subgraph for large model ([#38793](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38793&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607573710&sdata=8XqGYuUyFD5LCC%2BGxHr7yTd0GeU70dwySb4kaTXZUfc%3D&reserved=0))\r\n\r\n### Operator Benchmark\r\n\r\n* Added benchmark for quantized batchnorm ([#35389](https://github.com/pytorch/pytorch/pull/35389))\r\n* Added more quantized activation benchmarks and input sizes ([#35729](https://github.com/pytorch/pytorch/pull/35729))\r\n* Added `__torch_function__` benchmarks ([#36138](https://github.com/pytorch/pytorch/pull/36138))\r\n* Aligned qconv benchmark to conv ([#36673](https://github.com/pytorch/pytorch/pull/36673))\r\n* Aligned the qlinear benchmark to linear ([#36674](https://github.com/pytorch/pytorch/pull/36674))\r\n* Added CUDA support for the observer benchmark ([#39360](https://github.com/pytorch/pytorch/pull/39360))\r\n\r\n### Profiler\r\n\r\n* `torch.autograd.profiler`: Make RecordFunction callbacks thread local and modernize interface ([#37491](https://github.com/pytorch/pytorch/pull/37491))\r\n* `torch.autograd.profiler`: Make profiler thread local ([#36291](https://github.com/pytorch/pytorch/pull/36291))\r\n\r\n### Quantization\r\n\r\n* Add ConvBn3d, ConvBnReLU3d, BNReLU2d, BNReLU3d to eager mode quantization ([#33540](https://github.com/pytorch/pytorch/pull/33540))\r\n* Enabled per channel quantized static linear/conv in QNNPACK ([#37622](https://github.com/pytorch/pytorch/pull/37622))\r\n* Enable per-channel quantization for LSTM Modules ([#39666](https://github.com/pytorch/pytorch/pull/39666), [#39041](https://github.com/pytorch/pytorch/pull/39041))\r\n* Dynamic quantization support for LSTMCell, RNNCell and GRUCell ([#40102](https://github.com/pytorch/pytorch/pull/40102))\r\n* Quantization aware training now works with nn.DataParallel and nn.DistributedDataParallel\r\n    * Make quantization modules work with nn.DataParallel ([#37032](https://github.com/pytorch/pytorch/pull/37032))\r\n    * fake_quant: move observer and fake_quant flags into buffers ([#38368](https://github.com/pytorch/pytorch/pull/38368))\r\n    * Make QAT Conv-BN work with nn.DistributedDataParallel and nn.SyncBatchNorm ([#38478](https://github.com/pytorch/pytorch/pull/38478))\r\n    * fake_quantize: respect device affinity ([#39031](https://github.com/pytorch/pytorch/pull/39031))\r\n* Add quantized tensor support on CUDA ([#37081](https://github.com/pytorch/pytorch/pull/37081))\r\n* Add reduce_range params for quantized_lstm ([#39604](https://github.com/pytorch/pytorch/pull/39604))\r\n* Use TorchBind for ConvPackedParams ([#35923](https://github.com/pytorch/pytorch/pull/35923))\r\n* Use TorchBind for Linear PackedParams\" ([#38101](https://github.com/pytorch/pytorch/pull/38101))\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc.RRef`: Throw an actionable error message on user call `RRef.to_here()` in TorchScript ([#35369](https://github.com/pytorch/pytorch/pull/35369))\r\n* `torch.distributed.rpc.RRef`: Handle exceptions returned via `remote()` calls ([#35331](https://github.com/pytorch/pytorch/pull/35331))\r\n* `torch.distributed.rpc.RRef`: Make RRef type hint mismatch exception message more actionable to users ([#35943](https://github.com/pytorch/pytorch/pull/35943))\r\n* `torch.distributed.rpc`:Allow abort `RecvWork::wait()` in `ProcessGroupAgent::listenLoop` ([#36084](https://github.com/pytorch/pytorch/pull/36084))\r\n* `torch.distributed.autograd`: Appropriately handle exceptions in autograd engine. ([#36019](https://github.com/pytorch/pytorch/pull/36019))\r\n* `torch.distributed.autograd`: Catch exception in distributed engine callbacks. ([#36118](https://github.com/pytorch/pytorch/pull/36118))\r\n* `torch.distributed.autograd`: Avoid some future callback self-captures. ([#36502](https://github.com/pytorch/pytorch/pull/36502))\r\n* `torch.distributed.rpc`: Propagate error from RPC retries to the original attempt ([#35263](https://github.com/pytorch/pytorch/pull/35263))\r\n* `torch.distributed.autograd`: Ensure future is complete when exiting `Engine::mark_graph_task_completed()` ([#36856](https://github.com/pytorch/pytorch/pull/36856))\r\n* `torch.distributed.autograd`: Trigger pre/post hooks of output function nodes under distributed autograd ([#34501](https://github.com/pytorch/pytorch/pull/34501))\r\n* `torch.distributed.rpc`: Supporting create an RPC gang of world size 1 ([#32731](https://github.com/pytorch/pytorch/pull/32731))\r\n* `torch.distributed.autograd`: Improve Error Message for Dist Autograd Context Cleanup Failure ([#37255](https://github.com/pytorch/pytorch/pull/37255))\r\n* `torch.distributed.rpc`: Guard against negative `rpcTimeout` being passed in to `RpcBackendOptions` ([#38267](https://github.com/pytorch/pytorch/pull/38267))\r\n* `torch.distributed.rpc`: Use infinite timeout for operations in ProcessGroup RPC backend ([#38577](https://github.com/pytorch/pytorch/pull/38577))\r\n* `torch.distributed.rpc.WorkerInfo`: Add stringify `WorkerInfo` ([#39974](https://github.com/pytorch/pytorch/pull/39974))\r\n* `torch.distributed.rpc`: Avoid using default process group in ProcessGroupAgent. ([#39909](https://github.com/pytorch/pytorch/pull/39909))\r\n* `torch.distributed.rpc`: Ignore expected errors in TensorPipe RPC backend ([#39182](https://github.com/pytorch/pytorch/pull/39182))\r\n* `torch.distributed.rpc`: Don't use separate heap allocation for metrics  in TensorPipe RPC backend ([#39183](https://github.com/pytorch/pytorch/pull/39183))\r\n* `torch.distributed.rpc`: Bind to hostname's IP address instead of localhost in TensorPipe RPC backend ([#39184](https://github.com/pytorch/pytorch/pull/39184))\r\n* `torch.distributed.rpc`: Use PrefixStore to avoid conflicting keys in TensorPipe RPC backend ([#39185](https://github.com/pytorch/pytorch/pull/39185))\r\n\r\n### TorchScript\r\n\r\n## Improvements\r\n\r\n* Add `id` function ([#34975](https://github.com/pytorch/pytorch/pull/34975))\r\n* Add lazy script decorator ([#34935](https://github.com/pytorch/pytorch/pull/34935))\r\n* Make Future type annotation available in Python ([#27637](https://github.com/pytorch/pytorch/pull/27637))\r\n* Support converting `str` to `float` ([#35352](https://github.com/pytorch/pytorch/pull/35352))\r\n* Enable recording of TorchScript functions ([#34710](https://github.com/pytorch/pytorch/pull/34710))\r\n* Improve the error message when registering a custom class twice ([#35568](https://github.com/pytorch/pytorch/pull/35568))\r\n* Improve optimization of `if` statements with statically determinable predicates ([#35834](https://github.com/pytorch/pytorch/pull/35834))\r\n* Fix reporting of error message in `toBool` ([#35570](https://github.com/pytorch/pytorch/pull/35570))\r\n* Better error when types of default value and parameter do not match ([#35888](https://github.com/pytorch/pytorch/pull/35888))\r\n* Improve serialization for lists and dictionary ([#35741](https://github.com/pytorch/pytorch/pull/35741))\r\n* Add type hints on `hardsigmoid`, `hardswish`, and `elu` to make them scriptable ([#35885](https://github.com/pytorch/pytorch/pull/35885))\r\n* Add `strict` tracer flag to guard against risky behaviors ([#36277](https://github.com/pytorch/pytorch/pull/36277))\r\n* Add support of `Dict` as output when connecting script and tracing ([_#36265_](https://github.com/pytorch/pytorch/pull/36265))\r\n* Use current default `dtype` with `torch.tensor` when `dtype` is not specified ([_#36587_](https://github.com/pytorch/pytorch/pull/36587))\r\n* Add dictionary as output of tracer ([_#36696_](https://github.com/pytorch/pytorch/pull/36696))\r\n* Allowing casting `str` to `int` ([_#36016_](https://github.com/pytorch/pytorch/pull/36016))\r\n* Convert float Tensor argument to double in `Tensor.tolist` ([_#37465_](https://github.com/pytorch/pytorch/pull/37465))\r\n* Add a `code_with_constants` method to module printing ([_#37586_](https://github.com/pytorch/pytorch/pull/37586))\r\n* Support indexing using list literal as index ([_#37848_](https://github.com/pytorch/pytorch/pull/37848))\r\n* Support indexing using list variable as index ([_#37966_](https://github.com/pytorch/pytorch/pull/37966))\r\n* Support `del` statements with variables as targets in TorchScript ([_#37608_](https://github.com/pytorch/pytorch/pull/37608))\r\n* Recursively compile TorchScript class types ([_#38050_](https://github.com/pytorch/pytorch/pull/38050))\r\n* Better error message when missing `init` on custom C++ classes ([_#37474_](https://github.com/pytorch/pytorch/pull/37474))\r\n* Fix `@staticmethod` access from `self` on modules ([_#37702_](https://github.com/pytorch/pytorch/pull/37702))\r\n* Allow `@torch.jit.unused` to be used on TorchScript classes ([_#38522_](https://github.com/pytorch/pytorch/pull/38522), [_#39336_](https://github.com/pytorch/pytorch/pull/39336))\r\n* Add support for `%=` operator in TorchScript ([_#38983_](https://github.com/pytorch/pytorch/pull/38983))\r\n* Provide error messages when JIT infers the type of an argument as `Tensor` ([_#38527_](https://github.com/pytorch/pytorch/pull/38527))\r\n* Allow self-referential type annotations in TorchScript classes ([_#39821_](https://github.com/pytorch/pytorch/pull/39821))\r\n* Support having a different forward method when not in scripting mode ([_#38158_](https://github.com/pytorch/pytorch/pull/38158))\r\n* Fix `index_put_` error in subscript assignment ([_#38378_](https://github.com/pytorch/pytorch/pull/38378))\r\n* Refactor attributes to support buffers and parameters as first class citizens, add support for iterating over named_buffers() ([_#37905_](https://github.com/pytorch/pytorch/pull/37905))\r\n* Add ROCm-specific `half_support_literal` ([_#38899_](https://github.com/pytorch/pytorch/pull/38899))\r\n* Make `torch.unique_consecutive` compilable ([_#39339_](https://github.com/pytorch/pytorch/pull/39339))\r\n* Make `deepcopy()` of Objects call **`g/setstate`** if present ([_#39500_](https://github.com/pytorch/pytorch/pull/39500))\r\n* Allow slicing sequential container (fe45c2c986)\r\n* Support `torch.Tensor` subclasses (like `Parameter`) as inputs to functions ([_#39487_](https://github.com/pytorch/pytorch/pull/39487))\r\n* Add `dtype` as supported type annotation ([_#39741_](https://github.com/pytorch/pytorch/pull/39741))\r\n* Improve error message when type annotation Future without a contained type ([_#39751_](https://github.com/pytorch/pytorch/pull/39751))\r\n* Fix inconsistent results of string `split` func ([_#38772_](https://github.com/pytorch/pytorch/pull/38772))\r\n* Support `pad_sequence/pack_sequence` ([_#39844_](https://github.com/pytorch/pytorch/pull/39844))\r\n* Enable `copy.deepcopy` and `copy.copy` for `RecursiveScriptModule` ([_#32685_](https://github.com/pytorch/pytorch/pull/32685))\r\n* Fix zip serialization for file > 2GiB (0c90b6da5c)\r\n* Fix `dictConstruct` ordering and enable dict mix (41816dc97f)\r\n* Fix delegating to `jit.load` from `torch.load` (#41013)\r\n* Add distributed `backward` support ([#38494](https://github.com/pytorch/pytorch/pull/38494))\r\n\r\n# Bug Fixes\r\n\r\n### Python API\r\n\r\n* `torch.cat`: fixed missing type promotion ([#35030](https://github.com/pytorch/pytorch/pull/35030), [#39777](https://github.com/pytorch/pytorch/pull/39777))\r\n* `torch.gather`: fixed silently incorrect results when in-place gather tries to use incorrect shapes  ([#37102](https://github.com/pytorch/pytorch/pull/37102))\r\n* `torch.median`: fixed `NaN` comparison ([#38216](https://github.com/pytorch/pytorch/pull/38216))\r\n* `torch.cdist`: fixed backward calculation for `p=2` ([#37337](https://github.com/pytorch/pytorch/pull/37337))\r\n* `torch.eig`: fixed segfault when input has NaNs and infs ([#37642](https://github.com/pytorch/pytorch/pull/37642))\r\n* `torch.irfft`: stopped modifying the input in-place ([#35219](https://github.com/pytorch/pytorch/pull/35219))\r\n* `torch.max`, `torch.min`, `torch.median`: fixed incorrect backwards implementation ([#36316](https://github.com/pytorch/pytorch/pull/36316))\r\n* `torch.fmod`: fixed crash on division by zero ([#38919](https://github.com/pytorch/pytorch/pull/38919))\r\n* `torch.multinomial`: fixed support for tensors with empty batch ([#39873](https://github.com/pytorch/pytorch/pull/39873))\r\n* `torch.einsum`: fixed incorrect `__torch_function__` handling ([#38741](https://github.com/pytorch/pytorch/pull/38741))\r\n* `torch.remainder`: fixed overflow when dividend is very large ([#37758](https://github.com/pytorch/pytorch/pull/37758))\r\n* `torch.remainder`: fixed precision issues for CPU tensors ([#38293](https://github.com/pytorch/pytorch/pull/38293))\r\n* `torch.argmax`, `torch.argmin`: fixed bug for big CPU tensors with `dim=2` ([#39576](https://github.com/pytorch/pytorch/pull/39576))\r\n* `torch.histc:` fixed support when passed empty tensor ([#38987](https://github.com/pytorch/pytorch/pull/38987))\r\n* `torch.as_strided`: added error message when passed a negative stric=de ([#39508](https://github.com/pytorch/pytorch/pull/39508))\r\n* `torch.argmax`, `torch.argmin`: fixed bogus returns when called on a scalar tensor ([#37214](https://github.com/pytorch/pytorch/pull/37214))\r\n* `torch.topk`: Fixed bogus results with 4d+ input tensors with topk dimension >= 1024/2048 on CUDA (depending on GPU) ([#40349](https://github.com/pytorch/pytorch/pull/40349))\r\n* `torch.mv`: Fixed bug when grad has  `stride=0` on GPU in the backward pass ([#38321](https://github.com/pytorch/pytorch/pull/38321))\r\n* `>>`, `<<` on CUDA changed to match the behavior on CPU for certain compiler variants ([#35339](https://github.com/pytorch/pytorch/pull/35339))\r\n* `Tensor.exponential_(0)` fixed to return a Tensor filled with `inf` ([#36837](https://github.com/pytorch/pytorch/pull/36837))\r\n* `Tensor.to(..., non_blocking=True)`: fixed regression where `non_blocking` is ignored ([#35144](https://github.com/pytorch/pytorch/pull/35144))\r\n* `Tensor.to`: fixed CUDA negative float to uint8 cast to be consistent with CPU ([#36832](https://github.com/pytorch/pytorch/pull/36832))\r\n* Fixed incorrect binary pointwise operations when the first argument is a scalar ([#39956](https://github.com/pytorch/pytorch/pull/39956))\r\n* `Tensor.copy_`: Fixed error when used with AMD devices ([#38003](https://github.com/pytorch/pytorch/pull/38003))\r\n* `torch.tensor`: fix segfault in error checking in Tensor constructor ([#40106](https://github.com/pytorch/pytorch/pull/40106))\r\n* Fix overflow issues when constructing tensors with large numbers ([#39140](https://github.com/pytorch/pytorch/pull/39140))\r\n* Fixed regression in unary ops casting to output dtype (#41097)\r\n* `nn.Module`: fixed AttributeError reporting for `nn.Module`'s properties ([#34324](https://github.com/pytorch/pytorch/pull/34324))\r\n* `nn.MaxPool2d`: fix for returning wrong shape with `return_indices=True` on CUDA ([#38992](https://github.com/pytorch/pytorch/pull/38992))\r\n* `nn.MaxPool2d`: fix NCHW backward bug ([#38953](https://github.com/pytorch/pytorch/pull/38953))\r\n* `nn.MaxPool2d`: fixed dilated case ([#36288](https://github.com/pytorch/pytorch/pull/36288))\r\n* `nn.MultiheadAttention`: Removed weights from `__constants__` to fix warnings when converting to TorchScript.\r\n* `nn.ConvTranspose2d`: fixed error in backward pass for fp16 inputs. ([#37569](https://github.com/pytorch/pytorch/pull/37569))\r\n* `nn.ConvTranspose3d`: fixed index overflow ([#39198](https://github.com/pytorch/pytorch/pull/39198))\r\n* `nn.RReLU`: fixed memory leak ([#39347](https://github.com/pytorch/pytorch/pull/39347))\r\n* `nn.PReLU`: fixed stack overflow in backward pass ([#36134](https://github.com/pytorch/pytorch/pull/36134))\r\n* `nn.MultiheadAttention`: fixed assertion to support FP16 training ([#37539](https://github.com/pytorch/pytorch/pull/37539))\r\n* `nn.MultiheadAttention`: Updated assert to remove check on 3rd dim for MHA ([#39402](https://github.com/pytorch/pytorch/pull/39402))\r\n* `nn.ModuleDict`, `nn.ParameterDict`: fixed bug in updating with another `ModuleDict/ParameterDict`, respectively ([#27814](https://github.com/pytorch/pytorch/pull/27814))\r\n* `nn.BatchNorm`: fixed buffer update when `track_running_stats` is set to False ([#38084](https://github.com/pytorch/pytorch/pull/38084))\r\n* `nn.MaxPool3d`: fixed incorrect CUDA backward results for non-square output ([#36820](https://github.com/pytorch/pytorch/pull/36820))\r\n* `nn.DataParallel`: fixed support for empty tensors ([#35965](https://github.com/pytorch/pytorch/pull/35965))\r\n* `nn.functional.grid_sample`: fixed out of boundary bug when grid contains large numbers ([#35506](https://github.com/pytorch/pytorch/pull/35506))\r\n* `nn.functional.max_pool2d`, `nn.functional.avg_pool2d`: fixed issue when stride=None ([#39221](https://github.com/pytorch/pytorch/pull/39221))\r\n* `nn.functional.max_pool2d`: fixed erroneous dimension out of range on CUDA ([#36095](https://github.com/pytorch/pytorch/pull/36095))\r\n* `nn.grad._grad_input_padding`: fixed support for dilation argument ([#33872](https://github.com/pytorch/pytorch/pull/33872))\r\n* `nn.functional.log_softmax`: improved accuracy on CUDA ([#38945](https://github.com/pytorch/pytorch/pull/38945))\r\n* `nn.utils.prune`, `nn.utils.weight_norm`: fixed problems when used with RNNs ([#34170](https://github.com/pytorch/pytorch/pull/34170))\r\n* Fixed nan, inf in GPU {fractional,adaptive} max_pool{2,3}d ([#39903](https://github.com/pytorch/pytorch/pull/39903))\r\n* `nn.functional.interpolation`: nearest interpolation implementation fix for CUDA ([#39055](https://github.com/pytorch/pytorch/pull/39055))\r\n* `torch.utils.mkldnn.to_mkdnn`: cover `nn.Conv1d` in mkldnn model conversion logic ([#38528](https://github.com/pytorch/pytorch/pull/38528))\r\n* `torch.utils.data.DataLoader`: Relax sampler check in BatchSampler ([#38403](https://github.com/pytorch/pytorch/pull/38403))\r\n* `torch.utils.data.DataLoader`: The exception raised when RandomSampler.replacement is non-boolean should be TypeError ([#36547](https://github.com/pytorch/pytorch/pull/36547))\r\n* `torch.utils.data.DataLoader`: Correct a ValueError in dataloader to TypeError ([#36244](https://github.com/pytorch/pytorch/pull/36244))\r\n* `torch.utils.data.DataLoader`: Allow shuffle when auto-batching is disabled ([#39865](https://github.com/pytorch/pytorch/pull/39865))\r\n* `torch.utils.data.DataLoader`: Kill DataLoader workers when we can't join to clean up gracefully ([#39869](https://github.com/pytorch/pytorch/pull/39869))\r\n* `torch.utils.data.Dataloader`: Added error when using `default_collate` on lists of unequal size ([#38492](https://github.com/pytorch/pytorch/pull/38492))\r\n* Fixed crashes on `import torch` related to defining static data in Vec256 ([#37767](https://github.com/pytorch/pytorch/pull/37767), [#38088](https://github.com/pytorch/pytorch/pull/38088)) \r\n* For `out=` operations, preserve output tensor's strides if it is correctly sized ([#38895](https://github.com/pytorch/pytorch/pull/38895))\r\n* `cuda`: fixed a bug where it was possible to incorrectly access the CUDA device before it was initialized ([#36714](https://github.com/pytorch/pytorch/pull/36714))\r\n* `torch.device`: Added better device idx parse checks ([#37376](https://github.com/pytorch/pytorch/pull/37376))\r\n* `torch.autograd`: fixed init-shutdown race condition in autograd engine ([#39194](https://github.com/pytorch/pytorch/pull/39194))\r\n* `torch.autograd`: Fixed error when using hooks with no `__name__` attribute\r\n* `torch.autograd`: Fixed error message ([#39729](https://github.com/pytorch/pytorch/pull/39729))\r\n* `torch.autograd`: wait for non-reentrant threads to shutdown ([#34529](https://github.com/pytorch/pytorch/pull/34529))\r\n* `torch.autograd`: Add undefined tensor gradient support to all backward functions ([#39400](https://github.com/pytorch/pytorch/pull/39400))\r\n* `torch.autograd`: fixed engine flakiness ([#35599](https://github.com/pytorch/pytorch/pull/35599))\r\n* `torch.autograd.Function`: fixed ability to report error messages inside ([#34845](https://github.com/pytorch/pytorch/pull/34845))\r\n* `torch.autograd`: move scalar input to a different device when needed; fixes backward passes of binary-pointwise operators with scalar inputs ([#35286](https://github.com/pytorch/pytorch/pull/35286))\r\n* `torch.autograd.gradcheck`: Fixed behavior for `stride=0` ([#38774](https://github.com/pytorch/pytorch/pull/38774))\r\n* `torch.autograd.Function`: prevent custom Functions from creating non differentiable type that requires grad ([#38326](https://github.com/pytorch/pytorch/pull/38326))\r\n* `torch.no_grad`: Fixed bad interaction between `torch.no_grad` and `tensor.numpy()` conversion  ([#38906](https://github.com/pytorch/pytorch/pull/38906))\r\n* `torch.optim.AdamW`: fixed error message ([#36088](https://github.com/pytorch/pytorch/pull/36088))\r\n* `torch.optim.Optimizer.state_dict()` fixed non-determinism ([#37347](https://github.com/pytorch/pytorch/pull/37347))\r\n* `torch.hub`: added optional request headers to avoid \u201cconnection refused\u201d errors ([#39740](https://github.com/pytorch/pytorch/pull/39740))\r\n* `torch.hub.hub_dir`: fixed inconsistencies ([#38969](https://github.com/pytorch/pytorch/pull/38969))\r\n* OpenMP: fixed memory leak for `num_threads==1` with operations that use OpenMP ([#39533](https://github.com/pytorch/pytorch/pull/39533))\r\n* `torch.multiprocessing`: Fixed deadlock when sharing CUDA tensors ([#40347](https://github.com/pytorch/pytorch/pull/40347))\r\n* `torch.distributions.Binomial`: fix bug where there is a small chance of incorrectly returning -1 ([#38456](https://github.com/pytorch/pytorch/pull/38456))\r\n* `torch.cuda.amp.GradScalar`: fixed bug where `GradScalar` was not pickle-able ([#38296](https://github.com/pytorch/pytorch/pull/38296))\r\n* Fixed uninitialized value in helper function `vec_reduce_all` ([#37853](https://github.com/pytorch/pytorch/pull/37853))\r\n* Fixed potential memory corruption in helper function `cpu_serial_kernel` ([#37869](https://github.com/pytorch/pytorch/pull/37869))\r\n* Synchronize MAGMA functions with the current CUDA stream ([#36605](https://github.com/pytorch/pytorch/pull/36605))\r\n* Windows support: Fix openmp detection with the clang-cl compiler ([#35365](https://github.com/pytorch/pytorch/pull/35365))\r\n* Windows support: Use `ProgramFiles` environment variable on Windows for portability ([#39707](https://github.com/pytorch/pytorch/pull/39707))\r\n* Windows support: Fix AVX detection with clang-cl ([#35653](https://github.com/pytorch/pytorch/pull/35653))\r\n* Windows support: Delay loading the cuda library until it is necessary ([#37811](https://github.com/pytorch/pytorch/pull/37811))\r\n* Windows support: Fix `_copysign` is not a member of std ([#35199](https://github.com/pytorch/pytorch/pull/35199))\r\n* Windows support: Fix zip serialization for files > 2GiB (#40783)\r\n* Windows support: Add runtime check for MSVC redist, fixed `import torch` errors ([#39841](https://github.com/pytorch/pytorch/pull/39841))\r\n* Windows support: More fixes about using Windows API through ctypes ([#39376](https://github.com/pytorch/pytorch/pull/39376))\r\n* Windows support: fixed `import torch` errors ([#39334](https://github.com/pytorch/pytorch/pull/39334))\r\n* Windows support: Fix wrong MSVC version constraint for CUDA 9.2 ([#40794](https://github.com/pytorch/pytorch/pull/40794))\r\n* Windows support: Use LoadLibraryEX, fix problems when loading dlls ([#38302](https://github.com/pytorch/pytorch/pull/38302))\r\n* Windows support: Fix dll load failure in virtual environments ([#39622](https://github.com/pytorch/pytorch/pull/39622))\r\n* Windows support: Make `find_first_set` work on x86 MSVC ([#38637](https://github.com/pytorch/pytorch/pull/38637), [#38706](https://github.com/pytorch/pytorch/pull/38706))\r\n* Removes pickle deprecation warning ([#39003](https://github.com/pytorch/pytorch/pull/39003))\r\n* dockerfile: Sync submodules ([#35423](https://github.com/pytorch/pytorch/pull/35423))\r\n* Fix crashes in `manywheels` builds related to having special `CUDNN` search path rules for `torch_python` ([#37349](https://github.com/pytorch/pytorch/pull/37349))\r\n* *`torch._six.PY37` should be true for Python-3.8 as well (#40868) *\r\n\r\n### AMD/ROCm\r\n\r\n* Stopped erroneously warning about CUDA compute capabilities ([#35949](https://github.com/pytorch/pytorch/pull/35949))\r\n* Stopped using MIOpen for tensors with more than `INT_MAX` number of elements ([#37110](https://github.com/pytorch/pytorch/pull/37110))\r\n* Enable HgemmBatched for ROCm ([#37483](https://github.com/pytorch/pytorch/pull/37483))\r\n* Fix encoding errors for hipify tool ([#37906](https://github.com/pytorch/pytorch/pull/37906))\r\n* Added HIP version guard for occupancy API compatibility ([#38551](https://github.com/pytorch/pytorch/pull/38551))\r\n* Fix the processing logic of `bernoulli` ([#40001](https://github.com/pytorch/pytorch/pull/40001))\r\n* Use correct device type when exporting tensors to DLPack ([#40124](https://github.com/pytorch/pytorch/pull/40124))\r\n\r\n### C++ API\r\n\r\n* Fixed the crash problem when using `BuildExtension.with_options `([#40121](https://github.com/pytorch/pytorch/pull/40121)) \r\n* Fixed the dir permission denied problem when multiple user building cpp_ext on the same machine ([#34239](https://github.com/pytorch/pytorch/pull/34239)) \r\n\r\n### Distributed\r\n\r\n* `torch.nn.SyncBatchNorm`: Fix batch size check. ([#37133](https://github.com/pytorch/pytorch/pull/37133)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Fix DDP error checking for unused parameters ([#36054](https://github.com/pytorch/pytorch/pull/36054)).\r\n* `torch.nn.DataParallel`: Ensure `DataParallel` replicas can be pickled ([#37307](https://github.com/pytorch/pytorch/pull/37307)).\r\n* `torch.distributed`: Ensure `NCCL_BLOCKING_WAIT=1` works for `dist.barrier()` ([#40249](https://github.com/pytorch/pytorch/pull/40249)).\r\n* `torch.nn.SyncBatchNorm`: Avoid blocking host thread when using  `SyncBatchNorm` ([#36659](https://github.com/pytorch/pytorch/pull/36659)).\r\n* `torch.cuda.comm.gather`: Fix `Gather::apply` to avoid accessing moved tensors ([#39733](https://github.com/pytorch/pytorch/pull/39733)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Add a guard to allow DDP\u2019s autograd engine callback to work in a with non-default CUDA streams ([#40115](https://github.com/pytorch/pytorch/pull/40115)).\r\n\r\n### Internals\r\n\r\n* Add missing mutex for listener removal ([#35486](https://github.com/pytorch/pytorch/pull/35486))\r\n* Add missing mutex for fallback register/deregister ([#36628](https://github.com/pytorch/pytorch/pull/36628))\r\n* Improved boxed dispatch performance ([#33313](https://github.com/pytorch/pytorch/pull/33313))\r\n* Refactored jit::Operator to more clearly distinguish the two possible states: c10 vs jit ([#33905](https://github.com/pytorch/pytorch/pull/33905), [#36634](https://github.com/pytorch/pytorch/pull/36634))\r\n* Per device initialization now occurs in backend kernels via code generation, rather than during backend selection ([#37402](https://github.com/pytorch/pytorch/pull/37402))\r\n* Improved support for dispatcher on mobile\r\n    * Unconditionally register schema even on mobile build (61b680c012, [#36250](https://github.com/pytorch/pytorch/pull/36250), [#35148](https://github.com/pytorch/pytorch/pull/35148), [#35193](https://github.com/pytorch/pytorch/pull/35193))\r\n    * Forced schema registration output now generated into a separate file ([#36284](https://github.com/pytorch/pytorch/pull/36284))\r\n* Improved error messages\r\n    * Print the class name when a custom class in kernel signature is invalid ([#39491](https://github.com/pytorch/pytorch/pull/39491))\r\n    * Add operator name to callBoxed() error message ([#39562](https://github.com/pytorch/pytorch/pull/39562))\r\n\r\n### ONNX\r\n\r\n* Fixes default dtype value for onnx hardtanh export (opset11) ([#35467](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35467&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607573710&sdata=lkFXOuIzi1OHmLfA8UA7sGXx6n%2FKhEBG%2F6aqmbZu9JU%3D&reserved=0))\r\n* disable size optimizations for onnx ([#36243](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36243&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607583699&sdata=l4xM3lRRXDMf5sZHq7mZzo7k21nkN9nyFaogqrsLv40%3D&reserved=0))\r\n* Adding a pass to replace interpolate function with `aten::__interpolate` ([#35744](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F35744&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607583699&sdata=uhPb%2FgDMJusIp%2BGA9eXzpwqLPy%2FXAT2Jl2zLCJh3ZSA%3D&reserved=0))\r\n* fix `provider_version` and add consistency test ([#36797](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36797&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607593694&sdata=O4HsT%2BiyJ5i9ddZZOaH04zxntDt0ZXrlV1t2RWJ%2FzdA%3D&reserved=0))\r\n*  Fix numerical errors in softmax when dim is not last dimension ([#37326](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37326&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607593694&sdata=%2FdLdlHCzoPuKmBb4p%2Bp50U7tKRV54CAzXDmyqwcKey0%3D&reserved=0))\r\n*  make onnx expect tests resilient to producer_version changes ([#39002](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39002&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607603698&sdata=ILk%2FKvQELZbvfqQ8RFejGY8wkcGGh30qB3xRAZQ0L4M%3D&reserved=0))\r\n* Enable models tests ([#38791](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38791&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607603698&sdata=HsUuISwEDItvGutFLR5Pjdl%2Fr2dqWe6Gonc8XN2sk4A%3D&reserved=0))\r\n*  Enable Constant Folding Tests ([#38751](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38751&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607613689&sdata=Uu5iNJACcpg0V67u4POepfzMYenbslAr0pORiHe6iwE%3D&reserved=0))\r\n* Bump up ONNX submodule to a82c6a7010e2e332d8f74ad5b0c726fd47c85376 ([#39372](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39372&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607613689&sdata=YiRARqN0Ck5SeMCwi84vllPydPsi1qQ2EJwi1jXfK8w%3D&reserved=0))\r\n* Fix type casting for reduce ops ([#38829](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F38829&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607623677&sdata=CLbIPtrcC%2BoFuFj7tFF52hwWafH6b86h6ANOL842hn4%3D&reserved=0))\r\n* Fix ONNX export of RNNs with no bias ([#36894](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F36894&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607623677&sdata=NNh1t3IOy3IyecUnR4MiiuqRG5VtdNVVtVbR%2Fe0OTMA%3D&reserved=0))\r\n* Fix regression disabling checker ([#39073](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39073&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607633671&sdata=Pr8FqnUI0SnQXSbEibwLnzHMEm7gyfEbwmZX8UyXWLk%3D&reserved=0))\r\n* Fix KeypointRCNN test ([#39589](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39589&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607633671&sdata=MpMkw8vK9c2dYnfhUL5xxMtO9G5Nr%2BctU%2Fqp8H8p6Vc%3D&reserved=0))\r\n* Fix bug in export of ops involving torch.bool type ([#40006](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F40006&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607643667&sdata=qenOAbymKJ0hVHND3cQUYNW%2Fff4%2FKntBUrWStn3Cgno%3D&reserved=0))\r\n* Fix bug in export of cumsum operator ([#40044](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F40044&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607643667&sdata=GdfZcHFg9EoA8QQCdyXMdtsKfKJM%2Ff2qf%2BhIQfkqHZE%3D&reserved=0))\r\n* Set onnx opset version before model select ([#37466](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37466&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607653661&sdata=NjHYlOkTKEidVr2fa%2FJAAFI671fUhf617%2FJ6L8mLqNw%3D&reserved=0))\r\n* Enable tests for opset 12 ([#37846](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37846&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607653661&sdata=WJY2Cyhh3LG902%2BDLGztnzCG77XUe87h9rRvQQx1qYU%3D&reserved=0))\r\n* Enable tests in `test_pytorch_onnx_onnxruntime` ([#37868](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F37868&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607663657&sdata=SJQeNlkkqnTyK3DPJqopSBklEHiFlr8668kMEIoRPuE%3D&reserved=0))\r\n* Enable tests in test_operators.py ([#39431](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fpytorch%2Fpytorch%2Fpull%2F39431&data=02%7C01%7Csptiwari%40microsoft.com%7C98bafc94587f40abee0e08d822e0e5dc%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637297696607663657&sdata=tnEHdrvEjIbPfWIxzAic%2FC0Lnll9UPiu3ZMGKalLVzI%3D&reserved=0))\r\n\r\n### Operator Benchmark\r\n\r\n* Fixed missing comma in activation benchmarks ([#35104](https://github.com/pytorch/pytorch/pull/35104))\r\n* Fixed bug where activation benchmarks didn\u2019t run anything ([#35731](https://github.com/pytorch/pytorch/pull/35731))\r\n* Replaced `import cpp_benchmark` with `torch.utils.cpp_benchmark` ([#38832](https://github.com/pytorch/pytorch/pull/38832))\r\n\r\n### Profiler\r\n\r\n* `torch.autograd.profiler`: Use `high_resolution_clock` for profiling on Mac ([#37280](https://github.com/pytorch/pytorch/pull/37280))\r\n* `torch.autograd.profiler`: Fixes for profiling JIT code ([#38453](https://github.com/pytorch/pytorch/pull/38453))\r\n* `torch.autograd.profiler`: Destroy CUDA events after profiling ([#39962](https://github.com/pytorch/pytorch/pull/39962))\r\n\r\n### Quantization\r\n\r\n* Fix a bug for convolution bias in QAT Conv-BN ([#36173](https://github.com/pytorch/pytorch/pull/36173))\r\n* Ensure that histogram observers have zero-point of zero for post ReLU activations ([#37107](https://github.com/pytorch/pytorch/pull/37107))\r\n* Unify numerics between fakequant and quant/dequant ([#37188](https://github.com/pytorch/pytorch/pull/37188))\r\n* Release qnnpack original weights for conv/linear ([#37595](https://github.com/pytorch/pytorch/pull/37595))\r\n* Fix histogram observer with 0 input ([#40191](https://github.com/pytorch/pytorch/pull/40191))\r\n* Histogram observer bug fix with min == max ([#40310](https://github.com/pytorch/pytorch/pull/40310))\r\n* Add save/load state_dict to quantized dynamic RNNs ([#39105](https://github.com/pytorch/pytorch/pull/39105))\r\n* Ensure qconv doesn't assert with empty batch ([#38252](https://github.com/pytorch/pytorch/pull/38252))\r\n* Support empty batch input for quantized ops ([#38508](https://github.com/pytorch/pytorch/pull/38508))\r\n* Fixed CUDA memory pinning (#41139)\r\n\r\n### RPC\r\n\r\n* `torch.distributed.autograd`: Respect dist autograd context in `torch.jit._fork`. ([#34360](https://github.com/pytorch/pytorch/pull/34360))\r\n* `torch.distributed.autograd`: Continue trying `send()` even if one `send()` failed when cleanup distributed autograd contexts ([#34943](https://github.com/pytorch/pytorch/pull/34943))\r\n* `torch.distributed.rpc`: In ProcessGroup RPC backend, avoid read-after-free ([#35252](https://github.com/pytorch/pytorch/pull/35252))\r\n* `torch.distributed.rpc`: Fix `aten::wait` for RPC futures([#35695](https://github.com/pytorch/pytorch/pull/35695))\r\n* `torch.distributed.rpc`: Fix `prim::rpc_async` for RPC futures ([#35994](https://github.com/pytorch/pytorch/pull/35994))\r\n* `torch.distributed.rpc`: Only Schedule Retries before Agent Shutdown ([#35554](https://github.com/pytorch/pytorch/pull/35554))\r\n* `torch.distributed.rpc`: Call `threadPool.waitWorkComplete` after `listenerThread.join()` to fix graceful shutdown ([#35394](https://github.com/pytorch/pytorch/pull/35394))\r\n* `torch.distributed.rpc`: Fixing Potential TSAN issue with joining RPC helper threads ([#36094](https://github.com/pytorch/pytorch/pull/36094))\r\n* `torch.distributed.rpc`: Fix race during RPC shutdown. ([#36113](https://github.com/pytorch/pytorch/pull/36113))\r\n* `torch.distributed.rpc`: Fixing RPC shutdown and thread joining ([#36239](https://github.com/pytorch/pytorch/pull/36239))\r\n* `torch.distributed.autograd`: Capture global state, distributed autograd current context id, before thread switching triggered by JIT `future.wait()` ([#36395](https://github.com/pytorch/pytorch/pull/36395))\r\n* `torch.distributed.autograd`: Fix race in `mark_graph_task_completed`. ([#36640](https://github.com/pytorch/pytorch/pull/36640))\r\n* `torch.distributed.rpc`: Acquire GIL when constructing/destructing `ConcretePyObjectHolder` ([#37870](https://github.com/pytorch/pytorch/pull/37870))\r\n* `torch.distributed.rpc`: Explicitly decref `py::object` in `ConcretePyObjectHolder` and `PythonFunctionGuard` ([#38364](https://github.com/pytorch/pytorch/pull/38364))\r\n* `torch.distributed.rpc`: Explicitly decref `py::object` in `PythonRpcHandler` ([#38366](https://github.com/pytorch/pytorch/pull/38366))\r\n* `torch.distributed.rpc`: Keep `py::object` alive until `jit::toIValue` returns ([#38348](https://github.com/pytorch/pytorch/pull/38348))\r\n* `torch.distributed.rpc`: Use GIL to guard decref of `jit::toPyObj` return value in `processRpc` ([#38376](https://github.com/pytorch/pytorch/pull/38376))\r\n* `torch.distributed.rpc`: Use Future's `then()` API to make sure profiling logic is completed when the Future completes ([#38352](https://github.com/pytorch/pytorch/pull/38352))\r\n* `torch.distributed.rpc`: Fix timeout computation in TensorPipe agent([#38928](https://github.com/pytorch/pytorch/pull/38928))\r\n* `torch.distributed.rpc`: Fix lock inversion upon response read error handling ([#38929](https://github.com/pytorch/pytorch/pull/38929))\r\n* `torch.distributed.rpc`: Acquire lock when adding message to timeout map to fix race in TensorPipe RPC backend ([#39398](https://github.com/pytorch/pytorch/pull/39398))\r\n* `torch.distributed.rpc`: Explicitly decref in `UnpickledPythonCall` dtor ([#38398](https://github.com/pytorch/pytorch/pull/38398))\r\n* `torch.distributed.rpc`: Fix possible deadlock in `_wait_all_workers` ([#39535](https://github.com/pytorch/pytorch/pull/39535))\r\n* `torch.distributed.rpc`: Release GIL when deleting users and unforked owners ([#39555](https://github.com/pytorch/pytorch/pull/39555))\r\n* `torch.distributed.rpc`: Fix error handling for `rpc.remote` ([#39605](https://github.com/pytorch/pytorch/pull/39605))\r\n* `torch.distributed.rpc`: Fix RRef alias annotation ([#39933](https://github.com/pytorch/pytorch/pull/39933))\r\n* `torch.distributed.rpc`: Fix TensorPipeAgent shutdown to ensure it drains all outstanding work. ([#40060](https://github.com/pytorch/pytorch/pull/40060))\r\n* `torch.futures`: Let `torch.futures.wait_all()` re-throw errors ([#40291](https://github.com/pytorch/pytorch/pull/40291))\r\n* `torch.distributed.autograd`: Add basic GPU support to distributed autograd. ([#40312](https://github.com/pytorch/pytorch/pull/40312))\r\n\r\n### TensorBoard\r\n\r\n* `summary.hparams`: Support `None` in `hparams_dict` ([#36497](https://github.com/pytorch/pytorch/pull/36497))\r\n* `SummaryWriter.add_scalars()`: Removed incorrect documentation ([#36495](https://github.com/pytorch/pytorch/pull/36495))\r\n* `SummaryWriter.add_embedding`: Fix error where NaN appears in some cases ([#36496](https://github.com/pytorch/pytorch/pull/36496))\r\n* `SummaryWriter.add_hparams`: Fix input parameters ([#31301](https://github.com/pytorch/pytorch/pull/31301))\r\n* `SummaryWriter.add_image_with_boxes`: Added option to add strings to image boxes ([#30941](https://github.com/pytorch/pytorch/pull/30941))\r\n* `SummaryWriter.add_graph`: Fixed missing documentation ([#37504](https://github.com/pytorch/pytorch/pull/37504))\r\n* `SummaryWriter.add_hparams` Let hparam render values correctly ([#31544](https://github.com/pytorch/pytorch/pull/31544))\r\n* Enforce tensorboard minimum version as 1.15 ([#35952](https://github.com/pytorch/pytorch/pull/35952))\r\n\r\n### TorchScript\r\n\r\n* Fix scope of writes in comprehensions ([#36105](https://github.com/pytorch/pytorch/pull/36105))\r\n* Fix name collision during module loading ([#35720](https://github.com/pytorch/pytorch/pull/35720))\r\n* Fix `NamedTuple` resolution ([#35409](https://github.com/pytorch/pytorch/pull/35409))\r\n* Fix copying of bound method from `Module` to `ScriptModule` ([_#36546_](https://github.com/pytorch/pytorch/pull/36546))\r\n* Fix lifting bug in tracing module calls ([_#37189_](https://github.com/pytorch/pytorch/pull/37189))\r\n* Fix tracing of return types for modules that return heterogenous tuples ([_#37190_](https://github.com/pytorch/pytorch/pull/37190))\r\n* Add type-hint check for default arguments in TorchScript C++ frontend ([_#39021_](https://github.com/pytorch/pytorch/pull/39021))\r\n* Fix recursive compilation of function annotated with `@torch.jit._script_if_tracing`` (#40468) ([_#40468_](https://github.com/pytorch/pytorch/pull/40468))\r\n* Fix parsing of subscript expressions using python resolver ([_#39269_](https://github.com/pytorch/pytorch/pull/39269))\r\n* Fix compilation error with gcc 5.5 ([#38112](https://github.com/pytorch/pytorch/pull/38112))\r\n* Fix handling of `aten::masked_select`, properly update type of the `aten::unsqueeze`'s output in shape analysis ([#40716](https://github.com/pytorch/pytorch/pull/40716))\r\n* Fix handling of `aten::unfold`, properly handle default dtype, and fix a gradient thrashing issue in shape analysis ([#41044](https://github.com/pytorch/pytorch/pull/41044))\r\n* Fix a bug with incorrect handling of `aten::view` in autodiff graph construction ([#42029](https://github.com/pytorch/pytorch/pull/42029))\r\n* Fix a bug with incorrect handling of constructor operations with tensor inputs tensor properties based on an input tensor rather than defaults in shape analysis ([#41016](https://github.com/pytorch/pytorch/pull/41016))\r\n* Fix  bug with incorrect  handling of `prim::grad` operation for `Undefined` values in shape analysis ([#41015](https://github.com/pytorch/pytorch/pull/41015))\r\n* Fix the incorrect requires_grad property propagation on loop\u2019s block inputs ([#41014](https://github.com/pytorch/pytorch/pull/41014))\r\n\r\n# Performance\r\n\r\n### Misc\r\n\r\n* `F.avg_pool2d`: added specialized kernel for channels-last ([#35855](https://github.com/pytorch/pytorch/pull/35855))\r\n* Relax cudnn conditions for channels-last convolutions ([#38904](https://github.com/pytorch/pytorch/pull/38904))\r\n* `torch.cat`: Enabled fast path for channels-last inputs ([#39448](https://github.com/pytorch/pytorch/pull/39448))\r\n* `torch.index_put` parallelized accumulate CPU float path with `cpu_atomic_add_float` ([#29705](https://github.com/pytorch/pytorch/pull/29705))\r\n* Make discontiguous tensors also benefit from unrolling ([#34708](https://github.com/pytorch/pytorch/pull/34708))\r\n* `torch.scatter`, `torch.gather`: removed some redundant checks to achieve some speedups ([#34690](https://github.com/pytorch/pytorch/pull/34690))\r\n* `torch.scatter`, `torch.gather` improved performance on CUDA ([#36181](https://github.com/pytorch/pytorch/pull/36181))\r\n* `torch.min(tensor, dim)`, `torch.max(tensor, dim)`: Optimize performance on CPU ([#34875](https://github.com/pytorch/pytorch/pull/34875))\r\n* `torch.index_select`: Optimize performance for 1D inputs ([#35243](https://github.com/pytorch/pytorch/pull/35243))\r\n* Vectorize (CPU) generic types for binary bitwise operators ([#34338](https://github.com/pytorch/pytorch/pull/34338))\r\n* `torch.linspace` vectorized on CPU. ([#27957](https://github.com/pytorch/pytorch/pull/27957), [#34555](https://github.com/pytorch/pytorch/pull/34555), [#35842](https://github.com/pytorch/pytorch/pull/35842), ([#37981](https://github.com/pytorch/pytorch/pull/37981), [#38093](https://github.com/pytorch/pytorch/pull/38093))\r\n* Set device only when device index are different ([#35438](https://github.com/pytorch/pytorch/pull/35438))\r\n* Don't replace TensorImpl for inplace min/max dim ([#35591](https://github.com/pytorch/pytorch/pull/35591), [#39696](https://github.com/pytorch/pytorch/pull/39696))\r\n* `torch.clamp` vectorized for bfloat16 ([#35082](https://github.com/pytorch/pytorch/pull/35082))\r\n* bfloat16: vectorized many unary ops ([#35092](https://github.com/pytorch/pytorch/pull/35092))\r\n* `torch.bincount` optimized for CPU by removing extra `size()` calls ([#35822](https://github.com/pytorch/pytorch/pull/35822))\r\n* Improve reduction op performance on CUDA for large tensors ([#35997](https://github.com/pytorch/pytorch/pull/35997), [#36014](https://github.com/pytorch/pytorch/pull/36014))\r\n* Vectorize in-place comparison operators ([#35117](https://github.com/pytorch/pytorch/pull/35117))\r\n* Vectorize reduction when reducing on fastest striding dimension ([#36873](https://github.com/pytorch/pytorch/pull/36873))\r\n* `nn.EmbeddingBag`: add a fast path that calls FBGEMM ([#36679](https://github.com/pytorch/pytorch/pull/36679))\r\n* `nn.Conv3d`: Optimized grouped Conv3d performance ([#36355](https://github.com/pytorch/pytorch/pull/36355))\r\n* Reduce overheads on several CPU kernels by avoiding restrides. ([#36875](https://github.com/pytorch/pytorch/pull/36875))\r\n* `nn.EmbeddingBag`: uninitialize output and `bag_size` in the fast path to save overhead ([#36681](https://github.com/pytorch/pytorch/pull/36681))\r\n* `nn.SmoothL1Loss`: vectorize forward (CPU) ([#37114](https://github.com/pytorch/pytorch/pull/37114), [#37115](https://github.com/pytorch/pytorch/pull/37115))\r\n* `nn.Unfold`: optimized backward pass ([#36612](https://github.com/pytorch/pytorch/pull/36612), [#38871](https://github.com/pytorch/pytorch/pull/38871))\r\n* Add per-device allocator object in CUDACachingAllocator, reducing lock contention between operations on different devices. ([#37567](https://github.com/pytorch/pytorch/pull/37567))\r\n* Lazily initialize thread local num_threads value ([#37461](https://github.com/pytorch/pytorch/pull/37461))\r\n* Vectorize non-persistent Softmax ([#38557](https://github.com/pytorch/pytorch/pull/38557))\r\n* `nn.GroupNorm`: performance optimized on CPU and CUDA ([#28203](https://github.com/pytorch/pytorch/pull/28203), [#28204](https://github.com/pytorch/pytorch/pull/28204))\r\n* `torch.cumsum`, `torch.cumprod`: Restore thrust path for 1d tensors cumulative ops ([#39180](https://github.com/pytorch/pytorch/pull/39180))\r\n* TensorIterator: Remove unnecessary `!op.is_read_write` test  ([#39747](https://github.com/pytorch/pytorch/pull/39747))\r\n* `torch.multinomial` : fast-path for replacement=False ([#39742](https://github.com/pytorch/pytorch/pull/39742))\r\n* Vectorize on output for reduction kernels ([#37206](https://github.com/pytorch/pytorch/pull/37206))\r\n* `nn.UpSample`: optimized performance for linear modes on CPU ([#34864](https://github.com/pytorch/pytorch/pull/34864))\r\n* Make dynamic casting case also benefit from unrolling ([#34749](https://github.com/pytorch/pytorch/pull/34749))\r\n* `torch.sinh`, `torch.cosh`: vectorized on CPU ([#36396](https://github.com/pytorch/pytorch/pull/36396))\r\n* Speed up sparse tensor gradient accumulation ([#36292](https://github.com/pytorch/pytorch/pull/36292))\r\n* `torch.masked_select` sped up ([#36539](https://github.com/pytorch/pytorch/pull/36539), [#33269](https://github.com/pytorch/pytorch/pull/33269))\r\n* `torch.var`, `torch.std` sped up ([#39967](https://github.com/pytorch/pytorch/pull/39967))\r\n* `torch.max(tensor, dim)` , `torch.min(tensor, dim)` sped up ([#39029](https://github.com/pytorch/pytorch/pull/39029))\r\n\r\n### Distributed\r\n\r\n* `torch.nn.SyncBatchNorm`: Speed up `SyncBatchNorm` by batching distributed communication ([#38246](https://github.com/pytorch/pytorch/pull/38246)).\r\n* `torch.nn.parallel.DistributedDataparallel`: Dynamically adjust DDP bucketing order using the signals collected from the first iteration ([#35137](https://github.com/pytorch/pytorch/pull/35137)).\r\n\r\n### Mobile\r\n\r\n* Use XNNPACK to improve performance for some instances of convolution and linear. ([#35790](https://github.com/pytorch/pytorch/pull/35790)) ([#35791](https://github.com/pytorch/pytorch/pull/35791))\r\n* Use a custom allocator on mobile to automatically include padding for {Q,X}NNPACK, reducing reallocation costs.  ([#36032](https://github.com/pytorch/pytorch/pull/36032))\r\n* Use updated open-source pthreadpool to improve multi-threading performance. (#40951)\r\n\r\n### Quantization\r\n\r\n* qmul and qadd should preserve input memory format ([#34834](https://github.com/pytorch/pytorch/pull/34834))\r\n* remove the slow path(NCHW) for avg_pool3d ([#34994](https://github.com/pytorch/pytorch/pull/34994))\r\n* Optimized qadd_scalar ([#34925](https://github.com/pytorch/pytorch/pull/34925))\r\n* Optimize qavg_pool3d_nhwc ([#35740](https://github.com/pytorch/pytorch/pull/35740))\r\n* Changes to qadd for perf improvement. (602b51eb30)\r\n* improve the quantized batch_norm performance ([#35639](https://github.com/pytorch/pytorch/pull/35639))\r\n* Add vector path to copy kernel for quantized data types ([#36189](https://github.com/pytorch/pytorch/pull/36189))\r\n* Speed up calculate Qparams for per-channel observers ([#30485](https://github.com/pytorch/pytorch/pull/30485))\r\n* Enable float requantization for avgpool/gavgpool ops. ([#37037](https://github.com/pytorch/pytorch/pull/37037))\r\n* Move to using MemoryFormat::ChannelsLast for avgpool2d. ([#36812](https://github.com/pytorch/pytorch/pull/36812))\r\n* Use `gpu_kernel` in Affine Quantizer ([#37312](https://github.com/pytorch/pytorch/pull/37312))\r\n* Perf optimization for conv and gemm kernels. ([#37626](https://github.com/pytorch/pytorch/pull/37626))\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc`: In RPC Server, handle TorchScript continuations asynchronously ([#34109](https://github.com/pytorch/pytorch/pull/34109))\r\n* `torch.distributed.autograd`: Avoid holding lock when completing GraphTask futureResult ([#35101](https://github.com/pytorch/pytorch/pull/35101))\r\n* `torch.distributed.autograd`: Lock optimizations for `DistAutogradContainer` ([#36529](https://github.com/pytorch/pytorch/pull/36529))\r\n* `torch.distributed.rpc.RRef`:Prevent `RRef.to_here()` to block an RPC thread on the callee using Future callbacks ([#36805](https://github.com/pytorch/pytorch/pull/36805))\r\n* `torch.distributed.rpc.RRef`:Prevent `RRef` unpickle to block waiting for `OwnerRRef` creation ([#36785](https://github.com/pytorch/pytorch/pull/36785))\r\n* `torch.distributed.autograd`: Remove spinning for dist engine ([#36606](https://github.com/pytorch/pytorch/pull/36606))\r\n* `torch.distributed.rpc`: Avoid Releasing, Reacquiring lock per iteration in RPC Retry Thread ([#38521](https://github.com/pytorch/pytorch/pull/38521))\r\n\r\n### TorchScript\r\n\r\n* Add vectorized load/store support for JIT generated CUDA kernel ([_#36555_](https://github.com/pytorch/pytorch/pull/36555))\r\n* Speed up alias analysis ([_#36345_](https://github.com/pytorch/pytorch/pull/36345))\r\n* Make new zip serialization for torch save/load significantly (~70%) faster ([_#38379_](https://github.com/pytorch/pytorch/pull/38379))\r\n* Run extra optimizations after inlining ([#35562](https://github.com/pytorch/pytorch/pull/35562))\r\n\r\n# Documentation\r\n\r\n* Split up documentation into subpages, greatly improving performance and search-ability ([#37419](https://github.com/pytorch/pytorch/pull/37419))\r\n* Rename `torch._C.Generator` to `torch.Generator` ([#38773](https://github.com/pytorch/pytorch/pull/38773))\r\n* FAQ: Add note about recovering from OOM ([#35214](https://github.com/pytorch/pytorch/pull/35214))\r\n* `torch.histc`: Add a note on elements outside of given bounds ([#34889](https://github.com/pytorch/pytorch/pull/34889))\r\n* `functional.hardswish`, `functional.hardsigmoid`: improve docs ([#35431](https://github.com/pytorch/pytorch/pull/35431))\r\n* `Tensor.is_complex` doc fix ([#35680](https://github.com/pytorch/pytorch/pull/35680))\r\n* `nn.KLDivLoss` doc fix ([#36137](https://github.com/pytorch/pytorch/pull/36137))\r\n* `torch.min`, `torch.max`, `torch.median`: added note on deterministic/non-deterministic gradient ([#36481](https://github.com/pytorch/pytorch/pull/36481))\r\n* Amp gradient accumulation example ([#36601](https://github.com/pytorch/pytorch/pull/36601))\r\n* `functional.softmax` doc fix ([#36600](https://github.com/pytorch/pytorch/pull/36600))\r\n* Update `contribution_guide.rst` ([#36438](https://github.com/pytorch/pytorch/pull/36438))\r\n* Documentation LU Decomposition: deriving L, U, and P ([#36907](https://github.com/pytorch/pytorch/pull/36907))\r\n* CONTRIBUTING.md: Fixed missing links ([#37131](https://github.com/pytorch/pytorch/pull/37131))\r\n* Add links to more subdir READMEs in CONTRIBUTING.md ([#38049](https://github.com/pytorch/pytorch/pull/38049))\r\n* `torch.isclose`: Adds missing documentation . ([#37295](https://github.com/pytorch/pytorch/pull/37295))\r\n* Improve checkpoint docs to warn users about detached gradient issues ([#37266](https://github.com/pytorch/pytorch/pull/37266))\r\n* Add documentation about multithread autograd ([#37020](https://github.com/pytorch/pytorch/pull/37020))\r\n* `tensor.view` doc improved ([#36728](https://github.com/pytorch/pytorch/pull/36728))\r\n* `nn.MultiheadAttention`: Fixed typo in documentation ([#37496](https://github.com/pytorch/pytorch/pull/37496))\r\n*  `contribution_guide.rst` and `governance.rst` : fixed broken links ([#37820](https://github.com/pytorch/pytorch/pull/37820))\r\n*  `nn.Hardsigmoid` and `nn.functional.hardsigmoid` documentation added ([#38120](https://github.com/pytorch/pytorch/pull/38120))\r\n* `nn.FeatureAlphaDropout` documentation added ([#36295](https://github.com/pytorch/pytorch/pull/36295))\r\n* `nn.functional.hardwish` documentation added ([#37989](https://github.com/pytorch/pytorch/pull/37989))\r\n* `torch.bucketize`, `torch.searchsorted` documentation added ([#38119](https://github.com/pytorch/pytorch/pull/38119))\r\n* Documented bfloat16 dtype and BFloat16Tensor ([#37051](https://github.com/pytorch/pytorch/pull/37051))\r\n* `nn.Linear` fix sample code ([#38002](https://github.com/pytorch/pytorch/pull/38002))\r\n* `torch.index_add`: add missing args for index_add ([#38213](https://github.com/pytorch/pytorch/pull/38213))\r\n* `Tensor.is_nonzero` doc added ([#37845](https://github.com/pytorch/pytorch/pull/37845))\r\n* `functional.upsample`: Correct upsample doc to match interpolation ([#38455](https://github.com/pytorch/pytorch/pull/38455))\r\n* `nn.CTCLoss`: added target un-pad example ([#38393](https://github.com/pytorch/pytorch/pull/38393))\r\n* `nn.SyncBatchNorm`: improved doc ([#38423](https://github.com/pytorch/pytorch/pull/38423), [#38890](https://github.com/pytorch/pytorch/pull/38890), [#39646](https://github.com/pytorch/pytorch/pull/39646))\r\n* `torch.utils.cmake_prefix_path` documented ([#38727](https://github.com/pytorch/pytorch/pull/38727))\r\n*  `torch.logcumsumexp`: fixed formula ([#38952](https://github.com/pytorch/pytorch/pull/38952))\r\n* Add C++ Landing Page ([#38450](https://github.com/pytorch/pytorch/pull/38450))\r\n* use version number instead of 'master' in html header title ([#38149](https://github.com/pytorch/pytorch/pull/38149))\r\n* Docs fix: Added missing indent ([#35017](https://github.com/pytorch/pytorch/pull/35017))\r\n* Fix many doc issues ([#37099](https://github.com/pytorch/pytorch/pull/37099))\r\n* docs: Fixed docstring indentation for documentation ([#37739](https://github.com/pytorch/pytorch/pull/37739))\r\n* Remove old section of the aten doc that is not true anymore ([#35807](https://github.com/pytorch/pytorch/pull/35807))\r\n* Removed Python 2 references ([#36336](https://github.com/pytorch/pytorch/pull/36336))\r\n* Fix multiline signatures in docstring ([#38768](https://github.com/pytorch/pytorch/pull/38768))\r\n* Removed java documentation ([#38920](https://github.com/pytorch/pytorch/pull/38920))\r\n* Fix missing code in 'Installing C++ distribution of Pytorch' ([#39237](https://github.com/pytorch/pytorch/pull/39237))\r\n* `nn.MultiheadAttention`: Update `key_padding_mask` arg docs ([#39321](https://github.com/pytorch/pytorch/pull/39321))\r\n* `torch.squeeze`, `torch.split`, `torch.set_printoption`, `torch.save` docs updated. ([#39303](https://github.com/pytorch/pytorch/pull/39303))\r\n* `utils.cpp_extension`: correct some usages and docs ([#39766](https://github.com/pytorch/pytorch/pull/39766))\r\n* `nn.BatchNorm`, `nn.InstanceNorm`: Clarify that variance estimator is biased for normalization layers ([#39752](https://github.com/pytorch/pytorch/pull/39752))\r\n* Remove duplicated entries in `random.rst` ([#39725](https://github.com/pytorch/pytorch/pull/39725))\r\n* Fix `Tensor.tolist` signature in the docstring ([#39732](https://github.com/pytorch/pytorch/pull/39732)\r\n* `torch.save`: added note ([#40394](https://github.com/pytorch/pytorch/pull/40394))\r\n* Update docs feature classifications (#40539)\r\n* Fix autograd doc subsubsection display issue (#40796)\r\n* Add specific list of supported types in autograd ([#38325](https://github.com/pytorch/pytorch/pull/38325))\r\n\r\n### C++ API\r\n\r\n* Added C++ autograd APIs and C++ Tensor indexing docs ([#35777](https://github.com/pytorch/pytorch/pull/35777))\r\n\r\n### Distributed\r\n\r\n* `torch.distributed.launch`: Include launcher script docs in the distributed doc page ([#40963](https://github.com/pytorch/pytorch/pull/40963))\r\n* `torch.nn.parallel.DistributedDataparallel`: Enhance DDP doc strings for DDP + RPC support. ([#39916](https://github.com/pytorch/pytorch/pull/39916))\r\n\r\n### Quantization\r\n\r\n* Minor fix to quantized conv docstring ([#35134](https://github.com/pytorch/pytorch/pull/35134))\r\n* quant docs: clean up hardswish ([#40323](https://github.com/pytorch/pytorch/pull/40323))\r\n* quant docs: add and clean up hardsigmoid ([#40340](https://github.com/pytorch/pytorch/pull/40340))\r\n* quant docs: add and clean up hardtanh ([#40341](https://github.com/pytorch/pytorch/pull/40341))\r\n* quant docs: add and clean up LayerNorm ([#40342](https://github.com/pytorch/pytorch/pull/40342))\r\n* quant docs: add and clean up GroupNorm ([#40343](https://github.com/pytorch/pytorch/pull/40343))\r\n* quant docs: add and clean up InstanceNorm{n}d ([#40345](https://github.com/pytorch/pytorch/pull/40345))\r\n* quant docs: add and clean up BatchNorm{n}d ([#40346](https://github.com/pytorch/pytorch/pull/40346))\r\n* quant docs: add and clean up ELU ([#40377](https://github.com/pytorch/pytorch/pull/40377))\r\n* Docstring changes for dynamic quantized classes (#40931)\r\n\r\n### RPC\r\n\r\n* `torch.distributed.rpc.RRef`:Updated RRef docs to indicate RPC Retries ([#36678](https://github.com/pytorch/pytorch/pull/36678))\r\n* `torch.distributed.rpc`: Add pointer to RPC parameter server tutorial ([#37667](https://github.com/pytorch/pytorch/pull/37667))\r\n* `torch.distributed.rpc`: Add TensorPipe RPC Backend documents ([#39467](https://github.com/pytorch/pytorch/pull/39467), [#40222](https://github.com/pytorch/pytorch/pull/40222))\r\n* `torch.distributed.rpc`: Fix `ProcessGroupRpcBackendOptions` Doc ([#39787](https://github.com/pytorch/pytorch/pull/39787))\r\n* `torch.distributed.rpc`: fix RPC reference in top-level index ([#40077](https://github.com/pytorch/pytorch/pull/40077))\r\n* `torch.futures`: Add torch.futures to API docs ([#40051](https://github.com/pytorch/pytorch/pull/40051))\r\n* `torch.distributed.rpc`: Fix typos in RPC Docs ([#40219](https://github.com/pytorch/pytorch/pull/40219))\r\n* `torch.futures`: Improve torch.futures docs ([#40245](https://github.com/pytorch/pytorch/pull/40245))\r\n* `torch.distributed.rpc`: Minor improvements for RPC documents ([#40296](https://github.com/pytorch/pytorch/pull/40296), [#40298](https://github.com/pytorch/pytorch/pull/40298), [#40299](https://github.com/pytorch/pytorch/pull/40299), [#40300](https://github.com/pytorch/pytorch/pull/40300), [#40305](https://github.com/pytorch/pytorch/pull/40305), [#35809](https://github.com/pytorch/pytorch/pull/35809))\r\n* `torch.distributed.rpc.functions.async_execution`: Add a warning to mention that async_execution does not work with autograd profiler ([#40309](https://github.com/pytorch/pytorch/pull/40309))\r\n* `torch.distributed.rpc.functions.async_execution`: Add examples and tests for combining static/class method with async execution (#40619) ([#40619](https://github.com/pytorch/pytorch/pull/40619))\r\n* `torch.distributed`: Add a link in RPC doc page to point to PT Distributed overview ([#41108](https://github.com/pytorch/pytorch/pull/41108)) \r\n\r\n### TorchScript\r\n\r\n* Remove import statement reference in serialization docs ([#38578](https://github.com/pytorch/pytorch/pull/38578))\r\n* Fix Markdown in overview.md for proper rendering of `Node*` ([#37686](https://github.com/pytorch/pytorch/pull/37686))\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.6.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.6.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.6.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/29030094", "dateCreated": "2020-07-24T22:18:09Z", "datePublished": "2020-07-28T17:13:18Z"}, {"tagName": "v1.5.1", "name": "Bug Fix release", "authorName": "zou3519", "authorType": "User", "body": "# PyTorch 1.5.1 Release Notes\r\n- Backwards Incompatible Changes\r\n- Known Issues and Workarounds\r\n- Critical Fixes\r\n- Crashes and Error Fixes\r\n- Other Fixes\r\n\r\n# Backwards Incompatible Changes\r\n\r\n### Autograd: Operations that return integer-type tensors now always returns tensors that don\u2019t require grad  (#37789).\r\n\r\nThis most notably affects `torch.argmin`, `torch.argmax`, and `torch.argsort`. This change is BC-Breaking because previously one could obtain an integer-type tensor that requires grad in 1.5.0. However, said tensors were not usable by autograd; calling `.backward()` on them resulted in an error, so most users are likely to not have been relying on this behavior.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> torch.argmax(tensor).requires_grad\r\nTrue\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> torch.argmax(tensor).requires_grad\r\nFalse\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n# Known Issues and Workarounds\r\n\r\n### When using multiprocessing, PyTorch 1.5.1 and 1.5.0 may error out with complaints about incompatibility between MKL and libgomp (#37377)\r\n\r\nYou may see error messages like the following when using the `torch.multiprocessing` package. This bug has primarily affected users with AMD CPUs.\r\n\r\n```\r\n`Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\r\n        Try to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.`\r\n```\r\n\r\nYou can get rid of the error and the error message by setting the environment `MKL_THREADING_LAYER=GNU`. This can be done either by including the following in your python code:\r\n\r\n```\r\nimport os\r\nos.environ['MKL_THREADING_LAYER'] = 'GNU'\r\n```\r\n\r\nor by specifying the environment variable when running your script:\r\n\r\n```\r\nMKL_THREADING_LAYER=GNU python my_script.py\r\n```\r\n\r\nTo learn more about what triggers this bug and other workarounds if the above isn\u2019t working, please [read this comment on the issue](https://github.com/pytorch/pytorch/issues/37377#issuecomment-629610327).\r\n\r\n\r\n# Critical Fixes\r\n\r\n### [`torch.multinomial`:](https://pytorch.org/docs/stable/torch.html#torch.multinomial) Fixed a bug where CUDA `multinomial` generated the same sequence over and over again with a shift of 4. (#38046)\r\n\r\n### [`nn.Conv2d`](https://pytorch.org/docs/stable/nn.html#conv2d): Fixed a bug where circular padding applied padding across the wrong dimension (#37881)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> circular = nn.Conv2d(6, 1, (3, 3), padding=(0, 1), padding_mode='circular')\r\n>>> circular(torch.zeros(1, 6, 10, 10)).shape\r\n# Notice the padding is incorrectly on the H dimension, not the W dimension.\r\ntorch.Size([1, 1, 10, 8])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> other = tensor + 1\r\n>>> output = nn.LeakyReLU(0, inplace=True)(other)\r\n>>> output.sum().backward()\r\ntorch.Size([1, 1, 8, 10])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Fixed bug where asserts in CUDA kernels were mistakingly disabled, leading to many silent kernel errors. (#38943, #39047, #39218)\r\n\r\n### [`torch.gather`](https://pytorch.org/docs/stable/torch.html#torch.gather), [`torch.scatter`](https://pytorch.org/docs/stable/torch.html#torch.scatter): added checks for illegal input dtypes that caused silently incorrect behaviors (#38025, #38646)\r\n\r\n### [`torch.argmin`](https://pytorch.org/docs/stable/torch.html#torch.argmin), [`torch.argmax`](https://pytorch.org/docs/stable/torch.html?highlight=argmax#torch.argmax): Fixed silently incorrect result for inputs with more than 2^32 elements (#39212)\r\n\r\n### C++ Custom Operators: fixed a bug where custom operators stopped working with autograd and ignored the `requires_grad=True` flag. (#37355)\r\n\r\n\r\n\r\n## Crashes and Error Fixes\r\n\r\n### Fixed CUDA reduction operations on inputs with more than 2^32 elements (#37788)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> `torch.zeros(5, 14400, 14400, device='cuda').sum(0)`\r\n`RuntimeError: sub_iter.strides(0)[0] == 0 INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Reduce.cuh:706, please report a bug to PyTorch.`      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.zeros(5, 14400, 14400, device='cuda').sum(0)\r\n# No problem\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### Fixed pickling of PyTorch operators (#38033)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> `pickle.dumps(torch.tanh)`\r\nPicklingError: Can't pickle <class 'torch._C._VariableFunctions'>: it's not the same object as torch._C._VariableFunctions\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> pickle.dumps(torch.tanh)\r\n# No problem\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### [`nn.LeakyReLU`](https://pytorch.org/docs/stable/nn.html?highlight=leaky#torch.nn.LeakyReLU): Fixed a bug where using autograd with in-place `nn.LeakyReLu` with a slope of 0 incorrectly errored out. (#37453, #37559)\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> other = tensor + 1\r\n>>> output = nn.LeakyReLU(0, inplace=True)(other)\r\n>>> output.sum().backward()\r\nRuntimeError: In-place leakyReLu backward calculation is triggered with a non-positive slope which is not supported. This is caused by calling in-place forward function with a non-positive slope, please call out-of-place version instead.\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.randn(3, requires_grad=True)\r\n>>> other = tensor + 1\r\n>>> output = nn.LeakyReLU(0, inplace=True)(other)\r\n>>> output.sum().backward()\r\n# No error\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### [`torch.as_strided`](https://pytorch.org/docs/stable/torch.html#torch.as_strided) : Fixed crash when passed `sizes` and `strides` of different lengths. (#39301)\r\n\r\n### [`nn.SyncBatchNorm.convert_sync_batchnorm`](https://pytorch.org/docs/stable/nn.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm): Fixed bug where it did not respect the devices of the original BatchNorm module, resulting in device mismatch errors (#39344)\r\n\r\n### [`nn.utils.clip_grad_norm_`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_): Fixed ability to operate on tensors on different devices (#38615)\r\n\r\n### [`torch.min`](https://pytorch.org/docs/stable/torch.html#torch.min), [`torch.max`](https://pytorch.org/docs/stable/torch.html#torch.max): added check for illegal output dtypes (#38850)\r\n\r\n### MacOS: Fixed `import torch` error (#36941).\r\n\r\n### C++ Extensions: fixed compilation error when building with older versions of nvcc (#37221)\r\n\r\nThis bug mainly affected users of ubuntu 16.04. We\u2019re certain it affected the following configurations:\r\n\r\n* ubuntu 16.04 + cuda 9.2 + gcc 5\r\n* ubuntu 16.04 + cuda 9.2 + gcc 7\r\n* ubuntu 16.04 + cuda 10.0 + gcc 5\r\n\r\n### C++ Extensions: fixed ability to compile with paths that include spaces (#38860, #38670)\r\n\r\n### C++ Extensions: fixed ability to compile with relative `include_dirs` for ahead-of-time compilation (#38264)\r\n\r\n\r\n\r\n# Other Fixes\r\n\r\n### [`nn.Conv1d`](https://pytorch.org/docs/stable/nn.html#conv1d), [`nn.Conv2d`](https://pytorch.org/docs/stable/nn.html#conv2d), [`nn.Conv3d`](https://pytorch.org/docs/stable/nn.html#conv3d): Fixed a bug where convolutions were using more memory than previous versions of PyTorch. (#38674)\r\n\r\n### Fixed in-place floor division magic method (#38695)\r\n\r\nIn 1.5.0, the in-place floor division magic method mistakingly performed the floor division out-of-place. We\u2019ve fixed this in 1.5.1. \r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.ones(1)\r\n>>> expected_data_ptr = tensor.data_ptr()\r\n>>> tensor //= 1\r\n>>> tensor.data_ptr() == expected_data_ptr\r\nFalse\r\n      <td><sub><pre lang=\"python\">\r\n>>> tensor = torch.ones(1)\r\n>>> expected_data_ptr = tensor.data_ptr()\r\n>>> tensor //= 1\r\n>>> tensor.data_ptr() == expected_data_ptr\r\nTrue\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Documentation: fixed link to java docs. (#39039)\r\n\r\n### Quantization: Fixed weight quantization inaccuracies for LSTM (#35961)\r\n\r\nWeight quantization was done incorrectly for LSTMs, the statistics for all weights (across layers) were combined in the observer. This meant that weights for later layers in a LSTM would use sub-optimal scales impacting accuracy. The problem gets worse as the number of layers increases.\r\n\r\n### DistributedDataParallel: Fixed single-process multi-GPU use case (#36503)\r\n\r\n### RPC: Fixed future callbacks not capturing and restoring autograd context id (#38512)\r\n\r\n### TorchScript: Fixed support with [`torch.unique`](https://pytorch.org/docs/stable/torch.html#torch.unique) (#38156)\r\n\r\n### ONNX: Fix `pow` operator export (#39791)\r\n\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.5.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.5.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.5.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/27656022", "dateCreated": "2020-06-11T22:26:46Z", "datePublished": "2020-06-18T16:43:46Z"}, {"tagName": "v1.5.0", "name": "Stable C++ Frontend, Distributed RPC framework, and more. New experimental higher-level autograd API, Channels Last memory format, and more.", "authorName": "zou3519", "authorType": "User", "body": "# PyTorch 1.5.0 Release Notes\r\n\r\n* Highlights\r\n* Known Issues\r\n* Backwards Incompatible Changes\r\n    * Python\r\n    * C++ API\r\n    * JIT\r\n    * Quantization\r\n    * RPC\r\n* New Features\r\n* Improvements\r\n* Bug Fixes\r\n* Performance\r\n* Documentation\r\n* Deprecations\r\n    * Python\r\n    * C++ API\r\n* Miscellaneous\r\n\r\n# Highlights\r\n\r\nThis release includes several major new API additions and improvements. These include new APIs for autograd allowing for easy computation of hessians and jacobians, a significant update to the C++ frontend, \u2018channels last\u2019 memory format for more performant computer vision models, a stable release of the distributed RPC framework used for model parallel training, and a new API that allows for the creation of Custom C++ Classes that was inspired by PyBind. Additionally `torch_xla` 1.5 is now available and tested with the PyTorch 1.5 release providing a mature Cloud TPU experience. \r\n\r\n### C++ Frontend API [Now Stable]\r\n\r\nThe C++ frontend API is now at parity with Python and the features overall has been moved to \u2018stable\u2019. (previously tagged as experimental). Some of the major highlights include:\r\n\r\n* C++ torch::nn module/functional are now at ~100% parity with Python API, with appropriate documentation. Now users can easily translate their model from Python API to C++ API, making the model authoring experience much smoother.\r\n* C++ optimizers now behave identically to the Python API. In the past, optimizers in C++ had deviated from the Python equivalent: C++ optimizers couldn\u2019t take parameter groups as input while the Python ones could. Also step function implementations were not exactly the same. With the 1.5 release, C++ optimizers will always behave the same as the Python equivalent.\r\n* New C++ tensor multi-dim indexing API which looks and behaves the similar to the Python API. The previous workaround was to use a combination of `narrow` / `select` / `index_select` / `masked_select`, which is clunky and error-prone compared to the Python API\u2019s elegant `tensor[:, 0, ..., mask]` syntax. With the 1.5 release users can use `tensor.index({Slice(), 0, \"...\", mask})` to achieve the same result.\r\n\r\n### Channels last memory format for Computer Vision models [Experimental]\r\n\r\nChannels Last memory format is an alternative way of ordering NCHW tensors in memory while preserving the NCHW semantic dimensions ordering. Channels Last tensors are ordered in memory in such a way that channels become the densest dimension (aka storing images pixel-per-pixel).\r\n\r\nChannels Last memory format unlocks the ability to use performance efficient convolution algorithms and hardware (NVidia\u2019s Tensor Cores, FBGEMM, QNNPACK). Additionally it was designed to automatically propagate through the operators, which allows easy switching between memory layouts.\r\n\r\nLearn more [here](https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators) on how to write memory format aware operators.\r\n\r\n### Custom C++ Classes [Experimental]\r\n\r\nThis release adds a new API for binding custom C++ classes into TorchScript and Python simultaneously. This API is almost identical in syntax to [pybind11](https://pybind11.readthedocs.io/en/stable/). It allows users to expose their C++ class and its methods to the TorchScript type system and runtime system such that they can instantiate and manipulate arbitrary C++ objects from TorchScript and Python. An example C++ binding:\r\n\r\n```\r\ntemplate <class T>\r\nstruct MyStackClass : torch::CustomClassHolder {\r\n  std::vector<T> stack_;\r\n  MyStackClass(std::vector<T> init) : stack_(std::move(init)) {}\r\n\r\n  void push(T x) {\r\n    stack_.push_back(x);\r\n  }\r\n  T pop() {\r\n    auto val = stack_.back();\r\n    stack_.pop_back();\r\n    return val;\r\n  }\r\n};\r\n\r\nstatic auto testStack =\r\n  torch::class_<MyStackClass<std::string>>(\"myclasses\", \"MyStackClass\")\r\n      .def(torch::init<std::vector<std::string>>())\r\n      .def(\"push\", &MyStackClass<std::string>::push)\r\n      .def(\"pop\", &MyStackClass<std::string>::pop)\r\n      .def(\"size\", [](const c10::intrusive_ptr<MyStackClass>& self) {\r\n        return self->stack_.size();\r\n      });\r\n```\r\n\r\nWhich exposes a class you can use in Python and TorchScript like so:\r\n\r\n```\r\n@torch.jit.script\r\ndef do_stacks(s : torch.classes.myclasses.MyStackClass):\r\n    s2 = torch.classes.myclasses.MyStackClass([\"hi\", \"mom\"])\r\n    print(s2.pop()) # \"mom\"\r\n    s2.push(\"foobar\")\r\n    return s2 # [\"hi\", \"foobar\"]\r\n```\r\n\r\nYou can try it out in the tutorial [here](https://pytorch.org/tutorials/advanced/torch_script_custom_classes.html).\r\n\r\n\r\n### Distributed RPC framework APIs [Now Stable]\r\n\r\nThe `torch.distributed.rpc` package aims at supporting a wide range of distributed training paradigms that do not fit into `DistributedDataParallel`. Examples include parameter server training, distributed model parallelism, and distributed pipeline parallelism. Features in the `torch.distributed.rpc` package can be categorized into four main sets of APIs.\r\n\r\n* The **RPC** API allows running a function on a specified destination worker with given arguments and fetches the return value or creates a distributed reference to the return value. \r\n* The **RRef** (Remote REFerence) serves as a reference to an object on another worker. A worker holding an RRef can explicitly request copies of the object, and it can also share the light-weight RRef with other workers without worrying about reference counting. This is especially useful when multiple workers need to repeatedly access different versions of the same remote object. \r\n* With **Distributed Autograd**, applications can automatically compute gradients even if a model is split on multiple workers using RPC. This is achieved by stitching together local autograd graphs at RPC boundaries in the forward pass and reaching out to participants to transparently launch local autograd in the backward pass. \r\n* The **Distributed Optimizer** uses gradients computed by Distributed Autograd to update model parameters. Its constructor takes a local optimizer (e.g., `SGD`, `Adagrad`, etc.) and a list of parameter RRefs, and its `step()` function automatically uses the local optimizer to update parameters on all distinct RRef owner workers.\r\n\r\nLearn more [here](https://pytorch.org/docs/stable/rpc.html).\r\n\r\n\r\n### **torch_xla 1.5 now available**\r\n\r\n[torch_xla](http://pytorch.org/xla/) is a Python package that uses the [XLA linear algebra compiler](https://www.tensorflow.org/xla) to accelerate the [PyTorch deep learning framework](https://pytorch.org/) on [Cloud TPUs](https://cloud.google.com/tpu/) and [Cloud TPU Pods](https://cloud.google.com/tpu/docs/tutorials/pytorch-pod). torch_xla aims to give PyTorch users the ability to do everything they can do on GPUs on Cloud TPUs as well while minimizing changes to the user experience. This release of [torch_xla](http://pytorch.org/xla/) is aligned and tested with PyTorch 1.5 to reduce friction for developers and to provide a stable and mature PyTorch/XLA stack for training models using Cloud TPU hardware. You can [try it for free](https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc) in your browser on an 8-core Cloud TPU device with [Google Colab](https://colab.research.google.com/), and you can use it at a much larger scale [on Google Cloud](https://cloud.google.com/gcp).\r\n\r\nSee the full torch_xla release notes [here](https://github.com/pytorch/xla/releases) and the full docs [here](https://pytorch.org/xla/).\r\n\r\n\r\n### **New High level autograd API [Experimental]**\r\n\r\nPyTorch 1.5 brings new functions including jacobian, hessian, jvp, vjp, hvp and vhp to the `torch.autograd.functional.*` submodule. This feature builds on the current API and allow the user to easily perform these functions. \r\n\r\nSee the full docs [here](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api).\r\n\r\n\r\n### Python 2 no longer supported\r\n\r\nFor PyTorch 1.5.0 we will no longer support Python 2, specifically version 2.7. Going forward support for Python will be limited to Python 3, specifically Python 3.5, 3.6, 3.7 and 3.8 (first enabled in PyTorch 1.4.0). \r\n\r\n\r\n# Known Issues\r\n\r\n### `torch.nn.parallel.DistributedDataParallel` does not work in Single-Process Multi-GPU mode.\r\n\r\n`DistributedDataParallel` (DDP) used to support two modes \r\n\r\n1. Single-Process Multi-GPU (SPMG): In this mode, each DDP process replicates the input `module` to all specified devices and trains on all `module` replicas. This mode is enabled when application passes in a `device_ids` argument that contains multiple devices. Or if `device_ids` is not presented, DDP will try to use all available devices. \r\n2. Multi-Process Single-GPU (MPSG): This is the **recommended** mode, as it is faster than SPMG. In this mode, each DDP process directly works on the provided `module` without creating additional replicas. This mode is enabled when `device_ids` only contains a single device or if there is only one visible device (e.g., by setting `CUDA_VISIBLE_DEVICES`). \r\n\r\nA recent change (#33907)  in `torch.nn.parallel.replicate` breaks DDP\u2019s assumption on replicated modules and leads to failures in the SPMG mode. However, since SPMG is known to be slower due to GIL contention and additional overhead caused by scattering input and gathering output, we are planning to retire this mode in future releases and make MPSG the only supported mode in DDP. The code below shows an example of the recommended way to construct DDP.\r\n\r\n\r\n```\r\nimport torch\r\nfrom torch.nn.parallel import DistributedDataParallel as DDP\r\n\r\n# use \"cuda:1\" as the target device\r\ntarget_device = 1 \r\nlocal_model = torch.nn.Linear(2, 2).to(target_device)\r\nddp_model = DDP(local_model, device_ids=[target_device])\r\n```\r\n\r\n\r\nSee [#36268](https://github.com/pytorch/pytorch/issues/36268) for more discussion.\r\n\r\n\r\n### `Tensor.exponential_(0)` used to return `Inf`, now it incorrectly returns `0` \r\n\r\nPreviously in 1.4, `x.exponential_(0)` gives a tensor full of `inf`. On 1.5.0, it wrongly gives a tensor full of zeros.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.randn(3).exponential_(0)\r\ntensor([inf, inf, inf])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.randn(3).exponential_(0)\r\n# This is wrong!\r\ntensor([0., 0., 0.])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nSee [#36798](https://github.com/pytorch/pytorch/issues/36798) for more details\r\n\r\n\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## Python\r\n\r\n### `Tensor.clone`, `Tensor.to`, `Tensor.empty_like`, and similar functions preserve stride information instead of returning contiguous tensors\r\n\r\n`clone`, `to`, `type`, `cuda`, `cpu`, `byte`, `char`, `double`, `bool`, `half`, `int`, `long`, `short`, `float`, `bfloat16`, `empty_like`, `full_like`, `ones_like`, `zeros_like`, `rand_like`, `randn_like`, `randint_like` operators now propagate memory format (roughly, the strides) of the input tensor to the output tensor.\r\n\r\nSince PyTorch operators generally support non-contiguous tensors, this should have no functional effect on most PyTorch programs.\r\n\r\nThe most common incompatibility with Python programs is with the `view` operator, which has specific stride requirements. If these requirements are no longer met as a result of this change, you will get an error message indicating that you should use reshape instead, i.e. \"RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\"\r\n\r\nAnother possible exception incompatibility is if you have a (usually) C++ operator implementation that works directly on memory (i.e. calls data_ptr and relies on the strides being contiguous).\r\n\r\n In the following example, we go through the implementation of a simple `clone` operation and see how it needs to change between versions.\r\n\r\n```\r\n# Version 1.4.0\r\nTensor simple_clone(const Tensor& input) {\r\n    TORCH_CHECK(input.dim() == 1);\r\n    auto output = at::empty_like(input);\r\n    auto input_stride = input.strides()[0];\r\n    auto* output_ptr = output.data_ptr<float>();\r\n    auto* input_ptr = input.data_ptr<float>();\r\n    // Before 1.5.0, the result of `empty_like` is always contiguous.\r\n    for (int64_t idx = 0; idx < input.size(); idx++) {\r\n        output[idx] = input[idx * input_stride]\r\n    }\r\n}\r\n```\r\n\r\n```\r\n# Version 1.5.0\r\nTensor simple_clone(const Tensor& input) {\r\n    TORCH_CHECK(input.dim() == 1);\r\n    // From 1.5.0 on, the result of `empty_like` may not be contiguous.\r\n    auto output = at::empty_like(input);\r\n    \r\n    // As a result, we need to keep track of the output stride.\r\n    auto input_stride = input.strides()[0];\r\n    auto output_stride = output.strides()[0];\r\n    auto* output_ptr = output.data_ptr<float>();\r\n    auto* input_ptr = input.data_ptr<float>();\r\n    for (int64_t idx = 0; idx < input.size(); idx++) {\r\n        output[idx * output_stride] = input[idx * input_stride]\r\n    }\r\n}\r\n\r\n```\r\n\r\n\r\n### The inferred dtype of np.float_, np.float64 scalars in tensor constructors (e.g. torch.tensor(...), torch.as_tensor(...) is now torch.float64 instead of the default dtype (usually torch.float32). (#30486 (https://github.com/pytorch/pytorch/pull/30486))\r\n\r\nPlease explicitly pass in the desired dtype when constructing tensors with NumPy float64 scalars to get the old behavior.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n# Old behavior: return torch.float32 tensor (by default)\r\n>>> torch.tensor(np.float64(0))\r\ntensor(0.)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# To keep the old behavior, please explicitly pass the dtype\r\n >>> torch.tensor(np.float64(0), dtype=torch.get_default_dtype())\r\ntensor(0.)\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThis can cause your program to execute in torch.float64, potentially slowing down your program or can lead to errors for operators that don't support torch.float64 or mixed-dtypes.\r\n\r\nnumpy integer scalars are now treated as integers for the purposes of type promotion (#30486 (https://github.com/pytorch/pytorch/pull/30486))\r\n\r\nPreviously, in 1.4.0, they were mistakenly treated as floats (so for example, torch.ones(3) * np.int64(3) would return a float32 tensor. In 1.5.0, we\u2019ve fixed that behavior; torch.ones(3) * np.int64(3) returns an int32 tensor.\r\n\r\nThis can cause your code to fail if you performed operations between PyTorch tensors and numpy scalars and then passed the result into an operation that does not support integral types or mixed types. To fix your code, please cast the resulting tensor to the desired dtype.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.ones(3) * np.int64(3)\r\ntensor([3., 3., 3.])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> (torch.ones(3) * np.int64(3)).float()\r\ntensor([3., 3., 3.])\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n###  numpy integer scalars are now treated as integers for the purposes of type promotion (#30486)\r\n\r\nPreviously, in 1.4.0, they were mistakenly treated as floats (so for example, `torch.ones(3) * np.int64(3)` would return a float32 tensor. In 1.5.0, we\u2019ve fixed that behavior; `torch.ones(3) * np.int64(3)` returns an int32 tensor.\r\n\r\nThis can cause your code to fail if you performed operations between PyTorch tensors and numpy scalars and then passed the result into an operation that does not support integral types or mixed types. To fix your code, please cast the resulting tensor to the desired dtype.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.ones(3) * np.int64(3)\r\ntensor([3., 3., 3.])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> (torch.ones(3) * np.int64(3)).float()\r\ntensor([3., 3., 3.])\r\n    </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `torch.autograd.Function`: dropped support for old-style Functions ([#33956](https://github.com/pytorch/pytorch/pull/33956)).\r\n\r\nIn previous versions of PyTorch, there were two ways to write autograd Functions. We deprecated one of them in 1.3.0 and dropped support for it entirely in 1.5.0. Old-style autograd Functions will no longer work in user code.\r\n\r\nThese Functions be identified by not having `staticmethod` `forward` and `backward` functions (see the example below) Please see [the current documentation](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) for how to write new-style Functions.\r\n\r\n```\r\n# Version 1.4.0\r\nclass Exp(torch.autograd.Function):\r\n    def forward(self, i):\r\n        result = i.exp()\r\n        self.save_for_backward(result)\r\n        return result\r\n\r\n    def backward(self, grad_output):\r\n        result, = self.saved_tensors\r\n        return grad_output * result\r\n\r\nExp()(torch.tensor(1.))\r\n```\r\n\r\n```\r\n# Version 1.5.0\r\nclass Exp(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, i):\r\n        result = i.exp()\r\n        ctx.save_for_backward(result)\r\n        return result\r\n        \r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        result, = ctx.saved_tensors\r\n        return grad_output * result\r\n\r\nExp.apply(torch.tensor(1.))   \r\n```\r\n\r\n\r\n\r\n### `torch.optim` optimizers changed to fix in-place checks for the changes made by the optimizer  ([#33640](https://github.com/pytorch/pytorch/pull/33640), [#34211](https://github.com/pytorch/pytorch/pull/34211))\r\n\r\nIf this causes your code to fail, there are two possible reasons:\r\n\r\nReason 1: The value of that parameter was actually saved and used and we were computing incorrect gradients in previous versions of PyTorch. This would result in an error message mentioning incorrect version numbers. You should replace code that uses `self.my_param` by `self.my_param.clone()` to make sure the saved version is different from the one that is modified by the optimizer. For example:\r\n\r\nBefore 1.5.0, the following may have worked.\r\n\r\n```\r\ndef model(input, target, param):\r\n    return `(input * param ** 2 - target).norm()`\r\n\r\nparam = torch.randn(2, requires_grad=True)\r\ninput = torch.randn(2)\r\ntarget = torch.randn(2)\r\nsgd = optim.SGD([param], lr=0.001)\r\nloss = model(input, target, param)\r\nloss.backward(retain_graph=True)\r\nsgd.step()\r\nloss.backward()\r\nparam.grad\r\n```\r\n\r\n\r\nIf after upgrading to 1.5.0, the above fails due to a version counter error, then that means the gradient computed was incorrect. To remedy this, clone `param` before using it in the model:\r\n\r\n```\r\ndef model(input, target, param):\r\n    return (input * param ** 2 - target).norm()\r\n\r\nparam = torch.randn(2, requires_grad=True)\r\ninput = torch.randn(2)\r\ntarget = torch.randn(2)\r\nsgd = optim.SGD([param], lr=0.001)\r\nloss = model(input, target, param.clone())\r\nloss.backward(retain_graph=True)\r\nsgd.step()\r\nloss.backward()\r\nparam.grad\r\n```\r\n\r\n\r\nReason 2: You know what you're doing and change the values back to the right thing before the next backward. However, you're running into an error because the version counter cannot be decremented. Open an issue with your particular use case and we will help you to work around the version counter issue.\r\n\r\n\r\n### `utils.cpp_extensions` now use `ninja` as the default compilation backend ([#32495](https://github.com/pytorch/pytorch/pull/32495))\r\n\r\n`ninja` enables parallel compilation of your C++ extension, greatly speeding up compilation. This change will not break most user code; if you do not have `ninja` installed, we fallback to the old `distutils` backend.\r\n\r\nHowever, if you do have `ninja` installed, it is possible that this change will cause your C++ extension build to fail by oversubscribing your system with too many worker processes. There are two potential workarounds to this.\r\n\r\nMethod 1: If a previously succeeding `python setup.py install` now fails, try setting the `MAX_JOBS` environment variable.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"sh\">\r\npython setup.py install\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"sh\">\r\nMAX_JOBS=2 python setup.py install\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nMethod 2: Switch back to the old `distutils` backend inside your `setup.py`\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ncmdclass={'clean': clean,\r\n          'build_ext': BuildExtension},\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\ncmdclass={'clean': clean,\r\n          'build_ext': BuildExtension.with_options(use_ninja=False)},\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `torch.optim.Adam`, `torch.optim.SGD`  changed to not modify gradients in-place ([#30257](https://github.com/pytorch/pytorch/pull/30257))\r\n\r\nIn previous versions of PyTorch, the Adam and SGD optimizers modified gradients (e.g. `param.grad`) in-place via in-place addition of `params.grad += weight_decay * param`. To make this consistent with the behavior of other optimizers and to prevent surprises about the behavior, we\u2019ve changed them to stop modifying gradients in-place.\r\n\r\nThis should not have an effect on most PyTorch programs unless they relied on this behavior. The easiest way to replicate the old behavior is to create a custom optimizer that implements it.\r\n\r\n\r\n###  `torch.masked_select` now always returns a 1D tensor ([#29923](https://github.com/pytorch/pytorch/pull/29923))\r\n\r\nThe behavior of `torch.masked_select` when both \"self\" and \"mask\" are 0-dimensional was changed. In previous versions of PyTorch, this would return a 0-dimensional tensor. Now, we return a 1-dimensional tensor to be consistent with other input sizes and our documentation.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.masked_select(torch.tensor(0), torch.tensor(True))\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.masked_select(torch.tensor(0), torch.tensor(True))\r\ntensor([0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `torch.index_select` on a 0-d tensor now returns a 0-d tensor. ([#30790](https://github.com/pytorch/pytorch/pull/30790))\r\n\r\nIn previous versions of PyTorch, the output of `torch.index_select` on a 0D input tensor produced a 1D tensor. This was inconsistent with our documentation on it, which stated \"The returned tensor has the same number of dimensions as the original tensor (input).\" Now, we return a 0D tensor.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.index_select(torch.tensor(5), 0, torch.tensor([0]))\r\ntensor([5])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.index_select(torch.tensor(5), 0, torch.tensor([0]))\r\ntensor(5)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `nn.MultiLabelMarginLoss:` 'none' reduction on 1D tensor now returns a 0D tensor ([#30768](https://github.com/pytorch/pytorch/pull/30768))\r\n\r\nIn previous versions of PyTorch, the output of `nn.MultiLabelMarginLoss` on 1D and 0D tensors incorrectly produced 1-D tensors. Now, those cases return a 0D tensor to be consistent with the 2-D tensor case.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiLabelMarginLoss(reduction='none')(torch.randn(3), torch.zeros(3, dtype=torch.long))\r\ntensor([0.2959])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiLabelMarginLoss(reduction='none')(torch.randn(3), torch.zeros(3, dtype=torch.long))\r\ntensor(0.2959)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `nn.MultiMarginLoss:` \u2018none' reduction on 1D target now returns a 1D tensor ([#30826](https://github.com/pytorch/pytorch/pull/30826))\r\n\r\nIn previous versions of PyTorch, the output of `nn.MultiMarginLoss` on a 1D `target` tensor produced a 0D output. We changed this to return a 1D `target` tensor to make it consistent with other input sizes which return an output that matches the target shape.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiMarginLoss(reduction='none')(torch.tensor([1.]), torch.tensor([0]))\r\ntensor(0.)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> nn.MultiMarginLoss(reduction='none')(torch.tensor([1.]), torch.tensor([0]))\r\ntensor([0.])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### `Tensor.exponential_(lambda)` no longer supports `lambda < 0` ([#32501](https://github.com/pytorch/pytorch/pull/32501))\r\n\r\n`lambda`, the rate parameter of the exponential distribution, mathematically should be greater than 0. We\u2019ve disabled support `lambda < 0` to be mathematically correct; most users will not have used a lambda less than zero.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntensor = torch.empty(3).exponential_(-1.5)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# Negative lambda not supported!\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `nn.BCELoss`, `nn.functional.binary_cross_entropy` no longer accept inputs with the same number of elements that are not broadcastable ([#31365](https://github.com/pytorch/pytorch/pull/31365))\r\n\r\nPreviously, we supported accepting inputs with the same number of elements. However, this behavior was deprecated and we removed it in 1.5.0. In order to replicate the old behavior, please explicitly `reshape` your input and target tensors to have the same shape.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.rand(3, 3)\r\n>>> target = torch.randn(9)\r\n>>> torch.nn.functional.binary_cross_entropy(input, target)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.rand(3, 3)\r\n>>> target = torch.randn(9)\r\n>>> torch.nn.functional.binary_cross_entropy(input, target.reshape_as(input))\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n### `torch.normal` out argument is now required to have the same size as the computed output ([#32031](https://github.com/pytorch/pytorch/pull/32031))\r\n\r\nPreviously, on CPU devices, `torch.normal(mean, std, out=out)`  would resize `out` to the correct size. To be consistent with the CUDA implementation, we\u2019ve changed it so that `out` must either already have the correct size, or be an empty tensor with size `[0]`. To work around this, please ensure that your `out` tensor has the correct size.\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.normal(torch.zeros(3), torch.ones(3), out=torch.randn(2))\r\ntensor([ 0.0300,  0.7830, -1.3579])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.normal(torch.zeros(3), torch.ones(3), out=torch.randn(2))\r\nRuntimeError: inconsistent tensor, output size ([2]) is not the same as broadcasted mean and std size (3)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n### `Tensor.geometric_` no longer supports integral Tensors ([#31878](https://github.com/pytorch/pytorch/pull/31878))\r\n\r\nPreviously, on CPU devices, `Tensor.geometric_` supported Tensors with integral dtype. Now, it only supports floating point. We removed support for this because it doesn\u2019t make sense for `geometric_` to operate on integral dtypes.\r\n\r\n\r\n### Changed `torch.floor_divide` `input` positional argument name to `self`  ([#34552](https://github.com/pytorch/pytorch/pull/34552))\r\n\r\nBefore PyTorch 1.5, `torch.floor_divide` took two positional arguments: `torch.floor_divide(input, other)`. We\u2019ve changed the name of the `input` argument to `self`; this will break code that called `torch.floor_divide` via keyword argument. For example:\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\ntorch.floor_divide(input=x, other=y)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n# Either of the following works.\r\ntorch.floor_divide(self=x, other=y)\r\ntorch.floor_divide(x, y)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n\r\n## C++ API\r\n\r\n### RNN / GRU / LSTM layers ([#34322](https://github.com/pytorch/pytorch/pull/34322))\r\n\r\n* Instead of returning `RNNOutput`, RNN / GRU `forward` method now returns `std::tuple<Tensor, Tensor>`, and LSTM `forward` method now returns `std::tuple<Tensor, std::tuple<Tensor, Tensor>>`, matching Python API.\r\n* LSTM forward method\u2019s hidden state parameter now has type `torch::optional<std::tuple<Tensor, Tensor>>`, matching Python API.\r\n* RNN / LSTM / GRU layers now have `forward_with_packed_input` method which accepts `PackedSequence` as input and optionally hidden state, matching the `forward(PackedSequence, ...)` variant in Python API.\r\n* RNN / LSTM / GRU layers no longer have these fields: `w_ih` / `w_hh` / `b_ih` / `b_hh`. Instead, to access the weights and biases of the gates, users should do e.g. `rnn->named_parameters()[\"weight_ih_l0\"]`, which mirrors the Python API `rnn.weight_ih_l0`.\r\n* In `RNNOptions`\r\n    * `tanh()` / `relu()` / `activation` are removed. Instead, `nonlinearity` is added which takes either `torch::kTanh` or `torch::kReLU`\r\n    * `layers` is renamed to `num_layers`\r\n    * `with_bias` is renamed to `bias`\r\n* In `LSTMOptions`\r\n    * `layers` is renamed to `num_layers`\r\n    * `with_bias` is renamed to `bias`\r\n* In `GRUOptions`\r\n    * `layers` is renamed to `num_layers`\r\n    * `with_bias` is renamed to `bias`\r\n\r\n\r\n\r\n### Upsample layer / F::interpolate function ([#35025](https://github.com/pytorch/pytorch/pull/35025))\r\n\r\n* There are changes to `UpsampleOptions` and `InterpolateFuncOptions`:\r\n    * `size` is changed from `std::vector<int64_t>` to `c10::optional<std::vector<int64_t>>`. If you want to pass a list of `int64_t` to this argument, you must pass it as `std::vector<int64_t>`.\r\n    * `scale_factor` is changed from `std::vector<double>` to `c10::optional<std::vector<double>>`. If you want to pass a list of `double` to this argument, you must pass it as `std::vector<double>`.\r\n* F::multilabel_margin_loss / F::multilabel_soft_margin_loss functions ([#35163](https://github.com/pytorch/pytorch/pull/35163))\r\n* `torch::nn::functional::MultiLabelMarginLossFuncOptions` is renamed to `torch::nn::functional::MultilabelMarginLossFuncOptions`\r\n* `torch::nn::functional::MultiLabelSoftMarginLossFuncOptions` is renamed to `torch::nn::functional::MultilabelSoftMarginLossFuncOptions`\r\n* The deprecated `torch::nn::BatchNorm` is removed in favor of `torch::nn::BatchNorm{1,2,3}d`\r\n* The deprecated `torch::nn::FeatureDropout` is removed in favor of `torch::nn::Dropout{2,3}d` \r\n* The deprecated `torch::nn::modules_ordered_dict` is removed. User should do `Sequential sequential({{\"m1\", MyModule(1)}, {\"m2\", MyModule(2)}})` instead.\r\n* The deprecated `torch::nn::init::Nonlinearity` is removed, in favor of these enums: `torch::kLinear `/ `torch::kConv1D` / `torch::kConv2D` / `torch::kConv3D` / `torch::kConvTranspose1D` / `torch::kConvTranspose2D` / `torch::kConvTranspose3D` / `torch::kSigmoid` / `torch::kTanh` / `torch::kReLU` / `torch::kLeakyReLU`\r\n* The deprecated `torch::nn::init::FanMode` is removed, in favor of these enums: `torch::kFanIn` / `torch::kFanOut`\r\n\r\n\r\n\r\n### Optimizers\r\n\r\n* `Optimizer::step` now accepts closure function as optional input and returns a tensor, and `LossClosureOptimizer` is removed (#34790) (#34957). If you had a custom optimizer class defined as:\r\n\r\n```\r\nstruct MyOptimizer : Optimizer {\r\n  using Optimizer::Optimizer;\r\n  void step() override {...}\r\n};\r\n```\r\n\r\n    * you would need to update your optimizer class definition as follows:\r\n\r\n```\r\nstruct MyOptimizer : Optimizer {\r\n  using Optimizer::Optimizer;\r\n  torch::Tensor step(LossClosure closure = nullptr) override {\r\n    ...\r\n    // return `torch::Tensor()` if `closure` is nullptr\r\n    // (i.e. we are not computing the loss)\r\n    return torch::Tensor();\r\n  }\r\n};\r\n```\r\n\r\n* Adagrad ([#29335](https://github.com/pytorch/pytorch/pull/29335))\r\n    * In `AdagradOptions`, `learning_rate` is renamed to `lr`.\r\n    * In `Adagrad`, `sum_buffers` and `step_buffers` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<AdagradParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.sum()\r\n// param_state.step()\r\n```\r\n\r\n* SGD ([#32592](https://github.com/pytorch/pytorch/pull/32592))\r\n    * In `SGDOptions`, `learning_rate` is renamed to `lr`.\r\n    * In `SGD`, `momentum_buffers` is now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<SGDParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.momentum_buffer()\r\n```\r\n\r\n* Adam ([#33730](https://github.com/pytorch/pytorch/pull/33730))\r\n    * In `AdamOptions`:\r\n        * `learning_rate` is renamed to `lr`\r\n        * `beta1` and `beta2` are replaced by a tuple `betas`\r\n    * In `Adam`, `step_buffers`, `exp_average_buffers`, `exp_average_sq_buffers` and `max_exp_average_sq_buffers` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<AdamParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.step()\r\n// param_state.exp_avg()\r\n// param_state.exp_avg_sq()\r\n// param_state.max_exp_avg_sq()\r\n```\r\n\r\n* RMSprop ([#33450](https://github.com/pytorch/pytorch/pull/33450))\r\n    * In `RMSpropOptions`:\r\n        * `learning_rate` is renamed to `lr`\r\n    * In `RMSprop`, `square_average_buffers`, `momentum_buffers` and `grad_average_buffers` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<RMSpropParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.square_avg()\r\n// param_state.momentum_buffer()\r\n// param_state.grad_avg()\r\n```\r\n\r\n* LBFGS ([#34564](https://github.com/pytorch/pytorch/pull/34564)) ([#34957](https://github.com/pytorch/pytorch/pull/34957))\r\n\r\n    * In `LBFGSOptions`:\r\n        * `learning_rate` is renamed to `lr`\r\n        * `max_eval`\u2018s type is changed from `int64_t` to `c10::optional<int64_t>`\r\n        * `tolerance_grads type` is changed from `float` to `double`\r\n        * `tolerance_change type` is changed from `float` to `double`\r\n        * `history_size type` is changed from `size_t` to `int64_t`\r\n    * In `LBFGS`, `d`, `H_diag`, `prev_flat_grad`, `t`, `prev_loss`, `ro`, `al`, `old_dirs`, `old_stps`, `func_evals` and `state_n_iter` are now removed, and parameter state should be accessed by calling the accessors on the parameter\u2019s corresponding state object. For example:\r\n\r\n```\r\nauto& param_state = static_cast<LBFGSParamState&>(\r\n  *optimizer.state()[c10::guts::to_string(parameter.unsafeGetTensorImpl())]);\r\n\r\n// Use the following to access parameter state:\r\n//\r\n// param_state.d()\r\n// param_state.H_diag()\r\n// param_state.prev_flat_grad()\r\n// param_state.t()\r\n// param_state.prev_loss()\r\n// param_state.ro()\r\n// param_state.al()\r\n// param_state.old_dirs()\r\n// param_state.old_stps()\r\n// param_state.func_evals()\r\n// param_state.n_iter()\r\n```\r\n\r\n\r\n\r\n### Removed `AutoGIL/AutoNoGIL` in favor of `pybind11::gil_scoped_*` functions (#[34301](https://github.com/pytorch/pytorch/pull/34301))\r\n\r\nIf your code released or acquired the GIL via AutoNoGIL or AutoGIL, please change the invocations to `pybind11::gil_scoped_release` or `pybind11::gil_scoped_release`, respectively.\r\n\r\n\r\n### Others\r\n\r\n* `torch::tensor(floating-point values)` will always produce tensor of default dtype, and `torch::tensor(integer values)` will always produce tensor of `torch::kLong` dtype, matching Python API behavior ([#32367](https://github.com/pytorch/pytorch/pull/32367)).\r\n* `torch::Tensor::base()` is renamed to `torch::Tensor::_base()` , matching Python API. (#33316)\r\n* Renamed TensorTypeId to DispatchKey ([#32154](https://github.com/pytorch/pytorch/pull/32154))\r\n* Throw an error if nbytes is called on a sparse tensor. ([#33897](https://github.com/pytorch/pytorch/pull/33897))\r\n\r\n\r\n\r\n## JIT \r\n\r\n### Simple Executor Is Now On By Default\r\n\r\nThe simple executor skips the number of fusion-related passes and analyses that are very time-consuming. Disabling these optimizations fixes pathologically long compilation times. The users that rely on GPU fusion to have their desired performance profile, should turn on the profiling executor. We provide C++ and python API to enable the profiling executor:\r\n\r\n* in python, call `torch._C._jit_set_profiling_mode(True)` before you call your model for the first time.\r\n* in C++, include `#include <torch/csrc/jit/runtime/graph_executor.h>` and set `getProfilingMode() = true` before you invoke your model for the first time.\r\n\r\n\r\n\r\n## Quantization\r\n\r\n### **Remove qconfig_dict in top level eager mode quantization API** ([#31972](https://github.com/pytorch/pytorch/pull/31972)).\r\n\r\nIn eager mode quantization, one needs to manually insert quant and dequant stubs in a model to specify where activations are quantized. Having a qconfig_dict that specifies the quantization configuration for each module is not useful as one needs to manually modify the model with quant/dequant stubs. The new API makes it explicit that the model needs to be manually modified for quantization.\r\n\r\n```\r\n# previously qconfig_dict was an optional argument to prepare\r\ndef prepare(model, qconfig_dict=None, inplace=False):\r\n\r\n# now replaced with\r\ndef prepare(model, inplace=False):\r\n```\r\n\r\n## RPC\r\n\r\n### Functional API for Distributed Autograd and Distributed Optimizer\r\n\r\nMore specifically, callers must pass `context_id` to `torch.distributed.autograd.backward()` and `torch.distributed.optim.step()`.  ([#33711](https://github.com/pytorch/pytorch/pull/33711))\r\n\r\n\r\n```\r\n# Before\r\nimport torch.distributed.autograd as dist_autograd\r\nimport torch.distributed.rpc as rpc\r\nfrom torch import optim\r\nfrom torch.distributed.optim import DistributedOptimizer\r\n\r\nwith dist_autograd.context() as context_id:\r\n    # Forward pass.\r\n    rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n    rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\r\n    loss = rref1.to_here() + rref2.to_here()\r\n    # Backward pass.\r\n    dist_autograd.backward([loss.sum()])\r\n    # Optimizer.\r\n    dist_optim = DistributedOptimizer(\r\n        optim.SGD,\r\n        [rref1, rref2],\r\n        lr=0.05,\r\n    )\r\n```\r\n```\r\n# After\r\nimport torch.distributed.autograd as dist_autograd\r\nimport torch.distributed.rpc as rpc\r\nfrom torch import optim\r\nfrom torch.distributed.optim import DistributedOptimizer\r\n\r\nwith dist_autograd.context() as context_id:\r\n    # Forward pass.\r\n    rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\r\n    rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\r\n    loss = rref1.to_here() + rref2.to_here()\r\n    # Backward pass.\r\n    dist_autograd.backward(context_id, [loss.sum()])\r\n    # Optimizer.\r\n    dist_optim = DistributedOptimizer(\r\n        optim.SGD,\r\n        [rref1, rref2],\r\n        lr=0.05,\r\n    )\r\n    \r\n    dist_optim.step(context_id)    \r\n```\r\n\r\n### Disallow sending CUDA tensors over RPC\r\n\r\nThe motivation is to prevent potential invalid device errors when the number of devices on the sender and the receiver does not match. However applications, can always move CUDA tensors to CPU before sending (#33604).\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.4.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.distributed.rpc as rpc\r\nrpc.init_rpc(\"worker0\", rank=0, world_size=2)\r\nx = torch.zeros(2, device=0)\r\nret = rpc.rpc_sync(\"worker1\", torch.add, args=(x, 3))\r\nrpc.shutdown()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\nimport torch.distributed.rpc as rpc\r\nrpc.init_rpc(\"worker0\", rank=0, world_size=2)\r\nx = torch.zeros(2, device=0)\r\nret = rpc.rpc_sync(\"worker1\", torch.add, args=(x.cpu(), 3))\r\nrpc.shutdown()\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n\r\n\r\n# New Features\r\n\r\n## Python\r\n\r\n### Added new functional autograd API ([#34066](https://github.com/pytorch/pytorch/pull/34066))\r\n\r\n* See Highlights for more details\r\n\r\n\r\n### New `__torch_function__` API Override Mechanism ([#30730](https://github.com/pytorch/pytorch/pull/30730), [#32194](https://github.com/pytorch/pytorch/pull/32194), [#32799](https://github.com/pytorch/pytorch/pull/32799), [#34240](https://github.com/pytorch/pytorch/pull/34240), [#34303](https://github.com/pytorch/pytorch/pull/34303)).\r\n\r\nWe introduced `__torch_function__`, an API override mechanism for subclassing `torch.Tensor` in Python. This is useful for creating custom objects that implement the `torch.*` APIs. These currently support overriding most `torch.*`, and `torch.nn.functional` APIs; we\u2019ve also planned future support for subclassing `torch.Tensor` (see tracking issue [#22402](https://github.com/pytorch/pytorch/issues/22402)).\r\n\r\n\r\n## New Operators\r\n\r\n* `torch.logical_and` and `torch.logical_or` operations added ([#30521](https://github.com/pytorch/pytorch/pull/30521)).\r\n* `torch.square` added ([#30719](https://github.com/pytorch/pytorch/pull/30719)).\r\n* `torch.bitwise_and` added ([#31104](https://github.com/pytorch/pytorch/pull/31104)).\r\n* `torch.cummax`, `torch.cummin` added ([#32169](https://github.com/pytorch/pytorch/pull/32169), [#32238](https://github.com/pytorch/pytorch/pull/32238), [#32537](https://github.com/pytorch/pytorch/pull/32537), [#33492](https://github.com/pytorch/pytorch/pull/33492)).\r\n* `torch.floor_divide` ,  `Tensor.floor_divide` added ([#30493](https://github.com/pytorch/pytorch/pull/30493), [#34552](https://github.com/pytorch/pytorch/pull/34552)).\r\n* `torch.true_divide` , `Tensor.true_divide` added, analogous to Python 's, and NumPy's (true) division ([#34236](https://github.com/pytorch/pytorch/pull/34236), [#34794](https://github.com/pytorch/pytorch/pull/34794))\r\n* `nn.functional.hardsigmoid` added([#34545](https://github.com/pytorch/pytorch/pull/34545)).\r\n* Added PCA and SVD for low-rank matrices (`torch.pca_lowrank`,  `torch.svd_lowrank`), `torch.lobpcg` for positive-defined generalized eigenvalue problem ([#34721](https://github.com/pytorch/pytorch/pull/34721)).\r\n\r\n\r\n\r\n## Distributions\r\n\r\n* `distributions.von_mises` added ([#33418](https://github.com/pytorch/pytorch/pull/33418)).\r\n* `distributions.mixture_same_family` : Added support for mixture distributions ([#22742](https://github.com/pytorch/pytorch/pull/22742), [#33408](https://github.com/pytorch/pytorch/pull/33408)).\r\n* `distributions.transforms.TanhTransform`  added([#19785](https://github.com/pytorch/pytorch/pull/19785)).\r\n* `distributions.continuous_bernoulli` added ([#34619](https://github.com/pytorch/pytorch/pull/34619)).\r\n\r\n\r\n\r\n## C++ API\r\n\r\n* NN modules / functionals\r\n    * `torch::nn::MultiheadAttention` ([#27309](https://github.com/pytorch/pytorch/pull/27309))\r\n    * `torch::nn::RNNCell` / `LSTMCell` / `GRUCell` ([#34400](https://github.com/pytorch/pytorch/pull/34400))\r\n    * `torch::nn::AdaptiveLogSoftmaxWithLoss` ([#29076](https://github.com/pytorch/pytorch/pull/29076)).\r\n    * `torch::nn::utils::rnn::PackedSequence` / `pack_padded_sequence` / `pad_packed_sequence` / `pack_sequence` / `pad_sequence` ([#32387](https://github.com/pytorch/pytorch/pull/32387), [#33652](https://github.com/pytorch/pytorch/pull/33652), [#34185](https://github.com/pytorch/pytorch/pull/34185))\r\n* C++ tensor indexing ([#30424](https://github.com/pytorch/pytorch/pull/30424), [#32841](https://github.com/pytorch/pytorch/pull/32841), [#30427](https://github.com/pytorch/pytorch/pull/30427), [#34255](https://github.com/pytorch/pytorch/pull/34255))\r\n    * Please see docs: https://pytorch.org/cppdocs/notes/tensor_indexing.html\r\n* Operators\r\n    * C++ API parity: `isinf` ([#31099](https://github.com/pytorch/pytorch/pull/31099)).\r\n* Autograd\r\n    * Add `at::Tensor::retain_grad` API ([#33349](https://github.com/pytorch/pytorch/pull/33349)).\r\n* C++ extensions\r\n    * Add option to use ninja to compile ahead-of-time `cpp_extensions` (#32495, [#33084](https://github.com/pytorch/pytorch/pull/33084))\r\n    * Added support for Pytorch C++ extensions to use HIP ([#32669](https://github.com/pytorch/pytorch/pull/32669)).\r\n\r\n\r\n\r\n## Distributed\r\n\r\n* Allows Python application to create subclass of C++ `c10d.Store` using pybind11 trampoline class  [#30415](https://github.com/pytorch/pytorch/pull/30415).\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Loading module from android asset ([#30378](https://github.com/pytorch/pytorch/pull/30378)).\r\n* Torchscript print to logcat ([#31456](https://github.com/pytorch/pytorch/pull/31456)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* qnnpack TanH ([#31013](https://github.com/pytorch/pytorch/pull/31013)).\r\n* Adding quantized clamp kernel ([#30541](https://github.com/pytorch/pytorch/pull/30541)).\r\n* Quantized H Tangent function ([#31031](https://github.com/pytorch/pytorch/pull/31031)).\r\n* QNNPACK: Add support for dynamic quantization. ([#31896](https://github.com/pytorch/pytorch/pull/31896)).\r\n* Add operator support for dynamic quant on mobile ([#32479](https://github.com/pytorch/pytorch/pull/32479)).\r\n* Adding native qconcat ([#32252](https://github.com/pytorch/pytorch/pull/32252)).\r\n* FP16 dynamic quantized Linear ([#32331](https://github.com/pytorch/pytorch/pull/32331)).\r\n* Add support for Dynamic LSTM quantization on Mobile ([#32757](https://github.com/pytorch/pytorch/pull/32757)).\r\n* Quantized sigmoid function ([#31851](https://github.com/pytorch/pytorch/pull/31851)).\r\n* Quantized leaky relu ([#33004](https://github.com/pytorch/pytorch/pull/33004)).\r\n* Add a quantized batch_norm operator ([#33080](https://github.com/pytorch/pytorch/pull/33080)).\r\n* Add Quantized BatchNorm2d module ([#33109](https://github.com/pytorch/pytorch/pull/33109)).\r\n* Add the 3d avg pool for video related model ([#33339](https://github.com/pytorch/pytorch/pull/33339)).\r\n* Add quantized_hardtanh ([#34097](https://github.com/pytorch/pytorch/pull/34097)).\r\n* Add quantized ELU activation ([#34267](https://github.com/pytorch/pytorch/pull/34267)).\r\n* Add the 3d upsample quantized op for video model ([#34594](https://github.com/pytorch/pytorch/pull/34594)).\r\n* Add the quantized batch_norm3d and also batch_norm3d fused with relu operators ([#34702](https://github.com/pytorch/pytorch/pull/34702)).\r\n* Add quantized implementation of hard sigmoid ([#34607](https://github.com/pytorch/pytorch/pull/34607)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* [Experimental] Enable autograd profiler to work with RPC ([#31381](https://github.com/pytorch/pytorch/pull/31381), [#34398](https://github.com/pytorch/pytorch/pull/34398), [#30677](https://github.com/pytorch/pytorch/pull/30677), [#31346](https://github.com/pytorch/pytorch/pull/31346), [#31380](https://github.com/pytorch/pytorch/pull/31380)). \r\n* [Experimental] Allow calling remote TorchScript functions using RPC ([#32466](https://github.com/pytorch/pytorch/pull/32466), [#33190](https://github.com/pytorch/pytorch/pull/33190), [#32990](https://github.com/pytorch/pytorch/pull/32990), [#32959](https://github.com/pytorch/pytorch/pull/32959),  [#33526](https://github.com/pytorch/pytorch/pull/33526), [#33992](https://github.com/pytorch/pytorch/pull/33992), [#33582](https://github.com/pytorch/pytorch/pull/33582), [#32197](https://github.com/pytorch/pytorch/pull/32197), [#33329](https://github.com/pytorch/pytorch/pull/33329), [#34183](https://github.com/pytorch/pytorch/pull/34183)).\r\n\r\n\r\n\r\n# Improvements\r\n\r\n## AMD/ROCm\r\n\r\n*  `nn.RNN`: Ensure MIOpen is called on same stream as operator ([#30672](https://github.com/pytorch/pytorch/pull/30672))\r\n* Fixed asserts in CUDA kernels ([#31276](https://github.com/pytorch/pytorch/pull/31276), [#31297](https://github.com/pytorch/pytorch/pull/31297)).\r\n* Enable BFloat16 support for convolutions ([#30948](https://github.com/pytorch/pytorch/pull/30948)).\r\n* Abstract atomic add calls ([#31992](https://github.com/pytorch/pytorch/pull/31992)).\r\n* Install complete set of headers for ROCm build ([#32076](https://github.com/pytorch/pytorch/pull/32076)).\r\n* Adjust `elementwise_kernel` settings on ROCm ([#32609](https://github.com/pytorch/pytorch/pull/32609)).\r\n* `nn.BatchNorm{1,2,3}d`: Use `C10_WARP_SIZE` to fix functionality on HIP vs CUDA for gradient computation ([#33098](https://github.com/pytorch/pytorch/pull/33098)).\r\n* Enabled Bfloat16 type for activation functions and `batch_norm` ([#32065](https://github.com/pytorch/pytorch/pull/32065)).\r\n* Added ability to enable/disable MIOpen at runtime ([#33118](https://github.com/pytorch/pytorch/pull/33118)).\r\n* Enable BFloat16 type for pooling ops ([#34166](https://github.com/pytorch/pytorch/pull/34166)).\r\n* `torch.pdist`: improved precision by enabling double `__shfl_down` ([#34103](https://github.com/pytorch/pytorch/pull/34103)).\r\n* Enabled BFloat16 type for loss functions and few misc ops required for resnet50 ([#34469](https://github.com/pytorch/pytorch/pull/34469)).\r\n* Enabled BFloat16 type for EmbeddingBag, Index, and Sigmoid ops ([#34630](https://github.com/pytorch/pytorch/pull/34630)).\r\n* Enabled 3D batch norms through MIOpen ([#33262](https://github.com/pytorch/pytorch/pull/33262)).\r\n* Enabled 3D convolutions through ROCm ([#33067](https://github.com/pytorch/pytorch/pull/33067)).\r\n* `nn.RNN`: Check if weights need to be flattened ([#34265](https://github.com/pytorch/pytorch/pull/34265)).\r\n\r\n\r\n\r\n## C++ API\r\n\r\n* NN modules / functionals\r\n    * Allow skipping default arguments in module's forward method when module is used in `torch::nn::Sequential` ([#33027](https://github.com/pytorch/pytorch/pull/33027)) ([#33718](https://github.com/pytorch/pytorch/pull/33718))\r\n    * Make `torch::nn::Sequential::push_back(AnyModule)` methods public ([#34208](https://github.com/pytorch/pytorch/pull/34208)).\r\n    * Refactor RNN / GRU / LSTM layers to match Python API ([#34322](https://github.com/pytorch/pytorch/pull/34322)).\r\n    * For `Conv{1,2,3}d`, `padding_mode` now accepts `torch::kZeros` / `torch::kReflect` / `torch::kReplicate` / `torch::kCircular`, matching Python API behavior. ([#35023](https://github.com/pytorch/pytorch/pull/35023))\r\n    * Fix `F::interpolate` and `torch::nn::Upsample` implementation to match Python API behavior ([#35025](https://github.com/pytorch/pytorch/pull/35025)) ([#36274](https://github.com/pytorch/pytorch/pull/36274))\r\n    * Renaming: MultiLabelMarginLossFuncOptions -> MultilabelMarginLossFuncOptions, MultiLabelSoftMarginLossFuncOptions -> MultilabelSoftMarginLossFuncOptions ([#35163](https://github.com/pytorch/pytorch/pull/35163))\r\n* Optimizers\r\n    * All existing optimizers in the C++ API (Adagrad / SGD / Adam / RMSprop / LBFGS) have the following changes to achieve parity with the Python API: ([#29335](https://github.com/pytorch/pytorch/pull/29335)) ([#30739](https://github.com/pytorch/pytorch/pull/30739)) ([#32592](https://github.com/pytorch/pytorch/pull/32592)) ([#33730](https://github.com/pytorch/pytorch/pull/33730)) ([#33450](https://github.com/pytorch/pytorch/pull/33450)) ([#34790](https://github.com/pytorch/pytorch/pull/34790)) ([#34564](https://github.com/pytorch/pytorch/pull/34564)) ([#34957](https://github.com/pytorch/pytorch/pull/34957)) ([#35001](https://github.com/pytorch/pytorch/pull/35001)) ([#36033](https://github.com/pytorch/pytorch/pull/36033)) (#36245)\r\n        * step function implementation is changed to behave the same as Python equivalent\r\n        * Constructor now accepts `std::vector<OptimizerParamGroup>` as input\r\n        * `optimizer.add_param_group(...)` can be used to add parameter group to an existing optimizer\r\n        * `optimizer.state()` should be used to access parameter state\r\n* autograd\r\n    * Renamed `at::Tensor::base()` to `_base()`, matching Python API (#33316)\r\n\r\n\r\n\r\n## Distributed\r\n\r\n* Allow TCPStore to pick a port to bind to ([#31674](https://github.com/pytorch/pytorch/pull/31674)).\r\n* Enhance NCCL watchdog to actively abort communicators for timed out ops ([#32338](https://github.com/pytorch/pytorch/pull/32338)).\r\n* Adding DDP Design Note ([#32158](https://github.com/pytorch/pytorch/pull/32158)).\r\n* Recommend using DDP over DataParallel ([#35063](https://github.com/pytorch/pytorch/pull/35063/files))\r\n\r\n\r\n\r\n## Distributions\r\n\r\n* `distributions.independent`: added explicit string representation ([#33676](https://github.com/pytorch/pytorch/pull/33676)).\r\n* `categorical.sample`: Reduced memory overhead ([#34900](https://github.com/pytorch/pytorch/pull/34900)).\r\n* `distributions.MultivariateNormal`: improved numeric stability and performance ([#32092](https://github.com/pytorch/pytorch/pull/32092)).\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Add module level qpl logging. ([#30906](https://github.com/pytorch/pytorch/pull/30906)).\r\n* Expose setNumThreads to android api ([#31033](https://github.com/pytorch/pytorch/pull/31033)).\r\n* remove unused SparseCPUType from mobile build ([#33517](https://github.com/pytorch/pytorch/pull/33517)).\r\n* make sure mobile build work with dynamic dispatch ([#34038](https://github.com/pytorch/pytorch/pull/34038)).\r\n* support for custom mobile build with dynamic dispatch ([#34055](https://github.com/pytorch/pytorch/pull/34055)).\r\n* Add watchOS support ([#33318](https://github.com/pytorch/pytorch/pull/33318)).\r\n* speed_benchmark_torch switch to log latency from dataset level to row level ([#34598](https://github.com/pytorch/pytorch/pull/34598)).\r\n\r\n\r\n\r\n## ONNX\r\n\r\n### ****Exporting More Torch Operators to ONNX****\r\n\r\nIn PyTorch 1.5, we have added support for 10 additional operators and also enhanced support for another set of 10+ existing operators. We have also added support for exporting large models (> 2GB) to ONNX. Additionally, we have made enhancements and optimizations to the export of ScriptModules and will continue to do that in the next release. We have also made improvements to the custom op export experience. \r\n\r\n* Export dynamic unbind, split and getitem ([#29136](https://github.com/pytorch/pytorch/pull/29136)).\r\n* Export torch.new_zeros ([#34077](https://github.com/pytorch/pytorch/pull/34077)).\r\n* Export Im2col ([#30972](https://github.com/pytorch/pytorch/pull/30972)).\r\n* Export bitwise_not for bool ([#28439](https://github.com/pytorch/pytorch/pull/28439)).\r\n* Export logsoftmax with dim != -1 ([#30433](https://github.com/pytorch/pytorch/pull/30433)).\r\n* Export einsum ([#32716](https://github.com/pytorch/pytorch/pull/32716)).\r\n* Export aten::copy_ and aten::index_put to ONNX opset 11 ([#26941](https://github.com/pytorch/pytorch/pull/26941)).\r\n* Export floor_divide ([#31081](https://github.com/pytorch/pytorch/pull/31081)).\r\n* Export one_hot ([#34454](https://github.com/pytorch/pytorch/pull/34454)).\r\n* Export torch.take ([#33061](https://github.com/pytorch/pytorch/pull/33061)).\r\n* Export bool type index mask ([#32445](https://github.com/pytorch/pytorch/pull/32445)).\r\n* Export split with list of sizes ([#33161](https://github.com/pytorch/pytorch/pull/33161)).\r\n* Export scalar tensor for split ([#32493](https://github.com/pytorch/pytorch/pull/32493)).\r\n* Export flatten to accept negative indices in opset 11 ([#30751](https://github.com/pytorch/pytorch/pull/30751)).\r\n* Export sort with negative axes ([#31971](https://github.com/pytorch/pytorch/pull/31971)).\r\n* Export Interpolate to support scale ([#28324](https://github.com/pytorch/pytorch/pull/28324), [#31526](https://github.com/pytorch/pytorch/pull/31526), [#32554](https://github.com/pytorch/pytorch/pull/32554)).\r\n* Export quantized concat ([#30887](https://github.com/pytorch/pytorch/pull/30887)).\r\n\r\n### ****Enhancing the Support for ScriptModule****\r\n\r\n* Fixed access to element in size tensor for scripting  ([#32652](https://github.com/pytorch/pytorch/pull/32652)).\r\n* Export Conv in TorchScript module ([#30618](https://github.com/pytorch/pytorch/pull/30618)).\r\n* Export Dim operation in TorchScript module ([#31928](https://github.com/pytorch/pytorch/pull/31928)).\r\n* Export randnlike in TorchScript module ([#32830](https://github.com/pytorch/pytorch/pull/32830)).\r\n* Partially support tensor lists in loop/concat/stack ([#30126](https://github.com/pytorch/pytorch/pull/30126))\r\n\r\n### ****Enhancing Existing Export Logic****\r\n\r\n* Updating ONNX checker logic. ([#33522](https://github.com/pytorch/pytorch/pull/33522)).\r\n* Adding ONNX large model export support in  exporter ([#33062](https://github.com/pytorch/pytorch/pull/33062)).\r\n* Extend op registration ([#32943](https://github.com/pytorch/pytorch/pull/32943)).\r\n* Support op registration if name starts with underscore ([#32017](https://github.com/pytorch/pytorch/pull/32017)).\r\n\r\n### ****Optimizing Exported ONNX Graph****\r\n\r\n* Try exporting ONNX with force_outplace=False ([#29466](https://github.com/pytorch/pytorch/pull/29466)).\r\n* Enable constant folding ([#29834](https://github.com/pytorch/pytorch/pull/29834)).\r\n* Added cons folding for ONNX mul, div, sqrt ops ([#32077](https://github.com/pytorch/pytorch/pull/32077)).\r\n* Enable constant folding for Reshape ([#31054](https://github.com/pytorch/pytorch/pull/31054)).\r\n\r\n### ****Adding Utility Functions and Refactoring****\r\n\r\n* Added ONNX model checker to ONNX export ([#32298](https://github.com/pytorch/pytorch/pull/32298)).\r\n* Export custom ops ([#29752](https://github.com/pytorch/pytorch/pull/29752)).\r\n* Upgrade exported ONNX IR version to 6 ([#31025](https://github.com/pytorch/pytorch/pull/31025)).\r\n* Provide names for operator nodes in ONNX exported graph ([#27342](https://github.com/pytorch/pytorch/pull/27342)).\r\n* Update ONNX landing page since 1.3 ([#32805](https://github.com/pytorch/pytorch/pull/32805)).\r\n* Turn ONNX_ML into a proper build option ([#33424](https://github.com/pytorch/pytorch/pull/33424)).\r\n\r\n\r\n\r\n## Operator Benchmark\r\n\r\n* Added small input shapes to test operator overhead ([#30617](https://github.com/pytorch/pytorch/pull/30617)).\r\n* Added `binary_test` to benchmark binary ops ([#31326](https://github.com/pytorch/pytorch/pull/31326)).\r\n* Added `Tensor.copy_` operator ([#31327](https://github.com/pytorch/pytorch/pull/31327)).\r\n* Removed option to wipe cache because it did not help with variance ([#31334](https://github.com/pytorch/pytorch/pull/31334)).\r\n* Added `torch.diag` ([#32597](https://github.com/pytorch/pytorch/pull/32597)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Guard against copying from quantized Tensor to non-quantized Tensor ([#29660](https://github.com/pytorch/pytorch/pull/29660)).\r\n* Add assert for min, max, qmin, qmax for ChooseQuantizationParams ([#32739](https://github.com/pytorch/pytorch/pull/32739)).\r\n* Support broadcast for quantized mul kernel ([#30442](https://github.com/pytorch/pytorch/pull/30442)).\r\n* Make FakeQuant use `REGISTER_DISPATCH` ([#33682](https://github.com/pytorch/pytorch/pull/33682)).\r\n* Set alias analysis kind to `FROM_SCHEMA` for qadd, qmul, qclamp, qconcat ([#33359](https://github.com/pytorch/pytorch/pull/33359)).\r\n* Migrate `fake_quant_slice` to TensorIterator ([#33744](https://github.com/pytorch/pytorch/pull/33744)).\r\n* Parallelize quantize and dequantize ([#33765](https://github.com/pytorch/pytorch/pull/33765)).\r\n* Make FP16 RNN use new prepack op ([#34339](https://github.com/pytorch/pytorch/pull/34339)).\r\n* Refactor QAT Conv module for better extensibility ([#30362](https://github.com/pytorch/pytorch/pull/30362)).\r\n* Use non-inplace for insert observer pass ([#34190](https://github.com/pytorch/pytorch/pull/34190)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Add default arguments for `init_method` ([#30208](https://github.com/pytorch/pytorch/pull/30208)).\r\n* By default ignore RRef leaks during shutdown ([#30217](https://github.com/pytorch/pytorch/pull/30217)).\r\n* Robustify `rpc_agent` handlers with generic Future ([#31224](https://github.com/pytorch/pytorch/pull/31224)).\r\n* Fix error message in incorrect `rref.localValue()` call ([#31199](https://github.com/pytorch/pytorch/pull/31199)).\r\n* Add `RpcAgent::getWorkerInfos()` API to return all `WorkInfo`s in the group ([#30241](https://github.com/pytorch/pytorch/pull/30241)).\r\n* Add local shutdown to process group agent ([#30330](https://github.com/pytorch/pytorch/pull/30330)).\r\n* Add `RRef.str()` API to return a string representation of the RRef ([#30609](https://github.com/pytorch/pytorch/pull/30609)).\r\n* Adding Debug Info for RRef Context ([#30610](https://github.com/pytorch/pytorch/pull/30610)).\r\n* Add `get_metrics` and `get_debug_info` to RPC agent ([#30833](https://github.com/pytorch/pytorch/pull/30833)).\r\n* Adding debugging metrics to process group agent ([#30884](https://github.com/pytorch/pytorch/pull/30884)).\r\n* Add glue code to collect debug info from all components ([#30888](https://github.com/pytorch/pytorch/pull/30888)).\r\n* Make RRef leak detection always print a warning log ([#31922](https://github.com/pytorch/pytorch/pull/31922)).\r\n* Allow multiple backward passes to accumulate gradients. ([#32506](https://github.com/pytorch/pytorch/pull/32506)).\r\n* Allow RRef local creation with IValue objects ([#33263](https://github.com/pytorch/pytorch/pull/33263)).\r\n* Improve ProcessGroup `RpcBackendOptions` Constructor API ([#34081](https://github.com/pytorch/pytorch/pull/34081)).\r\n* Enhanced Error Reporting in Dist Autograd/RPC ([#34179](https://github.com/pytorch/pytorch/pull/34179)).\r\n* Delete all user forks tracked in `RRefContext` before graceful shutdown ([#31893](https://github.com/pytorch/pytorch/pull/31893)).\r\n* Best-effort Error Detection for Using Deleted UserRRefs ([#34673](https://github.com/pytorch/pytorch/pull/34673)).\r\n* Don't run user function until all UserRRefs in the args are confirmed ([#34497](https://github.com/pytorch/pytorch/pull/34497)).\r\n* Support using self as the destination in `rpc.remote` for builtin operators ([#34931](https://github.com/pytorch/pytorch/pull/34931)).\r\n* Add debug info API for distributed autograd. ([#30642](https://github.com/pytorch/pytorch/pull/30642)).\r\n* Propagate errors in `clearAndWaitForOutstandingRpcsAsync`. ([#32952](https://github.com/pytorch/pytorch/pull/32952)).\r\n\r\n\r\n\r\n## Type Hints\r\n\r\n* DataLoader `default_collate` type hint added ([#28935](https://github.com/pytorch/pytorch/pull/28935)).\r\n* `Tensor.rsub, Tensor.rpow, Tensor.rtruediv, Tensor.map_` type hints were added ([#30576](https://github.com/pytorch/pytorch/pull/30576)).\r\n* `torch.optim`: added more missing type hints ([#31130](https://github.com/pytorch/pytorch/pull/31130)).\r\n* `nn.functional.grid_sample`, `nn.functional.affine_grid`: added missing align_corners annotation ([#32492](https://github.com/pytorch/pytorch/pull/32492)).\r\n* `torch.nn.Parameter` constructor type hint was fixed ([#32617](https://github.com/pytorch/pytorch/pull/32617)).\r\n* `nn.MultiheadAttention`, `nn.Transformer`: added type hints ([#28396](https://github.com/pytorch/pytorch/pull/28396)).\r\n* `torch.optim.LambdaLR` constructor type hint was fixed ([#33271](https://github.com/pytorch/pytorch/pull/33271)).\r\n* `torch.optim`: added missing default value for `LRScheduler.step()` ([#32411](https://github.com/pytorch/pytorch/pull/32411)).\r\n* Make type of `Tensor.type()` more specific ([#32353](https://github.com/pytorch/pytorch/pull/32353)).\r\n* `torch.optim.optimizer.Optimizer`  type hints were fixed ([#32900](https://github.com/pytorch/pytorch/pull/32900)).\r\n* `optim.AdamW` type hints were fixed ([#34299](https://github.com/pytorch/pytorch/pull/34299)).\r\n* `torch.utils.data.Sampler`  subclasses type hints were added ([#33679](https://github.com/pytorch/pytorch/pull/33679)).\r\n* `nn.Sequential`, `nn.ModuleList`, `nn.ParameterList`, `nn.ParameterDict` type hints were fixed ([#33686](https://github.com/pytorch/pytorch/pull/33686)).\r\n* `Tensor.bfloat16()` type hint was added ([#33747](https://github.com/pytorch/pytorch/pull/33747)).\r\n* Binary operator type hints were fixed ([#33748](https://github.com/pytorch/pytorch/pull/33748)).\r\n* `torch.bfloat16`, `nn.Module.training`, `Tensor.cuda`, and 10s of other type hints added ([#33762](https://github.com/pytorch/pytorch/pull/33762)).\r\n* `torch.add` type hint was fixed([#33935](https://github.com/pytorch/pytorch/pull/33935)).\r\n* `Tensor.shape` type hint was fixed ([#34595](https://github.com/pytorch/pytorch/pull/34595)).\r\n* Fixed `utils.data` imports ([#33543](https://github.com/pytorch/pytorch/pull/33543)).\r\n* `Tensor.__radd__` type hint was fixed ([#35231](https://github.com/pytorch/pytorch/pull/35231))\r\n\r\n\r\n\r\n## Other\r\n\r\n* `autograd.detect_anomaly`: added support for Sparse Tensors ([#29803](https://github.com/pytorch/pytorch/pull/29803)).\r\n* `autograd.detect_anomaly`: Error messages now print the current Node name ([#33875](https://github.com/pytorch/pytorch/pull/33875)).\r\n* `autograd.profiler`: added better error message when crashing while profiling multi-worker DataLoader ([#31473](https://github.com/pytorch/pytorch/pull/31473)).\r\n* `autograd.profiler` Enable using `torch.autograd.profiler.record_function` as decorator ([#30861](https://github.com/pytorch/pytorch/pull/30861)).\r\n* `autograd.profiler` Speed up `export_chrome_trace` by up to 4x ([#30724](https://github.com/pytorch/pytorch/pull/30724)).\r\n* `torch.autograd`: added better error message when attempting to fork ([#33885](https://github.com/pytorch/pytorch/pull/33885)).\r\n* `torch.cuda.memory.caching_allocator_alloc`, `torch.cuda.memory.caching_allocator_delete` exposed in Python API ([#33860](https://github.com/pytorch/pytorch/pull/33860)).\r\n* `torch.roll`: added bool tensor support ([#31194](https://github.com/pytorch/pytorch/pull/31194)).\r\n* `torch.flip`: added support for bool tensors ([#31267](https://github.com/pytorch/pytorch/pull/31267)).\r\n* `torch.equal`: added support for bfloat16 CPU scalar types ([#30817](https://github.com/pytorch/pytorch/pull/30817)).\r\n* `torch.save`, `torch.load`: added error message for minimum dill version support ([#30985](https://github.com/pytorch/pytorch/pull/30985)).\r\n* `torch.diagonal`: added named tensor support([#30193](https://github.com/pytorch/pytorch/pull/30193)).\r\n* `torch.linspace`: added support for integral types on CPU ([#32218](https://github.com/pytorch/pytorch/pull/32218)).\r\n* `torch.eig`: Added autograd support in the case where eigenvalues are real ([#33090](https://github.com/pytorch/pytorch/pull/33090)).\r\n* `torch.mvlgamma`: improved error message ([#32665](https://github.com/pytorch/pytorch/pull/32665)).\r\n* `torch.no_grad`, `torch.enable_grad`: added support for decorating generator functions ([#31792](https://github.com/pytorch/pytorch/pull/31792)).\r\n* `torch.narrow`: added Tensor overload for `start` ([#34317](https://github.com/pytorch/pytorch/pull/34317)).\r\n* `Tensor.random_`: enabled support for half on CPU ([#34030](https://github.com/pytorch/pytorch/pull/34030)).\r\n* `Tensor.grad`: added warnings when accessing it if it won't be populated for known reasons ([#30531](https://github.com/pytorch/pytorch/pull/30531)).\r\n* `torch.cuda.comm.gather`: improved error message ([#27456](https://github.com/pytorch/pytorch/pull/27456)).\r\n* `nn.functional.max_pool{1,2,3}d`: added named tensor support ([#31669](https://github.com/pytorch/pytorch/pull/31669)).\r\n* `nn.Module.load_state_dict`: Include the contents of the exception in error messages ([#32693](https://github.com/pytorch/pytorch/pull/32693)).\r\n* `nn.MultiheadAttention`: add support for 3D attention mask ([#31996](https://github.com/pytorch/pytorch/pull/31996)).\r\n* `nn.MSELoss` : Added performance warning for using CPU Half ([#33021](https://github.com/pytorch/pytorch/pull/33021)).\r\n* `nn.ModuleList`, `nn.ParameterDict`, `nn.ParameterDict`: added more descriptive error messages when attempting to call these like Modules ([#29991](https://github.com/pytorch/pytorch/pull/29991)).\r\n* `nn.init.dirac_`: Added `groups` option for compatibility with initializing group convolutions ([#32825](https://github.com/pytorch/pytorch/pull/32825)).\r\n* Added error message to indicate that reduction operations are not supported for dim >= 64 ([#31476](https://github.com/pytorch/pytorch/pull/31476)).\r\n* Type Promotion: added supports for sparse tensors and arithmetic operations ([#30429](https://github.com/pytorch/pytorch/pull/30429)).\r\n* Enabled indexing for bfloat16 tensors ([#31692](https://github.com/pytorch/pytorch/pull/31692)).\r\n* Add 64-bit indexing support for CUDA Tensors ([#33405](https://github.com/pytorch/pytorch/pull/33405)).\r\n* Added warning when converting a read-only NumPy array to `torch.Tensor` ([#33615](https://github.com/pytorch/pytorch/pull/33615)).\r\n* Set rpath for JNI library on Mac ([#32247](https://github.com/pytorch/pytorch/pull/32247)).\r\n* Updated MAGMA to 2.5.2 for Windows ([#30513](https://github.com/pytorch/pytorch/pull/30513), [#34205](https://github.com/pytorch/pytorch/pull/34205)).\r\n* Marked PyTorch incompatible with Python-3.6.0 ([#34724](https://github.com/pytorch/pytorch/pull/34724)).\r\n* Consider `hub_dir` alongside `TORCH_HOME` env variable for storing hub models ([#32844](https://github.com/pytorch/pytorch/pull/32844)).\r\n* Improved dll loading logic on Windows ([#33856](https://github.com/pytorch/pytorch/pull/33856)).\r\n* Error out if legacy `Tensor.new ` is called on alternate layouts or dtypes ([#31485](https://github.com/pytorch/pytorch/pull/31485)).\r\n* `utils.checkpoint.checkpoint_sequential`: Removed deprecated variadic arguments behavior ([#25985](https://github.com/pytorch/pytorch/pull/25985)).\r\n\r\n# Bug Fixes\r\n\r\n## C++ API\r\n\r\n* NN modules / functionals\r\n    * `output_ratio` for `FractionalMaxPool{2,3}d `module and `fractional_max_pool{2,3}d` functional should accept double as data type ([#33304](https://github.com/pytorch/pytorch/pull/33304))\r\n    * For `AdaptiveAvgPool{2,3}d `and `AdaptiveMaxPool{2,3}d`, `output_size` is changed to accept `c10::nullopt` in its elements, matching Python API behavior. ([#35022](https://github.com/pytorch/pytorch/pull/35022))\r\n    * Fix bug in `fractional_max_pool3d_with_indices` implementation ([#35024](https://github.com/pytorch/pytorch/pull/35024))\r\n    * Remove `namespace F = torch::nn::functional` from torch/nn/modules/batchhnorm.h, so that people don't have to use `F` to alias `torch::nn::functional` if they don't want to ([#30684](https://github.com/pytorch/pytorch/pull/30684))\r\n* autograd\r\n    * For `AutogradContext`, `get_dirty()` is removed and `get_and_bump_dirty()` is added, and the latter always bumps the version counter of the returned tensors (#33068)\r\n    * Fix allow_unused checking for C++ API (#34035)\r\n    * Remove `using namespace torch::autograd` from `torch/csrc/api/include/torch/nn/modules/_functions.h` ([#34423](https://github.com/pytorch/pytorch/pull/34423))\r\n* Operators\r\n    * `torch::tensor(floating-point values)` will always produce tensor of default dtype, and `torch::tensor(integer values)` will always produce tensor of `torch::kLong` dtype, matching Python API behavior ([#32367](https://github.com/pytorch/pytorch/pull/32367))\r\n    * Fix `torch::allclose` to handle `std::numeric_limits::lowest()` for integral types ([#32978](https://github.com/pytorch/pytorch/pull/32978))\r\n    * Switch `torch::empty_like` to use `merge_in` to process TensorOptions (#33505)\r\n\r\n\r\n\r\n## Distributed\r\n\r\n* Allow DDP to detect globally unused parameters ([#28883](https://github.com/pytorch/pytorch/pull/28883)).\r\n* Accept url query when `rank` or `world_size` is specified in Process Group `init_method` URL ([#32016](https://github.com/pytorch/pytorch/pull/32016)).\r\n* Add ability to abort NCCL communicators from the store. ([#32895](https://github.com/pytorch/pytorch/pull/32895)).\r\n* Fix timeout support when initializing process group with TCP store ([#33434](https://github.com/pytorch/pytorch/pull/33434)).\r\n* Abort NCCL communicators before throwing operation timed out ([#31128](https://github.com/pytorch/pytorch/pull/31128)).\r\n* Fix logging for aborted communicators in ProcessGroupNCCL ([#33147](https://github.com/pytorch/pytorch/pull/33147)).\r\n* Fix handling of replica parameters in DataParallel ([#33907](https://github.com/pytorch/pytorch/pull/33907)).\r\n* Specify `requires_grad` for Parameter replica so it's not always set to True by default ([#32356](https://github.com/pytorch/pytorch/pull/32356))\r\n* Put sparse `allreduce` results to input tensors ([#32226](https://github.com/pytorch/pytorch/pull/32226))\r\n* Issue a warning when `zero_grad` is used in `DataParallel` ([#33064](https://github.com/pytorch/pytorch/pull/33064))\r\n\r\n\r\n\r\n## JIT\r\n\r\n* TorchScript compilation fixed for ([#33783](https://github.com/pytorch/pytorch/pull/33783)):  \r\n    * `torch.stft`\r\n    * `torch.lu`, \r\n    * `torch.lu_unpack`\r\n    * `torch.cdist`\r\n    * `torch.norm`\r\n* `tensor.tolist()` compilation now supported, requires output type annotation ([#33472](https://github.com/pytorch/pytorch/pull/34554))\r\n```\r\ndef foo(float_matrix, scalar_ten):\r\n    # type: (Tensor, Tensor) -> Tuple[List[List[float]], bool]\r\n    out1 : List[List[float]] = float_matrix.tolist()\r\n    out2 = torch.jit.annotate(bool, scalar_ten.tolist())\r\n    return out1, out2\r\n```\r\n* `torch.rand_like` and other `_like` constructors no longer require additional arguments in TorchScript\r\n* Compilation for `nn.Module` APIs added [(#29495)](https://github.com/pytorch/pytorch/pull/29495):\r\n    * `children`\r\n    * `named_children`\r\n    * `modules`\r\n    * `named_modules`\r\n* Support for ModuleList Indexing with Integer Literal ([#29236)](https://github.com/pytorch/pytorch/pull/29236/)\r\n* Fixed flipped outputs for `PackedSequence`  ([#32955)](https://github.com/pytorch/pytorch/pull/32955)\r\n* Support `index` and `type` properties on `Device` ([#32953](https://github.com/pytorch/pytorch/pull/32953))\r\n    * `device.index`\r\n    * `device.type`\r\n* Add remaining `Tensor` properties ([#33906](https://github.com/pytorch/pytorch/pull/33906))\r\n    * `tensor.ndim`\r\n    * `tensor.T`\r\n    * `tensor.name`\r\n    * `tensor.is_leaf`\r\n* Fix augmented assignment to non-tensor attributes [#32993](https://github.com/pytorch/pytorch/pull/32993)\r\n* Fixed type resolution for function arguments [#29623](https://github.com/pytorch/pytorch/pull/29623)\r\n    * Previously we resolved types by parsing their names directly, but now TorchScript uses the value of the type directly from Python\r\n    * This allows types types like `torch.device` to be used\r\n* `len` on tuples containing different types [#35768](https://github.com/pytorch/pytorch/pull/35768)\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Fix exception message in Java Tensor ([#30205](https://github.com/pytorch/pytorch/pull/30205)).\r\n* Fix the crashes for c++ not able to find java class through Jni ([#30390](https://github.com/pytorch/pytorch/pull/30390)).\r\n* Add @DoNotStrip to nativeNewTensor method. ([#30472](https://github.com/pytorch/pytorch/pull/30472)).\r\n* GenericDict/List type use unshapedType() ([#30428](https://github.com/pytorch/pytorch/pull/30428)).\r\n* Support tensors with a storage offset in Java ([#31584](https://github.com/pytorch/pytorch/pull/31584)).\r\n* Fix SIGABORT caused by double exception in PyTorchStreamReader when file not found. ([#33243](https://github.com/pytorch/pytorch/pull/33243)).\r\n* Fix `SELECTED_OP_LIST` file path issue ([#33942](https://github.com/pytorch/pytorch/pull/33942)).\r\n* Fix for handling batch size 0. ([#34599](https://github.com/pytorch/pytorch/pull/34599)).\r\n* fixed AutoGradMode/AutoNonVariableTypeMode uses for mobile callsites\r\n* Use `gettimeofday` on iOS ([#30361](https://github.com/pytorch/pytorch/pull/30361)).\r\n\r\n\r\n\r\n## ONNX\r\n\r\n* Fix `weight_norm` export for dim=0 ([#31015](https://github.com/pytorch/pytorch/pull/31015)).\r\n* Fix for constant folding flaky tests ([#32546](https://github.com/pytorch/pytorch/pull/32546)).\r\n* Fix export for avg_pool with default stride  ([#33017](https://github.com/pytorch/pytorch/pull/33017)).\r\n* Fix ONNX CI by moving test data to aws ([#33200](https://github.com/pytorch/pytorch/pull/33200)).\r\n* Fix for random generators export ([#33789](https://github.com/pytorch/pytorch/pull/33789)).\r\n* Fix export of index_put  ([#31552](https://github.com/pytorch/pytorch/pull/31552)).\r\n* Fix for expand -1 dim value ([#34069](https://github.com/pytorch/pytorch/pull/34069)).\r\n* Reduce ONNX test time on CI ([#33242](https://github.com/pytorch/pytorch/pull/33242)).\r\n* ONNX Error Message on Missing Op ([#33593](https://github.com/pytorch/pytorch/pull/33593)).\r\n* Fix exporting `copy_` with index as tensor input ([#32801](https://github.com/pytorch/pytorch/pull/32801)).\r\n* Fix for `rand_like` as well ([#33095](https://github.com/pytorch/pytorch/pull/33095)).\r\n* Added torchvision tests as part of ORT tests ([#31835](https://github.com/pytorch/pytorch/pull/31835)).\r\n* Remove non-ascii character from `torch/onnx/symbolic_opset11.py` ([#31814](https://github.com/pytorch/pytorch/pull/31814)).\r\n* Add flag to enable script tests ([#32654](https://github.com/pytorch/pytorch/pull/32654)).\r\n* Skip same tests in ONNX Python3 CI as in Python2 ([#31827](https://github.com/pytorch/pytorch/pull/31827)).\r\n* Fixed `torch.mm` export ([#34794](https://github.com/pytorch/pytorch/pull/34661))\r\n* Fixed `aten::size` for opset 11 ([#35984](https://github.com/pytorch/pytorch/pull/35984))\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Bug fix: Handle missing keys in observer state dict during load ([#30357](https://github.com/pytorch/pytorch/pull/30357)).\r\n* Fix BC for quantized linear ([#30481](https://github.com/pytorch/pytorch/pull/30481)).\r\n* Fix mapping white list to avoid attaching qconfig for DeQuantStub ([#30636](https://github.com/pytorch/pytorch/pull/30636)).\r\n* Fix default instantation of dynamic quantized LSTM ([#31433](https://github.com/pytorch/pytorch/pull/31433)).\r\n* Use default scale/zero_point in fake_quantize module instead of None ([#32318](https://github.com/pytorch/pytorch/pull/32318)).\r\n* Fix ASAN / potential segfault in quantized Tensor memory allocations. ([#29882](https://github.com/pytorch/pytorch/pull/29882)).\r\n* Don't serialize None values in observer ([#32733](https://github.com/pytorch/pytorch/pull/32733)).\r\n* Enable inplace relu fusion for training ([#33105](https://github.com/pytorch/pytorch/pull/33105)).\r\n* Bug fix in dynamic quantization kernels + better test coverage. ([#33320](https://github.com/pytorch/pytorch/pull/33320)).\r\n* Run weight_post_process for QAT ([#33852](https://github.com/pytorch/pytorch/pull/33852)).\r\n* Fix histogram observer to work with QAT on GPU ([#34232](https://github.com/pytorch/pytorch/pull/34232)).\r\n* Fix the quantized batchnorm2d ([#34579](https://github.com/pytorch/pytorch/pull/34579)).\r\n* Move QScheme ops to c10 ([#30134](https://github.com/pytorch/pytorch/pull/30134)).\r\n* Remove incorrect fp16 dynamic linear/relu op ([#32774](https://github.com/pytorch/pytorch/pull/32774)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Fix serialization memory lifetime issue. ([#30603](https://github.com/pytorch/pytorch/pull/30603)).\r\n* Don't crash callee when function does not exist on it, instead return an Exception ([#32726](https://github.com/pytorch/pytorch/pull/32726)).\r\n* Throw the correct Exception on local client based on the `RemoteException` ([#32936](https://github.com/pytorch/pytorch/pull/32936)).\r\n* Attach autograd edges only for tensors requiring grad. ([#30904](https://github.com/pytorch/pytorch/pull/30904)).\r\n* `WireSerializer` should check `has_storage()` ([#34626](https://github.com/pytorch/pytorch/pull/34626)).\r\n* Fixed potential deadlock in python exception handling ([#35283](https://github.com/pytorch/pytorch/pull/35283/))\r\n\r\n\r\n\r\n## Other\r\n\r\n* `torch.split`: Fixed incorrect gradient computation that assumed the output was not a view ([#32044](https://github.com/pytorch/pytorch/pull/32044)).\r\n* Allowed numpy integer types to be used where we accept Python integers ([#30486](https://github.com/pytorch/pytorch/pull/30486)).\r\n* `torch.unique`, `torch.unique_consecutive`:  fixed bug with zero-element input support ([#31211](https://github.com/pytorch/pytorch/pull/31211)).\r\n* `Tensor.to_sparse`: fixed backward in the non-contiguous tensor case ([#31223](https://github.com/pytorch/pytorch/pull/31223)).\r\n* `torch.index_put`: Added error checks for input tensors\u2019 devices (#31280) ([#31280](https://github.com/pytorch/pytorch/pull/31280)).\r\n* Ensure we switch the CUDA stream correctly in CUDA operations ([#31537](https://github.com/pytorch/pytorch/pull/31537), [#31538](https://github.com/pytorch/pytorch/pull/31538), [#31541](https://github.com/pytorch/pytorch/pull/31541)).\r\n* `torch.SparseTensor`: ensure the legacy sparse constructor doesn't interpret Python data as tensor data. ([#31490](https://github.com/pytorch/pytorch/pull/31490)).\r\n* `torch.argmax`, `torch.argmin`: Fixed incorrect behavior on large tensors ([#33310](https://github.com/pytorch/pytorch/pull/33310)).\r\n* `torch.div`: Fixed to throw an error when dividing by integer zero on CPU  ([#32629](https://github.com/pytorch/pytorch/pull/32629)).\r\n* `torch.cos`: Fixed incorrect gradient computation caused by not properly initializing temporary vectors in avx2 code ([#32722](https://github.com/pytorch/pytorch/pull/32722), [#34281](https://github.com/pytorch/pytorch/pull/34281)).\r\n* `torch.logspace`: Added missing integer dtype support, fixed precision issues in floating-point implementation ([#32744](https://github.com/pytorch/pytorch/pull/32744)).\r\n* `torch.prod`: Fixed behavior when passed a `torch.half` input tensor and `torch.float` output tensor ([#32831](https://github.com/pytorch/pytorch/pull/32831)).\r\n* `torch.max`, `torch.min`: Fixed NaN handling ([#32541](https://github.com/pytorch/pytorch/pull/32541)).\r\n* `torch.max`, `torch.min`: Added error check that operand and outputs are on the same device type ([#32862](https://github.com/pytorch/pytorch/pull/32862)).\r\n*  `torch.stack`: Added missing input size checks ([#32931](https://github.com/pytorch/pytorch/pull/32931)).\r\n* `torch.add`: Fixed memory leak on certain platforms ([#32478](https://github.com/pytorch/pytorch/pull/32478)).\r\n* `torch.normal`: Fixed shape checks ([#33050](https://github.com/pytorch/pytorch/pull/33050)).\r\n* `torch.cumsum`: fixed to handle inputs with zero-sized dimensions correctly ([#31694](https://github.com/pytorch/pytorch/pull/31694)).\r\n* `torch.device`: Disallow incorrectly formatted device strings ([#29087](https://github.com/pytorch/pytorch/pull/29087)).\r\n* `torch.cat`: Disallow passing `out` as one of the input tensors ([#30577](https://github.com/pytorch/pytorch/pull/30577)).\r\n* `torch.pdist`: Added support for large batch sizes ([#31593](https://github.com/pytorch/pytorch/pull/31593)).\r\n* `torch.stft`: Fixed crash when used with `nn.DataParallel` ([#31861](https://github.com/pytorch/pytorch/pull/31861)).\r\n* `torch.autograd`: Ensure the original grad mode is restored during backward ([#31884](https://github.com/pytorch/pytorch/pull/31884)).\r\n* `torch.autograd`: Fixed a race condition by locking graph_task before writing leaf_streams. (#31995) ([#31995](https://github.com/pytorch/pytorch/pull/31995)).\r\n* `torch.tensordot`: Fixed support for negative dimensions ([#31954](https://github.com/pytorch/pytorch/pull/31954)).\r\n* `torch.cumprod`: Fixed to handle inputs with zero-sized dimensions correctly ([#32070](https://github.com/pytorch/pytorch/pull/32070)).\r\n* `torch.pow`: Fixed the gradient computation when the base is a Tensor or Scalar of zeros ([#32062](https://github.com/pytorch/pytorch/pull/32062), [#32063](https://github.com/pytorch/pytorch/pull/32063)).\r\n* `torch.baddbmm`: Fixed bug in corner case ([#33538](https://github.com/pytorch/pytorch/pull/33538)).\r\n* `torch.where`: Added check for consistent devices ([#33432](https://github.com/pytorch/pytorch/pull/33432)).\r\n* `torch.cdist`: Fixed gradient computation for `p=2` and large inputs ([#31167](https://github.com/pytorch/pytorch/pull/31167)).\r\n* `torch.mv`: Fixed NaN handling ([#31666](https://github.com/pytorch/pytorch/pull/31666)).\r\n* `torch.index_put`: Added handling for large input tensors ([#33753](https://github.com/pytorch/pytorch/pull/33753)).\r\n* `torch.addmm`: Fixed incorrect output when using BLAS backend ([#33819](https://github.com/pytorch/pytorch/pull/33819)).\r\n* `torch.topk` fixed double backward when input has non-finite values ([#35253](https://github.com/pytorch/pytorch/pull/35253))\r\n* `torch.load`: Avoid problematic pickle usages on Python 3.8.0 and 3.8.1 ([#33824](https://github.com/pytorch/pytorch/pull/33824)).\r\n* `Tensor.to`: Fixed race condition for gradient computation that spans CUDA devices ([#31930](https://github.com/pytorch/pytorch/pull/31930)).\r\n* `Tensor.random_` added check that `from` and `to` are within the Tensor\u2019s dtype bounds ([#34033](https://github.com/pytorch/pytorch/pull/34033)).\r\n* `Tensor.copy_`: Fixed memory overlap check and allowed outputs to be zero-strided tensors if the size is <= 1 along that dimension ([#34100](https://github.com/pytorch/pytorch/pull/34100)).\r\n* `nn.BatchNorm{1,2,3}d`: fixed gradient computation for empty inputs ([#32820](https://github.com/pytorch/pytorch/pull/32820)).\r\n* `nn.BatchNorm`: Fixed behavior for inputs with large batch sizes ([#32763](https://github.com/pytorch/pytorch/pull/32763)).\r\n* `nn.Conv2d`: Fixed 5d weight handling with MKLDNN backend ([#34115](https://github.com/pytorch/pytorch/pull/34115)).\r\n* `nn.Conv3d`: Fixed unstable gradient computation ([#34358](https://github.com/pytorch/pytorch/pull/34358)).\r\n* `nn.Conv{1,2,3}d`: added support for empty batch size([#32709](https://github.com/pytorch/pytorch/pull/32709)).\r\n* `nn.Conv{1,2,3}d`: fixed `CUDNN_STATUS_NOT_SUPPORTED` errors by trying multiple algorithms ([#33073](https://github.com/pytorch/pytorch/pull/33073)).\r\n* `nn.Conv{1,2,3}d`: fixed padding mode support and added additional padding modes (reflection and replication) ([#31784](https://github.com/pytorch/pytorch/pull/31784)).\r\n* `nn.Conv2d`, `nn.Conv3d`, `nn.Conv1d`, `nn.ConvTranspose2d`: Fixed support for batch sizes greater than 2^32 ([#31383](https://github.com/pytorch/pytorch/pull/31383), [#31379](https://github.com/pytorch/pytorch/pull/31379), [#31889](https://github.com/pytorch/pytorch/pull/31889), [#34407,](https://github.com/pytorch/pytorch/pull/34407)[#31510](https://github.com/pytorch/pytorch/pull/31510)).\r\n* `nn.InstanceNorm`, `nn.GroupNorm`: Added error check for input with exactly one element ([#29082](https://github.com/pytorch/pytorch/pull/29082)).\r\n* `nn.RNN`: Fixed moving RNNs to a device after applying weight norm ([#32563](https://github.com/pytorch/pytorch/pull/32563), [#32989](https://github.com/pytorch/pytorch/pull/32989)).\r\n* `nn.MultiLabelMarginLoss`: added support for 0-d tensors ([#30765](https://github.com/pytorch/pytorch/pull/30765)).\r\n* `nn.GroupNorm`: added support for empty batch ([#32401](https://github.com/pytorch/pytorch/pull/32401)).\r\n* `nn.NLLLoss`: fixed to support empty tensors on CUDA ([#31491](https://github.com/pytorch/pytorch/pull/31491)).\r\n* `nn.GroupNorm`: corrected input size check ([#33008](https://github.com/pytorch/pytorch/pull/33008))\r\n* `nn.MultiLabelMarginLoss`: fixed memory leak on CUDA ([#30767](https://github.com/pytorch/pytorch/pull/30767)).\r\n* `nn.MultiMarginLoss`: fixed error checking on CUDA for the 1D case.  ([#30825](https://github.com/pytorch/pytorch/pull/30825)).\r\n* `nn.Softmax`: Fixed half->float case of softmax backward ([#30838](https://github.com/pytorch/pytorch/pull/30838)).\r\n* `nn.Softshrink`: Added check that lambda is no less than zero ([#33201](https://github.com/pytorch/pytorch/pull/33201)).\r\n* `nn.functional.interpolate`: added support for empty batch size input for interpolate. ([#32400](https://github.com/pytorch/pytorch/pull/32400)).\r\n* `nn.functional.pad`: Also return a new tensor instead of sometimes returning a view ([#32350](https://github.com/pytorch/pytorch/pull/32350)).\r\n* `nn.functional.grid_sample`: Fixed gradient computation at image borders ([#32829](https://github.com/pytorch/pytorch/pull/32829)).\r\n* `nn.functional.leaky_relu_`: disabled incorrect leaky_relu_ negative slope backward calculation ([#33639](https://github.com/pytorch/pytorch/pull/33639)).\r\n* `optim.LambdaLR`: removed unintentional side effects ([#32848](https://github.com/pytorch/pytorch/pull/32848)).\r\n* `optim.Adam`, `optim.AdamW`: Added missing `weight_decay` parameter validation ([#33126](https://github.com/pytorch/pytorch/pull/33126)).\r\n* `optim.MultiStepLR`: Fix \u201cunbound local variable\u201d error by removing return value for `__exit__` ([#32997](https://github.com/pytorch/pytorch/pull/32997)).\r\n* `optim.MultiStepLR`: Fixed broken `step()` method ([#33356](https://github.com/pytorch/pytorch/pull/33356)).\r\n* `torch.autograd`: added new error message if incorrect usage would cause a deadlock ([#32295](https://github.com/pytorch/pytorch/pull/32295)).\r\n* `torch.autograd`: Prohibited copying autograd engines ([#34567](https://github.com/pytorch/pytorch/pull/34567)).\r\n* `torch.autograd`: Fixed incorrect handling of functions that return multiple views ([#32790](https://github.com/pytorch/pytorch/pull/32790)).\r\n* `autograd.Function`: Fixed error if `Function` returned a view in a `torch.no_grad` block ([#33896](https://github.com/pytorch/pytorch/pull/33896)).\r\n* `autograd.Function`: Added more error checks for incorrect behavior ([#33069](https://github.com/pytorch/pytorch/pull/33069)).\r\n* `autograd.Function`: Added nice error message if missing overrides ([#33142](https://github.com/pytorch/pytorch/pull/33142)).\r\n* `autograd.Function`: Fixed version check for `grad_fn` for views ([#34145](https://github.com/pytorch/pytorch/pull/34145)).\r\n* `autograd.profiler`: Fix incorrect chrome trace formatting output for CUDA traces ([#33987](https://github.com/pytorch/pytorch/pull/33987)).\r\n* `multiprocessing.util.register_after_fork`: fixed crash on Windows  ([#30809](https://github.com/pytorch/pytorch/pull/30809)).\r\n* `utils.data.DataLoader`: Fixed potential hang when exiting main process ([#33721](https://github.com/pytorch/pytorch/pull/33721)).\r\n* `utils.tensorboard.SummaryWriter` fixed `scale_factor` calculation for uint8 tensor ([#31778](https://github.com/pytorch/pytorch/pull/31778)).\r\n* `utils.tensorboard` Fix for when PyTorch model trace has RecursiveScriptModules ([#30430](https://github.com/pytorch/pytorch/pull/30430)).\r\n* Fixed `CPU_INTEL` flag error on Windows ([#30564](https://github.com/pytorch/pytorch/pull/30564)).\r\n* Don't use `RTLD_GLOBAL` to load `_C`, resolving a multitude of weird segfaults and crashes\r\n    when PyTorch is imported along with other packages ([#31162](https://github.com/pytorch/pytorch/pull/31162)).\r\n* Fixed dll load logic for Python 3.8 on Windows ([#32215](https://github.com/pytorch/pytorch/pull/32215)).\r\n* `quasirandom.SobolEngine`: Fixed crash when default tensor type is CUDA ([#32496](https://github.com/pytorch/pytorch/pull/32496)).\r\n\r\n* Fixed error message when converting NumPy array with negative strides to a `torch.Tensor` ([#33254](https://github.com/pytorch/pytorch/pull/33254)).\r\n* Fixed crash when indexing a `torch.Tensor` with a single-element array ([#33456](https://github.com/pytorch/pytorch/pull/33456)).\r\n* Fixed crash when converting CUDA tensors and non-strided tensors to NumPy arrays ([#33612](https://github.com/pytorch/pytorch/pull/33612)).\r\n* Prevented crash on exit from static destructor race on Windows ([#33955](https://github.com/pytorch/pytorch/pull/33955)).\r\n* Fixed uncaught `std::domain_error` on macOS ([#34301](https://github.com/pytorch/pytorch/pull/34301)).\r\n* Don\u2019t reset worker affinity when using operators that call into OpenMP ([#29006](https://github.com/pytorch/pytorch/pull/29006)).\r\n* `torch.backends.mkldnn`: changed to be usable without import ([#32055](https://github.com/pytorch/pytorch/pull/32055)).\r\n\r\n\r\n\r\n\r\n# Performance\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Java Tensor hybrid, owns at::Tensor, no memcopy for java outputs. ([#30501](https://github.com/pytorch/pytorch/pull/30501)).\r\n* Tensor prep from image in native ([#31426](https://github.com/pytorch/pytorch/pull/31426)).\r\n* Pass to remove prepacking ops. ([#34319](https://github.com/pytorch/pytorch/pull/34319)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Per channel quantization performance improvement ([#33772](https://github.com/pytorch/pytorch/pull/33772)).\r\n* Speed up per-channel min-max observer ([#34118](https://github.com/pytorch/pytorch/pull/34118)).\r\n* Vectorized qmul and more methods on qint data types ([#34376](https://github.com/pytorch/pytorch/pull/34376)).\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Improve `ProcessGroupAgent` serialization speed ([#29785](https://github.com/pytorch/pytorch/pull/29785)).\r\n* Avoid sending large unneeded data over wire in `ProcessGroupAgent`. ([#31357](https://github.com/pytorch/pytorch/pull/31357)).\r\n* Integrate async mode for autograd engine with distributed autograd. ([#31508](https://github.com/pytorch/pytorch/pull/31508)).\r\n* Make handling of `FORWARD_AUTOGRAD_REQ` in `request_callback_impl` nonblocking ([#32476](https://github.com/pytorch/pytorch/pull/32476)).\r\n\r\n\r\n\r\n## Other\r\n\r\n* Major multithreaded performance regression when doing operator calls resolved ([#30333](https://github.com/pytorch/pytorch/pull/30333))\r\n* Improved performance of comparison ops on CUDA ([#29743](https://github.com/pytorch/pytorch/pull/29743)).\r\n* `Tensor.view` improved performance ([#30554](https://github.com/pytorch/pytorch/pull/30554)).\r\n* Improved tensor creation overhead ([#30452](https://github.com/pytorch/pytorch/pull/30452), [#30709](https://github.com/pytorch/pytorch/pull/30709))\r\n* `nn.SmoothL1Loss`: vectorized gradient computation on CPU. ([#30046](https://github.com/pytorch/pytorch/pull/30046)).\r\n* `nn.EmbeddingBag`: improved performance on CPU ([#30701](https://github.com/pytorch/pytorch/pull/30701), [#27477](https://github.com/pytorch/pytorch/pull/27477)).\r\n* `nn.LayerNorm`: optimized with explicit vectorization using Vec256 ([#31127](https://github.com/pytorch/pytorch/pull/31127)).\r\n* `Tensor.copy_`: fixed kernel speed regression introduced in #29631 ([#31279](https://github.com/pytorch/pytorch/pull/31279)).\r\n* Moved a number of debug asserts to not compile in release builds ([#31240](https://github.com/pytorch/pytorch/pull/31240)).\r\n* `Tensor::has_names` sped up for unnamed tensors ([#31436](https://github.com/pytorch/pytorch/pull/31436)).\r\n* `torch.index_select`: optimized performance on CPU ([#30598](https://github.com/pytorch/pytorch/pull/30598)).\r\n* `nn.Conv{1,2,3}d`: Improved performance by refactoring `bias` handling for cuDNN backend ([#31524](https://github.com/pytorch/pytorch/pull/31524)).\r\n* `torch.norm`: Optimized case where `p = 2` ([#31903](https://github.com/pytorch/pytorch/pull/31903)).\r\n* `nn.utils.clip_grad_norm_`: Refactored the computation for more performance ([#32020](https://github.com/pytorch/pytorch/pull/32020)).\r\n* Made an assert on a hotpath trigger only in DEBUG mode ([#32117](https://github.com/pytorch/pytorch/pull/32117)).\r\n* First steps toward TensorIterator unrolling and vectorized load ([#31974](https://github.com/pytorch/pytorch/pull/31974)).\r\n* `nn.functional.normalize`: changed to use `clamp_min_`  ([#32360](https://github.com/pytorch/pytorch/pull/32360)).\r\n* Stopped refreshing numel on a stride update ([#32116](https://github.com/pytorch/pytorch/pull/32116)).\r\n* `nn.functional.softplus`: vectorized operator and gradient computation on CPU ([#32944](https://github.com/pytorch/pytorch/pull/32944)).\r\n* `torch.gather` regression fixed by not materializing loop vars in error message ([#33108](https://github.com/pytorch/pytorch/pull/33108)).\r\n* `nn.ELU` forward and backward vectorized on CPU ([#32985](https://github.com/pytorch/pytorch/pull/32985), [#32986](https://github.com/pytorch/pytorch/pull/32986))\r\n* `torch.cat`: optimized performance on CPU ([#30806](https://github.com/pytorch/pytorch/pull/30806), [#33534](https://github.com/pytorch/pytorch/pull/33534)).\r\n* `torch.conv3d`: optimized Unfold3d to improve performance ([#33191](https://github.com/pytorch/pytorch/pull/33191)).\r\n* Workaround performance bug and memory leak in GOMP for AMD CPUs ([#32875](https://github.com/pytorch/pytorch/pull/32875)).\r\n* Improved TensorIterator overhead ([#33165](https://github.com/pytorch/pytorch/pull/33165)).\r\n* `torch.conv3d`: optimized Unfold3dAcc to improve gradient computation performance ([#33317](https://github.com/pytorch/pytorch/pull/33317)).\r\n* `torch.roll` improved performance ([#33623](https://github.com/pytorch/pytorch/pull/33623)).\r\n* Bounds checking for functor execution in vectorized/unrolled kernels ([#33642](https://github.com/pytorch/pytorch/pull/33642)).\r\n* `nn.EmbeddingBag`: improved performance on CUDA ([#33589](https://github.com/pytorch/pytorch/pull/33589)).\r\n* Remove unnecessary tensor copies while calling operators ([#33732](https://github.com/pytorch/pytorch/pull/33732)).\r\n* clang intrinsics targeting on Windows ([#33958](https://github.com/pytorch/pytorch/pull/33958)).\r\n* `nn.Dropout`: added vectorized CUDA implementation ([#33879](https://github.com/pytorch/pytorch/pull/33879)).\r\n* `nn.UpSampleNearest{1, 2, 3}d` performance on CPU optimized (#31452) ([#31452](https://github.com/pytorch/pytorch/pull/31452)).\r\n* Remove `cudaMemcpy` on full memory overlap ([#34548](https://github.com/pytorch/pytorch/pull/34548)).\r\n* CUDA Loops: move address computation into policy, make `policy.load` load all arguments ([#33720](https://github.com/pytorch/pytorch/pull/33720)).\r\n* `nn.BatchNorm{1, 2, 3}d` contiguous case's performance improved ([#34530](https://github.com/pytorch/pytorch/pull/34530)).\r\n* Add the build for runtime dispatch for AVX, AVX2 instruction set ([#26125](https://github.com/pytorch/pytorch/pull/26125)).\r\n* `nn.RReLU` performance improved up to 5x for inference on CPU  ([#31094](https://github.com/pytorch/pytorch/pull/31094)).\r\n* `nn.LogSigmoid` performance improved up to 10x on CPU ([#30958](https://github.com/pytorch/pytorch/pull/30958)).\r\n* `torch.dist` performance improved up to 2x ([#29714](https://github.com/pytorch/pytorch/pull/29714)).\r\n* `torch.max`, `torch.min` performance improved up to 1.5x on CPU ([#33936](https://github.com/pytorch/pytorch/pull/33936)). \r\n* `nn.GLU` performance improved up to 1.5X on CPU  ([#33179](https://github.com/pytorch/pytorch/pull/33179)).\r\n* `nn.LeakyReLU` performance improved up to 4x ([#29899](https://github.com/pytorch/pytorch/pull/29899)).\r\n* `nn.HardTanh` performance improved up to 5x  ([#30152](https://github.com/pytorch/pytorch/pull/30152)). \r\n\r\n\r\n\r\n# Documentation\r\n\r\n## Python\r\n\r\n* Added documentation for `nn.functional.softplus` ([#30055](https://github.com/pytorch/pytorch/pull/30055), [#32945](https://github.com/pytorch/pytorch/pull/32945)).\r\n* `torch.max`: Added warning about different, nondeterministic behavior on CPU and CUDA ([#31115](https://github.com/pytorch/pytorch/pull/31115)).\r\n* Clarified the documentation for `nn.NLLLoss`  ([#31488](https://github.com/pytorch/pytorch/pull/31488)).\r\n* Exclude generated source docs from Google search indexing ([#31484](https://github.com/pytorch/pytorch/pull/31484)).\r\n* `torch.poisson` docstring added to documentation (#31667) ([#31667](https://github.com/pytorch/pytorch/pull/31667)).\r\n* `torch.eq` fixed incorrect examples in documentation ([#32399](https://github.com/pytorch/pytorch/pull/32399)).\r\n* `torch.load`: added warning regarding pickle insecurity ([#32593](https://github.com/pytorch/pytorch/pull/32593)).\r\n* `optim.CosineAnnealingLR`: fixed the usage in examples ([#31358](https://github.com/pytorch/pytorch/pull/31358)).\r\n* Added doc previewing instructions ([#31905](https://github.com/pytorch/pytorch/pull/31905)).\r\n* Removed legacy `.data` usages from the `torch.nn` documentation ([#31481](https://github.com/pytorch/pytorch/pull/31481)).\r\n* Fixed description of convolution modules ([#30079](https://github.com/pytorch/pytorch/pull/30079)).\r\n* `Tensor.t()`, `Tensor.permute()`, `Tensor.unfold()`, and `Tensor.select()` clarified to note that they return views ([#32512](https://github.com/pytorch/pytorch/pull/32512)).\r\n* `torch.multiprocessing` Updated documentation indicating that start_method is ignored for `mp.spawn()` ([#33070](https://github.com/pytorch/pytorch/pull/33070)).\r\n* Improved CPU threading documentation ([#33083](https://github.com/pytorch/pytorch/pull/33083)).\r\n* `nn.BCELoss`: documented how it avoids infinite results ([#33160](https://github.com/pytorch/pytorch/pull/33160)).\r\n* `nn.utils.rnn.pack_padded_sequence`: Improved the description of `enforce_sorted`  ([#33617](https://github.com/pytorch/pytorch/pull/33617)).\r\n* `nn.utils.pad_packed_sequence`: doc improvement ([#33768](https://github.com/pytorch/pytorch/pull/33768)).\r\n* `nn.LPPool{1,2}d` : removed nonexistent parameter ([#33714](https://github.com/pytorch/pytorch/pull/33714)).\r\n* Created a Tensor View documentation page that documents all PyTorch operations that return views ([#32560](https://github.com/pytorch/pytorch/pull/32560)).\r\n* Added grad context manager doc to top level torch module. ([#33877](https://github.com/pytorch/pytorch/pull/33877)).\r\n* Enhanced reproducibility documentation ([#33795](https://github.com/pytorch/pytorch/pull/33795)).\r\n* Numerous typo fixes ([#30448](https://github.com/pytorch/pytorch/pull/30448), [#30518](https://github.com/pytorch/pytorch/pull/30518), [#30614](https://github.com/pytorch/pytorch/pull/30614), [#30464](https://github.com/pytorch/pytorch/pull/30464), [#30608](https://github.com/pytorch/pytorch/pull/30608), [#24335](https://github.com/pytorch/pytorch/pull/24335), [#34581](https://github.com/pytorch/pytorch/pull/34581), [#34624](https://github.com/pytorch/pytorch/pull/34624), [#34008](https://github.com/pytorch/pytorch/pull/34008), [#31395](https://github.com/pytorch/pytorch/pull/31395), [#31677](https://github.com/pytorch/pytorch/pull/31677), [#31617](https://github.com/pytorch/pytorch/pull/31617), [#31973](https://github.com/pytorch/pytorch/pull/31973), [#32068](https://github.com/pytorch/pytorch/pull/32068), [#33689](https://github.com/pytorch/pytorch/pull/33689), [#30385](https://github.com/pytorch/pytorch/pull/30385), [#32003](https://github.com/pytorch/pytorch/pull/32003), [#31682](https://github.com/pytorch/pytorch/pull/31682), [#30846](https://github.com/pytorch/pytorch/pull/30846), [#33478](https://github.com/pytorch/pytorch/pull/33478), [#33549](https://github.com/pytorch/pytorch/pull/33549), [#32307](https://github.com/pytorch/pytorch/pull/32307), [#33144](https://github.com/pytorch/pytorch/pull/33144), [#33805](https://github.com/pytorch/pytorch/pull/33805), [#33836](https://github.com/pytorch/pytorch/pull/33836), [#34053](https://github.com/pytorch/pytorch/pull/34053)).\r\n* Numerous formatting and/or rendering fixes ([#30377](https://github.com/pytorch/pytorch/pull/30377), [#30779](https://github.com/pytorch/pytorch/pull/30779), [#32667](https://github.com/pytorch/pytorch/pull/32667), [#34027](https://github.com/pytorch/pytorch/pull/34027), [#32911](https://github.com/pytorch/pytorch/pull/32911), [#30814](https://github.com/pytorch/pytorch/pull/30814), [#30815](https://github.com/pytorch/pytorch/pull/30815), [#31760](https://github.com/pytorch/pytorch/pull/31760), [#34503](https://github.com/pytorch/pytorch/pull/34503)).\r\n\r\n\r\n\r\n## C++ API\r\n\r\n* Fix `at::Tensor` docs generation and make it accessible again at https://pytorch.org/cppdocs/api/classat_1_1_tensor.html ([#34467](https://github.com/pytorch/pytorch/pull/34467))\r\n* Add docs for all `torch::nn modules` and functionals ([#34522](https://github.com/pytorch/pytorch/pull/34522)) ([#34688](https://github.com/pytorch/pytorch/pull/34688)) ([#34752](https://github.com/pytorch/pytorch/pull/34752))\r\n* Improve C++ autograd and tensor indexing docs ([#35919](https://github.com/pytorch/pytorch/pull/35919))\r\n* Fix example in `torch::nn::ModuleList` docs ([#34463](https://github.com/pytorch/pytorch/pull/34463))\r\n\r\n\r\n\r\n## RPC\r\n\r\n* Reorganize RPC API doc and add introduction ([#30491](https://github.com/pytorch/pytorch/pull/30491), [#35109](https://github.com/pytorch/pytorch/pull/35109)).\r\n* Make doc source format consistent in `rpc/init.cpp` ([#30515](https://github.com/pytorch/pytorch/pull/30515)).\r\n* Add examples to RRef doc ([#30516](https://github.com/pytorch/pytorch/pull/30516)).\r\n* Add more details to explain `rpc_backend_options` arg in `init_rpc` ([#30855](https://github.com/pytorch/pytorch/pull/30855)).\r\n* Fix examples in API doc ([#30856](https://github.com/pytorch/pytorch/pull/30856)).\r\n* Fix examples in RRef API doc ([#30857](https://github.com/pytorch/pytorch/pull/30857)).\r\n* Document WorkerInfo and `RpcBackendOptions` structures in RPC docs. ([#31077](https://github.com/pytorch/pytorch/pull/31077)).\r\n* Explain RPC behavior when using Tensor as arg or return value ([#31968](https://github.com/pytorch/pytorch/pull/31968)).\r\n* Update RPC docs to reflect correct use of dist_autograd backwards and dist_optim `step() `([#34670](https://github.com/pytorch/pytorch/pull/34670)).\r\n* Minor doc tweak to use mp.spawn in example ([#30381](https://github.com/pytorch/pytorch/pull/30381)).\r\n* Update distributed autograd note ([#34657](https://github.com/pytorch/pytorch/pull/34657)).\r\n\r\n\r\n\r\n## Mobile\r\n\r\n* Add info about transitive dependencies in case of using local aars ([#30128](https://github.com/pytorch/pytorch/pull/30128)).\r\n* Update Docs for building PyTorch for Android. ([#32578](https://github.com/pytorch/pytorch/pull/32578)).\r\n* Javadoc changes ([#31956](https://github.com/pytorch/pytorch/pull/31956)).\r\n\r\n\r\n\r\n## Quantization\r\n\r\n* Updates to quantization documentation ([#30288](https://github.com/pytorch/pytorch/pull/30288)).\r\n* Fix docs so that the example works ([#30120](https://github.com/pytorch/pytorch/pull/30120)).\r\n* Add the explicit per-tensor/per-channel quant info when we print the module ([#30591](https://github.com/pytorch/pytorch/pull/30591)).\r\n* Fixed typos in quantization docs / docstrings ([#34182](https://github.com/pytorch/pytorch/pull/34182)).\r\n* Docs entry for the `is_quantized` ([#32075](https://github.com/pytorch/pytorch/pull/32075)).\r\n\r\n\r\n\r\n# Deprecations\r\n\r\n## Python\r\n\r\n### How to figure out which line in your code is raising a warning\r\n\r\nAttempting to use deprecated behavior will raise warnings. Unfortunately, sometimes it is not entirely obvious what line of code the warning corresponds to, especially if the the warning comes from our C++ backend. For example, with a file named `foo.py` with the following contents,\r\n\r\n```\r\nimport torch\r\n# This is newly deprecated behavior, see the next section\r\ntorch.tensor(1) / torch.tensor(2)\r\n```\r\n\r\nrunning it doesn\u2019t give us the location of the warning:\r\n\r\n```\r\n> python foo.py\r\n../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true\r\n division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\r\n```\r\n\r\nWe can use the `warnings` module to tell us where the warning is by asking it to treat warnings as errors:\r\n\r\n```\r\nimport torch\r\nimport warnings\r\nwarnings.filterwarnings('error', message='Integer division')\r\n# This is newly deprecated behavior, see the next section\r\ntorch.tensor(1) / torch.tensor(2)\r\n```\r\n\r\nRunning the file now tells us exactly where the warning is:\r\n\r\n```\r\n> python foo.py\r\nTraceback (most recent call last):\r\n  File \"foo.py\", line 5, in <module>\r\n    torch.tensor(1) / torch.tensor(2)\r\nUserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide\r\nor floor_divide (// in Python) instead.\r\n```\r\n\r\n\r\n\r\n### Deprecated `torch.div` and `torch.addcdiv` integer floor division behavior ([#34570](https://github.com/pytorch/pytorch/pull/34570))\r\n\r\nIn 1.5.0 and older PyTorch releases `torch.div` and the `/` operator perform integer floor division. In a future PyTorch release, torch.div (including the `/` operator) will perform \"true\" division as in Python3 and NumPy.\r\n\r\nTo floor divide integer tensors, please use `torch.floor_divide` instead.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Before</th><th>After</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(3) / torch.tensor(2)\r\n../aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\r\ntensor(1)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> NB: the following is equivalent to `torch.floor_divide(torch.tensor(3), torch.tensor(2))\r\n>>> torch.tensor(3) // torch.tensor(2)\r\ntensor(1)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThe fix for `torch.addcdiv` is similar.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Before</th><th>After</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> torch.addcdiv(input, tensor, other, value=value)\r\n../aten/src/ATen/native/PointwiseOps.cpp:81: UserWarning: Integer division with addcdiv is deprecated, and in a future  release addcdiv will perform a true division of tensor1 and tensor2. The current addcdiv behavior can be replicated using floor_divide for integral inputs (self + value * tensor1 // tensor2) and division for float inputs (self + value * tensor1 / tensor2). The new addcdiv behavior can be implemented with true_divide (self + value * torch.true_divide(tensor1, tensor2).\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.tensor(0)\r\n>>> tensor = torch.tensor(1)\r\n>>> other = torch.tensor(3)\r\n>>> value = 1\r\n>>> (input + torch.floor_divide(value * tensor, other))\r\ntensor(0)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n\r\n### Deprecated `torch.full` returning float tensors if no dtype is specified ([#34709](https://github.com/pytorch/pytorch/pull/34709)).\r\n\r\nIn a future PyTorch release, `torch.full` will infer its dtype from its fill value when the optional dtype and out parameters are unspecified, matching NumPy's inference for `numpy.full`. For example, `torch.full(size, 1)` will return a tensor of `torch.long` dtype, unlike today where it returns a tensor of `torch.float` dtype.\r\n\r\n\r\n### Deprecated `torch.nn.modules.conv._ConvTransposeMixin` ([#31784](https://github.com/pytorch/pytorch/pull/31784)).\r\n\r\nThis is an internal-facing class that is not a part of our public API. We\u2019ve refactored some PyTorch internals to work without it and will remove it in a future release. \r\n\r\n\r\n### Deprecated positional args in multiple `torch` function signatures ([#32009](https://github.com/pytorch/pytorch/pull/32009), [#33428](https://github.com/pytorch/pytorch/pull/33428))\r\n\r\nBelow please find a list of deprecated signatures and what to change them to.\r\n\r\n* `torch.add(self: Tensor, alpha: Scalar, other: Tensor)`, `torch.sub(self: Tensor, alpha: Scalar, other: Tensor)` please use `alpha` as a keyword-only arg instead of positional args\r\n* `torch.addbmm(beta: Scalar, self: Tensor, alpha: Scalar, batch1: Tensor, batch2: Tensor)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.addcdiv(self: Tensor, value: Scalar, tensor1: Tensor, tensor2: Tensor)`, `torch.addmdiv(self: Tensor, value: Scalar, tensor1: Tensor, tensor2: Tensor)`: please use `value` as a keyword-only arg\r\n* `torch.addmm(beta: Scalar, self: Tensor, alpha: Scalar, mat1: Tensor, mat2: Tensor)`, `torch.sspaddmm(beta: Scalar, self: Tensor, alpha: Scalar, mat1: Tensor, mat2: Tensor)` please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.addmv(beta: Scalar, self: Tensor, alpha: Scalar, mat: Tensor, vec: Tensor)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.addr(beta: Scalar, self: Tensor, alpha: Scalar, vec1: Tensor, vec2: Scalar)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n* `torch.baddbmm(beta: Scalar, self: Tensor, alpha: Scalar, batch1: Tensor, batch2: Tensor)`: please use `alpha` and `beta` as keyword only args instead of positional args.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Before</th><th>After</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.zeros(2,3).add(2, torch.ones(2, 3))\r\n../torch/csrc/utils/python_arg_parser.cpp:750: UserWarning: This overload of add is deprecated:\r\n        add(Number alpha, Tensor other)\r\nConsider using one of the following signatures instead:\r\n        add(Tensor other, Number alpha)\r\ntensor([[2., 2., 2.],\r\n        [2., 2., 2.]])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.zeros(2, 3).add(torch.ones(2, 3), alpha=2)\r\ntensor([[2., 2., 2.],\r\n        [2., 2., 2.]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Deprecate modifying in-place a view that returned by a custom autograd Function ([#32839](https://github.com/pytorch/pytorch/pull/32839)). \r\n\r\nModifying in-place a view that was created by a custom Function leads to the custom backward not being called or being called with a partial gradient. This behavior will be removed in 1.6.\r\n\r\nPlease clone() the output of the Function to avoid incorrect gradient computation.\r\n\r\n```\r\nclass Id(Function):\r\n    @staticmethod\r\n    def forward(ctx, input):\r\n        return input.view_as(input)\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_input):\r\n        return grad_input\r\n```\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.5.0</th><th>Version 1.5.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.randn(3, requires_grad=True)\r\n>>> other = torch.randn(3)\r\n>>> output = Id.apply(input)\r\n>>> output.copy_(other)\r\n# Warning: Incorrect gradients\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> input = torch.randn(3, requires_grad=True)\r\n>>> other = torch.randn(3)\r\n>>> output = Id.apply(input).clone()\r\n>>> output.copy_(other)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Deprecate modifying in-place a view created inside a no_grad block (#32839)\r\n\r\nModifying in-place a view created inside a no_grad block is ambiguous and error-prone so we have deprecated it. \r\n\r\nHere is an example of some code that we\u2019ve deprecated. In previous versions of PyTorch, the following code throws a non-descriptive error message, but we've added a deprecation in 1.5.0.\r\n\r\n```\r\n>>> base = torch.rand(10, requires_grad=True)\r\n>>> var = torch.rand([], requires_grad=True)\r\n>>> with torch.no_grad():\r\n>>>     view = base[1]\r\n>>> view.copy_(var)\r\n>>> torch.autograd.grad(base.sum(), var)\r\nRuntimeError: A view was created in no_grad mode and is being modified inplace with grad mode enabled. Given that this use case is ambiguous and error-prone,\r\nit is deprecated and will be forbidden  starting 1.6 (see https://github.com/pytorch/pytorch/pull/32839 for more details about this). You can clarify your code and remove this warning by moving both the view and the inplace either both inside the no_grad block (if you don't want the inplace to be tracked) or both outside (if you want the inplace to be tracked).\r\n```\r\nIf you want to differentiate, you should change the above code to\r\n```\r\n>>> base = torch.rand(10, requires_grad=True)\r\n>>> var = torch.rand([], requires_grad=True)\r\n>>> view = base[1]\r\n>>> view.copy_(var)\r\n>>> torch.autograd.grad(base.sum(), var)\r\n(tensor(1.),)\r\n```\r\n\r\nIf you don\u2019t want to differentiate, you should change it to\r\n\r\n```\r\n>>> base = torch.rand(10, requires_grad=True)\r\n>>> var = torch.rand([], requires_grad=True)\r\n>>> with torch.no_grad():\r\n>>>     view = base[1]\r\n>>>     view.copy_(var)\r\n```\r\n\r\n\r\n## C++ API\r\n\r\n### Deprecated `Tensor.type()` [(#30281](https://github.com/pytorch/pytorch/pull/30281))\r\n\r\nPlease use `Tensor.options()` instead.\r\n\r\n\r\n# Miscellaneous\r\n\r\n* Part of an automated mixed-precision solution ([#33366](https://github.com/pytorch/pytorch/pull/33366), [#33832](https://github.com/pytorch/pytorch/pull/33832)).\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.5.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.5.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.5.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/25725370", "dateCreated": "2020-04-20T23:59:38Z", "datePublished": "2020-04-21T16:26:30Z"}, {"tagName": "v1.4.0", "name": "Mobile build customization, Distributed model parallel training, Java bindings, and more", "authorName": "nairbv", "authorType": "User", "body": "# PyTorch 1.4.0 Release Notes\r\n- Highlights\r\n- Backwards Incompatible Changes\r\n  * Python\r\n  * JIT\r\n  * C++\r\n- New Features\r\n  * torch.optim\r\n  * Distributed\r\n  * RPC [Experimental]\r\n  * JIT\r\n  * Mobile\r\n- Improvements\r\n  * Distributed\r\n  * JIT\r\n  * Mobile\r\n  * Named Tensors\r\n  * C++ API\r\n  * AMD Support\r\n  * ONNX\r\n  * Quantization\r\n  * Visualization\r\n  * Other Improvements\r\n- Bug Fixes\r\n  * Distributed\r\n  * RPC\r\n  * C++ API\r\n  * JIT\r\n  * Quantization\r\n  * Mobile\r\n  * Other Bug fixes\r\n- Deprecations\r\n- Performance\r\n\r\nThe PyTorch v1.4.0 release is now available.\r\n\r\nThe release contains over 1,500 commits and a significant amount of effort in areas spanning existing areas like JIT, ONNX, Distributed, Performance and Eager Frontend Improvements and improvements to experimental areas like mobile and quantization. It also contains new experimental features including rpc-based model parallel distributed training and language bindings for the Java language (inference only). \r\n\r\n**PyTorch 1.4 is the last release that supports Python 2**.  For the C++ API, it is the last release that supports C++11: you should start migrating to Python 3 and building with C++14 to make the future transition from 1.4 to 1.5 easier.\r\n\r\n\r\n# Highlights\r\n\r\n## PyTorch Mobile - Build level customization \r\n\r\nFollowing the experimental release of [PyTorch Mobile in the 1.3 release](https://pytorch.org/blog/pytorch-1-dot-3-adds-mobile-privacy-quantization-and-named-tensors/), PyTorch 1.4 adds additional mobile support including the ability to customize build scripts at a fine-grain level. This allows mobile developers to optimize library size by only including the operators used by their models and, in the process, reduce their on device footprint significantly. Initial results show that, for example, a customized MobileNetV2 is 40% to 50% smaller than the prebuilt PyTorch mobile library. [Learn more](https://pytorch.org/mobile/home/) about how to create your own custom builds, and please engage with the community on the [PyTorch forums](https://discuss.pytorch.org/c/mobile) to provide any feedback you have.\r\n\r\n## Distributed Model Parallel Training [Experimental]\r\n\r\n With the scale of models, such as RoBERTa, continuing to increase into the billions of parameters, model parallel training has become ever more important to help researchers push the limits. This release provides a distributed RPC framework to support distributed model parallel training. It allows for running functions remotely and referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backwards and update parameters across RPC boundaries. \r\n\r\nTo learn more about the APIs and the design of this feature, see the links below:\r\n\r\n* [API documentation](https://pytorch.org/docs/stable/rpc.html)\r\n* [Distributed Autograd design doc](https://pytorch.org/docs/stable/notes/distributed_autograd.html)\r\n* [Remote Reference design doc](https://pytorch.org/docs/stable/notes/rref.html)\r\n\r\nFor the full tutorials, see the links below: \r\n\r\n* [A full RPC tutorial](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html)\r\n* [Examples using model parallel training for reinforcement learning and with an LSTM](https://github.com/pytorch/examples/tree/master/distributed/rpc)\r\n\r\n As always, you can connect with community members and discuss more on the [forums](https://discuss.pytorch.org/c/distributed/distributed-rpc). \r\n\r\n\r\n## Java bindings [Experimental] \r\n\r\n In addition to supporting Python and C++, this release adds experimental support for Java bindings. Based on the interface developed for Android in PyTorch Mobile, the new bindings allow you to invoke TorchScript models from any Java program. Note that the Java bindings are only available for Linux for this release, and for inference only. We expect support to expand in subsequent releases. See the code snippet below for how to use PyTorch within Java:\r\n\r\nLearn more about how to use PyTorch from Java [here](https://github.com/pytorch/java-demo), and see the full Javadocs API documentation [here](https://pytorch.org/docs/stable/packages.html).\r\n\r\n## Pruning\r\n\r\nPruning functionalities have been added to PyTorch in the `nn.utils.prune` module. This provides out-of-the-box support for common magnitude-based and random pruning techniques, both structured and unstructured, both layer-wise and global, and it also enables custom pruning from user-provided masks.\r\n\r\nTo prune a tensor, first select a pruning technique among those available in `nn.utils.prune` (or implement your own by subclassing `BasePruningMethod`). \r\n```python\r\nfrom torch.nn.utils import prune\r\nt = torch.rand(2, 5)\r\np = prune.L1Unstructured(amount=0.7)\r\npruned_tensor = p.prune(t)\r\n```\r\n\r\nTo prune a module, select one of the pruning functions available in `nn.utils.prune` (or implement your own) and specify which module and which parameter within that module pruning should act on.\r\n```python\r\nm = nn.Conv2d(3, 1, 2)\r\nprune.ln_structured(module=m, name='weight', amount=5, n=2, dim=1)\r\n```\r\n\r\nPruning reparametrizes the module by turning `weight` (in the example above) from a parameter to an attribute, and replacing it with a new parameter called `weight_orig` (i.e. appending `\"_orig\"` to the initial parameter `name`) that stores the unpruned version of the tensor. The pruning mask is stored as a buffer named `weight_mask` (i.e. appending `\"_mask\"` to the initial parameter `name`). Pruning is applied prior to each forward pass by recomputing `weight` through a multiplication with the updated mask using PyTorch's `forward_pre_hooks`.\r\n\r\nIterative pruning is seamlessly enabled by repeatedly calling pruning functions on the same parameter (this automatically handles the combination of successive masks by making use of a `PruningContainer` under the hood).\r\n\r\n`nn.utils.prune` is easily extensible to support new pruning functions by subclassing the `BasePruningMethod` base class and implementing the `compute_mask` method with the instructions to compute the mask according to the logic of the new pruning technique.\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## Python\r\n\r\n### `torch.optim`: It is no longer supported to use `Scheduler.get_lr()` to obtain the last computed learning rate.  to get the last computed learning rate, call `Scheduler.get_last_lr()` instead.  ([26423](https://github.com/pytorch/pytorch/pull/26423))\r\n\r\nLearning rate schedulers are now \u201cchainable,\u201d as mentioned in the *New Features* section below.  `Scheduler.get_lr` was sometimes used for monitoring purposes to obtain the current learning rate.  But since `Scheduler.get_lr` is also used internally for computing new learning rates, this actually returns a value that is \u201cone step ahead.\u201d  To get the last computed learning rate, use `Scheduler.get_last_lr` instead.\r\n\r\nNote that `optimizer.param_groups[0]['lr']` was in version 1.3.1 and remains in 1.4.0 a way of getting the current learning rate used in the optimizer.\r\n\r\n### `Tensor.unfold` on a 0-dimensional Tensor now properly returns a 1-dimensional Tensor.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.1</th><th>Version 1.4.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5).unfold(dimension=0, size=1, step=1)\r\ntensor(5)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(5).unfold(dimension=0, size=1, step=1)\r\ntensor([5])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### `torch.symeig` now return a 0-element eigenvectors tensor when `eigenvectors=False` (the default).\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.1</th><th>Version 1.4.0</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.symeig(torch.randn(3,3)).eigenvectors.shape\r\ntorch.Size([3, 3])\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.symeig(torch.randn(3,3)).eigenvectors.shape\r\ntorch.Size([0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n## JIT\r\n\r\n* Make `torch.jit.get_trace_graph` private (it is now `torch.jit._get_trace_graph`) ([29149](https://github.com/pytorch/pytorch/pull/29149))\r\n    * This function was intended only for ONNX integration; use `traced_module.graph` instead, like:\r\n    * traced_module = torch.jit.trace(my_module, example_inputs)\r\n        traced_graph = traced_module.graph\r\n* `@property` on `ScriptModule`s has been disabled ([28395](https://github.com/pytorch/pytorch/pull/28395))\r\n    * Scripted `@property` accesses were silently broken before, where we would evaluate the the `get` function once and store that as the attribute permanently. They properly error now; a workaround is to make your `@property` a regular method.\r\n* Custom ops: `torch::jit::RegisterOperators` has been removed, use `torch::RegisterOperators` instead ([28229](https://github.com/pytorch/pytorch/pull/28229)). The usage and behavior should remain the same.\r\n* Remove` torch.jit._register_*` bindings from Python (e.g. `torch.jit._register_attribute`). These were private functions that were not intended to be used.  ([29499](https://github.com/pytorch/pytorch/pull/29499))\r\n\r\n## C++\r\n\r\n### [C++] The distinction between Tensor and Variable has been eliminated at the C++ level. ([28287](https://github.com/pytorch/pytorch/pull/28287))\r\n\r\nThis change simplifies our C++ API and matches previous changes we did at the python level that merged Tensors and Variables into a single type.\r\n\r\nThis change is unlikely to affect user code; the most likely exceptions are:\r\n\r\n1) [Argument-dependent lookup](https://en.cppreference.com/w/cpp/language/adl) for `torch::autograd` may no longer work.  This can break because Variable is now defined as an alias for Tensor (`using Variable = Tensor;`).  In this case, you must explicitly qualify the calls to `torch::autograd` functions. \r\n\r\n2) Because `Variable` and `Tensor` are now the same type, code which assumes that they are different types (e.g., for the purposes of templating, or `std::enable_if` checks) will not work until you delete the (now) redundant overload/specialization.\r\n\r\n3) Some operators may trace differently.  If this happens, please [file a bug.](https://github.com/pytorch/pytorch/issues/new?template=bug-report.md)  The most likely situations are:\r\n\r\n1. There are now *more* operations in your trace than before (usually, calls to `aten::empty`)\r\n2. There are now *less* operations in your trace than before (e.g., the trace complains that `\"there is no observable dependence\"` with the inputs)\r\n\r\n### [C++] arguments in `torch::nn::LinearOptions` are renamed to match the Python API. ([27382](https://github.com/pytorch/pytorch/pull/27382))\r\n\r\n* Arguments that are renamed:\r\n    * `in` -> `in_features`\r\n    * `out` -> `out_features`\r\n    * `with_bias` -> `bias`\r\n\r\n### [C++] arguments in `torch::nn::Conv{1,2,3}dOptions` are renamed to match the Python API. ([28917](https://github.com/pytorch/pytorch/pull/28917)) ([29838](https://github.com/pytorch/pytorch/pull/29838))\r\n\r\n* Arguments that are renamed:\r\n    * `input_channels` -> `in_channels`\r\n    * `output_channels` -> `out_channels`\r\n    * `with_bias` -> `bias`\r\n\r\n### [C++] `torch::nn::Conv{1,2,3}dOptions` no longer has the `transposed` argument. ([31005](https://github.com/pytorch/pytorch/pull/31005))\r\n\r\n* If users have `transposed` originally set to `true` in `torch::nn::Conv{1,2,3}dOptions`, they should migrate their code to use `torch::nn::ConvTranspose{1,2,3}d` layers instead.\r\n\r\n### [C++] All Reduction enums for `torch::nn` layers and functionals are changed to have `torch::KEnumNAME` syntax. ([27942](https://github.com/pytorch/pytorch/pull/27942), [26837](https://github.com/pytorch/pytorch/pull/26837))\r\n\r\n* Example: previously, to specify \u201cmean\u201d as the reduction method in a torch::nn layer or functional, we would use `torch::Reduction::Mean`. Now, `torch::Reduction::Mean` has been renamed to the shorter `torch::kMean`.\r\n\r\n### [C++] `torch::tensor` constructor is improved to match Python API behavior. ([28523](https://github.com/pytorch/pytorch/pull/28523)) ([29632](https://github.com/pytorch/pytorch/pull/29632)) ([29066](https://github.com/pytorch/pytorch/pull/29066))\r\n\r\n* Shape checking fixes\r\n    * Example 1: previously, `torch::tensor({{1}, {2}})` produced a tensor of sizes `{2}`. Now, it produces a tensor of sizes `{2, 1}`.\r\n    * Example 2: previously, `torch::tensor(1.1)` produced a 1-dim tensor. Now it produces a 0-dim tensor.\r\n* Type inference improvements\r\n    * Example 1: previously, C++ `torch::tensor` with a double (e.g. `torch::tensor(1.1)`) or a (nested) braced-init-list of doubles (e.g. `torch::tensor({{1.1, 2.2}})` produces a tensor with dtype `torch::kDouble`. Now it produces a tensor with dtype `torch::get_default_dtype()`.\r\n    * Example 2: previously, C++ `torch::tensor` with an integer type (e.g. `torch::tensor(1)`) or a (nested) braced-init-list of integer types (e.g. `torch::tensor({{1, 2}})`) produces a tensor with the same dtype. Now it always produces a tensor of dtype `torch::kLong` (aka. `int64_t`).\r\n    * Example 3: previously, when passed a `TensorOptions` without a dtype set to the `torch::tensor` constructor, it always produces a tensor of dtype `torch::get_default_dtype()`. Now it produces a tensor of different dtypes based on the dtype of the braced-init-list and the default dtype.\r\n* Passing a `std::initializer_list` (NOT braced-init-list) to `torch::tensor` will no longer compile, and the user should pass the equivalent braced-init-list to `torch::tensor` instead. For example, write `torch::tensor({1.1, 1.2})` instead of `torch::tensor(std::initializer_list<double>({1.1, 1.2}))`.\r\n\r\n### [C++] Some activation modules\u2019 `forward` function now take `Tensor` instead of `Tensor&` as input. ([28501](https://github.com/pytorch/pytorch/pull/28501))\r\n\r\n`torch::nn` layers affected: `ELU` / `SELU` / `Hardtanh` / `LeakyReLU` / `ReLU` / `ReLU6` / `RReLU` / `CELU`\r\nThis change ensures that the above layers can be used in a `torch::nn::Sequential` module. If your C++ model uses any of the above layers, you must recompile your C++ code with the new libtorch binary.\r\n\r\n# New Features\r\n\r\n## torch.optim\r\n\r\nLearning rate schedulers (`torch.optim.lr_scheduler`) now support \u201cchaining.\u201d This means that two schedulers can be defined and stepped one after the other to compound their effect, see example below. Previously, the schedulers would overwrite each other.\r\n\r\n```\r\n>>> import torch\r\n>>> from torch.optim import SGD\r\n>>> from torch.optim.lr_scheduler import ExponentialLR, StepLR\r\n>>>\r\n>>> model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\r\n>>> optimizer = SGD(model, 0.1)\r\n>>>\r\n>>> scheduler1 = ExponentialLR(optimizer, gamma=0.9)\r\n>>> scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)\r\n>>>\r\n>>> for epoch in range(4):\r\n>>>     print(epoch, scheduler2.get_last_lr()[0])\r\n>>>\r\n>>>     optimizer.step()\r\n>>>     scheduler1.step()\r\n>>>     scheduler2.step()\r\n    \r\n0 0.1\r\n1 0.09000000000000001\r\n2 0.08100000000000002\r\n3 0.00729000000000002\r\n4 0.00656100000000002\r\n```\r\n\r\n## Distributed\r\n\r\n* Add `allgather_coalesced` API to `ProcessGroup` ([28634,](https://github.com/pytorch/pytorch/pull/28634)[29059](https://github.com/pytorch/pytorch/pull/29059))\r\n* Add `abort` API in `ProcessGroupGloo` Send/Recv Work ([29928](https://github.com/pytorch/pytorch/pull/29928)).\r\n* Add `--no_python` flag to allow using a bash script wrapper in the launch command ([29144](https://github.com/pytorch/pytorch/pull/29144)).\r\n\r\n\r\n\r\n## RPC [Experimental] \r\n\r\n`torch.distributed.rpc` is a newly introduced package. It contains basic building blocks to run functions remotely in model training and inference, which will be useful for scenarios like distributed model parallel or implementing parameter server frameworks. More specifically, it contains four pillars: RPC, Remote Reference, Distributed Autograd, and Distributed Optimizer. Please refer to the [documentation](https://pytorch.org/docs/master/rpc.html) and the [tutorial](https://pytorch.org/tutorials/intermediate/rpc_tutorial.html) for more details.\r\n\r\n* Add `rpc_sync` and `rpc_async` for builtin operators and Python user functions ([23228](https://github.com/pytorch/pytorch/pull/23228), [23569](https://github.com/pytorch/pytorch/pull/23569), [28392](https://github.com/pytorch/pytorch/pull/28392)).\r\n* Add `remote` and `RRef` for builtin operators and Python user functions ([25169](https://github.com/pytorch/pytorch/pull/25169), [25499](https://github.com/pytorch/pytorch/pull/25499)).\r\n* Distributed Autograd - FAST mode backward pass implementation. ([27022](https://github.com/pytorch/pytorch/pull/27022), [27576](https://github.com/pytorch/pytorch/pull/27576)).\r\n* Integrate `remote` and `RRef` with distributed autograd ([28630](https://github.com/pytorch/pytorch/pull/28630), [28656](https://github.com/pytorch/pytorch/pull/28656)).\r\n* Add a distributed optimizer ([29304](https://github.com/pytorch/pytorch/pull/29304), [30062](https://github.com/pytorch/pytorch/pull/30062)).\r\n* Add python API for `get_gradients()` method to retrieve gradients from distributed autograd context. ([28926](https://github.com/pytorch/pytorch/pull/28926)).\r\n* Support creating local `RRef`s on local values and to-self `remote` calls ([28948](https://github.com/pytorch/pytorch/pull/28948), [29634](https://github.com/pytorch/pytorch/pull/29634)).\r\n* Support custom pickler for RPC ([30185](https://github.com/pytorch/pytorch/pull/30185)).\r\n* Add default RPC agent options based on the backend type ([30201](https://github.com/pytorch/pytorch/pull/30201)).\r\n* Add local `shutdown` to `ProcessGroup` agent ([30330](https://github.com/pytorch/pytorch/pull/30330)).\r\n\r\n## JIT\r\n\r\n* `script::Module`: implement more of of the nn.Module API ([28828](https://github.com/pytorch/pytorch/pull/28828))\r\n    * In particular, adds the (optionally recursive) methods that iterate over submodules, parameters, etc.\r\n    * Adds a pybind-like `attr()` method to simplify attribute access.\r\n* Add support for `@staticmethod` on `ScriptModule`s ([27163](https://github.com/pytorch/pytorch/pull/27163))\r\n* Support Module Containers as Iterables ([26465](https://github.com/pytorch/pytorch/pull/26465))\r\n* Support Iterables In List Comprehensions ([26768)](https://github.com/pytorch/pytorch/pull/26768)\r\n* Dictionaries now preserve insertion order, and `OrderedDict` is supported ([26465](https://github.com/pytorch/pytorch/pull/26465))\r\n* Add support for `hasattr()` ([29332](https://github.com/pytorch/pytorch/pull/29332))\r\n* TorchScript classes can now be callable ([26743](https://github.com/pytorch/pytorch/pull/26743))\r\n* Add `clone_instance` for `ScriptModule`s ([30168](https://github.com/pytorch/pytorch/pull/30168))\r\n* Add `torch.memory_format` support to the TorchScript ([28544](https://github.com/pytorch/pytorch/pull/28544))\r\n* Custom `forward()` is now allowed on container modules ([28988](https://github.com/pytorch/pytorch/pull/28988))\r\n* Calls to submodules are now preserved in the traced graph ([29261](https://github.com/pytorch/pytorch/pull/29261))\r\n* Add support for module containers to be used as iterables ([28255](https://github.com/pytorch/pytorch/pull/28255))\r\n* Make JIT Serialization support arbitrary std::function<> IO ([28039](https://github.com/pytorch/pytorch/pull/28039))\r\n* Support `layout() `in script ([27100](https://github.com/pytorch/pytorch/pull/27100))\r\n* Methods and functions are no longer inlined in the serialized file format ([26706](https://github.com/pytorch/pytorch/pull/26706))\r\n\r\n## Mobile\r\n\r\n* Build level customization\r\n    * Add custom build script to only include selected operators ([30144](https://github.com/pytorch/pytorch/pull/30144)).\r\n    * Dump operator names used by a script module ([29374](https://github.com/pytorch/pytorch/pull/29374), [30467](https://github.com/pytorch/pytorch/pull/30467)).\r\n    * Disable JIT optimizer in Android wrapper for mobile custom build ([30285](https://github.com/pytorch/pytorch/pull/30285)).\r\n    * FBJNI Gradle ABI_FILTERS parameter ([30135](https://github.com/pytorch/pytorch/pull/30135)).\r\n\r\n# Improvements\r\n\r\n## Distributed\r\n\r\n### Improvements\r\n\r\n* Add timeout support in `ProcessGroupNCCL` ([27224](https://github.com/pytorch/pytorch/pull/27224)).\r\n* Ensure that DDP wrapped module has parameters that require gradients ([25858](https://github.com/pytorch/pytorch/pull/25858)).\r\n* Making `torch/csrc/cuda` NCCL usage safe for NCCL 2.5 ([29014](https://github.com/pytorch/pytorch/pull/29014)).\r\n* Enable `test_distributed` for ROCm but only with NCCL backend ([28814](https://github.com/pytorch/pytorch/pull/28814)).\r\n\r\n### RPC Improvements\r\n\r\n* Separate out RPC to `rpc_sync` and `rpc_async` APIs ([26570](https://github.com/pytorch/pytorch/pull/26570)).\r\n* Make python user function serialization format to be consistent with builtin operators ([27136](https://github.com/pytorch/pytorch/pull/27136)).\r\n* Clean up distributed autograd context on all participants on exit ([27951](https://github.com/pytorch/pytorch/pull/27951)).\r\n* Improve error handling for distributed autograd engine. ([27940](https://github.com/pytorch/pytorch/pull/27940)).\r\n* Scope pybind11 functions to `torch.distributed.{autograd,rpc}` ([27529](https://github.com/pytorch/pytorch/pull/27529)).\r\n* Lift `rpc_timeout` to `RpcAgent` to make it reusable for other `RpcAgent` implementations. ([29341](https://github.com/pytorch/pytorch/pull/29341)).\r\n* Support sending message to self in `process_group_agent` ([29253](https://github.com/pytorch/pytorch/pull/29253)).\r\n* Properly shutdown RPC even in the case of `clean_shutdown=False`. ([29148](https://github.com/pytorch/pytorch/pull/29148)).\r\n* Ensure `initializedContextIds_` map is cleaned up appropriately in distributed autograd engine. ([29787](https://github.com/pytorch/pytorch/pull/29787)).\r\n* Add hash and equality operators for `WorkerInfo` ([29958](https://github.com/pytorch/pytorch/pull/29958)).\r\n* Add `RpcAgentOptions` struct type to bundle arguments for different `RpcAgent`s ([29972](https://github.com/pytorch/pytorch/pull/29972)).\r\n* Mark timeout `FutureMessage`s and throw exceptions in `ProcessGroupAgent` ([29601](https://github.com/pytorch/pytorch/pull/29601)).\r\n* Re-throw python remote exception when using remote reference to itself ([29930](https://github.com/pytorch/pytorch/pull/29930)).\r\n* By default ignore `RRef` leaks during shutdown ([30217](https://github.com/pytorch/pytorch/pull/30217)).\r\n\r\n### Documentation\r\n\r\n* Add Design doc for Distributed Autograd Engine ([29175](https://github.com/pytorch/pytorch/pull/29175), [30068](https://github.com/pytorch/pytorch/pull/30068), [29927](https://github.com/pytorch/pytorch/pull/29927))\r\n* Add Design doc for Remote Reference ([30066](https://github.com/pytorch/pytorch/pull/30066)).\r\n* Add documentation page for `torch.distrbuted.rpc` ([29276](https://github.com/pytorch/pytorch/pull/29276), [28030](https://github.com/pytorch/pytorch/pull/28030), [29971](https://github.com/pytorch/pytorch/pull/29971), [30160](https://github.com/pytorch/pytorch/pull/30160), [30050](https://github.com/pytorch/pytorch/pull/30050), [30069](https://github.com/pytorch/pytorch/pull/30069), [30179](https://github.com/pytorch/pytorch/pull/30179), [30218](https://github.com/pytorch/pytorch/pull/30218), [30240](https://github.com/pytorch/pytorch/pull/30240), [30243](https://github.com/pytorch/pytorch/pull/30243), [30259](https://github.com/pytorch/pytorch/pull/30259)).\r\n\r\n### MISC\r\n\r\n* Add known worker IDs to distributed autograd context ([26324](https://github.com/pytorch/pytorch/pull/26324)).\r\n* Minor tweaks to RPC message API ([28326](https://github.com/pytorch/pytorch/pull/28326)).\r\n* Rename `PythonUDF{Call,Resp}` ([27530](https://github.com/pytorch/pytorch/pull/27530)).\r\n* Use `std::shared_ptr` for `DistAutogradContext` ([29770](https://github.com/pytorch/pytorch/pull/29770)).\r\n* Mark `c10d::~NCCLUtils` as noexcept ([29118](https://github.com/pytorch/pytorch/pull/29118)).\r\n\r\n## JIT\r\n\r\n* Move custom passes to last optimization step ([29256](https://github.com/pytorch/pytorch/pull/29256))\r\n* Represent the original Python name of a module type the same way in traced and scripted modules. ([29912](https://github.com/pytorch/pytorch/pull/29912))\r\n* Only print original SourceRange on highlight ([29708](https://github.com/pytorch/pytorch/pull/29708))\r\n* Error message and ergonomic improvements:\r\n    * Show full call stack in TorchScript exception even when calls were inlined. ([29911](https://github.com/pytorch/pytorch/pull/29911))\r\n    * Reduce error context from 10 -> 3 ([26765](https://github.com/pytorch/pytorch/pull/26765))\r\n    * Fix error report highlight for unmatched type annotation ([27195](https://github.com/pytorch/pytorch/pull/27195))\r\n    * Make default string arguments in schemas human readable ([27088](https://github.com/pytorch/pytorch/pull/27088))\r\n    * Print which output didn't have dependence during trace checking. ([29047](https://github.com/pytorch/pytorch/pull/29047))\r\n* Improvements to save/load and serialization performance:\r\n    * Modules can now share JIT types if their implementation is the same, improving save/load performance ([26666](https://github.com/pytorch/pytorch/pull/26666))\r\n    * Improve float pickling speed. ([28553](https://github.com/pytorch/pytorch/pull/28553))\r\n    * Pickler: convert `std::stringstream` cases for improved performance. ([29351](https://github.com/pytorch/pytorch/pull/29351))\r\n    * Buffer to speed Unpickler ([27727](https://github.com/pytorch/pytorch/pull/27727))\r\n    * Buffer in Pickler to improve performance. ([27720](https://github.com/pytorch/pytorch/pull/27720))\r\n    * In `torch::save()` avoid zip compressing small header records. ([28180](https://github.com/pytorch/pytorch/pull/28180))\r\n    * String optimizations related to serialization. ([28230](https://github.com/pytorch/pytorch/pull/28230))\r\n* Clean up serialized source format ([28129](https://github.com/pytorch/pytorch/pull/28129))\r\n* API for finding a common ancestor block for a pair of nodes ([28864](https://github.com/pytorch/pytorch/pull/28864))\r\n* Make inserted child module names unique ([27237](https://github.com/pytorch/pytorch/pull/27237))\r\n* Better hashing for constant pool ([27733](https://github.com/pytorch/pytorch/pull/27733))\r\n* Improve error messages when a method or attribute is missing ([27110](https://github.com/pytorch/pytorch/pull/27110))\r\n* Display original source range in `Node::print` ([27524](https://github.com/pytorch/pytorch/pull/27524))\r\n* Always use the closure to resolve variable names ([27515](https://github.com/pytorch/pytorch/pull/27515))\r\n\r\n## Mobile\r\n\r\n* Improve Java API / JNI\r\n    * Add module method to allow explicitly destructing native part ([27090](https://github.com/pytorch/pytorch/pull/27090)).\r\n    * Add methods to write image tensor content to buffer ([27359](https://github.com/pytorch/pytorch/pull/27359)).\r\n    * Various improvements to Android API ([27454](https://github.com/pytorch/pytorch/pull/27454), [27455](https://github.com/pytorch/pytorch/pull/27455)).\r\n    * Add support for PyTorch JNI build ([29412](https://github.com/pytorch/pytorch/pull/29412), [42faf961c8](https://github.com/pytorch/pytorch/commit/42faf961c8), [d22f61432d](https://github.com/pytorch/pytorch/commit/d22f61432d)).\r\n    * Various fixes to PyTorch JNI ([29350](https://github.com/pytorch/pytorch/pull/29350), [29861](https://github.com/pytorch/pytorch/pull/29861), [30206](https://github.com/pytorch/pytorch/pull/30206), [30207](https://github.com/pytorch/pytorch/pull/30207)).\r\n* Improve support for older Android NDK\r\n    * Introduce math_compat.h for older Android versions ([28567](https://github.com/pytorch/pytorch/pull/28567)).\r\n    * Define std::strtoll for older Android ([28603](https://github.com/pytorch/pytorch/pull/28603)).\r\n* Improve error message, documentation, debuggability\r\n    * Enable full error message for mobile builds ([29926](https://github.com/pytorch/pytorch/pull/29926)).\r\n    * Update iOS README.md ([27145](https://github.com/pytorch/pytorch/pull/27145)).\r\n    * Update Android README.md ([28533](https://github.com/pytorch/pytorch/pull/28533)).\r\n    * Rename function parameters to avoid [-Werror,-Wshadow] ([30276](https://github.com/pytorch/pytorch/pull/30276)).\r\n    * Fix exception message in Java Tensor ([30776](https://github.com/pytorch/pytorch/pull/30776)).\r\n* Improve support for benchmark and profiling\r\n    * Add Android and iOS test app for benchmark and profiling ([28405](https://github.com/pytorch/pytorch/pull/28405), [28406](https://github.com/pytorch/pytorch/pull/28406), [28469](https://github.com/pytorch/pytorch/pull/28469), [28622](https://github.com/pytorch/pytorch/pull/28622)).\r\n    * Integration with mobile benchmark in PEP ([28437](https://github.com/pytorch/pytorch/pull/28437)).\r\n    * Subscribe for record function and if android do atrace ([28708](https://github.com/pytorch/pytorch/pull/28708)).\r\n* Improve build / CI\r\n    * Improve Android Gradle build and publishing ([26833](https://github.com/pytorch/pytorch/pull/26833), [27389](https://github.com/pytorch/pytorch/pull/27389), [29262](https://github.com/pytorch/pytorch/pull/29262), [29738](https://github.com/pytorch/pytorch/pull/29738)).\r\n    * Misc fixes to the Android test project ([27453](https://github.com/pytorch/pytorch/pull/27453)).\r\n    * Improve XCode build script ([27358](https://github.com/pytorch/pytorch/pull/27358), [28996](https://github.com/pytorch/pytorch/pull/28996), [29002](https://github.com/pytorch/pytorch/pull/29002)).\r\n    * Add testing code to iOS CI jobs ([27593](https://github.com/pytorch/pytorch/pull/27593), [27594](https://github.com/pytorch/pytorch/pull/27594), [27784](https://github.com/pytorch/pytorch/pull/27784), [30133](https://github.com/pytorch/pytorch/pull/30133)).\r\n    * Misc fixes to the iOS TestApp ([27591](https://github.com/pytorch/pytorch/pull/27591), [28356](https://github.com/pytorch/pytorch/pull/28356), [28809](https://github.com/pytorch/pytorch/pull/28809), [29247](https://github.com/pytorch/pytorch/pull/29247), [29962](https://github.com/pytorch/pytorch/pull/29962), [29963](https://github.com/pytorch/pytorch/pull/29963)).\r\n    * Add support for host build to pytorch_android ([27662,](https://github.com/pytorch/pytorch/pull/27662)[27664](https://github.com/pytorch/pytorch/pull/27664)).\r\n    * Add host build Gradle publishing ([29749](https://github.com/pytorch/pytorch/pull/29749)).\r\n    * Add mobile build CI with host toolchain ([30292](https://github.com/pytorch/pytorch/pull/30292)).\r\n\r\n## Named Tensors\r\n\r\n* `torch.addcdiv`, `torch.addcmul` Added named tensor support ([28975](https://github.com/pytorch/pytorch/pull/28975)).\r\n* `torch.{ones,zeros,full,rand,randn}_like` Added named tensor support ([28981](https://github.com/pytorch/pytorch/pull/28981)).\r\n* `torch.cdist` Added named tensor support ([29129](https://github.com/pytorch/pytorch/pull/29129)).\r\n* `torch.equal` Added named tensor support ([29322](https://github.com/pytorch/pytorch/pull/29322)).\r\n* Added named tensor support for comparison ops ([27162](https://github.com/pytorch/pytorch/pull/27162)).\r\n* `Tensor.align_to` Fixed error message ([27221](https://github.com/pytorch/pytorch/pull/27221)).\r\n* `Tensor.align_to` Make method-only. ([27304](https://github.com/pytorch/pytorch/pull/27304)).\r\n* `Tensor.align_to` Accept partially named tensors ([27308](https://github.com/pytorch/pytorch/pull/27308)).\r\n* `torch.mean(Tensor, Dimname)` Fixed autograd support ([29199](https://github.com/pytorch/pytorch/pull/29199)).\r\n* `Tensor.unflatten` Fix when dim is a negative integer (#31208) ([31432](https://github.com/pytorch/pytorch/pull/31432)).\r\n* Fix type errors in examples about Named Tensor ([27828](https://github.com/pytorch/pytorch/pull/27828)).\r\n\r\n## C++ API\r\n\r\n### New torch::nn modules\r\n\r\n* Convolution layers\r\n    * torch::nn::ConvTranspose{1,2,3}d / Unfold ([29721](https://github.com/pytorch/pytorch/pull/29721)) ([27809](https://github.com/pytorch/pytorch/pull/27809)).\r\n* Pooling layers\r\n    * torch::nn::AdaptiveAvgPool{1, 2, 3}d / MaxUnpool{1, 2, 3}d / LPPool{1, 2}d / FractionalMaxPool{2,3}d ([26808](https://github.com/pytorch/pytorch/pull/26808), [26818](https://github.com/pytorch/pytorch/pull/26818), [26819](https://github.com/pytorch/pytorch/pull/26819)) ([26896](https://github.com/pytorch/pytorch/pull/26896), [26915](https://github.com/pytorch/pytorch/pull/26915), [27027](https://github.com/pytorch/pytorch/pull/27027)) ([27800](https://github.com/pytorch/pytorch/pull/27800), [28492](https://github.com/pytorch/pytorch/pull/28492), [29584](https://github.com/pytorch/pytorch/pull/29584)) ([29933](https://github.com/pytorch/pytorch/pull/29933)).\r\n* Loss layers\r\n    * torch::nn::HingeEmbeddingLoss / CosineEmbeddingLoss /MultiMarginLoss ([27101](https://github.com/pytorch/pytorch/pull/27101)) ([27345](https://github.com/pytorch/pytorch/pull/27345)) ([27424](https://github.com/pytorch/pytorch/pull/27424)) ([27770](https://github.com/pytorch/pytorch/pull/27770)).\r\n    * torch::nn::TripletMarginLoss / SoftMarginloss / MultiLabelMargin / MarginRankingLoss / MultiLabelSoftMarginLoss ([27713](https://github.com/pytorch/pytorch/pull/27713), [27956](https://github.com/pytorch/pytorch/pull/27956)) ([27660](https://github.com/pytorch/pytorch/pull/27660)) ([27659](https://github.com/pytorch/pytorch/pull/27659)) ([29000](https://github.com/pytorch/pytorch/pull/29000)) ([27669](https://github.com/pytorch/pytorch/pull/27669)).\r\n    * torch::nn::MSELoss / KLDivLoss / BCELoss / SmoothL1Loss / PoissonNLLLoss / BCEWithLogitsLoss ([27156](https://github.com/pytorch/pytorch/pull/27156)) ([28806](https://github.com/pytorch/pytorch/pull/28806)) ([30146](https://github.com/pytorch/pytorch/pull/30146)) ([27661](https://github.com/pytorch/pytorch/pull/27661)) ([28755](https://github.com/pytorch/pytorch/pull/28755)) ([28783](https://github.com/pytorch/pytorch/pull/28783)).\r\n    * torch::nn::NLLLoss / CrossEntropyLoss / CTCLoss ([29812](https://github.com/pytorch/pytorch/pull/29812)) ([28654](https://github.com/pytorch/pytorch/pull/28654)).\r\n* Normalization Layers\r\n    * torch::nn::LayerNorm / InstanceNorm{1,2,3}d / BatchNorm{1,2,3}d / GroupNorm / LocalResponseNorm / CrossMapLRN2d ([28032](https://github.com/pytorch/pytorch/pull/28032)) ([28790](https://github.com/pytorch/pytorch/pull/28790)) ([28176](https://github.com/pytorch/pytorch/pull/28176), [28936](https://github.com/pytorch/pytorch/pull/28936)) ([29920](https://github.com/pytorch/pytorch/pull/29920)) ([28759](https://github.com/pytorch/pytorch/pull/28759)) ([29039](https://github.com/pytorch/pytorch/pull/29039)).\r\n* Activation Layers\r\n    * torch::nn::ELU / LeakyReLU / SELU / PReLU / ReLU / ReLU6 / RRelu / CELU / GLU ([27028)](https://github.com/pytorch/pytorch/pull/27028) ([27059](https://github.com/pytorch/pytorch/pull/27059)) ([27434](https://github.com/pytorch/pytorch/pull/27434)) ([27429](https://github.com/pytorch/pytorch/pull/27429)) ([27435](https://github.com/pytorch/pytorch/pull/27435)) ([27436](https://github.com/pytorch/pytorch/pull/27436)) ([27437](https://github.com/pytorch/pytorch/pull/27437)) ([27487](https://github.com/pytorch/pytorch/pull/27487)) ([29922](https://github.com/pytorch/pytorch/pull/29922)).\r\n    * torch::nn::Sigmoid / LogSigmoid / LogSoftmax / Softmax / Softmax2d / Softplus / Softmin / Softsign / Softshrink / Hardshrink / Hardtanh / Tanh / Threshold ([27488](https://github.com/pytorch/pytorch/pull/27488)) ([27060](https://github.com/pytorch/pytorch/pull/27060)) ([27462](https://github.com/pytorch/pytorch/pull/27462)) ([27446](https://github.com/pytorch/pytorch/pull/27446)) ([27509](https://github.com/pytorch/pytorch/pull/27509)) ([27489](https://github.com/pytorch/pytorch/pull/27489)) ([27459](https://github.com/pytorch/pytorch/pull/27459)) ([27535](https://github.com/pytorch/pytorch/pull/27535)) ([27534](https://github.com/pytorch/pytorch/pull/27534)) ([27035](https://github.com/pytorch/pytorch/pull/27035)) ([27537](https://github.com/pytorch/pytorch/pull/27537)) ([27038](https://github.com/pytorch/pytorch/pull/27038)) ([27536](https://github.com/pytorch/pytorch/pull/27536)) ([27538](https://github.com/pytorch/pytorch/pull/27538)).\r\n* Dropout Layers\r\n    * torch::nn::Dropout / Dropout{2, 3}d / AlphaDropout / FeatureAlphaDropout ([29761](https://github.com/pytorch/pytorch/pull/29761)) ([28424](https://github.com/pytorch/pytorch/pull/28424)).\r\n* Padding Layers\r\n    * torch::nn::ReflectionPad{1, 2}d / ReplicationPad{1,2,3}d / ZeroPad2d / ConstantPad{1,2,3}d ([28538](https://github.com/pytorch/pytorch/pull/28538)) ([28539](https://github.com/pytorch/pytorch/pull/28539)) ([28540](https://github.com/pytorch/pytorch/pull/28540)) ([28541](https://github.com/pytorch/pytorch/pull/28541)).\r\n* Embedding layers\r\n    * torch::nn::Embedding / EmbeddingBag ([26358](https://github.com/pytorch/pytorch/pull/26358)).\r\n* Linear layers\r\n    * torch::nn::Bilinear / Flatten ([26082](https://github.com/pytorch/pytorch/pull/26082)) ([28072](https://github.com/pytorch/pytorch/pull/28072)).\r\n* Vision layers\r\n    * torch::nn::Upsample / PixelShuffle ([28413](https://github.com/pytorch/pytorch/pull/28413)) ([28140](https://github.com/pytorch/pytorch/pull/28140)).\r\n\r\n### New torch::nn::functional functions\r\n\r\n* Convolution functions\r\n    * torch::nn::functional::conv{1,2,3}d / conv_transpose{1,2,3}d / fold / unfold ([28917](https://github.com/pytorch/pytorch/pull/28917)) ([29721](https://github.com/pytorch/pytorch/pull/29721)) ([28732](https://github.com/pytorch/pytorch/pull/28732)) ([27809](https://github.com/pytorch/pytorch/pull/27809)).\r\n* Pooling functions\r\n    * torch::nn::functional::adaptive_avg_pool{1, 2, 3}d / lp_pool{1, 2}d / fractional_max_pool{2, 3}d / fractional_max_pool{2, 3}d_with_indices ([26808](https://github.com/pytorch/pytorch/pull/26808), [26818](https://github.com/pytorch/pytorch/pull/26818), [26819](https://github.com/pytorch/pytorch/pull/26819)) ([27800](https://github.com/pytorch/pytorch/pull/27800), [28492](https://github.com/pytorch/pytorch/pull/28492)) ([29584](https://github.com/pytorch/pytorch/pull/29584)) ([29933](https://github.com/pytorch/pytorch/pull/29933)).\r\n* Loss functions\r\n    * torch::nn::functional::hinge_embedding_loss / multi_margin_loss / multilabel_soft_margin_loss / triplet_margin_loss / soft_margin_loss / margin_ranking_loss ([27101](https://github.com/pytorch/pytorch/pull/27101)) ([27424](https://github.com/pytorch/pytorch/pull/27424)) ([27669](https://github.com/pytorch/pytorch/pull/27669)) ([27713](https://github.com/pytorch/pytorch/pull/27713)) ([27660](https://github.com/pytorch/pytorch/pull/27660)) ([29000](https://github.com/pytorch/pytorch/pull/29000)).\r\n    * torch::nn::functional::poisson_nll_loss / nll_loss / cross_entropy / binary_cross_entropy_with_logits ([28755](https://github.com/pytorch/pytorch/pull/28755)) ([29812](https://github.com/pytorch/pytorch/pull/29812)) ([28783](https://github.com/pytorch/pytorch/pull/28783)).\r\n    * torch::nn::functional::l1_loss / kl_div / mse_loss / binary_cross_entropy / smooth_l1_loss / ctc_loss ([27156](https://github.com/pytorch/pytorch/pull/27156)) ([28806](https://github.com/pytorch/pytorch/pull/28806)) ([30146](https://github.com/pytorch/pytorch/pull/30146)) ([27661](https://github.com/pytorch/pytorch/pull/27661)) ([28654](https://github.com/pytorch/pytorch/pull/28654)).\r\n* Normalization functions\r\n    * torch::nn::functional::layer_norm / instance_norm / clip_grad_norm_ / batch_norm / group_norm / local_response_norm / normalize ([28032](https://github.com/pytorch/pytorch/pull/28032)) ([28790](https://github.com/pytorch/pytorch/pull/28790), [30684](https://github.com/pytorch/pytorch/pull/30684)) ([26140](https://github.com/pytorch/pytorch/pull/26140), [29584](https://github.com/pytorch/pytorch/pull/29584), [30216](https://github.com/pytorch/pytorch/pull/30216)) ([28176](https://github.com/pytorch/pytorch/pull/28176), [28936](https://github.com/pytorch/pytorch/pull/28936)) ([29920](https://github.com/pytorch/pytorch/pull/29920)) ([28759](https://github.com/pytorch/pytorch/pull/28759)) ([27280](https://github.com/pytorch/pytorch/pull/27280)).\r\n* Activation functions\r\n    * torch::nn::functional::elu / leaky_relu / selu / prelu / relu / relu6 / rrelu / celu / glu / gelu ([27028](https://github.com/pytorch/pytorch/pull/27028)) ([27059](https://github.com/pytorch/pytorch/pull/27059)) ([27434](https://github.com/pytorch/pytorch/pull/27434)) ([27429](https://github.com/pytorch/pytorch/pull/27429)) ([27435](https://github.com/pytorch/pytorch/pull/27435)) ([27436](https://github.com/pytorch/pytorch/pull/27436)) ([27437](https://github.com/pytorch/pytorch/pull/27437)) ([27487](https://github.com/pytorch/pytorch/pull/27487)) ([29922](https://github.com/pytorch/pytorch/pull/29922)) ([28433](https://github.com/pytorch/pytorch/pull/28433)).\r\n    * torch::nn::functional:: log_sigmoid/ log_softmax / softmax / softplus / softmin / softsign / softshrink / hardshrink / tanhshrink / hardtanh / gumbel_softmax / threshold ([27060](https://github.com/pytorch/pytorch/pull/27060)) ([27462](https://github.com/pytorch/pytorch/pull/27462)) ([27446](https://github.com/pytorch/pytorch/pull/27446)) ([27489](https://github.com/pytorch/pytorch/pull/27489)) ([27459](https://github.com/pytorch/pytorch/pull/27459)) ([27535](https://github.com/pytorch/pytorch/pull/27535)) ([27534](https://github.com/pytorch/pytorch/pull/27534)) ([27035](https://github.com/pytorch/pytorch/pull/27035)) ([27537](https://github.com/pytorch/pytorch/pull/27537)) ([27038](https://github.com/pytorch/pytorch/pull/27038)) ([28121](https://github.com/pytorch/pytorch/pull/28121)) ([27538](https://github.com/pytorch/pytorch/pull/27538)).\r\n* Embedding functions\r\n    * torch::nn::functional::embedding  / embedding_bag / one_hot ([28669](https://github.com/pytorch/pytorch/pull/28669)) ([29673](https://github.com/pytorch/pytorch/pull/29673)) ([27177](https://github.com/pytorch/pytorch/pull/27177)).\r\n* Linear functions\r\n    * torch::nn::functional::linear / bilinear ([27382](https://github.com/pytorch/pytorch/pull/27382)) ([26082](https://github.com/pytorch/pytorch/pull/26082)).\r\n* Padding functions\r\n    * torch::nn::functional::pad ([26601](https://github.com/pytorch/pytorch/pull/26601), [28760](https://github.com/pytorch/pytorch/pull/28760)).\r\n* Vision functions\r\n    * torch::nn::functional::affine_grid / grid_sample / interpolate / pixel_shuffle ([27263](https://github.com/pytorch/pytorch/pull/27263)) ([28354](https://github.com/pytorch/pytorch/pull/28354)) ([28413](https://github.com/pytorch/pytorch/pull/28413)) ([28140](https://github.com/pytorch/pytorch/pull/28140)).\r\n* Distance functions\r\n    * torch::nn::functional::pdist ([27122](https://github.com/pytorch/pytorch/pull/27122)).\r\n* Utility functions\r\n    * torch::nn::utils::clip_grad_value_ / parameters_to_vector / vector_to_parameters ([28736](https://github.com/pytorch/pytorch/pull/28736), [29584](https://github.com/pytorch/pytorch/pull/29584)) ([30216](https://github.com/pytorch/pytorch/pull/30216)) ([29267](https://github.com/pytorch/pytorch/pull/29267)).\r\n\r\n\r\n\r\n## AMD Support\r\n\r\n* New features integration\r\n    * Enabled RCCL Integration ([23884](https://github.com/pytorch/pytorch/pull/23884), [27383](https://github.com/pytorch/pytorch/pull/27383), [27518](https://github.com/pytorch/pytorch/pull/27518), [29385](https://github.com/pytorch/pytorch/pull/29385))\r\n    * Enabled rocTX and rocTracer Integration ([27416](https://github.com/pytorch/pytorch/pull/27416))\r\n    * Improved hiprtc integration ([27390](https://github.com/pytorch/pytorch/pull/27390))\r\n    * bfloat16 enablement (initial) on ROCm ([27719](https://github.com/pytorch/pytorch/pull/27719))\r\n* Build/CI\r\n    * Upgrade to ROCm 2.9 ([27417](https://github.com/pytorch/pytorch/pull/27417))\r\n    * Upgrade ROCm CI to Python3.6 ([30119](https://github.com/pytorch/pytorch/pull/30119), [27353](https://github.com/pytorch/pytorch/pull/27353))\r\n    * Distribute hipify scripts as part of torch package ([27425](https://github.com/pytorch/pytorch/pull/27425))\r\n    * Build and test gfx908 architecture ([27388](https://github.com/pytorch/pytorch/pull/27388))\r\n    * Add `torch.version.hip` ([29815](https://github.com/pytorch/pytorch/pull/29815)).\r\n    * Build fixes ([29547](https://github.com/pytorch/pytorch/pull/29547), [29009](https://github.com/pytorch/pytorch/pull/29009))\r\n\r\n## ONNX\r\n\r\nIn PyTorch 1.4, we have mainly focused on expanding the coverage for ONNX Opset 11, and enabling exporting torchvision models. Most of the torchvision models can be exported to ONNX (Opset 11, with fixed input size), including FasterRCNN, MaskRCNN, and KeypointRCNN. We have also enhanced export support for some tensor indexing scenarios, with more enhancements to come in the next release. In addition, 20+ new PyTorch operators are enabled in ONNX exporter.\r\n\r\n### Expanding Coverage for ONNX Opset 11\r\n\r\n* `torch.sort/torch.topk` are supported in Opset 11 ([25739](https://github.com/pytorch/pytorch/pull/25739))\r\n* `torch.size/torch.squeeze/torch.unsqueeze/torch.mm/torch.index_fill/torch.index_copy` are supported in Opset 11 ([27578](https://github.com/pytorch/pytorch/pull/27578))\r\n* `torch.masked_select/torch.masked_scatter` are supported in Opset 11 ([25949](https://github.com/pytorch/pytorch/pull/25949))\r\n* `torch.arange` is supported in Opset 11 ([26875](https://github.com/pytorch/pytorch/pull/26875))\r\n* `avg_pool, constant_pad_nd, reflection_pad, replication_pad` Support enhanced in Opset 11 ([28225](https://github.com/pytorch/pytorch/pull/28225))\r\n* `torch.hardtanh` is supported in Opset 11 ([30169](https://github.com/pytorch/pytorch/pull/30169))\r\n* Enable ONNX constant folding for opset 11 ([29011](https://github.com/pytorch/pytorch/pull/29011))\r\n\r\n### Exporting More Torch Operators/Models to ONNX\r\n\r\n* `torch.remainder` is enabled in exporter ([24410](https://github.com/pytorch/pytorch/pull/24410))\r\n* `torch.unfold` is enabled in exporter ([24970](https://github.com/pytorch/pytorch/pull/24970))\r\n* `torch.slice/torch.select` with negative index are enabled in exporter ([25273](https://github.com/pytorch/pytorch/pull/25273), [26549](https://github.com/pytorch/pytorch/pull/26549))\r\n* `torch.ones/torch.ones_like/torch.zeros/torch.zeros_like/torch.full/torch.full_like` with default dtype are enabled in exporter ([27577](https://github.com/pytorch/pytorch/pull/27577))\r\n* `torch.unbind` is enabled in exporter ([27247](https://github.com/pytorch/pytorch/pull/27247))\r\n* `torch.nn.functional.interpolate` export is enhanced ([27179](https://github.com/pytorch/pytorch/pull/27179), [27566](https://github.com/pytorch/pytorch/pull/27566), [28560](https://github.com/pytorch/pytorch/pull/28560), [29489](https://github.com/pytorch/pytorch/pull/29489))\r\n* `torch.det` is enabled in exporter ([26958](https://github.com/pytorch/pytorch/pull/26958))\r\n* `torch.group_norm` is enabled in exporter ([27071](https://github.com/pytorch/pytorch/pull/27071))\r\n* `torch.meshgrid` is enabled in exporter ([26037](https://github.com/pytorch/pytorch/pull/26037))\r\n* `torch.randn/torch.randn_like` are enabled in exporter ([28470](https://github.com/pytorch/pytorch/pull/28470), [29354](https://github.com/pytorch/pytorch/pull/29354))\r\n* `torch.weight_norm` enabled in exporter ([28618](https://github.com/pytorch/pytorch/pull/28618))\r\n* `torch.scalar_tensor` is enabled in exporter ([28713](https://github.com/pytorch/pytorch/pull/28713))\r\n* `torch.logdet` is enabled in exporter ([29767](https://github.com/pytorch/pytorch/pull/29767))\r\n* `torch.batch_norm` 2D with affine=False is enabled in exporter ([29458](https://github.com/pytorch/pytorch/pull/29458))\r\n* `torch.bitshift` is enabled in exporter ([28210](https://github.com/pytorch/pytorch/pull/28210))\r\n\r\n### Enhancing Export/Test Infra\r\n\r\n* Use deepcopy inputs in ONNX ORT test cases ([27186](https://github.com/pytorch/pytorch/pull/27186))\r\n* Return NotImplemented from all binary math ops ([27423](https://github.com/pytorch/pytorch/pull/27423)).\r\n* Disabling ONNX IR v4 sematics for opset 8 or lower ([28990](https://github.com/pytorch/pytorch/pull/28990))\r\n* Add ONNX tests for torchvision models ([30121](https://github.com/pytorch/pytorch/pull/30121))\r\n* Keep output type information while exporting ONNX graph ([25906](https://github.com/pytorch/pytorch/pull/25906))\r\n\r\n## Quantization\r\n\r\nQuantization updates correspond to a mix of bug-fixes and feature improvements, with feature improvements adding improved operator coverage and performance improvements.   We have also made a lot of progress towards enabling graph mode quantization support.\r\n\r\n* Feature improvements:\r\n    * Enabling intra-op parallelism ([26692](https://github.com/pytorch/pytorch/pull/26692)).\r\n    * Enabling inplace relu ([28710](https://github.com/pytorch/pytorch/pull/28710)).\r\n    * Quantized Tensor support copy ([28612](https://github.com/pytorch/pytorch/pull/28612)).\r\n    * Add quantized torch mean implementation ([27675](https://github.com/pytorch/pytorch/pull/27675)).\r\n    * Add quantized avg_pool2d for pytorch mobile ([27631](https://github.com/pytorch/pytorch/pull/27631)).\r\n    * Add nn.quantized.Conv3d ([29813](https://github.com/pytorch/pytorch/pull/29813)).\r\n    * Adding inplace quantized relu6 ([29245](https://github.com/pytorch/pytorch/pull/29245)).\r\n    * Fast histogram observer ([29790](https://github.com/pytorch/pytorch/pull/29790)).\r\n    * PackedSequence support for quantized LSTM ([29585](https://github.com/pytorch/pytorch/pull/29585)).\r\n    * Improve legacy QuantizedLinear functions to reduce overhead ([29773](https://github.com/pytorch/pytorch/pull/29773)).\r\n    * Add support for quantized operator conversion from PT to C2 via ONNX ([29694](https://github.com/pytorch/pytorch/pull/29694)).\r\n    * enable per channel dynamic quantization ([30122](https://github.com/pytorch/pytorch/pull/30122)).\r\n* Scripting support:\r\n    * Make PerChannelMinMaxObserver scriptable using `torch.jit.ignore` ([29416](https://github.com/pytorch/pytorch/pull/29416)).\r\n    * Make HistogramObserver scriptable with `@torch.jit.ignore` ([27950](https://github.com/pytorch/pytorch/pull/27950)).\r\n    * Fix tracing for dynamic quantized LSTM ([29331](https://github.com/pytorch/pytorch/pull/29331)).\r\n\r\n## Visualization\r\n\r\n* Fixed graph visualization: displaying proper names after recent JIT changes ([30244](https://github.com/pytorch/pytorch/pull/30244))\r\n* Support logging embedding for TensorBoard visualizations to generic filesystem ([27716](https://github.com/pytorch/pytorch/pull/27716))\r\n\r\n## Other Improvements\r\n\r\n* `torch.argmax/argmin` Allow half type ([28787](https://github.com/pytorch/pytorch/pull/28787)).\r\n* `torch.cuda.memory_stats / memory_summary` instrumentation for CUDA memory allocator ([27361](https://github.com/pytorch/pytorch/pull/27361)).\r\n* `torch.set_num_threads` Allow calling multiple times with TBB ([27190](https://github.com/pytorch/pytorch/pull/27190)).\r\n* `torch.set_num_threads` Allow calling multiple times in parallel native ([27947](https://github.com/pytorch/pytorch/pull/27947)).\r\n* `torch.logical_xor` Allow non-bool tensors ([27248](https://github.com/pytorch/pytorch/pull/27248)).\r\n* `torch.promote_types` Nicer error message. ([27941](https://github.com/pytorch/pytorch/pull/27941)).\r\n* `torch.batch_norm_elemt` Add an out-variant ([27621](https://github.com/pytorch/pytorch/pull/27621)).\r\n* `torch.lerp` Implement derivative with respect to weight ([28219](https://github.com/pytorch/pytorch/pull/28219)).\r\n* `torch.complex32` Add type promotion support ([27929](https://github.com/pytorch/pytorch/pull/27929)).\r\n* `torch.unique` Support bool tensors ([28374](https://github.com/pytorch/pytorch/pull/28374)).\r\n* `torch.reshape` Improve backward for viewable geometries ([28901](https://github.com/pytorch/pytorch/pull/28901)).\r\n* `torch.lu` Generalized factorization ([28608](https://github.com/pytorch/pytorch/pull/28608)).\r\n* `torch.equal` Add the intra-op parallelism ([28810](https://github.com/pytorch/pytorch/pull/28810)).\r\n* `torch.randint` Accept generator=None ([29748](https://github.com/pytorch/pytorch/pull/29748)).\r\n* `torch.bfloat16` Enabled for cuda ([27259](https://github.com/pytorch/pytorch/pull/27259)).\r\n* `torch.multinomial` Enable for torch.half ([29266](https://github.com/pytorch/pytorch/pull/29266)).\r\n* `nn.RNN` Respect the current stream in cudnn ([27026](https://github.com/pytorch/pytorch/pull/27026)).\r\n* `nn.RNN` Preserve nonlinearity attribute ([28058](https://github.com/pytorch/pytorch/pull/28058)).\r\n* `nn.Linear` Support 0-batch size. ([27211](https://github.com/pytorch/pytorch/pull/27211)).\r\n* `nn.functional.binary_cross_entropy` implement double backwards ([26983](https://github.com/pytorch/pytorch/pull/26983)).\r\n* `nn.AdaptiveAvgPool2d` Add support for NHWC memory format ([24396](https://github.com/pytorch/pytorch/pull/24396)).\r\n* `nn.GELU` Add GELU activation ([28944](https://github.com/pytorch/pytorch/pull/28944)).\r\n* `nn.LayerNorm` Handle batch size of zero ([28614](https://github.com/pytorch/pytorch/pull/28614)).\r\n* `nn.BatchNorm` Add NHWC support on cudnn ([23861](https://github.com/pytorch/pytorch/pull/23861)).\r\n* `nn.BatchNorm2d` support torch.channels_last ([28982](https://github.com/pytorch/pytorch/pull/28982)).\r\n* `nn.BatchNorm2d` Handle empty inputs ([30035](https://github.com/pytorch/pytorch/pull/30035)).\r\n* `nn.LayerNorm` Enable the intra-op parallelism ([28464](https://github.com/pytorch/pytorch/pull/28464)).\r\n* `nn.utils.prune` Add pruning functionality ([24076](https://github.com/pytorch/pytorch/pull/24076)).\r\n* `nn.Sequential` Make iterable ([28987](https://github.com/pytorch/pytorch/pull/28987)).\r\n* `dtype.is_signed` Ability to differentiate signed dtypes ([29511](https://github.com/pytorch/pytorch/pull/29511)).\r\n* `optim.lr_scheduler.MultiplicativeLR `Add new multiplicative learning rate scheduler. ([27254](https://github.com/pytorch/pytorch/pull/27254)).\r\n* `cuda.comm.scatter, gather` Add channel-last support ([28077](https://github.com/pytorch/pytorch/pull/28077)).\r\n* `at::parallel_for` Choose number of OMP threads based on GRAIN_SIZE ([26963](https://github.com/pytorch/pytorch/pull/26963)).\r\n* Return NotImplemented from unsupported tensor arithmetic operators ([26507](https://github.com/pytorch/pytorch/pull/26507)).\r\n* Automatically select proper tqdm submodule ([27108](https://github.com/pytorch/pytorch/pull/27108)).\r\n* Pickle support for sparse tensors ([27062](https://github.com/pytorch/pytorch/pull/27062)).\r\n* Vectorized complex unary and binary op support. ([26500](https://github.com/pytorch/pytorch/pull/26500)).\r\n* Complex support for reduce and linpack ops on CPU ([27653](https://github.com/pytorch/pytorch/pull/27653)).\r\n* Complex support for compare and pointwise ops on CPU ([28735](https://github.com/pytorch/pytorch/pull/28735)).\r\n* Make PyTorch Python 3.8 compatible ([29302](https://github.com/pytorch/pytorch/pull/29302)).\r\n* Buffer python warning to avoid deadlocks ([26613](https://github.com/pytorch/pytorch/pull/26613)).\r\n* Use NNPACK for strided convolutions. ([29084](https://github.com/pytorch/pytorch/pull/29084)).\r\n\r\n\r\n\r\n# Bug Fixes\r\n\r\n## Distributed\r\n\r\n* Ensure NCCL error handling code is disabled for NCCL versions < 2.4 ([27124](https://github.com/pytorch/pytorch/pull/27124)).\r\n* Fix segmentation fault in `FileStore` with concurrent accesses. ([28812](https://github.com/pytorch/pytorch/pull/28812)).\r\n* Fix DDP incompatibility issue with `nn.MultiheadAttention` ([26826](https://github.com/pytorch/pytorch/pull/26826)).\r\n\r\n## RPC\r\n\r\n* Add `ProcessGroupAgent` termination detection algorithm ([26984](https://github.com/pytorch/pytorch/pull/26984)).\r\n* Fix pybind11 warnings in Python RPC handler implementation ([27284](https://github.com/pytorch/pytorch/pull/27284)).\r\n* Defer creating `ProcessGroupAgent` listener thread until contexts are initialized ([28013](https://github.com/pytorch/pytorch/pull/28013)).\r\n* Fix Python RPC handler exit crash ([27251](https://github.com/pytorch/pytorch/pull/27251)).\r\n* Fix distributed autograd initialization ([29069](https://github.com/pytorch/pytorch/pull/29069)).\r\n* Always include autograd context id in `rpc_*` / `remote` requests ([29781](https://github.com/pytorch/pytorch/pull/29781)).\r\n* Make `RRefContext` singleton leaky, deal with module destruct order race. ([30172](https://github.com/pytorch/pytorch/pull/30172)).\r\n\r\n## C++ API Bug Fixes\r\n\r\n* at::Tensor::requires_grad_ now supported ([26332](https://github.com/pytorch/pytorch/pull/26332)).\r\n* torch::isfinite now supported ([30083](https://github.com/pytorch/pytorch/pull/30083)).\r\n* torch::nn::modules_ordered_dict is deprecated ([28774](https://github.com/pytorch/pytorch/pull/28774)).\r\n* Add reset_parameters to torch::nn modules ([29832](https://github.com/pytorch/pytorch/pull/29832)).\r\n* Allow passing undefined Tensor to Module::register_parameter ([27948](https://github.com/pytorch/pytorch/pull/27948)).\r\n* Exclude undefined tensors in the result of Module::parameters() / named_paramters() / buffers() / named_buffers() ([30626](https://github.com/pytorch/pytorch/pull/30626)).\r\n* Include hierarchy information in C++ API loading error messages ([28499](https://github.com/pytorch/pytorch/pull/28499)).\r\n* Fix a bug: the C++ L-BFGS optimizer does not work properly if there are one or more registered tensors with no grad in the model ([27606](https://github.com/pytorch/pytorch/pull/27606)).\r\n* Use c10::variant-based enums for Nonlinearity and FanMode ([27933](https://github.com/pytorch/pytorch/pull/27933)). Support for `torch::nn::init::Nonlinearity` and `torch::nn::init::FanMode` will be removed in 1.5.\r\n\r\n## JIT\r\n\r\n* Make dropout properly condition on training. ([29436](https://github.com/pytorch/pytorch/pull/29436))\r\n* Fix aten::grad to return optional list ([29577](https://github.com/pytorch/pytorch/pull/29577))\r\n* Fix `torch.arange` dtype\r\n* Fix type sharing on loaded ScriptModules ([29826](https://github.com/pytorch/pytorch/pull/29826))\r\n* Fix type sharing between traced modules ([29583](https://github.com/pytorch/pytorch/pull/29583))\r\n* Check for mutable default parameters ([29833](https://github.com/pytorch/pytorch/pull/29833))\r\n* Fix tracing of autograd functions ([29791](https://github.com/pytorch/pytorch/pull/29791))\r\n* Check for unrolled loop in break & continue ([29474](https://github.com/pytorch/pytorch/pull/29474))\r\n* Fix negative string indexing ([22700](https://github.com/pytorch/pytorch/pull/22700))\r\n* Make jit.trace_module reentrant ([29411](https://github.com/pytorch/pytorch/pull/29411))\r\n* Fix jit outplace tracing and reapply changes to _like operators. ([28839](https://github.com/pytorch/pytorch/pull/28839))\r\n* Properly guard against inheritance on TorchScript classes ([28407](https://github.com/pytorch/pytorch/pull/28407))\r\n* Fix when giving jit format warning about unsupported options ([28616](https://github.com/pytorch/pytorch/pull/28616))\r\n* Fix handling of function attributes. ([28569](https://github.com/pytorch/pytorch/pull/28569))\r\n* Fix pushLong() issue in pickler. ([28057](https://github.com/pytorch/pytorch/pull/28057))\r\n* Fix broken name mangling ([27511](https://github.com/pytorch/pytorch/pull/27511))\r\n* Fix segfault while printing value type for an error msg in emitListComprehension ([27261](https://github.com/pytorch/pytorch/pull/27261))\r\n* Fix `toIValue` dict iteration ([26856](https://github.com/pytorch/pytorch/pull/26856))\r\n* Fix race condition in Function::optimized_graph(). ([27012](https://github.com/pytorch/pytorch/pull/27012))\r\n* Sanitize module names on legacy import ([27764](https://github.com/pytorch/pytorch/pull/27764))\r\n* Python None should have its type inferred as NoneType ([26665](https://github.com/pytorch/pytorch/pull/26665))\r\n* Properly set existing attributes under recursive script ([27514](https://github.com/pytorch/pytorch/pull/27514))\r\n\r\n## Quantization\r\n\r\n* Skip copy_same_type_transpose_ for quantized tensor ([29609](https://github.com/pytorch/pytorch/pull/29609)).\r\n* Add note that cuda quantization is not supported ([27829](https://github.com/pytorch/pytorch/pull/27829)).\r\n* Rename _intrinsic to intrinsic ([27194](https://github.com/pytorch/pytorch/pull/27194)).\r\n* Better error message for quantized dispatch ([28635](https://github.com/pytorch/pytorch/pull/28635)).\r\n* Update the misleading comments for zero_points and scale in dynamic quant linear module [1/2] ([28767](https://github.com/pytorch/pytorch/pull/28767)).\r\n* Avoid the misleading zero_point and scale [2/2] ([28827](https://github.com/pytorch/pytorch/pull/28827)).\r\n* Add the warning message for API with linear modules ([28766](https://github.com/pytorch/pytorch/pull/28766)).\r\n* Do not insert observers for empty sequential modules ([28384](https://github.com/pytorch/pytorch/pull/28384)).\r\n* Fix the padding issue of quantized average pool operator ([28260](https://github.com/pytorch/pytorch/pull/28260)).\r\n\r\n## Mobile\r\n\r\n* Fix deadlock issues in ThreadPool ([29885](https://github.com/pytorch/pytorch/pull/29885)).\r\n* Disable ProfilingGraphExecutorImpl for mobile ([30067](https://github.com/pytorch/pytorch/pull/30067)).\r\n\r\n## Other Bug fixes\r\n\r\n* `torch.kthvalue` Fix CUDA shared memory out of bound access in findPattern ([28989](https://github.com/pytorch/pytorch/pull/28989)).\r\n* `torch.save` Fix source files not being saved ([28965](https://github.com/pytorch/pytorch/pull/28965)).\r\n* `torch.load` Fix OSError loading files larger than 2GB. ([27125](https://github.com/pytorch/pytorch/pull/27125)).\r\n* `torch.linspace` clearer error message for negative step sizes. ([28274](https://github.com/pytorch/pytorch/pull/28274)).\r\n* `torch.histc` Add range checks to avoid segfaults ([27712](https://github.com/pytorch/pytorch/pull/27712)).\r\n* `torch.lu` Fix thread` `local issue on cpu ([28546](https://github.com/pytorch/pytorch/pull/28546)).\r\n* `torch.max_pool2d` Limit tensor size to max CUDA grid size ([28931](https://github.com/pytorch/pytorch/pull/28931)).\r\n* `torch.renorm` Fix a memory leak in CUDA renorm. ([29873](https://github.com/pytorch/pytorch/pull/29873)).\r\n* `torch.index_add` Fix bug in atomicAdd on CUDA for some dtypes ([29231](https://github.com/pytorch/pytorch/pull/29231)).\r\n* `torch.addmm` Fix handling of empty tensors ([28613](https://github.com/pytorch/pytorch/pull/28613)).\r\n* `nn.CTCLoss` Fix incorrect gradient for large target sizes ([27460](https://github.com/pytorch/pytorch/pull/27460)).\r\n* `nn.functional.ctc_loss` Fix incorrect gradient on cudnn ([27039](https://github.com/pytorch/pytorch/pull/27039)).\r\n* `nn.Embedding` Incorrect gradient at padding_idx in cuda kernel. ([27731](https://github.com/pytorch/pytorch/pull/27731)).\r\n\r\n* `nn.LayerNorm` Fix an illegal memory access error ([28196](https://github.com/pytorch/pytorch/pull/28196)).\r\n* `nn.Conv2d` handle zero stride ([28784](https://github.com/pytorch/pytorch/pull/28784)).\r\n* `nn.PoissonNLLLoss` Fix incorrect result with `full=True` ([28637](https://github.com/pytorch/pytorch/pull/28637)).\r\n* `nn.AvgPool2d` fix an overflow for 2^31-1 sized inputs ([30793](https://github.com/pytorch/pytorch/pull/30793)).\r\n* `nn.RNNBase` Fix an issue with use of children of RNN third party device types ([28562](https://github.com/pytorch/pytorch/pull/28562)).\r\n* `nn.Upsample` Fix \u201cinvalid configuration argument\u201d error ([28927](https://github.com/pytorch/pytorch/pull/28927)).\r\n* `nn.Upsample` Fix a CUDA launch config failure ([29016](https://github.com/pytorch/pytorch/pull/29016)).\r\n* `optim.lr_scheduler.OneCycleLR` Correctly handle div_factor parameter ([28217](https://github.com/pytorch/pytorch/pull/28217)).\r\n* `PackedSequence.to` Ensure all tensors are moved ([27245](https://github.com/pytorch/pytorch/pull/27245)).\r\n* `EventList.total_average` Fix a regression caused by missing __iadd__ ([27498](https://github.com/pytorch/pytorch/pull/27498)).\r\n* `Tensor.record_stream` Ensure stream is recorded for shifted view tensors ([27371](https://github.com/pytorch/pytorch/pull/27371)).\r\n* `torch.hub` Handle branch names containing a slash. ([27960](https://github.com/pytorch/pytorch/pull/27960)).\r\n* Fix error handling in Magma kernels ([29003](https://github.com/pytorch/pytorch/pull/29003)).\r\n* Fix avx for c++14 ([28207](https://github.com/pytorch/pytorch/pull/28207)).\r\n* Fix illegal memory access thread safety issue in sparse CUDA ([29426](https://github.com/pytorch/pytorch/pull/29426)).\r\n* `__cuda_array_interface__` Fix stride calculation ([31450](https://github.com/pytorch/pytorch/pull/31450)).\r\n\r\n# Deprecations\r\n\r\n### **Python 2 support is deprecated and will not be supported in the 1.5 release.**\r\n\r\n### `torch.optim`: `Scheduler.step(epoch)` is now deprecated; use `Scheduler.step()` instead.  ([26432](https://github.com/pytorch/pytorch/pull/26423))\r\n\r\nFor example:\r\n\r\n```\r\n>>> for epoch in range(10):\r\n>>>    optimizer.step()\r\n>>>    scheduler.step(epoch)\r\nDeprecationWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\r\n  warnings.warn(EPOCH_DEPRECATION_WARNING, DeprecationWarning)\r\n```\r\n\r\n### **[C++]** C++11 is deprecated and will not be supported in the 1.5 release.\r\n\r\n### **[C++]** `Tensor::is_variable()` has been deprecated.  As noted in the **Backwards Incompatible Changes** section, the distinction between variable and non-variable has been eliminated, so this check is no longer meaningful.  Generally, `is_variable()` will now return true except in some special circumstances (see [29653](https://github.com/pytorch/pytorch/pull/29653) for more details).  ([29653](https://github.com/pytorch/pytorch/pull/29653))\r\n\r\n### **[C++]** `torch::nn::modules_ordered_dict` has been deprecated.  It is generally no longer necessary and can just be removed.  ([28774](https://github.com/pytorch/pytorch/pull/28774/))\r\n\r\n### `torch.jit.quantized` API has been deprecated in favor of  `torch.quantization.quantize_dynamic` ([28766](https://github.com/pytorch/pytorch/pull/28766))\r\n\r\n# Performance\r\n\r\nA benchmark suite is available to easily measure the performance of operators with a range of input shapes. The generated benchmark data fully characterize the performance of operators in terms of execution time. For more details see README.md in the benchmarks/operator_benchmark directory.\r\n\r\n\r\n* `torch.nn.functional.threshold, torch.nn.functional.layer_norm, torch.cdist` Performance of threshold (CPU), layer norm (CUDA) and cdist operations was improved ([27155,](https://github.com/pytorch/pytorch/pull/27155)[27634](https://github.com/pytorch/pytorch/pull/27634), [25799](https://github.com/pytorch/pytorch/pull/25799))\r\n* `torch.Tensor.fill_` Performance for half and bfloat16 types on CPU was improved  ([28397](https://github.com/pytorch/pytorch/pull/28397)).\r\n* `torch.nn.MaxPool2d` implementation for channels_last format was added ([24872](https://github.com/pytorch/pytorch/pull/24872))\r\n* There is a fast pass reducing the overheads of pointwise operations relying on TensorIterator under certain conditions (contiguous inputs, no broadcast) ([29180](https://github.com/pytorch/pytorch/pull/29180)).\r\n* Overheads of operations with scalars/number literals was improved ([29915](https://github.com/pytorch/pytorch/pull/29915)).\r\n* In case of type promotion on the GPU, the values are converted on the fly, without explicit casting of the full tensor ([30018](https://github.com/pytorch/pytorch/pull/30018)).\r\n* reorder_dimensions in TensorIterator favors output write locality, improving overall performance when operating on discontiguous tensors ([28615](https://github.com/pytorch/pytorch/pull/28615)).\r\n* Float pickling speed was improved ([28553](https://github.com/pytorch/pytorch/pull/28553)).\r\n* GRAIN_SIZE for intra-op parallelization was unified between TH and ATen operations ([28770](https://github.com/pytorch/pytorch/pull/28770))\r\n* `tensor.numel`  devirtualized, improving performance ([27294](https://github.com/pytorch/pytorch/pull/27294))\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.4.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.4.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.4.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/22877176", "dateCreated": "2020-01-14T17:05:04Z", "datePublished": "2020-01-16T00:03:49Z"}, {"tagName": "v1.3.1", "name": "Bug Fix Release", "authorName": "gchanan", "authorType": "User", "body": "### Significant Fixes\r\n\r\n#### [Type Promotion](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype): fixed a bug where type promotion, combined with non-contiguous tensors could compute incorrect results.  ([28253](https://github.com/pytorch/pytorch/pull/28253/))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.0</th><th>Version 1.3.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([[True,  True],\r\n                      [False, True]])\r\n# get a non-contiguous tensor\r\n>>> a_transpose = a.t()\r\n# type promote by comparing across dtypes (bool -> long)\r\n>>> a_transpose == 0\r\n# POTENTIALLY INCORRECT VALUES\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.tensor([[True,  True],\r\n                      [False, True]])\r\n# get a non-contiguous tensor\r\n>>> a_transpose = a.t()\r\n# type promote by comparing across dtypes (bool -> long)\r\n>>> a_transpose == 0\r\ntensor([[False,  True],\r\n        [False, False]])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### [Type Promotion](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype) / Indexing: Fixed a Bug that Allowed Mixed-Dtype Indexing and assignment could lead to incorrect results.  Mixed dtype operations of this form are currently disabled, as they were in 1.2.  ([28231](https://github.com/pytorch/pytorch/pull/28231))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.0</th><th>Version 1.3.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.ones(5, 2, dtype=torch.float)\r\n>>> b = torch.zeros(5, dtype=torch.long)\r\n>>> a[:, [1]] = b.unsqueeze(-1)\r\n>>> a\r\n# POTENTIALLY INCORRECT VALUES\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> a = torch.ones(5, 2, dtype=torch.float)\r\n>>> b = torch.zeros(5, dtype=torch.long)\r\n>>> a[:, [1]] = b.unsqueeze(-1)\r\nRuntimeError: expected dtype Float but got dtype Long\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### [torch.where(condition, x, y)](https://pytorch.org/docs/stable/torch.html#torch.where): fixed a bug on CPU where incorrect results could be returned if `x` and `y` were of different dtypes.  Mixed dtype operations of this form are currently disabled, as they were in version 1.2.  ([29078](https://github.com/pytorch/pytorch/pull/29078))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.3.0</th><th>Version 1.3.1</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(2, 3)\r\n>>> y = torch.randint(0, 10, (2, 3))\r\n>>> torch.where(x < 0, x, y)\r\ntensor(...)\r\n# POTENTIALLY INCORRECT VALUES\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> x = torch.randn(2, 3)\r\n>>> y = torch.randint(0, 10, (2, 3))\r\n>>> torch.where(x < 0, x, y)\r\nRuntimeError: expected scalar type Float but found Long\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n### Other Fixes\r\n\r\n* `torch.argmax`: fix regression on CUDA that disabled support for `torch.float16` inputs.  ([28915](https://github.com/pytorch/pytorch/pull/28915/))\r\n* NamedTensor: fix Python refcounting bug with `Tensor.names`.  ([28922](https://github.com/pytorch/pytorch/pull/28922))\r\n* Quantization: support `deepcopy` for quantized tensors.  ([28612](https://github.com/pytorch/pytorch/pull/28612))\r\n* Quantization: support `nn.quantized.ReLU` with `inplace=True`.  ([28710](https://github.com/pytorch/pytorch/pull/28710))\r\n* Documentation: `torch.lgamma` and `torch.polygamma` are now documented.  ([28964](https://github.com/pytorch/pytorch/pull/28964))", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.3.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.3.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.3.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/21295658", "dateCreated": "2019-11-04T23:37:02Z", "datePublished": "2019-11-07T17:19:44Z"}, {"tagName": "v1.3.0", "name": "Mobile Support, Named Tensors, Quantization, Type Promotion and many more", "authorName": "nairbv", "authorType": "User", "body": "## Table of Contents\r\n\r\n- Breaking Changes\r\n- Highlights\r\n  * [Experimental]: Mobile Support\r\n  * [Experimental]: Named Tensor Support\r\n  * [Experimental]: Quantization support\r\n  * Type Promotion\r\n  * Deprecations\r\n- New Features\r\n  * TensorBoard: 3D Mesh and Hyperparameter Support\r\n  * Distributed\r\n  * Libtorch Binaries with C++11 ABI\r\n  * New TorchScript features\r\n- Improvements\r\n  * C++ Frontend Improvements\r\n    + Autograd\r\n    + New torch::nn modules\r\n    + New torch::nn::functional functions\r\n    + tensor Construction API\r\n    + Other C++ Improvements\r\n  * Distributed Improvements\r\n  * Performance Improvements\r\n  * JIT Improvements\r\n  * ONNX Exporter Improvements\r\n    + Adding Support for ONNX IR v4\r\n    + Adding Support for ONNX Opset 11\r\n    + Exporting More Torch Operators/Models to ONNX\r\n    + Enhancing ONNX Export Infra\r\n  * Other Improvements\r\n- Bug Fixes\r\n    + TensorBoard Bug Fixes\r\n    + C++ API Bug fixes\r\n    + JIT\r\n    + Other Bug Fixes\r\n- Documentation Updates\r\n    + Distributed\r\n    + JIT\r\n    + Other documentation improvements\r\n\r\n# Breaking Changes\r\n\r\n#### Type Promotion: Mixed dtype operations may return a different dtype and value than in previous versions.  ([22273](https://github.com/pytorch/pytorch/pull/22273), [26981](https://github.com/pytorch/pytorch/pull/26981))\r\n\r\nPrevious versions of PyTorch supported a limited number of mixed dtype operations. These operations could result in loss of precision by, for example, truncating floating-point zero-dimensional tensors or Python numbers.\r\n\r\nIn Version 1.3, PyTorch supports NumPy-style type promotion (with slightly modified rules, see [full documentation](https://pytorch.org/docs/master/tensor_attributes.html#torch-dtype)).  These rules generally will retain precision and be less surprising to users.\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.2</th><th>Version 1.3</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(1) + 2.5\r\ntensor(3)\r\n>>> torch.tensor([1]) + torch.tensor(2.5)\r\ntensor([3])\r\n>>> torch.tensor(**True**) + 5\r\ntensor(True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.tensor(1) + 2.5\r\ntensor(3.5000)\r\n>>> torch.tensor([1]) + torch.tensor(2.5)\r\ntensor([3.5000])\r\n>>> torch.tensor(True) + 5\r\ntensor(6)\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### Type Promotion: in-place operations whose result_type is a lower dtype category (bool < integer < floating-point) than the in-place operand now throw an Error.  ([22273](https://github.com/pytorch/pytorch/pull/22273), [26981](https://github.com/pytorch/pytorch/pull/26981))\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.2</th><th>Version 1.3</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> int_tensor = torch.tensor(1)\r\n>>> int_tensor.add_(1.5)\r\ntensor(2)\r\n>>> bool_tensor = torch.tensor(True)\r\n>>> bool_tensor.add_(5)\r\ntensor(True)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> int_tensor = torch.tensor(1)\r\n>>> int_tensor.add_(1.5)\r\nRuntimeError: result type Float cannot be cast to the desired output type Long\r\n>>> bool_tensor = torch.tensor(True)\r\n>>> bool_tensor.add_(5)\r\nRuntimeError: result type Long cannot be cast to the desired output type Bool\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nThese rules can be checked at runtime via [torch.can_cast](https://pytorch.org/docs/master/torch.html#torch.can_cast).\r\n\r\n#### `torch.flatten`: 0-dimensional inputs now return a 1-dim tensor.  ([25406](https://github.com/pytorch/pytorch/pull/25406)).\r\n\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Version 1.2</th><th>Version 1.3</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.flatten(torch.tensor(0))\r\ntensor(0)\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"python\">\r\n>>> torch.flatten(torch.tensor(0))\r\ntensor([0])\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\n#### `nn.functional.affine_grid`: when `align_corners = True`, changed the behavior of 2D affine transforms on 1D data and 3D affine transforms on 2D data (i.e., when one of the spatial dimensions has unit size).\r\n\r\nPreviously, all grid points along a unit dimension were considered arbitrarily to be at -1, now they are considered to be at 0 (the center of the input image).\r\n\r\n#### `torch.gels:` removed deprecated operator, use `torch.lstsq` instead.  ([26480](https://github.com/pytorch/pytorch/pull/26480)).\r\n\r\n#### `utils.data.DataLoader:` made a number of Iterator attributes private (e.g. `num_workers`, `pin_memory`).  ([22273](https://github.com/pytorch/pytorch/pull/22273))\r\n\r\n#### **[C++]** `Variable::backward` will no longer implicitly create a gradient for non-1-element Variables.  Previously, a gradient tensor of all 1s would be implicitly created . This behavior matches the Python API.  ([26150](https://github.com/pytorch/pytorch/pull/26150))\r\n\r\n```\r\nauto x = torch::randn({5, 5}, torch::requires_grad());\r\nauto y = x * x;\r\ny.backward()\r\n// ERROR: \"grad can be implicitly created only for scalar outputs\"\r\n```\r\n\r\n#### [C++] All option specifiers (e.g. `GRUOptions::bidirectional_`) are now private, use the function variants (`GRUOptions::bidirectional(...))` instead. ([26419](https://github.com/pytorch/pytorch/pull/26419)).\r\n\r\n# Highlights\r\n\r\n## [Experimental]: Mobile Support \r\n\r\nIn PyTorch 1.3, we are launching experimental support for mobile. Now you can run any TorchScript model directly without any conversion. Here are the full list of features in this release:\r\n\r\n* Support for full TorchScript inference on mobile;\r\n* Prebuilt LibTorch libraries for Android/iOS on JCenter/CocoaPods;\r\n* Java wrapper for Android with functionality to cover common inference cases (loading and invoking the model);\r\n* Support for all forward ops on mobile CPU (backward ops are not supported yet);\r\n* Some optimized fp32 operator implementations for ARM CPUs (based on Caffe2Go);\r\n* Some optimized int8 operator implementations for ARM CPUs (based on QNNPACK);\r\n\r\nWe decided not to create a new framework for mobile so that you can use the same APIs you are already familiar with to run the same TorchScript models on Android/iOS devices without any format conversion. This way you can have the shortest path from research ideas to production-ready mobile apps.\r\n\r\nThe tutorials, demo apps and download links for prebuilt libraries can be found at: https://pytorch.org/mobile/\r\n\r\nThis is an experimental release. We are working on other features like customized builds to make PyTorch smaller, faster and better for your specific use cases. Stay tuned and give us your feedback!\r\n\r\n## [Experimental]: Named Tensor Support\r\n\r\nNamed Tensors aim to make tensors easier to use by allowing users to associate explicit names with tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support \"broadcasting by name\" rather than \"broadcasting by position\".\r\n\r\nCreate a named tensor by passing a `names` argument into most tensor factory function.\r\n\r\n```python\r\n>>> tensor = torch.zeros(2, 3, names=('C', 'N'))\r\n    tensor([[0., 0., 0.],\r\n            [0., 0., 0.]], names=('C', 'N'))\r\n```\r\n\r\nNamed tensors propagate names across operations.\r\n\r\n```python\r\n>>> tensor.abs()\r\n    tensor([[0., 0., 0.],\r\n            [0., 0., 0.]], names=('C', 'N'))\r\n```\r\n\r\nRearrange to a desired ordering by using `align_to` .\r\n\r\n```python\r\n>>> tensor = tensor.align_to('N', 'C', 'H', 'W')\r\n>>> tensor.names, tensor.shape\r\n    (('N', 'C', 'H', 'W'), torch.Size([3, 2, 1, 1]))\r\n```\r\n\r\nAnd more! [Please see our documentation on named tensors.](https://pytorch.org/docs/master/named_tensor.html)\r\n\r\n## [Experimental]: Quantization support\r\n\r\nPyTorch now supports quantization from the ground up, starting with support for quantized tensors. Convert a float tensor to a quantized tensor and back by:\r\n\r\n```\r\nx = torch.rand(10,1, dtype=torch.float32)\r\nxq = torch.quantize_per_tensor(x, scale = 0.5, zero_point = 8, dtype=torch.quint8)\r\n# xq is a quantized tensor with data represented as quint8\r\nxdq = x.dequantize()\r\n# convert back to floating point\r\n```\r\n\r\nWe also support 8 bit quantized implementations of most common operators in CNNs, including:\r\n\r\n* Tensor operations:\r\n    * view, clone, resize, slice\r\n    * add, multiply, cat, mean, max, sort, topk\r\n* Modules/Functionals (in torch.nn.quantized)\r\n    * Conv2d\r\n    * Linear\r\n    * Avgpool2d, AdaptiveAvgpool2d, MaxPool2d, AdaptiveMaxPool2d\r\n    * Interpolate\r\n    * Upsample\r\n* Fused operations for preserving better accuracy (in torch.nn.intrinsic)\r\n    * ConvReLU2d, ConvBnReLU2d, ConvBn2d\r\n    * LinearReLU\r\n    * add_relu\r\n\r\nWe also support dynamic quantized operators, which take in floating point activations, but use quantized weights (in torch.nn.quantized.dynamic).\r\n\r\n* LSTM\r\n* Linear\r\n\r\nQuantization also requires support for methods to collect statistics from tensors and calculate quantization parameters (implementing interface torch.quantization.Observer). We support several methods to do so:\r\n\r\n* MinMaxObserver\r\n* MovingAverageMinMaxObserver\r\n* PerChannelMinMaxObserver\r\n* MovingAveragePerChannelMinMaxObserver\r\n* HistogramObserver\r\n\r\nFor quantization aware training, we support fake-quantization operators and modules to mimic quantization during training:\r\n\r\n* `torch.fake_quantize_per_tensor_affine`, `torch.fake_quantize_per_channel_affine`\r\n* `torch.quantization.FakeQuantize`\r\n\r\nIn addition, we also support workflows in torch.quantization for:\r\n\r\n* post-training dynamic quantization\r\n* static post training quantization\r\n* quantization aware training \r\n\r\nAll quantized operators are compatible with TorchScript.\r\n\r\nFor more details, see the documentation at: https://pytorch.org/docs/master/quantization.html\r\n\r\n## Type Promotion\r\n\r\nArithmetic and comparison operations may now perform mixed-type operations that promote to a common dtype. \r\n\r\nThis below example was not allowed in version 1.2. In version 1.3, the same code returns a tensor with `dtype=torch.float32`.\r\n\r\n```\r\n>>> torch.tensor([1], dtype=torch.int) + torch.tensor([1], dtype=torch.float32)\r\n```\r\n\r\nSee the full [documentation](https://github.com/pytorch/pytorch/blob/master/docs/source/tensor_attributes.rst#type-promotion-doc) for more details.\r\n\r\n* `torch.result_type` Provide function to determine result of mixed-type operations ([26012](https://github.com/pytorch/pytorch/pull/26012)).\r\n* `torch.can_cast` Expose casting rules for type promotion ([26805](https://github.com/pytorch/pytorch/pull/26805)).\r\n* `torch.promote_types` Expose promotion logic ([26655](https://github.com/pytorch/pytorch/pull/26655)).\r\n\r\n\r\n\r\n\r\n## Deprecations\r\n\r\n\r\n### `nn.functional.affine_grid` / `nn.functional.grid_sample`: USING The Align_CORNER Default value is now deprecated, because it will be changed in 1.4 release.\r\n\r\nThe `align_corner` parameter was added in this release; the behavior in the previous release was equivalent to setting the parameter to `True`.  This is also the current default value but it will be changed to `False` from 1.4 release. Note that using the default will trigger a warning as demonstrated below; set the value explicitly to remove the warning. \r\n\r\n    >>> torch.nn.functional.affine_grid(torch.randn(1,2,3),\r\n                                        (1,3,2,2))\r\n    UserWarning: Default grid_sample and affine_grid behavior will be changed\r\n    to align_corners=False from 1.4.0. \r\n    See the documentation of grid_sample for details.\r\n    ...\r\n    \r\n    >>> torch.nn.functional.affine_grid(torch.randn(1,2,3),\r\n                                        (1,3,2,2),\r\n                                        align_corners=True)\r\n    # NO WARNING!\r\n    ...\r\n\r\n### [C++] Deprecate `torch::Tensor::data<T>()` in favor of `torch::Tensor::data_ptr<T>()` ([24847](https://github.com/pytorch/pytorch/pull/24847), [24886](https://github.com/pytorch/pytorch/pull/24886)).\r\n\r\n# New Features\r\n\r\n## TensorBoard: 3D Mesh and Hyperparameter Support\r\n\r\n`torch.utils.tensorboard` supports 3D mesh and points plus hyperparameter logging. More details can be found in [the documentation](https://pytorch.org/docs/stable/tensorboard.html) for `SummaryWriter` with `add_mesh` and `add_hparams`.\r\n\r\nA simple example exercising both methods:\r\n\r\n```\r\nfrom torch.utils.tensorboard import SummaryWriter\r\n\r\nvertices_tensor = torch.as_tensor([\r\n    [1, 1, 1],\r\n    [-1, -1, 1],\r\n    [1, -1, -1],\r\n    [-1, 1, -1],\r\n], dtype=torch.float).unsqueeze(0)\r\ncolors_tensor = torch.as_tensor([\r\n    [255, 0, 0],\r\n    [0, 255, 0],\r\n    [0, 0, 255],\r\n    [255, 0, 255],\r\n], dtype=torch.int).unsqueeze(0)\r\nfaces_tensor = torch.as_tensor([\r\n    [0, 2, 3],\r\n    [0, 3, 1],\r\n    [0, 1, 2],\r\n    [1, 3, 2],\r\n], dtype=torch.int).unsqueeze(0)\r\n\r\nwith SummaryWriter() as w:\r\n    w.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\r\n    for i in range(5):\r\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\r\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\r\n\r\n```\r\n\r\n## Distributed\r\n\r\nThis release adds macOS support for `torch.distributed` with the Gloo backend. You can more easily switch from development (e.g. on macOS) to deployment (e.g. on Linux) without having to change a single line of code. The prebuilt binaries for macOS (stable and nightly) include support out of the box.\r\n\r\n\r\n* `torch.distributed.all_reduce_coalesced` Support allreduce of a list of same-device tensors ([24949](https://github.com/pytorch/pytorch/pull/24949), [25470](https://github.com/pytorch/pytorch/pull/25470), [24876](https://github.com/pytorch/pytorch/pull/24876))\r\n* `torch.distributed.all_reduce` Add bitwise reduction ops (BAND, BOR, BXOR) ([26824](https://github.com/pytorch/pytorch/pull/26824))\r\n\r\n## Libtorch Binaries with C++11 ABI\r\n\r\nWe now provide Libtorch binaries for building applications compatible with the C++11 ABI. The download links for libtorch binaries with C++11 ABI can be found in https://pytorch.org/ \u201cQUICK START LOCALLY\u201d.\r\n\r\n\r\n## New TorchScript features\r\n\r\n* Add `not in` support for TorchScript ([23637](https://github.com/pytorch/pytorch/pull/23637)).\r\n* You can now raise exceptions in one side of an if branch ([23565](https://github.com/pytorch/pytorch/pull/23565)).\r\n* Add `torch.jit.is_scripting()` API ([25955](https://github.com/pytorch/pytorch/pull/25955)).\r\n* Make assertions like `x is not None` unwrap the optional type of `x` ([23949](https://github.com/pytorch/pytorch/pull/23949)).\r\n* Add dictionary augmented assignment (`+=`) support to TorchScript ([23639](https://github.com/pytorch/pytorch/pull/23639)).\r\n* Support `grad` and `data` attribute for tensor in TorchScript ([23842](https://github.com/pytorch/pytorch/pull/23842)).\r\n* Add `@ignore` for TorchScript classes ([23614](https://github.com/pytorch/pytorch/pull/23614)).\r\n* Support nn.GRU in script ([23266](https://github.com/pytorch/pytorch/pull/23266)).\r\n* Support tensor as a key type in TorchScript ([23638](https://github.com/pytorch/pytorch/pull/23638)).\r\n* Add support for ModuleDict ([25715](https://github.com/pytorch/pytorch/pull/25715)).\r\n* Bind `set_grad_enabled()` into TorchScript ([25350](https://github.com/pytorch/pytorch/pull/25350)).\r\n* Add `in` membership checks for lists ([25796](https://github.com/pytorch/pytorch/pull/25796)).\r\n* Add `tuple` keyword ([25474](https://github.com/pytorch/pytorch/pull/25474)).\r\n* Add `__getitem__` to class types ([25664](https://github.com/pytorch/pytorch/pull/25664)).\r\n* Add `__setitem__` to class types ([25750](https://github.com/pytorch/pytorch/pull/25750)).\r\n* Make JIT dicts ordered, matching Python 3.6+ semantics ([26465](https://github.com/pytorch/pytorch/pull/26465)).\r\n* Added invert bitwise operation to TorchScript ([22324](https://github.com/pytorch/pytorch/pull/22324)).\r\n* Add `min()` and `max()` for lists to TorchScript ([26351](https://github.com/pytorch/pytorch/pull/26351)).\r\n* Support iterables and ranges in list comprehensions ([26768](https://github.com/pytorch/pytorch/pull/26768)).\r\n\r\n# Improvements\r\n\r\n\r\n\r\n## C++ Frontend Improvements\r\n\r\nWe are on our way to better API parity between our Python and C++ frontends. Specifically, we made the following improvements:\r\n\r\n### Autograd\r\n\r\n* Tensor autograd APIs\r\n    * `torch::Tensor::data` Added ([26008](https://github.com/pytorch/pytorch/pull/26008)).\r\n    * `torch::Tensor::grad` Don\u2019t create a gradient for non-1-element Variables [BC-breaking] ([26150](https://github.com/pytorch/pytorch/pull/26150)).\r\n    * `torch::Tensor::is_leaf` Added ([26186](https://github.com/pytorch/pytorch/pull/26186)). \r\n    * `torch::Tensor::output_nr` Added ([26216](https://github.com/pytorch/pytorch/pull/26216)). \r\n    * `torch::Tensor::_version` Added ([26217](https://github.com/pytorch/pytorch/pull/26217)).\r\n* Add support for custom autograd functions in C++ API\r\n    * For example usage, please see the PR description and test cases in ([23572](https://github.com/pytorch/pytorch/pull/23572), [23628](https://github.com/pytorch/pytorch/pull/23628), and [23803](https://github.com/pytorch/pytorch/pull/23803))\r\n* `torch::autograd::backward` and `torch::autograd::grad` ([24342](https://github.com/pytorch/pytorch/pull/24342))\r\n* `torch::autograd::Variable::register_hook` ([24393](https://github.com/pytorch/pytorch/pull/24393)).\r\n\r\n### New torch::nn modules\r\n\r\n* Containers\r\n    * torch::nn::ModuleList ([24317](https://github.com/pytorch/pytorch/pull/24317)).\r\n* Linear layers\r\n    * torch::nn::Identity ([26713](https://github.com/pytorch/pytorch/pull/26713)).\r\n* Convolution layers\r\n    * torch::nn::Fold ([24160](https://github.com/pytorch/pytorch/pull/24160)).\r\n* Pooling layers\r\n    * torch::nn::MaxPool1d / MaxPool2d / MaxPool3d ([24860](https://github.com/pytorch/pytorch/pull/24860), [26521](https://github.com/pytorch/pytorch/pull/26521)).\r\n    * torch::nn::AvgPool1d / AvgPool2d / AvgPool3d ([25800](https://github.com/pytorch/pytorch/pull/25800)).\r\n    * torch::nn::AdaptiveMaxPool1d / AdaptiveMaxPool2d / AdaptiveMaxPool3d ([26755](https://github.com/pytorch/pytorch/pull/26755), [26772](https://github.com/pytorch/pytorch/pull/26772), [26775](https://github.com/pytorch/pytorch/pull/26775)).\r\n* Loss functions\r\n    * torch::nn::L1Loss ([25902](https://github.com/pytorch/pytorch/pull/25902)).\r\n* Distance functions\r\n    * torch::nn::CosineSimilarity ([26424](https://github.com/pytorch/pytorch/pull/26424))\r\n    * torch::nn::PairwiseDistance ([26424](https://github.com/pytorch/pytorch/pull/26424))\r\n\r\n### New torch::nn::functional functions\r\n\r\n* Pooling functions\r\n    * torch::nn::functional::max_pool1d / max_pool2d / max_pool3d ([26262](https://github.com/pytorch/pytorch/pull/26262)).\r\n    * torch::nn::functional::max_pool1d_with_indices / max_pool2d_with_indices / max_pool3d_with_indices ([26521](https://github.com/pytorch/pytorch/pull/26521)).\r\n    * torch::nn::functional::avg_pool1d / avg_pool2d / avg_pool3d ([26262](https://github.com/pytorch/pytorch/pull/26262)).\r\n    * torch::nn::functional::adaptive_max_pool1d / adaptive_max_pool2d / adaptive_max_pool3d ([26755](https://github.com/pytorch/pytorch/pull/26755), [26772](https://github.com/pytorch/pytorch/pull/26772), [26775](https://github.com/pytorch/pytorch/pull/26775)).\r\n    * torch::nn::functional::adaptive_max_pool1d_with_indices / adaptive_max_pool2d_with_indices / adaptive_max_pool3d_with_indices ([26755](https://github.com/pytorch/pytorch/pull/26755), [26772](https://github.com/pytorch/pytorch/pull/26772), [26775](https://github.com/pytorch/pytorch/pull/26775)).\r\n* Distance functions\r\n    * torch::nn::functional::cosine_similarity ([26424](https://github.com/pytorch/pytorch/pull/26424)).\r\n    * torch::nn::functional::pairwise_distance ([26424](https://github.com/pytorch/pytorch/pull/26424)).\r\n\r\n### tensor Construction API\r\n\r\n* Add support for multidimensional inputs to `torch::tensor` ([26210](https://github.com/pytorch/pytorch/pull/26210), [26890](https://github.com/pytorch/pytorch/pull/26890), [26756](https://github.com/pytorch/pytorch/pull/26756)).\r\n    * From now on, we can use `torch::tensor({{1, 2}, {3, 4}})` in C++ to construct the same tensor as `torch.tensor([[1, 2], [3, 4]])` in Python. Some caveats are noted in [this comment](https://github.com/pytorch/pytorch/blob/e0ae3ce5e4b5a98c8dd67b9ec1ea0f81dfc52fef/tools/autograd/templates/variable_factories.h#L184-L194).\r\n* Add support for bool and BFloat16 dtypes to `torch::tensor` ([23337](https://github.com/pytorch/pytorch/pull/23337)).\r\n\r\n### Other C++ Improvements\r\n\r\n* Add `torch::nn::Module::unregister_module` function, for unregistering a submodule from a `torch::nn::Module` ([26088](https://github.com/pytorch/pytorch/pull/26088)).\r\n\r\n## Distributed Improvements\r\n\r\n* `torch.distributed` Detect and handle NCCL errors appropriately instead of blocking peers until timeout in `ProcessGroupNCCL` ([25012](https://github.com/pytorch/pytorch/pull/25012), [25905](https://github.com/pytorch/pytorch/pull/25905))\r\n* `torch.distributed` Make scatter/gather arguments optional ([25575](https://github.com/pytorch/pytorch/pull/25575))\r\n* `torch.distributed.launch` Add a -m flag to allow users to launch python modules ([24910](https://github.com/pytorch/pytorch/pull/24910)).\r\n* `torch.distributed` Add function to get NCCL version for logging ([26583](https://github.com/pytorch/pytorch/pull/26583)).\r\n* `torch.distributed` Add timeout parameter to connect function in TCPStore ([26554](https://github.com/pytorch/pytorch/pull/26554)).\r\n* `torch.distributed` use timeout in connect function to prevent against infinite loop ([26364](https://github.com/pytorch/pytorch/pull/26364)).\r\n* `torch.nn.modules.batchnorm` Allow SyncBatchNorm to run without DDP in inference mode ([24815](https://github.com/pytorch/pytorch/pull/24815))\r\n\r\n## Performance Improvements\r\n\r\n* `torch.argmax/argmin` Rewrite as TensorIterator reductions ([26181](https://github.com/pytorch/pytorch/pull/26181)).\r\n* `torch.erfinv` Vectorize unary operator ([26629](https://github.com/pytorch/pytorch/pull/26629)).\r\n* `torch.sin/cos/tan` Use intrinsics for trigonometric functions on CPU ([26431](https://github.com/pytorch/pytorch/pull/26431)).\r\n* Fix possible deadlock in SharedCache inside a forked child proc ([25158](https://github.com/pytorch/pytorch/pull/25158)).\r\n* `torch.qr` Fix a regression ([23591](https://github.com/pytorch/pytorch/pull/23591)).\r\n* `nn.Conv` Use Caffe2's implementation of grouped depthwise 3x3 convolutions ([26556](https://github.com/pytorch/pytorch/pull/26556)).\r\n* `nn.Conv` Use parallel_for in DepthwiseConvKernel ([26879](https://github.com/pytorch/pytorch/pull/26879)).\r\n* `nn.Conv` Change shape for conv and unary ops ([25477](https://github.com/pytorch/pytorch/pull/25477)).\r\n* Fix pin_memory_thread not exiting quickly ([23646](https://github.com/pytorch/pytorch/pull/23646)).\r\n* Increase predefined_minimum_secs to reduce variation ([23734](https://github.com/pytorch/pytorch/pull/23734)).\r\n* Enhance Tensor indexSelect performance ([23055](https://github.com/pytorch/pytorch/pull/23055)).\r\n* Separate input shapes to reduce default execution time ([24136](https://github.com/pytorch/pytorch/pull/24136)).\r\n* constraints.lower_cholesky Vectorize LowerCholeskyTransform ([24131](https://github.com/pytorch/pytorch/pull/24131)).\r\n* Speed up an integer to the power of a positive integer on CPU ([26020](https://github.com/pytorch/pytorch/pull/26020)).\r\n* [ROCm] Enable jit fusion ([22872](https://github.com/pytorch/pytorch/pull/22872)).\r\n* [ROCm] Use MIOpen for transpose convolutions ([26172](https://github.com/pytorch/pytorch/pull/26172)).\r\n\r\n## JIT Improvements\r\n\r\n* Enable CPU fused kernel on Windows ([25578](https://github.com/pytorch/pytorch/pull/25578)).\r\n* Expose an API to iterate all the registered operators ([23207](https://github.com/pytorch/pytorch/pull/23207)).\r\n* Include recursive class compilations in error call stack ([23454](https://github.com/pytorch/pytorch/pull/23454)).\r\n* Substantial improvements to saved model format speed and size.\r\n    * Compress debug symbols when serializing TorchScript models. ([23659](https://github.com/pytorch/pytorch/pull/23659)).\r\n    * Compress all non-Tensor components of a serialized TorchScript model. ([23723](https://github.com/pytorch/pytorch/pull/23723)).\r\n    * Perform string uniquing by value in pickle serialization. ([23741](https://github.com/pytorch/pytorch/pull/23741)).\r\n    * Implement a bunch of pickle serialization features that optimize for size. ([23759](https://github.com/pytorch/pytorch/pull/23759)).\r\n    * Implement more size-oriented opcodes in the depickler. ([26454](https://github.com/pytorch/pytorch/pull/26454)).\r\n* Cache node operators to speed up optimization ([24827](https://github.com/pytorch/pytorch/pull/24827)).\r\n* Allow forward hooks in tracing ([23613](https://github.com/pytorch/pytorch/pull/23613)).\r\n* Add Pickler C++ API ([23241](https://github.com/pytorch/pytorch/pull/23241)).\r\n* Open up AliasAnalysisKind for any ops ([23810](https://github.com/pytorch/pytorch/pull/23810)).\r\n* Add the ability to compile exports on traced modules ([24298](https://github.com/pytorch/pytorch/pull/24298)).\r\n* Make `NoneType` a subtype of `Optional[T]` ([25361](https://github.com/pytorch/pytorch/pull/25361)).\r\n\r\n## ONNX Exporter Improvements\r\n\r\nIn PyTorch 1.3, we have added support for exporting graphs with ONNX IR v4 semantics, and set it as default. We have achieved good initial coverage for ONNX Opset 11, which was released recently with ONNX 1.6. Further enhancement to Opset 11 coverage will follow in the next release. We have enabled export for about 20 new PyTorch operators. Also, we have focused on enabling the export for all models in torchvision. We have introduced some necessary groundwork for that in this release, e.g., accepting PyTorch models with inputs/outputs of Dict or String. We continue to work on torchvision models, such as FasterRCNN and MaskRCNN, to enable their export.\r\n\r\n### Adding Support for ONNX IR v4\r\n\r\n* Provide an option to exclude the weights from model inputs ([#23284](https://github.com/pytorch/pytorch/pull/26146))\r\n* Make graph inputs without weights as default ([#26146](https://github.com/pytorch/pytorch/pull/26146))\r\n\r\n### Adding Support for ONNX Opset 11\r\n\r\n* Introduce ONNX Opset 11 support ([#23739](https://github.com/pytorch/pytorch/pull/23739))\r\n* Add export for torch.Interpolate in Opset 11 ([#24805](https://github.com/pytorch/pytorch/pull/24805), [#27179](https://github.com/pytorch/pytorch/pull/27179))\r\n* Add export for tensor.gather, tensor.scatter and tensor.scatter_add in Opset 11 ([#24790](https://github.com/pytorch/pytorch/pull/24790))\r\n* Add export for tensor.clamp in Opset 11 ([#25797](https://github.com/pytorch/pytorch/pull/25797))\r\n* Add export for torch.topk and torch.sort in Opset 11 ([#25739](https://github.com/pytorch/pytorch/pull/25739))\r\n\r\n### Exporting More Torch Operators/Models to ONNX\r\n\r\n* Export torch.pixel_shuffle ([#23739](https://github.com/pytorch/pytorch/pull/23739))\r\n* Export torch.multinomial ([#23581](https://github.com/pytorch/pytorch/pull/23581))\r\n* Export torch.norm\u2019s frobenius_norm ([#23536](https://github.com/pytorch/pytorch/pull/23536))\r\n* Export torch.std ([#22310](https://github.com/pytorch/pytorch/pull/22310))\r\n* Export torch.empty and torch.empty_like ([#24166](https://github.com/pytorch/pytorch/pull/24166))\r\n* Export torch.rsqrt ([#24153](https://github.com/pytorch/pytorch/pull/24153))\r\n* Export torch.log1p ([#25808](https://github.com/pytorch/pytorch/pull/25808))\r\n* Export torch.unique ([#25050](https://github.com/pytorch/pytorch/pull/25050))\r\n* Export torch.gelu ([#24475](https://github.com/pytorch/pytorch/pull/24475))\r\n* Export tensor.index_fill and tensor.index_copy ([#23052](https://github.com/pytorch/pytorch/pull/23052))\r\n* Export torch.round ([#26126](https://github.com/pytorch/pytorch/pull/26126))\r\n* Export torch.baddbmm ([#25738](https://github.com/pytorch/pytorch/pull/25738))\r\n* Export torch.remainder ([#24410](https://github.com/pytorch/pytorch/pull/24410))\r\n* Export torch.cumsum ([#24476](https://github.com/pytorch/pytorch/pull/24476))\r\n* Export tensor.size with negative axis ([#26436](https://github.com/pytorch/pytorch/pull/26436))\r\n* Export RNN/LSTM with h0/c0 initial state ([#22813](https://github.com/pytorch/pytorch/pull/22813))\r\n\r\n### Enhancing ONNX Export Infra\r\n\r\n* Enable exporting PyTorch models which have Dict and String as inputs and outputs ([#25889](https://github.com/pytorch/pytorch/pull/25889))\r\n* Systematically solving mismatched types caused by implicit type conversion for binary arithmetic operators by adding an ONNX type conversions pass. ([#24378](https://github.com/pytorch/pytorch/pull/24378))\r\n* Correctly validate dynamic axes names. ([#23974](https://github.com/pytorch/pytorch/pull/23974))\r\n* Enable ONNX Runtime tests for Opset 10 and partially for Opset 11 ([#22993](https://github.com/pytorch/pytorch/pull/22993))\r\n\r\n## Other Improvements\r\n\r\n* Error checking: many operators now perform strides check of the output tensor and errors if it contains inner overlaps that would result in incorrect result ([23063](https://github.com/pytorch/pytorch/issues/23063)).\r\n* `torch.det/logdet/slogdet` Allowing batching ([22909](https://github.com/pytorch/pytorch/pull/22909)).\r\n* `torch.logical_not` Add new operator ([23839](https://github.com/pytorch/pytorch/pull/23839)).\r\n* `torch.logical_xor` Add new operator ([23847](https://github.com/pytorch/pytorch/pull/23847)).\r\n* `torch.symeig` Improve the stability of gradient updates ([23018](https://github.com/pytorch/pytorch/pull/23018)).\r\n* `torch.eye` Enable for bool and half ([24148](https://github.com/pytorch/pytorch/pull/24148)).\r\n* `torch.tril / triu` Enable for bool and half ([24163](https://github.com/pytorch/pytorch/pull/24163)).\r\n* `torch.logical_not/xor` support non-bool tensors. ([23916](https://github.com/pytorch/pytorch/pull/23916), [23978](https://github.com/pytorch/pytorch/pull/23978)).\r\n* `torch.index_select` Implement indexing methods for sparse tensors ([24937](https://github.com/pytorch/pytorch/pull/24937)).\r\n* `torch.lu_solve` Enable broadcasting of batch dimensions ([24333](https://github.com/pytorch/pytorch/pull/24333)).\r\n* `torch.cholesky` Enable batches greater than 262140 ([24438](https://github.com/pytorch/pytorch/pull/24438)).\r\n* `torch.det` Simplify generation of singular matrices to avoid numerical issue on PowerPC ([25773](https://github.com/pytorch/pytorch/pull/25773)).\r\n* `torch.erfinv` In the CUDA implementation, use erfinv() for double to preserve accuracy ([25337](https://github.com/pytorch/pytorch/pull/25337)).\r\n* `torch.erfinv` Add a float version of erfinv on CPU ([26070](https://github.com/pytorch/pytorch/pull/26070)).\r\n* `torch.cuda.stream` Updates autograd engine to respect streams set in forward ([8354](https://github.com/pytorch/pytorch/pull/8354)).\r\n* `torch.backends.mkldnn.enabled` Allow disabling MKLDNN at runtime ([25459](https://github.com/pytorch/pytorch/pull/25459)).\r\n* `torch.cholesky_solve` Add derivative ([26185](https://github.com/pytorch/pytorch/pull/26185)).\r\n* `torch.cholesky_inverse` Add derivative ([26451](https://github.com/pytorch/pytorch/pull/26451)).\r\n* `torch.polygamma` Ensure that n is non-negativ`e` ([26294](https://github.com/pytorch/pytorch/pull/26294)).\r\n* `torch.pinverse` Enable batching ([26095](https://github.com/pytorch/pytorch/pull/26095)).\r\n* `torch.digamma/trigamma` Fix type mismatches on CUDA ([25791](https://github.com/pytorch/pytorch/pull/25791)).\r\n* `torch.where` Enable for bool tensor on CUDA ([26430](https://github.com/pytorch/pytorch/pull/26430)).\r\n* `torch.load` default encoding change to 'utf-8' ([26421](https://github.com/pytorch/pytorch/pull/26421)).\r\n* `torch.repeat_interleave` Respect the current stream ([26946](https://github.com/pytorch/pytorch/pull/26946)).\r\n* `torch.bernoulli_` Implement for bool tensors ([25076](https://github.com/pytorch/pytorch/pull/25076)).\r\n* `torch.norm` Fix nuclear norm with requires_grad=True ([26303](https://github.com/pytorch/pytorch/pull/26303)).\r\n* `torch.hub.download_url_to_file` Make function public ([26723](https://github.com/pytorch/pytorch/pull/26723)).\r\n* `nn.modules.conv` add padding_mode to repr ([23996](https://github.com/pytorch/pytorch/pull/23996)).\r\n* `nn.Transformer` Extend to support BERT (gelu) ([24181](https://github.com/pytorch/pytorch/pull/24181)).\r\n* `nn.BatchNorm2d` Add support for non-affine batch norm with float stats and half inputs ([22750](https://github.com/pytorch/pytorch/pull/22750)).\r\n* `nn.Parameter` Fix type hints ([25586](https://github.com/pytorch/pytorch/pull/25586)).\r\n* `nn.CTCLoss` Improve error message ([26325](https://github.com/pytorch/pytorch/pull/26325)).\r\n* `nn.Conv` Allow batch size of 0 ([26214](https://github.com/pytorch/pytorch/pull/26214)).\r\n* `nn.LSTM/GRU` enable double backward for non-cudnn ([26660](https://github.com/pytorch/pytorch/pull/26660)).\r\n* `optim.Adagrad` Add epsilon argument ([24980](https://github.com/pytorch/pytorch/pull/24980)).\r\n* `optim.LBFGS`  Change default tolerance_grad to 1e-7 ([25240](https://github.com/pytorch/pytorch/pull/25240)).\r\n* `optim.lr_scheduler.OneCycleLR` Add new 1cycle learning rate scheduler ([25324](https://github.com/pytorch/pytorch/pull/25324)).\r\n* `optimizer.step` Fix type annotation ([26930](https://github.com/pytorch/pytorch/pull/26930)).\r\n* `bfloat16` Add support for sub, mul, and div on CPU ([22851](https://github.com/pytorch/pytorch/pull/22851)).\r\n* `bfloat16` Enabled comparison ops on CPU ([24182](https://github.com/pytorch/pytorch/pull/24182)).\r\n* `bfloat16` Enabled masked methods ([24183](https://github.com/pytorch/pytorch/pull/24183)).\r\n* `bfloat16` Enabled torch.mm and torch.mv ([24224](https://github.com/pytorch/pytorch/pull/24224)).\r\n* `bfloat16` Enable log_softmax and CrossEntropyLoss ([24457](https://github.com/pytorch/pytorch/pull/24457)).\r\n* `bfloat16` Enabled conv methods ([26167](https://github.com/pytorch/pytorch/pull/26167)).\r\n* `bfloat16` Enabled dtype on CUDA ([26407](https://github.com/pytorch/pytorch/pull/26407)).\r\n* `quasirandom.SobolEngine` Use random seed if not specified ([24884](https://github.com/pytorch/pytorch/pull/24884)).\r\n* `utils.data.dataloader` Add possible out of shared memory error message ([25730](https://github.com/pytorch/pytorch/pull/25730)).\r\n* `cuda.set_rng_state` Add type hint ([26200](https://github.com/pytorch/pytorch/pull/26200)).\r\n* Zero sized tensor support for repeat_interleave ([23717](https://github.com/pytorch/pytorch/pull/23717)).\r\n* Recommend `~` and `bitwise_not()` when user tries to apply neg (`-`) on a bool tensor. ([23621](https://github.com/pytorch/pytorch/pull/23621)).\r\n* Fix double backward of inplace op on view ([23502](https://github.com/pytorch/pytorch/pull/23502)).\r\n* `autograd.grad` Validate shapes of outputs ([25349](https://github.com/pytorch/pytorch/pull/25349)).\r\n* Enable libflame as a LAPACK choice ([25795](https://github.com/pytorch/pytorch/pull/25795)).\r\n* Fix race condition in CUDA initialization ([25788](https://github.com/pytorch/pytorch/pull/25788)).\r\n* Include `iteration_` in SGD optimizer serialization ([26906](https://github.com/pytorch/pytorch/pull/26906)).\r\n* [C++] `torch::tensor` Fix an ambiguous overload issues in constructor ([26890](https://github.com/pytorch/pytorch/pull/26890)).\r\n* [XLA] Check device before accessing data_ptr in PackLayer ([26056](https://github.com/pytorch/pytorch/pull/26056)).\r\n* [XLA] Allow overwriting catch-all kernels ([25947](https://github.com/pytorch/pytorch/pull/25947)).\r\n\r\n\r\n\r\n# Bug Fixes\r\n\r\n### TensorBoard Bug Fixes\r\n\r\n* `SummaryWriter.add_graph`: Fix empty graph output in some cases ([25599](https://github.com/pytorch/pytorch/pull/25599)).\r\n* Update Caffe2 contrib TensorBoard logging to not require TensorFlow ([25259](https://github.com/pytorch/pytorch/pull/25259)).\r\n* `SummaryWriter.make_video`: Fix write_gif call to moviepy for newer lib ([21218](https://github.com/pytorch/pytorch/pull/21218)).\r\n\r\n### C++ API Bug fixes\r\n\r\n* Fixes mismatch of device and data type when computing `step_size` in LBFGS optimizer ([25909](https://github.com/pytorch/pytorch/pull/25909)).\r\n\r\n### JIT\r\n\r\n* Fix list comprehension that change the type of the original iterable ([24271](https://github.com/pytorch/pytorch/pull/24271)).\r\n* Fix double copying of constants during recursive scripting ([24412](https://github.com/pytorch/pytorch/pull/24412)).\r\n* Fix frontend error message ([23576](https://github.com/pytorch/pytorch/pull/23576)).\r\n* Clear recursive error stack on each compilation ([23458](https://github.com/pytorch/pytorch/pull/23458)).\r\n* Fix bugs in assignment to optionals ([25059](https://github.com/pytorch/pytorch/pull/25059)).\r\n* Make `torch.jit.Attribute` work when `PYTORCH_ENABLED=0` ([23851](https://github.com/pytorch/pytorch/pull/23851)).\r\n* Fix unicode in comments causing compilation errors ([24218](https://github.com/pytorch/pytorch/pull/24218)).\r\n* Correctly raise an error if an `nn.Module` has not been initialized but you try to script it ([24852](https://github.com/pytorch/pytorch/pull/24852)).\r\n* Fix annotated assignment to variables ([25094](https://github.com/pytorch/pytorch/pull/25094)).\r\n* dictPop: dereference dict.find() iterator before calling dict.erase() ([25056](https://github.com/pytorch/pytorch/pull/25056)).\r\n* fix closures which always throw. ([25278](https://github.com/pytorch/pytorch/pull/25278)).\r\n* Add source location to class instantiation error ([24990](https://github.com/pytorch/pytorch/pull/24990)).\r\n* Fix `AliasAnalysisKind::PURE` on MSVC ([25375](https://github.com/pytorch/pytorch/pull/25375)).\r\n* Emit script function calls during tracing. ([25089](https://github.com/pytorch/pytorch/pull/25089)).\r\n* Resolve `NamedTuple` types properly in Python ([26443](https://github.com/pytorch/pytorch/pull/26443)).\r\n* Fix schema matching of tuples to vartype lists ([25944](https://github.com/pytorch/pytorch/pull/25944)).\r\n* Correctly preserve ignored function return value type ([25262](https://github.com/pytorch/pytorch/pull/25262)).\r\n* Fix missing newline in compiled from source range highlight ([25802](https://github.com/pytorch/pytorch/pull/25802)).\r\n* Fix use-after-free bug in `optional` ([25965](https://github.com/pytorch/pytorch/pull/25965)).\r\n* Fix torch.arange traced as constant ([25363](https://github.com/pytorch/pytorch/pull/25363)).\r\n* Preserve module names in recursive script ([24505](https://github.com/pytorch/pytorch/pull/24505)).\r\n* Properly resolve ignored module method type annotations ([26683](https://github.com/pytorch/pytorch/pull/26683)).\r\n* Make `is_optional` check more robust ([26312](https://github.com/pytorch/pytorch/pull/26312)).\r\n* Fix builtin lookup for Python functions ([26688](https://github.com/pytorch/pytorch/pull/26688)).\r\n* Typevar matching fix + implicit conversions from Scalar to int/float ([26453](https://github.com/pytorch/pytorch/pull/26453)).\r\n* Fix range for non-int inputs and pow implementation ([26926](https://github.com/pytorch/pytorch/pull/26926)).\r\n\r\n### Other Bug Fixes\r\n\r\n* `torch.is_pinned` pin_memory should not copy on already pinned tensors ([23484](https://github.com/pytorch/pytorch/pull/23484)).\r\n* `torch.cdist` Fix incorrect gradients on CUDA non-batch tensors ([22915](https://github.com/pytorch/pytorch/pull/22915)).\r\n* `torch.from_numpy` Fix failure on windows for int32 ([25139](https://github.com/pytorch/pytorch/pull/25139)).\r\n* `torch.tensor` Fix memory leak creating a tensor from numpy ([24267](https://github.com/pytorch/pytorch/pull/24267)).\r\n* `torch.index` Don't save `self` in `index` backward ([25594](https://github.com/pytorch/pytorch/pull/25594)).\r\n* `torch.bincount` Fix int32 overflow on CUDA ([25748](https://github.com/pytorch/pytorch/pull/25748)).\r\n* `torch.bernoulli` Fix the distribution sampler ([26864](https://github.com/pytorch/pytorch/pull/26864)).\r\n* `torch.pow` Fix precision ([25476](https://github.com/pytorch/pytorch/pull/25476)).\r\n* `torch.cdist` Fix gradient computation when first arg is 1xn ([26254](https://github.com/pytorch/pytorch/pull/26254)).\r\n* `torch.scatter_add_` Fix scatter CPU kernel when (input size, src size) > index size ([25839](https://github.com/pytorch/pytorch/pull/25839)).\r\n* `nn.ConvTranspose2d` Fixed an error with float16 inputs and weights on CUDA.  ([23552](https://github.com/pytorch/pytorch/pull/23552)).\r\n* `nn.CTCLoss` Fix zero-length targets on CUDA ([23298](https://github.com/pytorch/pytorch/pull/23298)).\r\n* `nn.Conv2d` Correct an overflow in an error message ([25146](https://github.com/pytorch/pytorch/pull/25146)).\r\n* `optim.Adam` apply a small mathematical fix. ([23737](https://github.com/pytorch/pytorch/pull/23737)).\r\n* `dataloader` Fix IndexError on shutdown if not all workers are started ([23761](https://github.com/pytorch/pytorch/pull/23761)).\r\n* `Tensor.repeat` Fix crash on for 0 repeats ([23766](https://github.com/pytorch/pytorch/pull/23766)).\r\n* `torch.pin_memory` only use one thread ([25111](https://github.com/pytorch/pytorch/pull/25111)).\r\n* `distributions.Uniform,HalfCauchy,Gamma` Fix `log_prob` when value is a float ([23017](https://github.com/pytorch/pytorch/pull/23017)).\r\n* Fix typing error for Padding with asymmetric signatures ([24895](https://github.com/pytorch/pytorch/pull/24895)).\r\n* Avoid race condition in `intrusive_ptr.reset_()` ([24464](https://github.com/pytorch/pytorch/pull/24464)).\r\n* `torch.hub`: Fix SSL cert issue for hub in Python 2 ([25042](https://github.com/pytorch/pytorch/pull/25042)).\r\n* Fix int overflow issue in CUDA kernels. ([24818](https://github.com/pytorch/pytorch/pull/24818)).\r\n* `Module.cuda` Fix type hints ([25018](https://github.com/pytorch/pytorch/pull/25018)).\r\n* Fix bug in assertNotEqual for int tensors ([25412](https://github.com/pytorch/pytorch/pull/25412)).\r\n* Fix 'in' return true incorrectly ([24156](https://github.com/pytorch/pytorch/pull/24156)).\r\n* Fix bugs in bulk loader when `batch_size=None` or with namedtuple ([26065](https://github.com/pytorch/pytorch/pull/26065)).\r\n* Fix serialization issue in big endian arch ([26383](https://github.com/pytorch/pytorch/pull/26383)).\r\n* Fix `Vec256::abs()` for floating point when applied on -0.0 ([26422](https://github.com/pytorch/pytorch/pull/26422)).\r\n* Fix cyclic reference in _LRScheduler ([25776](https://github.com/pytorch/pytorch/pull/25776)).\r\n* Fix a build failure on s390x ([26233](https://github.com/pytorch/pytorch/pull/26233)).\r\n* [XLA] Fix tensor construction from array ([24283](https://github.com/pytorch/pytorch/pull/24283)).\r\n\r\n# Documentation Updates\r\n\r\n### Distributed\r\n\r\n* `torch.distributed` Error phrasing in torch.distributed helper functions ([25574](https://github.com/pytorch/pytorch/pull/25574))\r\n* `torch.distributions.negative_binomial` clarified ambiguous doc string in NegativeBinomial ([25923](https://github.com/pytorch/pytorch/pull/25923))\r\n\r\n### JIT\r\n\r\n* Add technical documentation for the serialization format ([23456](https://github.com/pytorch/pytorch/pull/23456)).\r\n* Fix trace docs ([24191](https://github.com/pytorch/pytorch/pull/24191)).\r\n* Add `trace_module` to docs ([24258](https://github.com/pytorch/pytorch/pull/24258)).\r\n* Cleanup distinction around `script` and `trace` ([24208](https://github.com/pytorch/pytorch/pull/24208)).\r\n* Fix `item()` call in docs ([25404](https://github.com/pytorch/pytorch/pull/25404)).\r\n* Misc doc updates / fixes ([24371](https://github.com/pytorch/pytorch/pull/24371), [24445](https://github.com/pytorch/pytorch/pull/24445)).\r\n\r\n### Other documentation improvements\r\n\r\n* `torch.record_stream` Add documentation ([24078](https://github.com/pytorch/pytorch/pull/24078)).\r\n* `torch.fold` Describe the relation between fold and unfold operations ([24840](https://github.com/pytorch/pytorch/pull/24840)).\r\n* `torch.argmax` Fix incorrect doc ([23775](https://github.com/pytorch/pytorch/pull/23775)).\r\n* `torch.random` add docs ([23553](https://github.com/pytorch/pytorch/pull/23553)).\r\n* `torch.empty_strided` Add docs ([23735](https://github.com/pytorch/pytorch/pull/23735)).\r\n* `torch.bitwise_not` Document for bool tensors ([23800](https://github.com/pytorch/pytorch/pull/23800)).\r\n* `torch.cdist` Add documentation ([25221](https://github.com/pytorch/pytorch/pull/25221)).\r\n* `torch.where` Update parameter names in doc ([25554](https://github.com/pytorch/pytorch/pull/25554)).\r\n* `torch.atan2` Clarify and correct the doc ([26180](https://github.com/pytorch/pytorch/pull/26180)).\r\n* `nn.functional.bilinear` Added documentation ([24951](https://github.com/pytorch/pytorch/pull/24951)).\r\n* `nn.functional.upsample` Fix align_corners doc ([23707](https://github.com/pytorch/pytorch/pull/23707)).\r\n* `nn.Transformer` Fixed an error in the example ([24837](https://github.com/pytorch/pytorch/pull/24837)).\r\n* `optim.lr_scheduler.CosineAnnealingWarmRestarts` Add documentation ([25421](https://github.com/pytorch/pytorch/pull/25421)).\r\n* `optim.SGD` Updated with subscripts ([23985](https://github.com/pytorch/pytorch/pull/23985)).\r\n* `optim.RMSprop` Highlighting in the doc that square root comes before adding epsilon ([26735](https://github.com/pytorch/pytorch/pull/26735)).\r\n* `autograd.detect_anomaly` Add a warning ([26615](https://github.com/pytorch/pytorch/pull/26615)).\r\n* Improve dataloader docs on when auto-batching is disabled ([23671](https://github.com/pytorch/pytorch/pull/23671)).\r\n* Updated docs and added deprecation warnings to acknowledge a bool tensor ([22261](https://github.com/pytorch/pytorch/pull/22261)).\r\n* Document benchmarking practice for CUDA ([23910](https://github.com/pytorch/pytorch/pull/23910)).\r\n* Add ASAN instructions to CONTRIBUTING.md ([24848](https://github.com/pytorch/pytorch/pull/24848)).\r\n\r\n\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.3.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.3.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.3.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/20597721", "dateCreated": "2019-10-10T16:23:22Z", "datePublished": "2019-10-10T17:26:52Z"}, {"tagName": "v1.2.0", "name": "New TorchScript API with Improved Python Language Coverage, Expanded ONNX Export, NN.Transformer", "authorName": "gchanan", "authorType": "User", "body": "We have just released PyTorch v1.2.0.\r\n\r\nIt has over 1,900 commits and contains a significant amount of effort in areas spanning JIT, ONNX, Distributed, as well as Performance and Eager Frontend Improvements.\r\n\r\n## Highlights\r\n\r\n### [JIT] New TorchScript API\r\n\r\nVersion 1.2 includes a new, easier-to-use API for converting `nn.Module`s into `ScriptModule`s. A sample usage is:\r\n\r\n```\r\nclass MyModule(torch.nn.Module):\r\n    ...\r\n\r\n# Construct an nn.Module instance\r\nmodule = MyModule(args)\r\n\r\n# Pass it to `torch.jit.script` to compile it into a ScriptModule.\r\nmy_torchscript_module = torch.jit.script(module)\r\n```\r\n\r\n`torch.jit.script()` will attempt to recursively compile the given `nn.Module`, including any submodules or methods called from `forward()`. See the [migration guide](https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api) for more info on what's changed and how to migrate.\r\n\r\n### [JIT] Improved TorchScript Python language coverage\r\n\r\nIn 1.2, TorchScript has significantly improved its support for Python language constructs and Python's standard library. Highlights include:\r\n\r\n* Early returns, breaks and continues.\r\n* Iterator-based constructs, like `for..in` loops, `zip()`, and `enumerate()`.\r\n* `NamedTuples`.\r\n* `math` and `string` library support.\r\n* Support for most Python builtin functions.\r\n\r\nSee the detailed notes below for more information.\r\n\r\n### Expanded Onnx Export\r\n\r\n In PyTorch 1.2, working with Microsoft, we\u2019ve added full support to export ONNX Opset versions 7(v1.2), 8(v1.3), 9(v1.4) and 10 (v1.5). We\u2019ve have also enhanced the constant folding pass to support Opset 10, the latest available version of ONNX. Additionally, users now are able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export. Here is a summary of the all of the major improvements:\r\n\r\n* Support for multiple Opsets including the ability to export dropout, slice, flip and interpolate in Opset 10.\r\n* Improvements to ScriptModule including support for multiple outputs, tensor factories and tuples as inputs and outputs.\r\n* More than a dozen additional PyTorch operators supported including the ability to export a custom operator. \r\n\r\nUpdated docs can be found [here](https://pytorch.org/docs/stable/onnx.html) and also a refreshed tutorial using ONNXRuntime can be found [here](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html).\r\n\r\n### Tensorboard is no Longer Considered Experimental\r\n\r\nRead the [documentation](https://pytorch.org/docs/stable/tensorboard.html) or simply type **`from`**` torch.utils.tensorboard `**`import`**` SummaryWriter` to get started!\r\n\r\n### NN.Transformer\r\n\r\nWe include a standard [nn.Transformer](https://pytorch.org/docs/stable/nn.html?highlight=transformer#torch.nn.Transformer) module, based on the paper \u201c[_Attention is All You Need_](https://arxiv.org/abs/1706.03762)\u201d.  The `nn.Transformer` module relies entirely on an [attention mechanism](https://pytorch.org/docs/stable/nn.html?highlight=nn%20multiheadattention#torch.nn.MultiheadAttention) to draw global dependencies between input and output.  The individual components of the `nn.Transformer` module are designed so they can be adopted independently.  For example, the [nn.TransformerEncoder](https://pytorch.org/docs/stable/nn.html?highlight=nn%20transformerencoder#torch.nn.TransformerEncoder) can be used by itself, without the larger `nn.Transformer`. New APIs include:\r\n\r\n* `nn.Transformer`\r\n* `nn.TransformerEncoder` and `nn.TransformerEncoderLayer`\r\n* `nn.TransformerDecoder` and `nn.TransformerDecoderLayer`\r\n\r\nSee the [Transformer Layers](https://pytorch.org/docs/stable/nn.html#transformer-layers) documentation for more info.\r\n\r\n## Breaking Changes\r\n\r\n### Comparison operations (`lt (<), le (<=), gt (>), ge (>=), eq (==), ne, (!=)` ) return dtype has changed from `torch.uint8` to `torch.bool` ([21113](https://github.com/pytorch/pytorch/pull/21113))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2])\r\ntensor([1, 0, 0], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2])\r\ntensor([True, False, False])\r\n```\r\n\r\n\r\nFor most programs, we don't expect that any changes will need to be made as a result of this change. There are a couple of possible exceptions listed below.\r\n\r\n**Mask Inversion**\r\n\r\nIn prior versions of PyTorch, the idiomatic way to invert a mask was to call `1 - mask`.  This behavior is no longer supported; use the `~` or `bitwise_not()` operator instead.\r\n\r\n*Version 1.1*:\r\n\r\n```\r\n>>> 1 - (torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2]))\r\ntensor([0, 1, 1], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> 1 - (torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2]))\r\nRuntimeError: Subtraction, the `-` operator, with a bool tensor is not supported.\r\nIf you are trying to invert a mask, use the `~` or `bitwise_not()` operator instead.\r\n\r\n>>> ~(torch.tensor([1, 2, 3]) < torch.tensor([3, 1, 2]))\r\ntensor([False,  True,  True])\r\n```\r\n\r\n**sum(Tensor) (python built-in) does not upcast `dtype` like `torch.sum`**\r\n\r\nPython's built-in `sum` returns results in the same `dtype` as the tensor itself, so it will not return the expected result if the value of the sum cannot be represented in the `dtype` of the tensor.\r\n\r\n*Version 1.1*:\r\n\r\n```\r\n# value can be represented in result dtype\r\n>>> sum(torch.tensor([1, 2, 3, 4, 5]) > 2)\r\ntensor(3, dtype=torch.uint8)\r\n\r\n# value can NOT be represented in result dtype\r\n>>> sum(torch.ones((300,)) > 0)\r\ntensor(44, dtype=torch.uint8)\r\n\r\n# torch.sum properly upcasts result dtype\r\n>>> torch.sum(torch.ones((300,)) > 0)\r\ntensor(300)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n# value cannot be represented in result dtype (now torch.bool)\r\n>>> sum(torch.tensor([1, 2, 3, 4, 5]) > 2)\r\ntensor(True)\r\n\r\n# value cannot be represented in result dtype\r\n>>> sum(torch.ones((300,)) > 0)\r\ntensor(True)\r\n\r\n# torch.sum properly upcasts result dtype\r\n>>> torch.sum(torch.ones((300,)) > 0)\r\ntensor(300)\r\n```\r\n\r\n**TLDR**: use `torch.sum` instead of the built-in `sum`.  Note that the built-in `sum()` behavior will more closely resemble `torch.sum` in the next release. \r\n\r\nNote also that masking via `torch.uint8` Tensors is now deprecated, see the **Deprecations** section for more information.\r\n\r\n\r\n### `__invert__` / `~`: now calls `torch.bitwise_not` instead of `1 - tensor` and is supported for all integral+Boolean dtypes instead of only `torch.uint8`.  ([22326](https://github.com/pytorch/pytorch/pull/22326))\r\n\r\n*Version 1.1*:\r\n\r\n```\r\n>>> ~torch.arange(8, dtype=torch.uint8)\r\ntensor([ 1, 0, 255, 254, 253, 252, 251, 250], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2*:\r\n\r\n```\r\n>>> ~torch.arange(8, dtype=torch.uint8)\r\ntensor([255, 254, 253, 252, 251, 250, 249, 248], dtype=torch.uint8)\r\n```\r\n\r\n\r\n\r\n### `torch.tensor(bool)` and `torch.as_tensor(bool)` now infer `torch.bool` dtype instead of `torch.uint8`.  ([19097](https://github.com/pytorch/pytorch/pull/19097))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> torch.tensor([True, False])\r\ntensor([1, 0], dtype=torch.uint8)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> torch.tensor([True, False])\r\ntensor([ True, False])\r\n```\r\n\r\n\r\n\r\n### `nn.BatchNorm{1,2,3}D`: gamma (`weight`) is now initialized to all 1s rather than randomly initialized from *U(0, 1)*.  ([13774](https://github.com/pytorch/pytorch/pull/13774))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> torch.nn.BatchNorm2d(5).weight\r\nParameter containing:\r\ntensor([0.1635, 0.7512, 0.4130, 0.6875, 0.5496], \r\n       requires_grad=True)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> torch.nn.BatchNorm2d(5).weight\r\nParameter containing:\r\ntensor([1., 1., 1., 1., 1.], requires_grad=True)\r\n```\r\n\r\n\r\n\r\n### A number of deprecated Linear Algebra operators have been removed ([22841](https://github.com/pytorch/pytorch/pull/22841))\r\n\r\n| Removed        | Use Instead  | \r\n| ------------- | ------------- |\r\n| `btrifact`    | `lu` |\r\n| `btrifact_with_info`      | `lu` with `get_infos=True`      |\r\n| `btrisolve` | `lu_solve`     |\r\n| `btriunpack` | `lu_unpack`    |\r\n| `gesv` | `solve`     |\r\n| `pstrf` | `cholesky`     |\r\n| `potrf` | `cholesky`     |\r\n| `potri` | `cholesky_inverse`     |\r\n| `potrs` | `cholesky_solve`     |\r\n| `trtrs` | `triangular_solve`     |\r\n\r\n\r\n### Sparse Tensors: Changing the sparsity of a Tensor through `.data` is no longer supported.  ([17072](https://github.com/pytorch/pytorch/pull/17072))\r\n\r\n```\r\n>>> x = torch.randn(2,3)\r\n>>> x.data = torch.sparse_coo_tensor((2, 3))\r\nRuntimeError: Attempted to call `variable.set_data(tensor)`,\r\nbut `variable` and  `tensor` have incompatible tensor type.\r\n```\r\n\r\n\r\n\r\n### Sparse Tensors: in-place shape modifications of Dense Tensor Constructor Arguments will no longer modify the Sparse Tensor itself ([20614](https://github.com/pytorch/pytorch/pull/20614))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> i = torch.tensor([[0, 1]])\r\n>>> v = torch.ones(2)\r\n>>> s = torch.sparse_coo_tensor(i, v)\r\n>>> i.resize_(1, 1)\r\n>>> v.resize_(1)\r\n\r\n>>> s.coalesce().indices().shape\r\ntorch.Size([1, 1])\r\n\r\n>>> s.coalesce().values().shape\r\ntorch.Size([1])\r\n```\r\n\r\nNotice `indices()` and `values()` reflect the resized tensor shapes.\r\n\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> i = torch.tensor([[0, 1]])\r\n>>> v = torch.ones(2)\r\n>>> s = torch.sparse_coo_tensor(i, v)\r\n>>> i.resize_(1, 1)\r\n>>> v.resize_(1)\r\n\r\n>>> s.coalesce().indices().shape\r\ntorch.Size([1, 2])\r\n\r\n>>> s.coalesce().values().shape\r\ntorch.Size([2])\r\n```\r\n\r\nNotice `indices()` and `values()` reflect the original tensor shapes.\r\n\r\n### Sparse Tensors: Accumulating dense gradients into a sparse `.grad` will no longer retain Python object identity.  ([17072](https://github.com/pytorch/pytorch/pull/17072))\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> m = torch.nn.Embedding(10, 3, sparse=True)\r\n>>> m(torch.tensor([[1,2,4,5],[4,3,2,9]])).sum().backward()\r\n>>> assert m.weight.grad.layout == torch.sparse_coo\r\n>>> m_weight_grad_saved = m.weight.grad\r\n\r\n# accumulate dense gradient into sparse .grad, change sparsity\r\n>>> m.weight.sum().backward()\r\n>>> assert m.weight.grad.layout == torch.strided\r\n# m_weight_grad_saved still refers to the .grad of m's weight\r\n# even though the sparsity has changed\r\n>>> assert id(m_weight_grad_saved) == id (m.weight.grad)\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> m = torch.nn.Embedding(10, 3, sparse=True)\r\n>>> m(torch.tensor([[1,2,4,5],[4,3,2,9]])).sum().backward()\r\n>>> assert m.weight.grad.layout == torch.sparse_coo\r\n>>> m_weight_grad_saved = m.weight.grad\r\n\r\n# accumulate dense gradient into sparse .grad, change sparsity\r\n>>> m.weight.sum().backward()\r\n>>> assert m.weight.grad.layout == torch.strided\r\n# m_weight_grad_saved NO LONGER refers to the .grad of m's weight\r\n>>> assert id(m_weight_grad_saved) == id (m.weight.grad)\r\nAssertionError\r\n```\r\n\r\n\r\n\r\n### `nn.utils.convert_sync_batchnorm` has been replaced with `nn.SyncBatchNorm.convert_sync_batchnorm `([18787)](https://github.com/pytorch/pytorch/pull/18787)\r\n\r\nExample of new usage:\r\n\r\n```\r\n>>> # Network with nn.BatchNorm layer\r\n>>> module = torch.nn.Sequential(\r\n>>>     torch.nn.Linear(20, 100),\r\n>>>     torch.nn.BatchNorm1d(100)\r\n>>> ).cuda()\r\n>>> # creating process group (optional)\r\n>>> process_group = torch.distributed.new_group(process_ids)\r\n>>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\r\n```\r\n\r\n### Error Checking: `torch.addcmul` and `torch.lerp` operators enforce stronger shape requirements on the output tensor (`out=` keyword argument) and do not allow output tensor to be resized if it is also used as one of the inputs.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\n>>> x=torch.zeros(1)\r\n>>> torch.addcmul(x, x, torch.zeros(2,3), out=x)\r\ntensor([[0., 0., 0.],\r\n        [0., 0., 0.]])\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\n>>> x=torch.zeros(1)\r\n>>> torch.addcmul(x, x, torch.zeros(2,3), out=x)\r\nRuntimeError: output with shape [1] doesn't match the broadcast shape [2, 3]\r\n```\r\n\r\nIf you run into this error, please ensure the `out` parameter is of the correct output shape (post-broadcasting).\r\n\r\n### Error Checking: Improved Variable version tracking ([20391](https://github.com/pytorch/pytorch/pull/20391), [22821](https://github.com/pytorch/pytorch/pull/22821), [21865](https://github.com/pytorch/pytorch/pull/21865))\r\n\r\nPyTorch\u2019s autograd system uses a version tracking mechanism to ensure that Tensors that are saved for backwards computations retain their correct values when the backward pass is computed (i.e. that they haven\u2019t been updated in-place since they were saved).  See [In Place Correctness Checks](https://pytorch.org/docs/stable/notes/autograd.html#in-place-correctness-checks) in the docs for more information.\r\n\r\nIn PyTorch 1.2 we have enhanced the version tracking in a number of cases, which may flag issues that were not caught previously.  There is now additional tracking through the `Variable()` constructor, the `nn.Parameter()` constructor, after setting `.data`, and via `nn.Module._apply` (internal API).\r\n\r\n*Track changes through Variable constructor:*\r\n\r\n```\r\n>>> x = torch.ones(1, requires_grad=True)+1\r\n>>> y = x*x\r\n\r\n# do an in-place update through Variable constructor\r\n>>> torch.autograd.Variable(x).add_(1)\r\n>>> y.backward()\r\nRuntimeError: one of the variables needed for gradient computation has been modified\r\nby an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 \r\ninstead.\r\n```\r\n\r\n*Track changes on an nn.Parameter:*\r\n\r\n```\r\n>>> x = torch.ones(1)\r\n>>> p = torch.nn.Parameter(x)\r\n>>> y = p * p\r\n\r\n# do an in-place update on a saved Parameter\r\n>>> x.add_(1)\r\n>>> y.sum().backward()\r\nRuntimeError: one of the variables needed for gradient computation has been modified\r\nby an inplace operation: [torch.FloatTensor [1]] is at version 1; expected version 0 \r\ninstead.\r\n```\r\n\r\n*Track changes after setting `.data`:*\r\n\r\n```\r\n>>> x = torch.zeros(1, requires_grad=True)+1\r\n>>> y = x * x\r\n>>> x.data = torch.zeros(1, requires_grad=True)+1\r\n\r\n>>> x.add_(1)\r\n>>> y.backward()\r\nRuntimeError: one of the variables needed for gradient computation has been modified\r\nby an inplace operation: [torch.FloatTensor [1]], which is output 0 of AddBackward0,\r\nis at version 1; expected version 0 instead.\r\n```\r\n\r\n### [JIT] Python called from scripted modules must be `@ignore`d\r\n\r\n`torch.jit.script` now recursively compiles everything it finds in the original function, so if you had Python functions called from in your scripted function or module, you must now explicitly `@ignore` it. See [the new API guide](https://pytorch.org/docs/master/jit.html#migrating-to-pytorch-1-2-recursive-scripting-api) for more details.\r\n\r\n*Version 1.1*\r\n\r\n```\r\ndef my_unscriptable_python_fn():\r\n    # weird stuff\r\n\r\n@torch.jit.script\r\ndef fn():\r\n    # This gets inserted as a Python call, and only errors on `save()`.\r\n    my_unscriptable_python_fn()\r\n```\r\n\r\n*Version 1.2*\r\n\r\n```\r\n@torch.jit.ignore  # this needs to be added ...\r\ndef my_unscriptable_python_fn():\r\n    ...\r\n\r\n@torch.jit.script\r\ndef fn():\r\n    # ... or else recursive compilation will attempt to compile this call\r\n    my_unscriptable_python_fn()\r\n```\r\n\r\nNOTE: This is also a change to behavior of the `@torch.jit.ignore` decorator. In version 1.1, `@ignore` tells the compiler to omit compiling a function entirely, to mark Python functions that you know will not be called after export. In version 1.2 `@ignore`, tells the compiler to insert a call back to the Python interpreter instead of trying to compile the function.\r\n\r\nTo get the old behavior, use `@torch.jit.ignore(drop_on_export=True)` (`@torch.jit.ignore` with no arguments is equivalent to `@torch.jit.ignore(drop_on_export=False`)).\r\n\r\n### [JIT] `optimize` for ScriptModules is now a context manager\r\n\r\nWhether optimization passes are run is now a thread-local flag. This better reflects how optimization actually happens in the JIT (i.e. it is decided at runtime, not compilation time). \r\n\r\n*Version 1.1*\r\n\r\n```\r\n@torch.jit.script(optimize=False)\r\ndef fn(inputs):\r\n    ...\r\n\r\nfn(inputs)\r\n```\r\n\r\n*Version 1.2*\r\n\r\n```\r\n@torch.jit.script\r\ndef fn(inputs):\r\n    ...\r\n\r\nwith @torch.jit.optimized_execution(False):\r\n    fn(inputs)\r\n```\r\n\r\n### [jit] `script::Module` is now a reference type\r\n\r\nTo better align with the [PyTorch C++ API philosophy](https://github.com/pytorch/pytorch/wiki/Writing-Python-in-cpp-(a-manifesto)), `script::Module` and `script::Method` are now reference types. Our APIs have been updated to use `script::Module` instead of `std::shared_ptr<script::Module>`.\r\n\r\n*Version 1.1*\r\n\r\n```\r\nusing torch::jit::script::Module;\r\n\r\nstd::shared_ptr<Module> m = torch::jit::load(\"my_model.py\");\r\nm->forward(...);\r\n```\r\n\r\n*Version 1.2*\r\n\r\n```\r\nusing torch::jit::script::Module;\r\n\r\nModule m = torch::jit::load(\"my_model.py\");\r\nm.forward(...);\r\n```\r\n\r\n### [C++ only] mean() / sum() / prod() APIs have changed slightly ([21088](https://github.com/pytorch/pytorch/pull/21088))\r\n\r\n*Version 1.1 API*:\r\n\r\n```\r\nTensor sum(IntArrayRef dim, bool keepdim=false) const;    \r\nTensor sum(IntArrayRef dim, ScalarType dtype) const;\r\n```\r\n\r\n*Version 1.2 API*:\r\n\r\n```\r\nTensor sum(IntArrayRef dim, bool keepdim=false,\r\n           c10::optional<ScalarType> dtype=c10::nullopt) const;\r\n```\r\n\r\nthat is, to override `dtype`, `keepdim` must now be provided.\r\n\r\n### Binary distribution and nightly changes\r\n\r\nWe have streamlined our conda and wheel binary distributions, so that it is easier than ever to install the version of PyTorch appropriate for your needs. The install instructions on https://pytorch.org/ have been updated, but if you have tooling to download and install PyTorch, here is a detailed description of the changes we made:\r\n\r\n**Wheels now have local version identifiers.** Wheels that are for non-default CUDA configurations (the default CUDA version for this release is 10.0) now have local version identifiers like +cpu and +cu92. This means that, when installing, it is no longer necessary to specify a full wheel URL\u2014just specify an appropriate version constraint like `torch==1.2.0+cu92`.\r\n\r\n*Version 1.1 (for Python 3.7 on Linux only):*\r\n\r\n```\r\npip install numpy\r\npip install https://download.pytorch.org/whl/cpu/torch-1.1.0-cp37-cp37m-linux_x86_64.whl\r\n```\r\n\r\n*Version 1.2 (works for all versions of Python, and both Linux and Mac):*\r\n\r\n```\r\npip install torch==1.2.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\r\n```\r\n\r\n**CPU-only binaries on conda can be selected with the cpuonly feature.** We\u2019ve eliminated the pytorch-cpu conda package; instead, the cpu-only conda package can be enabled by installing the cpuonly metapackage. Similarly, there is no longer both a torchvision and torchvision-cpu package; the feature will ensure that the CPU version of torchvision is selected.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\nconda install -c pytorch pytorch-cpu\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\nconda install -c pytorch pytorch cpuonly\r\n```\r\n\r\n**Conda nightlies now live in the pytorch-nightly channel and no longer have \u201c-nightly\u201d in their name.** We have added a new dedicated channel for nightlies called pytorch-nightly; all nightlies (pytorch, torchvision, torchaudio, etc.) will now be uploaded to this channel, but with the same name as their corresponding stable versions (unlike before, when we had a separate pytorch-nightly, torchvision-nightly, etc. packages.) This makes it more difficult to accidentally install a copy of the nightly and stable at the same time.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\nconda install -c pytorch pytorch-nightly\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\nconda install -c pytorch-nightly pytorch\r\n```\r\n\r\n**Wheel nightlies no longer have -nightly in their name.** Similar to the changes we made in Conda, we no longer suffix wheel nightlies with \u201c-nightly\u201d, to make it harder to accidentally install a copy of nightly and stable at the same time.\r\n\r\n*Version 1.1:*\r\n\r\n```\r\npip install --pre torch_nightly -f https://download.pytorch.org/whl/nightly/torch_nightly.html\r\n```\r\n\r\n*Version 1.2:*\r\n\r\n```\r\npip install --pre torch -f https://download.pytorch.org/whl/nightly/torch_nightly.html\r\n```\r\n\r\n## New Features\r\n\r\n### Tensor Type Support\r\n\r\n* `torch.bool`: added support for many operators (masking, comparison, arithmetic operators) to achieve feature parity with `torch.uint8`.  See the **Breaking Changes** section for details about how this could affect existing programs. ([21032](https://github.com/pytorch/pytorch/pull/21032), etc.)\r\n* `torch.sparse.HalfTensor`: Added support for `torch.float16` sparse Tensors on both CPU and CUDA.  ([19695](https://github.com/pytorch/pytorch/pull/19695))\r\n* `torch.bfloat16`: Added basic creation and serialization support for [Brain Floating Point Tensors](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format). ([21522](https://github.com/pytorch/pytorch/pull/21522), [21523](https://github.com/pytorch/pytorch/pull/21523), [21860](https://github.com/pytorch/pytorch/pull/21860), [22852](https://github.com/pytorch/pytorch/pull/22852))\r\n\r\n### NN Package\r\n\r\n* `nn.Transformer`: added implementation of Transformer from [Attention is All You Need](https://arxiv.org/abs/1706.03762). ([20170](https://github.com/pytorch/pytorch/pull/20170), [22588](https://github.com/pytorch/pytorch/pull/22588))\r\n* `nn.Embedding`: support `float16` embeddings on CUDA.  ([19695](https://github.com/pytorch/pytorch/pull/19695))\r\n* `nn.Flatten`: added a Module that performs `torch.flatten`. ([22245](https://github.com/pytorch/pytorch/pull/22245/))\r\n* `nn.functional.gelu`: Added support for [Gaussian Error Linear Units](https://arxiv.org/abs/1606.08415). ([20665](https://github.com/pytorch/pytorch/pull/20665), [21237](https://github.com/pytorch/pytorch/pull/21237))\r\n* `nn.Module hooks`: add ability to replace input/output via `forward_pre_hook` and `forward_hook`. ([22285](https://github.com/pytorch/pytorch/pull/22285))\r\n* `nn.Module`: add `requires_grad_() `method for turning on/off `requires_grad` for Module parameters. ([22576](https://github.com/pytorch/pytorch/pull/22576))\r\n\r\n### Operators\r\n\r\n* `Tensor.to_sparse`: now supports autograd. ([20458](https://github.com/pytorch/pytorch/pull/20458))\r\n* `Tensor.fill_diagonal_`: operator to fill the main diagonal of a Tensor. ([21892](https://github.com/pytorch/pytorch/pull/21892))\r\n* `torch.qr`: supports autograd. ([21274](https://github.com/pytorch/pytorch/pull/21274))\r\n* `torch.bitwise_not`: add operator for boolean/integer types.  Also have python `~` operator use this. ([22283](https://github.com/pytorch/pytorch/pull/22283), [22320](https://github.com/pytorch/pytorch/pull/22320))\r\n* `torch.trapz`: integrate using the trapezoid rule; equivalent to [numpy.trapz](https://docs.scipy.org/doc/numpy/reference/generated/numpy.trapz.html). ([21610](https://github.com/pytorch/pytorch/pull/21610))\r\n* `torch.var_mean` / `torch.std_mean`: compute variance and mean at the same time.([18731](https://github.com/pytorch/pytorch/pull/18731))\r\n* `torch.utils.ThroughputBenchmark`: benchmark utility for measuring the throughput of PyTorch operators. ([20766](https://github.com/pytorch/pytorch/pull/20766)).\r\n* `Logging`: lightweight at-most-once logging to record operators that are used (`c10::Logging`). ([20745](https://github.com/pytorch/pytorch/pull/20745))\r\n\r\n### Optim Package\r\n\r\n* `optim.AdamW`: introduce AdamW optimizer from [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101). ([21250](https://github.com/pytorch/pytorch/pull/21250))\r\n* `optim.LBFGS`: added support for strong Wolfe line search. ([8824](https://github.com/pytorch/pytorch/pull/8824))\r\n\r\n### Distributed Package\r\n\r\n* `DistributedDataParallel`: support CPU modules.  ([20236](https://github.com/pytorch/pytorch/pull/20236))\r\n* `DistributedDataParallel`: support sparse tensors. ([19146](https://github.com/pytorch/pytorch/pull/19146))\r\n* `DistributedDataParallel`: support local gradient accumulation. ([21736](https://github.com/pytorch/pytorch/pull/21736))\r\n\r\n### IterableDataset\r\n\r\n* `IterableDataset`: introduces a new type of Dataset designed for data read from a stream. ([19228](https://github.com/pytorch/pytorch/pull/19228))\r\n\r\n### Tensorboard Package\r\n\r\n* TensorBoard support in PyTorch has improved and is no longer experimental!\r\n* `SummaryWriter.flush`: now supported. ([20607](https://github.com/pytorch/pytorch/pull/20607))\r\n* `SummaryWriter.add_mesh`: add support for 3D point clouds. ([20413](https://github.com/pytorch/pytorch/pull/20413))\r\n\r\n### JIT Features\r\n\r\n* Improved support for iterator infrastructure. TorchScript now supports looping through a `List`, `Tuple`, `Dict`, `Tensor`, `String` and you can also use `zip()`, `enumerate()`, and `for...in`. ([21801](https://github.com/pytorch/pytorch/pull/21801), [22006](https://github.com/pytorch/pytorch/pull/22006), [21990](https://github.com/pytorch/pytorch/pull/21990), [21985](https://github.com/pytorch/pytorch/pull/21985))\r\n* Support `in` membership checks. ([21527](https://github.com/pytorch/pytorch/pull/21527))\r\n* Improved support for strings and the string libraries. ([20826](https://github.com/pytorch/pytorch/pull/20826), [20188](https://github.com/pytorch/pytorch/pull/20188), [20761](https://github.com/pytorch/pytorch/pull/20761), [21656](https://github.com/pytorch/pytorch/pull/21656), [20617](https://github.com/pytorch/pytorch/pull/20617))\r\n* Improved `math` support. ([20979](https://github.com/pytorch/pytorch/pull/20979), [19707](https://github.com/pytorch/pytorch/pull/19707), [21151](https://github.com/pytorch/pytorch/pull/21151), [21131](https://github.com/pytorch/pytorch/pull/21131), [21129](https://github.com/pytorch/pytorch/pull/21129), [21130](https://github.com/pytorch/pytorch/pull/21130), [21512](https://github.com/pytorch/pytorch/pull/21512), [21126](https://github.com/pytorch/pytorch/pull/21126), [21127](https://github.com/pytorch/pytorch/pull/21127), [21128](https://github.com/pytorch/pytorch/pull/21128))\r\n* Support for various other Python builtin functions. ([21451](https://github.com/pytorch/pytorch/pull/21451))\r\n* Support for `NamedTuple`. ([21428](https://github.com/pytorch/pytorch/pull/21428))\r\n* All the rest of the `dict` methods. ([21979](https://github.com/pytorch/pytorch/pull/21979))\r\n* `sorted()` keyword for lists and dicts. ([23274](https://github.com/pytorch/pytorch/pull/23274))\r\n* Add support for breaks and continues. ([21692](https://github.com/pytorch/pytorch/pull/21692))\r\n* Improved custom operator API with several bugfixes and new features. It now allows more primitive types, supports `torch::List`, `torch::Dict` and `torch::Optional`, supports dispatch (i.e. registering a different function for CPU and CUDA for the same operator).\r\n* Support `nn.GRU` in script. ([23266](https://github.com/pytorch/pytorch/pull/23266))\r\n* Support `pack_padded_sequence` and `pad_packed_sequence`. ([23249](https://github.com/pytorch/pytorch/pull/23249))\r\n* Support `torch._C._get_tracing_state` in TorchScript. ([23248](https://github.com/pytorch/pytorch/pull/23248))\r\n* Support `torch.as_tensor` in TorchScript. ([23247](https://github.com/pytorch/pytorch/pull/23247))\r\n* add support for recursive compilation on `Modules`. ([20708](https://github.com/pytorch/pytorch/pull/20708))\r\n* add `all` builtin. ([20521](https://github.com/pytorch/pytorch/pull/20521))\r\n* Add `Final[T]` annotated members to `__constants__`. ([21603](https://github.com/pytorch/pytorch/pull/21603))\r\n* Add `save()` to scripted `Function`s. ([20386](https://github.com/pytorch/pytorch/pull/20386))\r\n* Support for serializing class attributes. ([22953](https://github.com/pytorch/pytorch/pull/22953))\r\n* Support for class annotations. ([21379](https://github.com/pytorch/pytorch/pull/21379))\r\n* support Python 3.8 `Constant` node. ([22007](https://github.com/pytorch/pytorch/pull/22007))\r\n* Support for type annotations instead of `torch.jit.annotate()`. ([21390](https://github.com/pytorch/pytorch/pull/21390))\r\n* Support operator overloading for user-defined classes. ([20033](https://github.com/pytorch/pytorch/pull/20033))\r\n* Support recursive `ModuleList` / `Sequential`. ([21306](https://github.com/pytorch/pytorch/pull/21306))\r\n* Trace multiple methods in a single `Module`. ([19905](https://github.com/pytorch/pytorch/pull/19905))\r\n\r\n## Improvements\r\n\r\n* `Tensor.pin_memory()`: only ask for context on current device. ([22229](https://github.com/pytorch/pytorch/pull/22229))\r\n* `Tensor.view()`: suggest using `reshape()` instead of `contiguous()` when the input is non-contiguous. ([20968](https://github.com/pytorch/pytorch/pull/20968))\r\n* `Tensor.numpy()`: throw `TypeError` instead of `ValueError` if the type isn\u2019t supported. ([21608](https://github.com/pytorch/pytorch/pull/21608))\r\n* `torch.norm`: add support for `p=\"nuc\"` with `dim` specified. ([21022](https://github.com/pytorch/pytorch/pull/21022))\r\n* `torch.qr`: support batching of input matrices. ([20689](https://github.com/pytorch/pytorch/pull/20689))\r\n* `torch.qr`: support `some` parameter akin to NumPy's `mode` option. ([20689](https://github.com/pytorch/pytorch/pull/20689))\r\n* `torch.det` / `torch.logdet` / `torch.slogdet`: added batching support. ([22909](https://github.com/pytorch/pytorch/pull/22909))\r\n* `torch.cdist`: support batching. ([20934](https://github.com/pytorch/pytorch/pull/20934))\r\n* `torch.symeig`: support batching. ([21858](https://github.com/pytorch/pytorch/pull/21858))\r\n* `torch._dirichlet_grad`: support CUDA. ([21191](https://github.com/pytorch/pytorch/pull/21191))\r\n* `torch.randperm`: support `torch.float16`. ([22102](https://github.com/pytorch/pytorch/pull/22102))\r\n* `torch.Size` is now pickle-able in Python2. ([20952](https://github.com/pytorch/pytorch/pull/20952))\r\n* `torch.tensor` / `torch.as_tensor`: infer device if input supports Numba\u2019s [`__cuda_array_interface__`](https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html). ([20584](https://github.com/pytorch/pytorch/pull/20584))\r\n* `torch.isinf` / `torch.isfinite`: throw `TypeError` instead of `ValueError` when a non-tensor is passed in. ([20817](https://github.com/pytorch/pytorch/pull/20817))\r\n* `nn.MultiheadedAttention`: add functional support. ([20415](https://github.com/pytorch/pytorch/pull/20415))\r\n* `nn.MultiheadedAttention`: added support for key/value to have different number of features. ([21288](https://github.com/pytorch/pytorch/pull/21288))\r\n* `nn.MultiheadAttention`: allow static key/values. ([21288](https://github.com/pytorch/pytorch/pull/21288))\r\n* `nn.Conv{1,2,3}D`: support `torch.int64` dtype in forward. ([20730](https://github.com/pytorch/pytorch/pull/20730), [22594](https://github.com/pytorch/pytorch/pull/22594))\r\n* `nn.AvgPool{1,2,3}D`: support `torch.int64` dtype in forward. ([22433](https://github.com/pytorch/pytorch/pull/22433))\r\n* `nn.Module`: make `_save_to_state_dict` overrideable. ([21933](https://github.com/pytorch/pytorch/pull/21933))\r\n* `autograd`: Checkpointing of modules inside large fanout networks no longer hits a recursion error. ([22397](https://github.com/pytorch/pytorch/pull/22397))\r\n* `autograd`: Track in-pace changes of Tensors through `Module._apply` (internal API). ([21865](https://github.com/pytorch/pytorch/pull/21865))\r\n* `autograd.profiler`: Add shape aggregation support.  [20035](https://github.com/pytorch/pytorch/pull/20035))\r\n* `autograd.profiler`: Profile custom c10 ops. ([20175](https://github.com/pytorch/pytorch/pull/20175))\r\n* `DataLoader`: support setting `batch_size=0` to disable automatic batching (collation) in `DataLoader` for easier bulk loading.  ([19228](https://github.com/pytorch/pytorch/pull/19228))\r\n* `DataLoader`: add `multiprocessing_context` parameter. ([22990](https://github.com/pytorch/pytorch/pull/22990))\r\n* `DataLoader`: added error detection for `worker_init_fn`. ([20150](https://github.com/pytorch/pytorch/pull/20150))\r\n* `DataLoader`: Retry on `EINTR`. ([21723](https://github.com/pytorch/pytorch/pull/21723))\r\n* `torch.cuda.set_rng_state` / `torch.cuda.get_rng_state`: accept string as `device` parameter. ([23448](https://github.com/pytorch/pytorch/pull/23448))\r\n* `CUDA`: add warning when using Turing GPUs and CUDA <= 9000. ([21468](https://github.com/pytorch/pytorch/pull/21468))\r\n* `CUDA`: warn on conditions that can trigger a cuBLAS 9.0 bug.  ([22034](https://github.com/pytorch/pytorch/pull/22034))\r\n* `CPU`: Improve CPUAllocator OOM message. ([20618](https://github.com/pytorch/pytorch/pull/20618))\r\n* `[memory_format]`: added support for `torch.empty`, `torch.empty_like`, `Tensor.contiguous()`, `Tensor.is_contiguous()` to specify / check the order in which dimensions are laid out in memory. ([20455](https://github.com/pytorch/pytorch/pull/20455), [20558](https://github.com/pytorch/pytorch/pull/20558))\r\n* `distributions.MultivariateNormal`: fix precision matrix instability. ([21366](https://github.com/pytorch/pytorch/pull/21366))\r\n* `distributions.transforms.SigmoidTransform`: fix numerical instability. ([19802](https://github.com/pytorch/pytorch/pull/19802))\r\n\r\n### Distributed Improvements\r\n\r\n* `DistributedDataParallel`: Support DDP forward/backward calls even if no module parameter is used. ([19821](https://github.com/pytorch/pytorch/pull/19821)) \r\n* `DistributedDataParallel`: Only call into reducer if grad is enabled. ([19897](https://github.com/pytorch/pytorch/pull/19897))\r\n* `DistributedDataParallel`:  Require finalize DDP backward only when there are indeed gradients computed, this allows application to completely discard DDP outputs and move on to the next iteration. ([19901](https://github.com/pytorch/pytorch/pull/19901))\r\n* `DistributedDataParallel`: Improve DDP backward reduction error messages. ([20586](https://github.com/pytorch/pytorch/pull/20586))\r\n* `DistributedDataParallel`:  make DDP failure recoverable. ([21591](https://github.com/pytorch/pytorch/pull/21591))\r\n* `DistributedDataParallel`:  Delay reduction of unused parameters until first autograd hook is called. ([22219](https://github.com/pytorch/pytorch/pull/22219))\r\n* `c10d:` support tensors shared across processes. ([21449](https://github.com/pytorch/pytorch/pull/21449))\r\n* `c10d:` `ProcessGroupMPI` Add device guard around MPI operations. ([22446](https://github.com/pytorch/pytorch/pull/22446))\r\n* `utils.data.distributed.DistributedSampler`: Make shuffling optional. ([22479](https://github.com/pytorch/pytorch/pull/22479))\r\n\r\n### Tensorboard Improvements\r\n\r\n* Usage of kwarg-only arguments has been removed. ([21786](https://github.com/pytorch/pytorch/pull/21786))  \r\n\r\n### Numpy Compatibility Improvements\r\n\r\n* `Tensor.T:` added numpy-like support for reversing dimensions. ([20598](https://github.com/pytorch/pytorch/pull/20598))\r\n* `Tensor.ndim`: NumPy equivalent property for the number of dimensions. ([20565](https://github.com/pytorch/pytorch/pull/20565))\r\n* `Tensor.nonzero`: added `as_tuple` argument (default `False`) that when `True`, will return a tuple of Tensors, which matches the behavior of [numpy.nonzero](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html). ([20293](https://github.com/pytorch/pytorch/pull/20293))\r\n* `torch.dtype`: support passing in NumPy dtypes as arguments. ([21215](https://github.com/pytorch/pytorch/pull/21215))\r\n* `torch.normal`: add `size` parameter when called with two floats. ([20545](https://github.com/pytorch/pytorch/pull/20545))\r\n* `torch.where`: add one-argument overload that is an alias for Numpy-like `nonzero`. ([21986](https://github.com/pytorch/pytorch/pull/21986))\r\n* support a number of argument name overrides, e.g. `axis` instead of `dim`. ([20451](https://github.com/pytorch/pytorch/pull/20451))\r\n\r\n### JIT Improvements\r\n\r\n* The original source code debug information is now saved with the model. If a model is saved and then loaded into another process, the loaded process can now print out error messages that point to the original source code. ([22177](https://github.com/pytorch/pytorch/pull/22177), [22178](https://github.com/pytorch/pytorch/pull/22178), [22179](https://github.com/pytorch/pytorch/pull/22179), [22180](https://github.com/pytorch/pytorch/pull/22180))\r\n* Error message source range highlighting now includes filename, line number, and column number. ([21157](https://github.com/pytorch/pytorch/pull/21157))\r\n* Better Constant Propagation through Tuples. ([22561](https://github.com/pytorch/pytorch/pull/22561))\r\n* Add `start` and `step` parameters for `range` in TorchScript. ([20795](https://github.com/pytorch/pytorch/pull/20795))\r\n* Support for threading options for TorchScript inference ([doc](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html))\r\n* Add `max_pool2d` to symbolic derivatives. ([19661](https://github.com/pytorch/pytorch/pull/19661))\r\n* Optimize `matmul` memory usage for certain cases. ([23433](https://github.com/pytorch/pytorch/pull/23433))\r\n* Avoid kernel launches for zero-sized tensor inputs. ([22790](https://github.com/pytorch/pytorch/pull/22790))\r\n* Add support for steps (strides) in tensor slices. ([20929](https://github.com/pytorch/pytorch/pull/20929))\r\n* Added error for classes that don't have an `__init__` function. ([21880](https://github.com/pytorch/pytorch/pull/21880))\r\n* Allow classes to be used in their own methods. ([20106](https://github.com/pytorch/pytorch/pull/20106))\r\n* Better error message when a variable is conditionally defined. ([20911](https://github.com/pytorch/pytorch/pull/20911))\r\n* Consider contained types in alias analysis. ([21431](https://github.com/pytorch/pytorch/pull/21431))\r\n* Convenience APIs for script objects. ([20226](https://github.com/pytorch/pytorch/pull/20226))\r\n* Don't print backtrace for interpreter errors. ([20925](https://github.com/pytorch/pytorch/pull/20925))\r\n* Improve error msg for missing attribute. ([20779](https://github.com/pytorch/pytorch/pull/20779))\r\n* Improve error msg on inferred type. ([21058](https://github.com/pytorch/pytorch/pull/21058))\r\n* Improve error msg on recursive class defs. ([21842](https://github.com/pytorch/pytorch/pull/21842))\r\n* Include module names in recursive error stacks. ([22921](https://github.com/pytorch/pytorch/pull/22921))\r\n* Improve recursive scripting error message. ([21841](https://github.com/pytorch/pytorch/pull/21841))\r\n* Index into a tuple with non constant integer. ([20081](https://github.com/pytorch/pytorch/pull/20081))\r\n* Let `ScriptModule` buffer attributes can also cast device/type. ([19700](https://github.com/pytorch/pytorch/pull/19700))\r\n* Lower batchmm to non-diff optimization. ([19987](https://github.com/pytorch/pytorch/pull/19987))\r\n* Make `ScriptModule.training` an attribute instead of a parameter. ([21078](https://github.com/pytorch/pytorch/pull/21078))\r\n* Make `strtod_c` compatible with different gcc abi. ([21293](https://github.com/pytorch/pytorch/pull/21293))\r\n* make magic methods work with casts too. ([20654](https://github.com/pytorch/pytorch/pull/20654))\r\n* Improve performance of alias analysis. ([20899](https://github.com/pytorch/pytorch/pull/20899))\r\n* Print a warning if a type annotation prefix is invalid according to mypy. ([20884](https://github.com/pytorch/pytorch/pull/20884))\r\n* schema_matching.cpp: improve error messages. ([21141](https://github.com/pytorch/pytorch/pull/21141))\r\n* Resolve with closed over variables instead of stack frame. ([22270](https://github.com/pytorch/pytorch/pull/22270))\r\n* Report errors through call stack. ([22280](https://github.com/pytorch/pytorch/pull/22280))\r\n* Reduce number of stack manipulation instructions in interpreter. ([21240](https://github.com/pytorch/pytorch/pull/21240))\r\n\r\n### C++ API Improvements\r\n\r\n* `nn::PoissonNLLLoss`: Added support. ([19316](https://github.com/pytorch/pytorch/pull/19316))\r\n* `nn::Module`: added `replace_module` API to overwrite submodules in C++ Frontend. ([22546](https://github.com/pytorch/pytorch/pull/22546))\r\n* `nn:Module::register_module` / `register_parameter` / `register_buffer`: make public ([23196](https://github.com/pytorch/pytorch/pull/23196))\r\n* `data::datasets::ChunkDataReader`: fix include headers and a vector issue. ([19485](https://github.com/pytorch/pytorch/pull/19485))\r\n* `data::datasets::ChunkDataset`: add new `get_batch` method. ([21797](https://github.com/pytorch/pytorch/pull/21797))\r\n* `data::datasets::ChunkDataset`: add checkpoint support. ([21889](https://github.com/pytorch/pytorch/pull/21889))\r\n* `data::datasets::ChunkDataset`: add support for cross-chunk shuffling. ([22347](https://github.com/pytorch/pytorch/pull/22347))\r\n* `data::datasets::ChunkDataset`: add sorting policy. ([23053](https://github.com/pytorch/pytorch/pull/23053))\r\n\r\n### MKLDNN Tensor Improvements\r\n\r\nAdd support for a number of operators on MKLDNN Tensors including:\r\n* `Tensor.is_mkldnn`: ([22386](https://github.com/pytorch/pytorch/pull/22386))\r\n* `Tensor.transpose()`: ([21943](https://github.com/pytorch/pytorch/pull/21943))\r\n* `Tensor.zero_()`: ([20573](https://github.com/pytorch/pytorch/pull/20573))\r\n* `torch.empty`: ([21184](https://github.com/pytorch/pytorch/pull/21184))\r\n* `torch.mul`: ([20575](https://github.com/pytorch/pytorch/pull/20575))\r\n* `nn.AdaptiveAvgPool{1,2,3}D`: ([19818](https://github.com/pytorch/pytorch/pull/19818))\r\n* `nn.Sigmoid`: ([20820](https://github.com/pytorch/pytorch/pull/20820))\r\n* `nn.Softmax`: ([21516](https://github.com/pytorch/pytorch/pull/21516))\r\n* `nn.Module`: support saving/loading MKLDNN modules. ([20799](https://github.com/pytorch/pytorch/pull/20799))\r\n* `nn.MaxPool{1,2,3}D`: support `ceil_mode`. ([21310](https://github.com/pytorch/pytorch/pull/21310))\r\n\r\n## Bug Fixes\r\n\r\n* Indexing: fix advanced indexing where there are more than (2^31)-1 bytes in the output. ([20919](https://github.com/pytorch/pytorch/pull/20919))\r\n* Indexing: fix indexing when there are more than 65535 elements in a non-indexing first dimension on CUDA. ([23123](https://github.com/pytorch/pytorch/pull/23123))\r\n* Indexing: fix issue with slicing empty tensors. ([20914](https://github.com/pytorch/pytorch/pull/20914))\r\n* `Tensor.index_copy_:` fix segfault by properly checking dimension is in range. ([21617](https://github.com/pytorch/pytorch/pull/21617))\r\n* `Tensor.copy_`: Fix a bug where non-blocking was not being respected. ([20305](https://github.com/pytorch/pytorch/pull/20305))\r\n* `Tensor.clone`: Fix an issue with MKLDNN tensors. ([20943](https://github.com/pytorch/pytorch/pull/20943))\r\n* Tensor subclassing: give a proper error instead of crashing. ([20283](https://github.com/pytorch/pytorch/pull/20283))\r\n* `torch.cat`:  Fix segfault with tensors that can't be indexed with 32-bit ints. ([21530](https://github.com/pytorch/pytorch/pull/21530))\r\n* `torch.range` / `torch.linspace` / `torch.logspace`: properly respect the current `Stream`. ([21619](https://github.com/pytorch/pytorch/pull/21619))\r\n* `torch.lu`: return the identity permutation instead of zeros when not using pivoting. ([22242](https://github.com/pytorch/pytorch/pull/22242))\r\n* `torch.einsum`: Fix an issue where the backward pass would potentially be skipped. ([22111](https://github.com/pytorch/pytorch/pull/22111))\r\n* `torch.cosh`: Fix an issue where `torch.cos` was instead calculated with `torch.double` dtype and vectorized instructions. ([20797](https://github.com/pytorch/pytorch/pull/20797))\r\n* `torch.triu` / `torch.tril`: handle strides correctly for in-place versions. ([22730](https://github.com/pytorch/pytorch/pull/22730)).\r\n* `torch.triu` / `torch.tril`: Fix handling of batches > 65535 on CUDA. ([21067](https://github.com/pytorch/pytorch/pull/21067))\r\n* `torch.inverse` / `torch.solve` / `torch.cholesky_solve` /  `torch.triangular_solve`: Fix batch sizes > 65535 on CUDA. ([21689](https://github.com/pytorch/pytorch/pull/21689))\r\n* `torch.histc`: return `dtype` is now the same as the input tensor on CUDA, matching CPU behavior. ([20369](https://github.com/pytorch/pytorch/pull/20369))\r\n* `torch.histc`: properly return 1-dim tensor on CPU with 0-dim input and 1 bin. ([21497](https://github.com/pytorch/pytorch/pull/21497))\r\n* `torch.randperm`: handle non-contiguous `out` parameter. ([23043](https://github.com/pytorch/pytorch/pull/23043))\r\n* `torch.unique`: Fix empty tensor handling when `dim` is passed as an argument. ([19000](https://github.com/pytorch/pytorch/pull/19000))\r\n* `torch.min` / `torch.max`: properly error on empty tensor inputs, as with CPU tensors. ([19612](https://github.com/pytorch/pytorch/pull/19612)).\r\n* `CUDA`: fix launch parameters for reductions. ([22827](https://github.com/pytorch/pytorch/pull/22827)).\r\n* `torch.hub`: fix an issue with `find_module`. ([20782](https://github.com/pytorch/pytorch/pull/20782))\r\n* `autograd`: Fix a number of custom autograd `Function` corner cases by inverting the relationship between PyFunction and THPFunction. ([22983](https://github.com/pytorch/pytorch/pull/22983))\r\n* `autograd`: give \u201cTrying to backward through the graph a second time\" error instead of internal assert when the buffers are a list of Tensors (with indexing). ([21533](https://github.com/pytorch/pytorch/pull/21533))\r\n* `optim.lr_scheduler.CosineAnnealingLR`: rename from CosineAnnealingLr. ([23242](https://github.com/pytorch/pytorch/pull/23242))\r\n* `distributions.Binomial`: Fix overflow of `log_prob` when `logits` is large. ([20679](https://github.com/pytorch/pytorch/pull/20679))\r\n* `distributions.SigmoidTransform`: Fix numerical issues that could result in `inf` / `-inf` return values. ([20288](https://github.com/pytorch/pytorch/pull/20288))\r\n* `distributions.Categorical.sample`: fix a view bug. ([23328](https://github.com/pytorch/pytorch/pull/23328))\r\n* `CUDA`: Give proper error message for bad cuda forks. ([23322](https://github.com/pytorch/pytorch/pull/23322))\r\n* `pickle`: Fix Unpickling error when loading multiple objects from a file. ([20270](https://github.com/pytorch/pytorch/pull/20270))\r\n* `NCCL`: Fix race condition. ([23040](https://github.com/pytorch/pytorch/pull/23040))\r\n\r\n### torch.nn Bug Fixes\r\n\r\n* `nn.Conv{1,2,3}D`: fix memory leak on MKLDNN code path. ([22392](https://github.com/pytorch/pytorch/pull/22392))\r\n* `nn.Conv{1,2,3}D`: properly unpickle older pickled versions. ([21687](https://github.com/pytorch/pytorch/pull/21687))\r\n* `nn.CTCLoss`: fix backward on CUDA when 2d target tensor is larger than `max_target_length`. ([20971](https://github.com/pytorch/pytorch/pull/20971))\r\n* `nn.CTCLoss`: fix some numerical stability issues. ([21392](https://github.com/pytorch/pytorch/pull/21392))\r\n* `nn.CTCLoss`: disable buggy non-deterministic CudNN algorithm. ([22977](https://github.com/pytorch/pytorch/pull/22977))\r\n* `nn.CTCLoss`: fixed empty target handling. ([21910](https://github.com/pytorch/pytorch/pull/21910), [23298](https://github.com/pytorch/pytorch/pull/23298))\r\n* `nn.SyncBatchNorm`: fix syncing of running statistics when count size differs between GPUs. ([22248](https://github.com/pytorch/pytorch/pull/22248))\r\n* `nn.SyncBatchNorm`: retain `requires_grad` value when converting from `nn.BatchNorm`. ([22569](https://github.com/pytorch/pytorch/pull/22569))\r\n* `nn.SyncBatchNorm`: correctly handle `process_group` in `convert_sync_batchnorm`. ([19240](https://github.com/pytorch/pytorch/pull/19240))\r\n* `nn.MultiheadedAttention`: fix for `torch.float16` dtype. ([21658](https://github.com/pytorch/pytorch/pull/21658)).\r\n* `nn.EmbeddingBag`: fix NaN output when input is empty. ([21400](https://github.com/pytorch/pytorch/pull/21400))\r\n* `nn.Dropout`: fix python crash (with SIGFPE) when called on an empty cuda tensor. ([20541](https://github.com/pytorch/pytorch/pull/20541))\r\n* `nn.MaxPool`: fix output size calculation in some corner cases. ([22304](https://github.com/pytorch/pytorch/pull/22304))\r\n* `nn.MaxPool`: return valid indices if all entries are `-inf`. ([23161](https://github.com/pytorch/pytorch/pull/23161))\r\n* `nn.Softmax`: respect the current Stream. ([22470](https://github.com/pytorch/pytorch/pull/22470))\r\n* `nn.LogSoftmax`: fix numerical stability issues. ([21672](https://github.com/pytorch/pytorch/pull/21672))\r\n* `nn.Module.load_state_dict`: break ref cycle. ([20397](https://github.com/pytorch/pytorch/pull/20397))\r\n* `nn.Module`: fix loading in 32-bit environments. ([20900](https://github.com/pytorch/pytorch/pull/20900))\r\n* `nn.utils.rnn.pack_padded_sequence`: Fix segfault on empty tensors. ([21461](https://github.com/pytorch/pytorch/pull/21461))\r\n* `nn.utils.spectral_norm`: fix loading `state_dict` when `strict=False`. ([22545](https://github.com/pytorch/pytorch/pull/22545))\r\n* `CudNN`: Fix uninitialized PoolWindow on Windows. ([22405](https://github.com/pytorch/pytorch/pull/22405))\r\n\r\n### Distributed Bug fixes\r\n\r\n* `nn.parallel.DataParallel`: fix error in `no_grad` mode. ([21262](https://github.com/pytorch/pytorch/pull/21262))\r\n* `torch.distributed.all_gather`: fix errors for views and aliases. ([21490](https://github.com/pytorch/pytorch/pull/21490))\r\n* `c10d`: fix collective communication errors on empty tensors. ([20658](https://github.com/pytorch/pytorch/pull/20658))\r\n\r\n### JIT Bug Fixes\r\n\r\n* Fix specialized list from dict keys. ([23267](https://github.com/pytorch/pytorch/pull/23267))\r\n* Switch keys to be sequential and stable in pickle serialization. ([23280](https://github.com/pytorch/pytorch/pull/23280))\r\n* `deepCopy` also copies type information of lists, ([23271](https://github.com/pytorch/pytorch/pull/23271))\r\n* `dictKeys` and `dictItems` ops on typed dicts return typed lists. ([23270](https://github.com/pytorch/pytorch/pull/23270))\r\n* Fix pickler bug where it would not load if no tensors were saved. ([23263](https://github.com/pytorch/pytorch/pull/23263))\r\n* Avoid multiple writes to files on export. ([21186](https://github.com/pytorch/pytorch/pull/21186))\r\n* Better error msg for mismatched `dict` key type. ([22231](https://github.com/pytorch/pytorch/pull/22231))\r\n* Better error msg for using Python `builtin_function_or_method`. ([22935](https://github.com/pytorch/pytorch/pull/22935))\r\n* Better error msg in `__get_state__` to let a user know that ScriptModules can't be deep-copied at the moment.([20885](https://github.com/pytorch/pytorch/pull/20885))\r\n* Better error msg when seeing a unsupported builtin function. ([21068](https://github.com/pytorch/pytorch/pull/21068))\r\n* `dropout` derivative should respect the `train` flag. ([20760](https://github.com/pytorch/pytorch/pull/20760))\r\n* Fix `__constants__` for some nn modules. ([21071](https://github.com/pytorch/pytorch/pull/21071))\r\n* Fix `ScriptModule.__dir__()`. ([22426](https://github.com/pytorch/pytorch/pull/22426))\r\n* Fix 3x DenseNet compile time regression by restoring earlier-out tests in AliasDB::writesToAlias. ([21425](https://github.com/pytorch/pytorch/pull/21425))\r\n* Fix a bug in loop unrolling. ([21239](https://github.com/pytorch/pytorch/pull/21239))\r\n* Fix alias annotations for dict ops. ([22900](https://github.com/pytorch/pytorch/pull/22900))\r\n* Fix inaccurate SourceRange reporting. ([21109](https://github.com/pytorch/pytorch/pull/21109))\r\n* Fix broken indexing when using None and ellipses indexing together. ([22905](https://github.com/pytorch/pytorch/pull/22905))\r\n* Fix bug in `CompilationUnit::define`. ([21886](https://github.com/pytorch/pytorch/pull/21886))\r\n* Fix compilation order for class methods. ([20094](https://github.com/pytorch/pytorch/pull/20094))\r\n* Fix dead code elimination over loops. ([22632](https://github.com/pytorch/pytorch/pull/22632))\r\n* Fix dead code elimination in onnx export. ([22476](https://github.com/pytorch/pytorch/pull/22476))\r\n* Fix incorrect default on `Graph::toString`. ([21370](https://github.com/pytorch/pytorch/pull/21370))\r\n* Fix optional type promotion for classes. ([21593](https://github.com/pytorch/pytorch/pull/21593))\r\n* Fix optional type unification. ([19813](https://github.com/pytorch/pytorch/pull/19813))\r\n* Fix `NameError` with `PYTORCH_JIT=0`. ([20120](https://github.com/pytorch/pytorch/pull/20120))\r\n* Fix overspecializing constants in compilation. ([22816](https://github.com/pytorch/pytorch/pull/22816))\r\n* Fix `pow()` bug on overloads. ([20824](https://github.com/pytorch/pytorch/pull/20824))\r\n* Fix recusive method compilation. ([21862](https://github.com/pytorch/pytorch/pull/21862))\r\n* Fix reflection on weak modules, copy attributes. ([20190](https://github.com/pytorch/pytorch/pull/20190))\r\n* Fix slow unpickling. ([21542](https://github.com/pytorch/pytorch/pull/21542))\r\n* Fix input/output type mismatch. ([20829](https://github.com/pytorch/pytorch/pull/20829))\r\n* Fix insert_guard for norm decomposation. ([19646](https://github.com/pytorch/pytorch/pull/19646))\r\n* Fix Trace inlining of graphs with optional inputs. ([22686](https://github.com/pytorch/pytorch/pull/22686))\r\n* Fix tracing bugs where using `1 - x` in C++ would cause the size of 1 to get hardcoded. ([20932](https://github.com/pytorch/pytorch/pull/20932))\r\n* Fix tuple indexing bug. ([21521](https://github.com/pytorch/pytorch/pull/21521))\r\n* Fix type hints for `None` constants. ([23029](https://github.com/pytorch/pytorch/pull/23029))\r\n* Fix weak module cuda() `_flat_weights bug`. ([21107](https://github.com/pytorch/pytorch/pull/21107))\r\n* Fix `WeakIValueEq`. ([21891](https://github.com/pytorch/pytorch/pull/21891))\r\n* Fixed gcd to use 64 bit integers. ([21041](https://github.com/pytorch/pytorch/pull/21041))\r\n* Fixed `list()` not making a copy. ([22093](https://github.com/pytorch/pytorch/pull/22093))\r\n* Fix race condition on `Module::forward` method. ([21398](https://github.com/pytorch/pytorch/pull/21398))\r\n* Made `a += b` for lists do an in place add. ([21896](https://github.com/pytorch/pytorch/pull/21896))\r\n* Made `floor/ceil` return ints. ([21124](https://github.com/pytorch/pytorch/pull/21124))\r\n* Out-of-memory on GPU due to the \"weak_script\" decorators. ([20588](https://github.com/pytorch/pytorch/issues/20588))\r\n* Override print when python is present. ([21625](https://github.com/pytorch/pytorch/pull/21625))\r\n* Set `__file__` for `torch.ops`. ([21888](https://github.com/pytorch/pytorch/pull/21888))\r\n* Set correct list type in pybind_utils. ([23188](https://github.com/pytorch/pytorch/pull/23188))\r\n\r\n### C++ Frontend bug fixes\r\n\r\n* `nn::RNN`: Fix assertions in bidirectional RNN. ([22850](https://github.com/pytorch/pytorch/pull/22850)).\r\n* `nn::MaxPool ` / ` nn::AvgPool`: expand incomplete kernel size, as in Python. ([22073](https://github.com/pytorch/pytorch/pull/22073), [22075](https://github.com/pytorch/pytorch/pull/22075))\r\n* `Optim`: Fix memory leak when `weight_decay` is applied to `Adam`, `Adagrad`,  `RMSProp`. ([23125](https://github.com/pytorch/pytorch/pull/23125))\r\n* `Optim::SGD`: fix memory leak with weight_decay. ([23007](https://github.com/pytorch/pytorch/pull/23007))\r\n* `torch::autograd::Scatter` `/ torch::autograd::Gather`: Fix nullptr bug. ([20286](https://github.com/pytorch/pytorch/pull/20286))\r\n* `torch::nn::parallel::data_parallel`: fix gradient computation error. ([20910](https://github.com/pytorch/pytorch/pull/20910))\r\n* [C++ Extensions] Fix an issue when building multiple extensions in the same directory. ([20221](https://github.com/pytorch/pytorch/pull/20221))\r\n\r\n## Deprecations\r\n\r\n### **Masking via `torch.uint8` Tensors is now deprecated in favor of masking via `torch.bool` Tensors.**\r\n\r\nSee the **Breaking Changes** section for more details about `torch.bool` Tensors and comparison operators.\r\n\r\n**`torch.masked_select`, `torch.masked_fill`, `torch.masked_scatter` now expect `torch.bool` masks rather than `torch.uint8`.**\r\n\r\n```\r\n>>> a = torch.tensor([1, 2, 3])\r\n>>> b = torch.tensor([3, 1, 2])\r\n\r\n>>> a.masked_select(tensor([0, 1, 1], dtype=torch.uint8))\r\nUserWarning: masked_select received a mask with dtype torch.uint8,\r\nthis behavior is now deprecated, please use a mask with dtype torch.bool instead.\r\n\r\ntensor([2, 3])\r\n\r\n# instead use torch.bool\r\n>>> a.masked_select(tensor([False,  True,  True]))\r\ntensor([2, 3])\r\n```\r\n\r\n\r\n**Comparison operators with `out=` parameters now expect `torch.bool` dtype rather than `torch.uint8`.**\r\n\r\n```\r\n>>> a = torch.tensor([1, 2, 3])\r\n>>> b = torch.tensor([3, 1, 2])\r\n>>> res = torch.empty_like(a, dtype=torch.uint8)\r\n>>> torch.gt(a, b, out=res)\r\nUserWarning: torch.gt received 'out' parameter with dtype torch.uint8, this behavior\r\nis now deprecated, please use 'out' parameter with dtype torch.bool instead.\r\n\r\ntensor([0, 1, 1], dtype=torch.uint8)\r\n\r\n# instead use torch.bool\r\n>>> res = torch.empty_like(a, dtype=torch.bool)\r\n>>> torch.gt(a, b, out=res)\r\ntensor([False, True, True])\r\n```\r\n\r\n\r\n\r\n### Legacy `autograd.Function` (Function without static forward method) is now deprecated\r\n\r\n```\r\n>>> class MyLegacyFunction(Function):\r\n>>>     def forward(self, x):\r\n>>>         return x\r\n>>>\r\n>>>     def backward(self, grad_output):\r\n>>>         return grad_output\r\n>>>\r\n>>> MyLegacyFunction()(torch.randn((3,), requires_grad=True)\r\nUserWarning: Legacy autograd function with non-static forward method is deprecated\r\nand will be removed in 1.3. Please use new-style autograd function\r\nwith static forward method.\r\n\r\n# instead use new-style Autograd Function\r\n>>> class MyFunction(Function):\r\n>>>     @staticmethod\r\n>>>     def forward(ctx, x):\r\n>>>         return x\r\n>>>\r\n>>>     @staticmethod\r\n>>>     def backward(ctx, grad_output):\r\n>>>         return grad_output\r\n>>>\r\n>>> MyFunction.apply(torch.randn((3,), requires_grad=True)\r\n```\r\n\r\nSee the [torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) documentation for more details.\r\n\r\n### `torch.gels`: has been renamed to `torch.lstsq`; `torch.gels` will work for this release but is now deprecated.  ([23460](https://github.com/pytorch/pytorch/pull/23460))\r\n\r\n## Performance\r\n\r\n* Advanced Indexing: significantly improve performance of advanced indexing backward. ([20557](https://github.com/pytorch/pytorch/pull/20557))\r\n* `Tensor.copy_`: increase broadcasting CUDA copy performance by 25%. ([20685](https://github.com/pytorch/pytorch/pull/20685))\r\n* `torch.matmul`: Optimize the case A.ndim <= 2 && B.ndim >= 3, shows up to 15x speed up. ([20448](https://github.com/pytorch/pytorch/pull/20448))\r\n* `torch.bmm`: Improve performance by up to 3x for small cases on CPU by applying TensorAccessor. ([20266](https://github.com/pytorch/pytorch/pull/20266))\r\n* `torch.inverse`: Move workspace query and allocation outside loop to improve performance by up to 5x. ([20904](https://github.com/pytorch/pytorch/pull/20904))\r\n* `torch.topk`: Optimize CPU perf using parallel and partial sort, up to 6x improvement. ([22865](https://github.com/pytorch/pytorch/pull/22865))\r\n* `torch.cdist`: Improve CPU perf by up to 10x for some cases. ([20605](https://github.com/pytorch/pytorch/pull/20605))\r\n* `torch.normal`: Move `normal`, `normal_means`, `normal_stddevs`, and `normal_means_stddevs` to ATen, increasing performance by up to 3x. ([21287](https://github.com/pytorch/pytorch/pull/21287))\r\n* `torch.bernoulli`: Speedup bernoulli_scalar_cuda_kernel with grid-stride loop, increasing performance by up to 2x. ([21300](https://github.com/pytorch/pytorch/pull/21300))\r\n* `torch.coalesce`: Use `_sparse_coo_tensor_unsafe` in `coalesce` for up to 10x speedup. ([21214](https://github.com/pytorch/pytorch/pull/21214))\r\n* `torch.sinh` / `torch.cosh`: Parallelize and vectorize on CPU. ([21115](https://github.com/pytorch/pytorch/pull/21115))\r\n* `torch.lerp`: Vectorize on CPU. ([22038](https://github.com/pytorch/pytorch/pull/22038))\r\n* `torch.eye`: Parallelize on CPU. ([21077](https://github.com/pytorch/pytorch/pull/21077))\r\n* `torch.randperm`: Parallelize initialization in randperm on CPU. ([21529](https://github.com/pytorch/pytorch/pull/21529))\r\n* Vectorization: Don't split 256-bit AVX2 load/store intrinsics. ([20609](https://github.com/pytorch/pytorch/pull/20609)).\r\n\r\n### Torch.NN Performance Improvements\r\n\r\n* `nn.Softmax`: Add persistent CUDA kernels that increase performance 2-10x on small inputs. ([20827](https://github.com/pytorch/pytorch/pull/20827))\r\n* `nn.Embedding` / `nn.EmbeddingBag`: Optimize CUDA kernel, increasing performance up to 2.7x. ([22016](https://github.com/pytorch/pytorch/pull/22016))\r\n* `nn.Linear`: optimize BERT model perf by using mkldnn inner product. ([21851](https://github.com/pytorch/pytorch/pull/21851))\r\n* `nn.Conv{1,2,3}D`: improve perf for depthwise convolutions in `torch.float16` on Volta and Turing GPUs. ([22302](https://github.com/pytorch/pytorch/pull/22302))\r\n* `nn.RNN`: optimize on CPU by fusing matmul ops. ([22512](https://github.com/pytorch/pytorch/pull/22512))\r\n* `nn.Upsample`: a number of significant perf improvements on CUDA. ([21879](https://github.com/pytorch/pytorch/pull/21879), [21694](https://github.com/pytorch/pytorch/pull/21694)).\r\n* `nn.functional.layer_norm`: optimize a fast path for layer_norm, increasing perf by up to 4x on CPU. ([20345](https://github.com/pytorch/pytorch/pull/20345), [20883](https://github.com/pytorch/pytorch/pull/20883))\r\n* Use `mkldnn` inner product for `nn.Linear()` to improve BERT perf. ([21851](https://github.com/pytorch/pytorch/pull/21851)).\r\n\r\n## Documentation\r\n\r\n* `torch.bool`: doc the Boolean tensor type. ([21601](https://github.com/pytorch/pytorch/pull/21601))\r\n* `torch.as_strided`: add docs. ([22842](https://github.com/pytorch/pytorch/pull/22842))\r\n* `torch.empty_strided`: add docs. ([23740](https://github.com/pytorch/pytorch/pull/23740))\r\n* `torch.lerp`: clarify broadcasting requirements. ([23268](https://github.com/pytorch/pytorch/pull/23268))\r\n* `torch.enable_grad` / `torch.no_grad` / `torch.set_grad_enable`: clarify interaction between these features. ([23310](https://github.com/pytorch/pytorch/pull/23310))\r\n* `torch.autograd.grad_mode`: Document that no_grad is thread local. ([21755](https://github.com/pytorch/pytorch/pull/21755))\r\n* `torch.multiprocessing`: Explain refcounting of CUDA tensors. ([19904](https://github.com/pytorch/pytorch/pull/19904))\r\n* `torch.Tensor`: Add a warning about memory usage. ([20801](https://github.com/pytorch/pytorch/pull/20801))\r\n* `torch.utils.data.Dataloader`: Document RNG state consumption. ([22540](https://github.com/pytorch/pytorch/pull/22540))\r\n* `torch.optim.lr_scheduler.CyclicLR`: Clarify `base_momentum` and `max_momentum`. ([20880](https://github.com/pytorch/pytorch/pull/20880)).\r\n* Document production environment features. ([23010](https://github.com/pytorch/pytorch/pull/23010))\r\n* Add note about contributing recently released research. ([23513](https://github.com/pytorch/pytorch/pull/23513))\r\n* Clarify performance implications of deterministic mode. ([21337](https://github.com/pytorch/pytorch/pull/21337))\r\n* Update cuda pinned memory note to include `tensor.to`. ([20977](https://github.com/pytorch/pytorch/pull/20977))\r\n\r\n### Torch.NN Documentation\r\n\r\n* `nn.functional / nn.init`: Break up NN in docs so they load faster. ([21291](https://github.com/pytorch/pytorch/pull/21291))\r\n* `nn.functional.conv{1,2,3}d`: Remove `padding_mode`.  ([20891](https://github.com/pytorch/pytorch/pull/20891))\r\n* `nn.functional.upsample` / `nn.functional.interpolate`: add note about overshooting with `mode=\u2018bicubic\u2019`. ([23321](https://github.com/pytorch/pytorch/pull/23321))\r\n* `nn.init.zeros_` / `nn.init.ones_`: add documentation. ([23145](https://github.com/pytorch/pytorch/pull/23145))\r\n* `nn.MultiheadAttention`: Add documentation for `add_bias_kv`, `add_zero_attn`, and `attn_mask`. ([20071](https://github.com/pytorch/pytorch/pull/20071))\r\n* `nn.MultiheadAttention`: Fix documentation for attention mask shape. ([20850](https://github.com/pytorch/pytorch/pull/20850))\r\n* `nn.Softmax`: Fixed to specify dimension to prevent warning in 1.1.0. ([20310](https://github.com/pytorch/pytorch/pull/20310)*)*\r\n\r\n### Contributor Documentation\r\n\r\n* Updated web links on contribution_guide and governance documentation. ([21243](https://github.com/pytorch/pytorch/pull/21243))\r\n* Improve documentation for publishing hub models. ([21307](https://github.com/pytorch/pytorch/pull/21307))\r\n* Suggest a faster linker in the contributing guide. ([21334](https://github.com/pytorch/pytorch/pull/21334))\r\n* Add CUDA C++11 and profiling notes to the contribution guide. ([21386](https://github.com/pytorch/pytorch/pull/21386))\r\n\r\n### Build Documentation\r\n\r\n* Add magma for CUDA 10.1 to Windows docs. ([19914](https://github.com/pytorch/pytorch/pull/19914))\r\n* Improve build-from-source instructions. ([20088](https://github.com/pytorch/pytorch/pull/20088))\r\n* Add `ninja` to build instructions. ([20079](https://github.com/pytorch/pytorch/pull/20079))\r\n* Update libtorch build docs. ([21150](https://github.com/pytorch/pytorch/pull/21150))\r\n\r\n### TensorBoard Documentation\r\n\r\n* Tensorboard Documentation has been greatly improved!  Browse the latest version [here](https://pytorch.org/docs/stable/tensorboard.html).\r\n\r\n### Torch HUB Documentation\r\n\r\n* Improve docs for publishing hub models. ([21307](https://github.com/pytorch/pytorch/pull/21307))\r\n* Update docs of entry point in hub. ([21568](https://github.com/pytorch/pytorch/pull/21568))\r\n\r\n## ONNX\r\n\r\n\r\nIn PyTorch 1.2, we have added the full support for ONNX Opset 7, 8, 9 and 10 in ONNX exporter, and we have also enhanced the constant folding pass to support Opset 10. The export of ScriptModule has better support. Additionally, users now are able to register their own symbolic to export custom ops, and specify the dynamic dimensions of inputs during export.\r\n\r\n### Supporting More ONNX Opsets\r\n\r\n* Add basic supports for multiple ONNX Opsets and support for Opset 10. ([19294](https://github.com/pytorch/pytorch/pull/19294))\r\n* Support ONNX Opset 7 and 8 in PyTorch ONNX Exporter. ([22421](https://github.com/pytorch/pytorch/pull/22421), [20036](https://github.com/pytorch/pytorch/pull/20036))\r\n* Export `Dropout` for Opset 10. ([20710](https://github.com/pytorch/pytorch/pull/20710))\r\n* Export `Slice` and `Flip` for Opset 10. ([20533](https://github.com/pytorch/pytorch/pull/20533))\r\n* Export `Interpolate (Resize)` for Opset 10. ([21434](https://github.com/pytorch/pytorch/pull/21434))\r\n\r\n### Enhancing the Support for ScriptModule\r\n\r\n* Support multiple outputs in ScriptModule in ONNX Exporter. ([20256](https://github.com/pytorch/pytorch/pull/20256))\r\n* Support tensor factories in ScriptModule in ONNX Exporter. ([20255](https://github.com/pytorch/pytorch/pull/20255))\r\n* Support tuples as inputs and outputs in ScriptModule. ([20784](https://github.com/pytorch/pytorch/pull/20784))\r\n\r\n### Exporting More Torch Operators to ONNX\r\n\r\n* Export custom ops. ([21321](https://github.com/pytorch/pytorch/pull/21321))\r\n* Export `torch.arange `. ([22601](https://github.com/pytorch/pytorch/pull/22601))\r\n* Export `torch.masked_fill`. ([22521](https://github.com/pytorch/pytorch/pull/22521))\r\n* Export `torch.floor`, ` torch.ceil`, `torch.log2` and `prim::shape`. ([17895](https://github.com/pytorch/pytorch/pull/17895))\r\n* Export `torch._dim_arange`. ([20078](https://github.com/pytorch/pytorch/pull/20078))\r\n* Export `torch.randn_like`. ([20093](https://github.com/pytorch/pytorch/pull/20093))\r\n* Export `torch._standard_gamma`. ([20126](https://github.com/pytorch/pytorch/pull/20126))\r\n* Export `torch.topk`. ([21104](https://github.com/pytorch/pytorch/pull/21104))\r\n* Export `__ and__`, `__or__`. ([17894](https://github.com/pytorch/pytorch/pull/17894))\r\n* Export `torch.sign`. ([20470](https://github.com/pytorch/pytorch/pull/20470))\r\n* Export `torch.scatter`. ([18543](https://github.com/pytorch/pytorch/pull/18543))\r\n* Export `torch.rand`. ([20559](https://github.com/pytorch/pytorch/pull/20559))\r\n* Export `torch.gather`. ([21235](https://github.com/pytorch/pytorch/pull/21235))\r\n* Export `torch.cosine_similarity`. ([21884](https://github.com/pytorch/pytorch/pull/21884))\r\n* Export `torch.sum`. ([22240](https://github.com/pytorch/pytorch/pull/22240))\r\n* Export `torch.logsumexp`. ([22306](https://github.com/pytorch/pytorch/pull/22306))\r\n* Export `torch.layer_norm`. ([22265](https://github.com/pytorch/pytorch/pull/22265))\r\n\r\n### Extending Existing Exporting Logic\r\n\r\n* Support `torch.min` and `torch.max` with dim. ([19689](https://github.com/pytorch/pytorch/pull/19689))\r\n* Support `maxpool` with dilations. ([18721](https://github.com/pytorch/pytorch/pull/18721))\r\n* Support `RNN` with `batch_first=True`. ([19766](https://github.com/pytorch/pytorch/pull/19766))\r\n* Support `Upsample` with dynamic input. ([20116](https://github.com/pytorch/pytorch/pull/20116))\r\n* Improve support for Loop export. ([20445](https://github.com/pytorch/pytorch/pull/20445))\r\n* Enable `torch.full` with scalar parameters. ([21931](https://github.com/pytorch/pytorch/pull/21931))\r\n* Added support for exporting models with variable length input/output to ONNX. ([20034](https://github.com/pytorch/pytorch/pull/20034))\r\n\r\n### Optimizing Exported ONNX Graph\r\n\r\n* Support constant folding in Opset 10. ([22515](https://github.com/pytorch/pytorch/pull/22515))\r\n* Support negative indexing for `Slice` in constant folding optimization. ([21811](https://github.com/pytorch/pytorch/pull/21811))\r\n\r\n### Bugfixes/Improvements\r\n\r\n* Fix the shape of `PReLU` weight. ([21330](https://github.com/pytorch/pytorch/pull/21330))\r\n* Fix the export for `torch.pixel_shuffle`. ([21486](https://github.com/pytorch/pytorch/pull/21486))\r\n* Fix the export for `torch.full`. ([21669](https://github.com/pytorch/pytorch/pull/21669))\r\n* Update logic for folding `onnx::Constant` nodes. ([20109](https://github.com/pytorch/pytorch/pull/20109))\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.2.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.2.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.2.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/19168826", "dateCreated": "2019-08-08T12:54:09Z", "datePublished": "2019-08-08T16:06:38Z"}, {"tagName": "v1.1.0", "name": "Official TensorBoard Support, Attributes, Dicts, Lists and User-defined types in JIT / TorchScript, Improved Distributed", "authorName": "soumith", "authorType": "User", "body": "Note: CUDA 8.0 is no longer supported\r\n\r\n## Highlights\r\n\r\n### TensorBoard (currently experimental)\r\n\r\nFirst-class and native support for visualization and model debugging with [TensorBoard](https://www.tensorflow.org/tensorboard), a web application suite for inspecting and understanding training runs, tensors, and graphs. PyTorch now supports TensorBoard logging with a simple `from torch.utils.tensorboard import SummaryWriter` command. Histograms, embeddings, scalars, images, text, graphs, and more can be visualized across training runs. TensorBoard support is currently experimental. You can browse the docs [here](https://pytorch.org/docs/stable/tensorboard.html).\r\n\r\n![](https://github.com/gchanan/pytorch/raw/tensorboard_screenshot/Screen%20Shot%202019-04-25%20at%204.53.42%20PM.png)\r\n\r\n### [JIT] Attributes in ScriptModules\r\nAttributes can be assigned on a `ScriptModule` by wrapping them with `torch.jit.Attribute` and specifying the type. Attributes are similar to parameters or buffers, but can be of any type. They will be serialized along with any paramters/buffers when you call `torch.jit.save()`, so they are a great way to store arbitrary state in your model. See [the docs](https://pytorch.org/docs/master/jit.html#module-attributes) for more info.\r\n\r\nExample:\r\n```\r\nclass Foo(torch.jit.ScriptModule):\r\n  def __init__(self, a_dict):\r\n    super(Foo, self).__init__(False)\r\n    self.words = torch.jit.Attribute([], List[str])\r\n    self.some_dict = torch.jit.Attribute(a_dict, Dict[str, int])\r\n\r\n  @torch.jit.script_method\r\n  def forward(self, input: str) -> int:\r\n    self.words.append(input)\r\n    return self.some_dict[input]\r\n```\r\n\r\n### [JIT] Dictionary and List Support in TorchScript\r\nTorchScript now has robust support for list and dictionary types. They behave much like Python lists and dictionaries, supporting most built-in methods, as well as simple comprehensions and `for\u2026in` constructs. \r\n\r\n### [JIT] User-defined classes in TorchScript (experimental)\r\nFor more complex stateful operations, TorchScript now supports annotating a class with `@torch.jit.script`. Classes used this way can be JIT-compiled and loaded in C++ like other TorchScript modules. See [the docs](https://pytorch.org/docs/master/jit.html#user-defined-types) for more info.\r\n```\r\n@torch.jit.script\r\nclass Pair:\r\n\tdef __init__(self, first, second)\r\n\t\tself.first = first\r\n\t\tself.second = second\r\n\r\n\tdef sum(self):\r\n\t\treturn self.first + self.second\r\n```\r\n\r\n\r\n### DistributedDataParallel new functionality and tutorials\r\n\r\n`nn.parallel.DistributedDataParallel`: can now wrap multi-GPU modules, which enables use cases such as model parallel ([tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html)) on one server and data parallel ([tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)) across servers. \r\n ([19271](https://github.com/pytorch/pytorch/pull/19271)).\r\n\r\n## Breaking Changes\r\n* `Tensor.set_`: the `device` of a Tensor can no longer be changed via `Tensor.set_`.  This would most commonly happen when setting up a Tensor with the default CUDA device and later swapping in a `Storage` on a different CUDA device.  Instead, set up the Tensor on the correct device from the beginning.  ([18832](https://github.com/pytorch/pytorch/pull/18832)).\r\n* Pay attention to the order change of `lr_scheduler.step()`. ([7889](https://github.com/pytorch/pytorch/pull/7889)).\r\n* `torch.unique`: changed the default value of `sorted` to `True`.  ([15379](https://github.com/pytorch/pytorch/pull/15379)).\r\n* **[JIT]** Rename isTensor api -> isCompleteTensor. [#18437](https://github.com/pytorch/pytorch/pull/18437) \r\n* **[JIT]** Remove GraphExecutor's python bindings. [#19141](https://github.com/pytorch/pytorch/pull/19141) \r\n* **[C++]**: many methods on `Type` no longer exist; use the functional or Tensor method equivalent.  ([17991](https://github.com/pytorch/pytorch/pull/17991)).\r\n* **[C++]**: the `Backend` constructor of `TensorOptions` no longer exists.  ([18137](https://github.com/pytorch/pytorch/pull/18137)).\r\n* **[C++, Distributed]**: Remove c10d `ProcessGroup::getGroupRank` has been removed.  ([19147](https://github.com/pytorch/pytorch/pull/19147)).\r\n\r\n\r\n## New Features\r\n\r\n### Operators\r\n* `torch.tril_indices`, `torch.triu_indices`: added operator with same behavior as NumPy.  ([14904](https://github.com/pytorch/pytorch/pull/14904), [15203](https://github.com/pytorch/pytorch/pull/15203)).\r\n* `torch.combinations`, `torch.cartesian_prod`: added new `itertools`-like operators.  ([9393](https://github.com/pytorch/pytorch/pull/9393)).\r\n* `torch.repeat_interleave`: new operator similar to `numpy.repeat`.  ([18395](https://github.com/pytorch/pytorch/pull/18395)).\r\n* `torch.from_file`: new operator similar to `Storage.from_file`, but returning a tensor.  ([18688](https://github.com/pytorch/pytorch/pull/18688)).\r\n* `torch.unique_consecutive`: new operator with semantics similar to `std::unique` in C++.  ([19060](https://github.com/pytorch/pytorch/pull/19060)).\r\n* `torch.tril`, `torch.triu`, `torch.trtrs`: now support batching.  ([15257](https://github.com/pytorch/pytorch/pull/15257), [18025](https://github.com/pytorch/pytorch/pull/18025)).\r\n* `torch.gather`: add support for `sparse_grad` option.  ([17182](https://github.com/pytorch/pytorch/pull/17182)).\r\n* `torch.std`, `torch.max_values`, `torch.min_values`, `torch.logsumexp` can now operate over multiple dimensions at once.  ([14535](https://github.com/pytorch/pytorch/pull/14535), [15892](https://github.com/pytorch/pytorch/pull/15892), [16475](https://github.com/pytorch/pytorch/pull/16475)).\r\n* `torch.cdist`: added operator equivalent to `scipy.spatial.distance.cdist`.  ([16168](https://github.com/pytorch/pytorch/pull/16168), [17173](https://github.com/pytorch/pytorch/pull/17173)).\r\n* `torch.__config__.show()`: reports detailed version of all libraries.  ([18579](https://github.com/pytorch/pytorch/pull/18579)).\r\n\r\n### NN\r\n* `nn.MultiheadedAttention`: new module implementing MultiheadedAttention from `Attention Is All You Need`.  ([18334](https://github.com/pytorch/pytorch/pull/18334)).\r\n* `nn.functional.interpolate`: added support for `bicubic`.  ([9849](https://github.com/pytorch/pytorch/pull/9849)).\r\n* `nn.SyncBatchNorm`: support synchronous Batch Normalization.  ([14267](https://github.com/pytorch/pytorch/pull/14267)).\r\n* `nn.Conv`: added support for Circular Padding via `mode='circular'`.  ([17240](https://github.com/pytorch/pytorch/pull/17240)).\r\n* `nn.EmbeddingBag`: now supports trainable `per_sample_weights.  ([18799](https://github.com/pytorch/pytorch/pull/18799)).\r\n* `nn.EmbeddingBag`: add support for `from_pretrained` method, as in `nn.Embedding`.  ([15273](https://github.com/pytorch/pytorch/pull/15273)).\r\n* `RNNs`: automatically handle unsorted variable-length sequences via `enforce_sorted`.  ([15225](https://github.com/pytorch/pytorch/pull/15225)).\r\n* `nn.Identity`: new module for easier model surgery.  ([19249](https://github.com/pytorch/pytorch/pull/19249)).\r\n\r\n## Tensors / dtypes\r\n* `torch.bool`: added support for `torch.bool` dtype and Tensors with that dtype (1-byte storage).  NumPy conversion is supported, but operations are currently limited.  ([16810](https://github.com/pytorch/pytorch/pull/16810)).\r\n\r\n### Optim\r\n* `optim.lr_scheduler.CyclicLR`: Support for Cyclical Learning Rate and Momentum.  ([18001](https://github.com/pytorch/pytorch/pull/18001)).\r\n* `optim.lr_scheduler.CosineAnnealingWarmRestarts`: new scheduler: Stochastic Gradient Descent with Warm Restarts).  ([17226](https://github.com/pytorch/pytorch/pull/17226)).\r\n* Support multiple simultaneous LR schedulers.  ([14010](https://github.com/pytorch/pytorch/pull/14010))\r\n\r\n\r\n### Distributions\r\n* `torch.distributions`: now support multiple inheritance.  ([16772](https://github.com/pytorch/pytorch/pull/16772)).\r\n\r\n### Samplers\r\n* `quasirandom.SobolEngine`: new sampler.  ([10505](https://github.com/pytorch/pytorch/pull/10505)).\r\n\r\n### DistributedDataParallel\r\n* `nn.parallel.DistributedDataParallel`: now supports modules with unused parameters (e.g. control flow, like adaptive softmax, etc). ([18251](https://github.com/pytorch/pytorch/pull/18251), [18953](https://github.com/pytorch/pytorch/pull/18953)).\r\n\r\n### TorchScript and Tracer\r\n* Allow early returns from if-statements. ([#154463](https://github.com/pytorch/pytorch/pull/15463))\r\n* Add an `@ignore` annotation, which statically tells the TorchScript compiler to ignore the Python function. ([#16055](https://github.com/pytorch/pytorch/pull/16055))\r\n* Simple `for...in`  loops on lists. ([#16726](https://github.com/pytorch/pytorch/pull/16726))\r\n* Ellipses (`...`) in Tensor indexing. ([#17763](https://github.com/pytorch/pytorch/pull/17763))\r\n* `None` in Tensor indexing. ([#18615](https://github.com/pytorch/pytorch/pull/18615))\r\n* Support for basic list comprehensions. ([#17267](https://github.com/pytorch/pytorch/pull/17267))\r\n* Add implicit unwrapping of optionals on `if foo is not None`. ([#15587](https://github.com/pytorch/pytorch/pull/15587))\r\n* Tensors, ints, and floats will once again be implicitly cast to bool if used in a conditional. ([#18755](https://github.com/pytorch/pytorch/pull/18755)).\r\n* Implement `to()`, `cpu()`, and `cuda()` on ScriptModules. ([#15340](https://github.com/pytorch/pytorch/pull/15340) ,  [#15904](https://github.com/pytorch/pytorch/pull/15904))\r\n* Add support for various methods on lists: ([`clear()`](https://github.com/pytorch/pytorch/pull/17050), [`pop()`](https://github.com/pytorch/pytorch/pull/17015), [`reverse()`](https://github.com/pytorch/pytorch/pull/17001), [`copy()`](https://github.com/pytorch/pytorch/pull/17092) ,  [`extend()`](https://github.com/pytorch/pytorch/pull/17092),[`index()`](https://github.com/pytorch/pytorch/pull/17446), [`count()`](https://github.com/pytorch/pytorch/pull/17446), [`insert()`](https://github.com/pytorch/pytorch/pull/17200), [`remove()`](https://github.com/pytorch/pytorch/pull/17200) ).\r\n* Add support for `sort()` on lists of specialized type (`Tensors`, `int`, `float`, `bool`). ([#19572](https://github.com/pytorch/pytorch/pull/19572))\r\n* Add support for various methods on strings: ([`index()`](https://github.com/pytorch/pytorch/pull/18247), [`slice()`](https://github.com/pytorch/pytorch/pull/18247), [`len()`](https://github.com/pytorch/pytorch/pull/19320))\r\n* Support `Tensor.to()` in TorchScript. ( [#15976](https://github.com/pytorch/pytorch/pull/15976) )\r\n* Support for `Torch.tensor()` in TorchScript. ([#14913](https://github.com/pytorch/pytorch/pull/14913),  [#19445](https://github.com/pytorch/pytorch/pull/19445))\r\n* Support for `torch.manual_seed()` in TorchScript. ([#19510](https://github.com/pytorch/pytorch/pull/19510))\r\n* Support for `nn.LSTM` in TorchScript. ([#15744](https://github.com/pytorch/pytorch/pull/15744))\r\n* Support for `nn.init` in TorchScript. ([#19640](https://github.com/pytorch/pytorch/pull/19640))\r\n* Add `hash()` builtin. ([#18258](https://github.com/pytorch/pytorch/pull/18258))\r\n* Add `min()` and `max()` builtins for numerical types. ([#15680](https://github.com/pytorch/pytorch/pull/15680))\r\n* Add `isinstance()` builtin, which performs a static type check. ([#15076](https://github.com/pytorch/pytorch/pull/15076))\r\n* Add `train()` / `eval()` / `is_training()` to C++ ScriptModule API. ([#16044](https://github.com/pytorch/pytorch/pull/16044))\r\n* Allow List arguments to Python functions called from TorchScript. ([#15721](https://github.com/pytorch/pytorch/pull/19086))\r\n* Allow using `std::vector` and `std::unordered_map` as arguments to custom operators. ([#17587](https://github.com/pytorch/pytorch/pull/17587))\r\n* Tracer: now allows passing static dicts and lists as trace inputs. ([#18092](https://github.com/pytorch/pytorch/pull/18092), [#19580](https://github.com/pytorch/pytorch/pull/19580))\r\n* Allow generic containers as ScriptModule inputs. ([#16482](https://github.com/pytorch/pytorch/pull/16482))\r\n* Allow `nn.Sequential` in ModuleList. ([#16882](https://github.com/pytorch/pytorch/pull/16882))\r\n\r\n### Experimental Features\r\n* [Quantization] **(API unstable)**: added limited support for quantized datatypes via `torch.qint8` dtype, `torch.quantize_linear` conversion function.  ([18230](https://github.com/pytorch/pytorch/pull/18230)).\r\n* [MKLDNN tensor] **(API unstable)**: Added limited (opaque) support for `MKLDNN` tensors via `Tensor.to_mkldnn()`; operators are currently limited to ResNext101 operators.  ([17748](https://github.com/pytorch/pytorch/pull/17748)).\r\n\r\n## Improvements\r\n\r\n* `torch.min`, `torch.max`, `torch.median`, `torch.mode`, `torch.kthvalue`, `torch.symeig`, `torch.eig`, `torch.pstrf`, `torch.qr`, `torch.geqrf`, `torch.solve`, `torch.slogdet`, `torch.sort`, `torch.topk`, `torch.gels`, `torch.triangular_solve`, `torch.svd` now return namedtuples describing their outputs. ([16186](https://github.com/pytorch/pytorch/pull/16186), [16950](https://github.com/pytorch/pytorch/pull/16950), [17093](https://github.com/pytorch/pytorch/pull/17093), [17195](https://github.com/pytorch/pytorch/pull/17195), [15429](https://github.com/pytorch/pytorch/pull/15429)).\r\n* `torch.empty` (and other factory functions): now take a `pin_memory` kwarg; can now pin without going through `torch.Storage` interface..  ([18455](https://github.com/pytorch/pytorch/pull/18455)).\r\n* `torch.histc`: Now supported on CUDA.  ([15842](https://github.com/pytorch/pytorch/pull/15842))\r\n* `torch.unique`: Add `return_counts`.  ([18391](https://github.com/pytorch/pytorch/pull/18391), [18651](https://github.com/pytorch/pytorch/pull/18651)).\r\n* `torch.logspace`: add the ability to specify a `base`.  ([19542](https://github.com/pytorch/pytorch/pull/19542)).\r\n* `torch.set_printoptions`: added scientific notation support.  ([16876](https://github.com/pytorch/pytorch/pull/16876)).\r\n* `torch.btrifact` now handles tensors with greater than 3 dimensions.  ([14964](https://github.com/pytorch/pytorch/pull/14964)).\r\n* `torch.kthvalue`: now supported on CUDA.  ([17544](https://github.com/pytorch/pytorch/pull/17544)).\r\n* `torch.abs`: now supported on `uint8` and `int8` dtypes.  ([16893](https://github.com/pytorch/pytorch/pull/16893)).\r\n* `torch.stack`, `torch.cat`: now supported for CPU half tensors.  ([16389](https://github.com/pytorch/pytorch/pull/16389)).\r\n* `torch.cross`: added support for negative dimensions. ([17582](https://github.com/pytorch/pytorch/pull/17582)).\r\n* `torch.lerp`: add support for `weight` as a Tensor.  ([17348](https://github.com/pytorch/pytorch/pull/17348)).\r\n* `torch.transpose`: Made consistent with NumPy: 1-d and 0-d arrays are accepted and returned as-is.  ([17462](https://github.com/pytorch/pytorch/pull/17462), [17535](https://github.com/pytorch/pytorch/pull/17535)).\r\n* `torch.linspace`, `torch.logspace` can now be used with `steps=1` and `start != end`.  ([14748](https://github.com/pytorch/pytorch/pull/14748)).\r\n* `torch.cholesky`: changed the derivative from a triangular matrix to symmetric matrix.  ([19116](https://github.com/pytorch/pytorch/pull/19116)).\r\n* `torch.lerp`: Improved numerical stability.  ([18871](https://github.com/pytorch/pytorch/pull/18871)).\r\n* `torch.logdet`, `torch.slogdet`: improve numerical precision.  ([18449](https://github.com/pytorch/pytorch/pull/18449)).\r\n* `Tensor.__contains__` is now supported. ([17733](https://github.com/pytorch/pytorch/pull/17733)).\r\n* `Tensor.fill_` and `torch.zeros` now support half on CPU.  ([17536](https://github.com/pytorch/pytorch/pull/17536)).\r\n* `Tensor.resize_as_`, `Tensor.view`: now supported on half CPU tensors.  ([18821](https://github.com/pytorch/pytorch/pull/18821)).\r\n* `Tensor indexing`: allow indexing via NumPy booleans.  ([14932](https://github.com/pytorch/pytorch/pull/14932)).\r\n* `nn.EmbeddingBag`: enable half precision dense backward.  ([19293](https://github.com/pytorch/pytorch/pull/19293)).\r\n* `nn.Embedding`: fix dense Embedding to work with double backwards.  ([9078](https://github.com/pytorch/pytorch/pull/9078)).\r\n* `nn.MaxPool1d`: Allow list and tuples to be passed as `output_size`.  ([16489](https://github.com/pytorch/pytorch/pull/16489)).\r\n* `nn.CTCLoss`:  support zeroing infinite losses via `zero_infinity` argument.  ([16199](https://github.com/pytorch/pytorch/pull/16199)).\r\n* `nn.Dropout`: add support for enabling during eval.  ([17549](https://github.com/pytorch/pytorch/pull/17549)).\r\n* `nn.MSELoss`: add warning about unexpected broadcasting.  ([18349](https://github.com/pytorch/pytorch/pull/18349)).\r\n* `nn.Module.load_state_dict`: also return `missing_keys` and `unexpected_keys`.  ([18668](https://github.com/pytorch/pytorch/pull/18668)).\r\n* `nn.parallel.data_parallel`: Enforce devices match `device_ids`.  ([17129](https://github.com/pytorch/pytorch/pull/17129)).\r\n* `torch.device`: handle in more places that used to accept only device ordinals.  ([14929](https://github.com/pytorch/pytorch/pull/14929))\r\n* `dtype.int8` tensors can now be converted to NumPy arrays.  ([14710](https://github.com/pytorch/pytorch/pull/14710)).\r\n* `nn.functional.gumbel_softmax`: allow multidimensional input with `dim` argument.  ([13339](https://github.com/pytorch/pytorch/pull/13339)).\r\n* `nn.functional.cosine_similarity`: improved precision.  ([18250](https://github.com/pytorch/pytorch/pull/18250)).\r\n* `torch.autograd`: Don't keep unnecessary saved_inputs alive, increasing memory efficiency.  ([16583](https://github.com/pytorch/pytorch/pull/16583)).\r\n* `torch.autograd.profiler`: add Self (non-nested) CPU Time Total, CPU time total ([19378](https://github.com/pytorch/pytorch/pull/19378)).\r\n* `DataLoader`: support accepting a custom memory pinning function.  ([16743](https://github.com/pytorch/pytorch/pull/16743)).\r\n* `DataLoader`: retry libshm on EINTR.  ([15964](https://github.com/pytorch/pytorch/pull/15964)).\r\n* `DataLoader`: fixed an issue with `pin_memory` and `PackedSequence`.  ([18079](https://github.com/pytorch/pytorch/pull/18079))\r\n* `data.utils.collate`, `data.utils.pin_memory`: now preserve namedtuples.  ([16440](https://github.com/pytorch/pytorch/pull/16440))\r\n* Use `IndexError` instead of `RuntimeError` on many indexing error cases.  ([17049](https://github.com/pytorch/pytorch/pull/17049), [17114](https://github.com/pytorch/pytorch/pull/17114)).\r\n* Support indexing a `torch.float16` tensor on CPU.  ([17645](https://github.com/pytorch/pytorch/pull/17645)).\r\n* Add (limited) error checking in case of internal overlap on inplace operators.  ([19317](https://github.com/pytorch/pytorch/pull/19317), [17927](https://github.com/pytorch/pytorch/pull/17927)).\r\n* `utils.checkpoint.checkpoint`: support `None` as an argument to checkpoint function.  ([17969](https://github.com/pytorch/pytorch/pull/17969)).\r\n* `torch.autograd`: added more information for `one of the variables needed for gradient computation has been modified by an inplace operation` exception.  ([18523](https://github.com/pytorch/pytorch/pull/18523)).\r\n* `cuda.synchronize`: add a device argument.  ([19573](https://github.com/pytorch/pytorch/pull/19573)).\r\n* `cuda.reset_max_memory_*`: now supported.  ([15985](https://github.com/pytorch/pytorch/pull/15985)).\r\n* `distributions.Independent`:  can now calculate KL Divergence.  ([17681](https://github.com/pytorch/pytorch/pull/17681)).\r\n* `torch.distributed.new_group`: now supports overriding default backend. ([18595](https://github.com/pytorch/pytorch/pull/18595)).\r\n* `torch.distributed.init_process_group`: will now propagate timeout to underlying Store. ([16571](https://github.com/pytorch/pytorch/pull/16571)).\r\n* **[JIT]** Preserve module hierarchy on traced modules. ([#15101](https://github.com/pytorch/pytorch/pull/15101))\r\n* **[JIT]** Add metadata for TracedModules. ([#17311](https://github.com/pytorch/pytorch/pull/17311))\r\n* **[JIT]** Improve portability of int and float checks. ([#19532](https://github.com/pytorch/pytorch/pull/19532))\r\n* **[JIT]** Preserve method parameter names during serialization. ([#16750](https://github.com/pytorch/pytorch/pull/16750))\r\n* **[JIT]** Add a correctness check for C++ types to custom operators. ([#15247](https://github.com/pytorch/pytorch/pull/15247))\r\n* **[JIT]** Added a few extra python bindings to help with walking the IR graph from Python. [#17822](https://github.com/pytorch/pytorch/pull/17822) \r\n* **[JIT Error Messages]** Print out operator suggestions for \"unknown builtin op\" error. ([#15183](https://github.com/pytorch/pytorch/pull/15183))\r\n* **[JIT Error Messages]** Better error message when creating a module instance in TorchScript. ([#16416](https://github.com/pytorch/pytorch/pull/16416))\r\n* **[JIT Error Messages]** Print suggestion to add `nn.Module` attributes to `__constants__` when they are using in TorchScript. ([#18164](https://github.com/pytorch/pytorch/pull/18164))\r\n* **[JIT Error Messages]** `torch.save()`: Improve error message when you try to save a ScriptModule. ([#15321](https://github.com/pytorch/pytorch/pull/15321))\r\n* **[JIT Error Messages]** `torch.jit.save()`: Improve error message when trying to save a model with Python code. ([#16850](https://github.com/pytorch/pytorch/pull/16850))\r\n* **[JIT Error Messages]** Better errors when trying to close over a Tensor with grad enabled while tracing. ([#18298](https://github.com/pytorch/pytorch/pull/18298), [#19645](https://github.com/pytorch/pytorch/pull/19645))\r\n* **[JIT Error Messages]** Better error when trying to add a Tensor to `__constants__`. ([#16724](https://github.com/pytorch/pytorch/pull/16724))\r\n* **[JIT Error Messages]** Better error when a module list isn't added to `__constants__`. ([#17167](https://github.com/pytorch/pytorch/pull/17167)) \r\n* **[JIT Error Messages]** Add a warning when attempting to trace legacy constructors. ([#16770](https://github.com/pytorch/pytorch/pull/16770))\r\n* **[JIT Error Messages]** Improve hint when trying to trace non-deterministic nodes. ([#17957](https://github.com/pytorch/pytorch/pull/17957))\r\n* **[C++]** `nn::Module`: added Python interop.  ([13481](https://github.com/pytorch/pytorch/pull/13481)).\r\n* **[C++]** `autograd::profiler`: is now supported.  ([16580](https://github.com/pytorch/pytorch/pull/16580))\r\n* **[C++]** allow detection of C++ ABI flag for cpp extensions from available runtime information.  ([18994](https://github.com/pytorch/pytorch/pull/18994)).\r\n* **[C++]** `torch.argsort` is now supported in C++.  ([17099](https://github.com/pytorch/pytorch/pull/17099)).\r\n* **[C++]** `Tensor.isnan`: now supported in C++.  ([15722](https://github.com/pytorch/pytorch/pull/15722)).\r\n* **[C++]**: Added named submodule support to `nn::Sequential`.  ([17552](https://github.com/pytorch/pytorch/pull/17552)).\r\n* **[C++]**: Kaiming Initialization.  ([14718](https://github.com/pytorch/pytorch/pull/14718)).\r\n* **[C++]** `torch::data::transforms::Normalize`: now supported in C++.  ([15891](https://github.com/pytorch/pytorch/pull/15891)).\r\n* **[C++]**: Support call operator on module holder calling forward.  ([15831](https://github.com/pytorch/pytorch/pull/15831)).\r\n Random and Sequential distributed samplers.  ([16910](https://github.com/pytorch/pytorch/pull/16910)).\r\n* **[C++]**: pretty printing of C++ Modules.  ([15326](https://github.com/pytorch/pytorch/pull/15326)).\r\n* **[C++]** Support serializing `std::vector<torch::Tensor>`.  ([19677](https://github.com/pytorch/pytorch/pull/19677)).\r\n\r\n## Bug Fixes\r\n\r\n### Serious\r\n* `torch.prod`: correct erroneous calculation on large tensors.  ([15653](https://github.com/pytorch/pytorch/pull/15653)).\r\n* `torch.mean` (and other reductions): fix incorrect calculation on CUDA on large inputs.  ([16023](https://github.com/pytorch/pytorch/pull/16023)).\r\n* `nn.Conv`: correctly handle non-contiguous inputs on MKLDNN convolution codepath.  ([16300](https://github.com/pytorch/pytorch/pull/16300)).\r\n* `Tensor.eq_`:  Fix erroneous calculation.  ([15475](https://github.com/pytorch/pytorch/pull/15475)).\r\n* `torch.mean`: Fix fp16 output calculation.  ([14878](https://github.com/pytorch/pytorch/pull/14878)).\r\n* `nn.PoissonNLLLoss`:  Properly handle `reduction=None`.  ([17358](https://github.com/pytorch/pytorch/pull/17358)).\r\n* **[JIT]** Fix bug where custom ops could get optimized out if their outputs weren't used. ([#18711](https://github.com/pytorch/pytorch/pull/18711)).\r\n* **[JIT]** Fix bug where the model serializer would accidentally reorder statements. ([#17557](https://github.com/pytorch/pytorch/pull/17557)).\r\n\r\n### Other\r\n* `Tensor.round` is now consistently half to even.  ([17443](https://github.com/pytorch/pytorch/pull/17443)).\r\n* `Tensor.resize_`: Fix some 0-element cases.  ([14874](https://github.com/pytorch/pytorch/pull/14874)).\r\n* `Tensor.numpy`: Fix conversion of `torch.int8` dtype.  ([15194](https://github.com/pytorch/pytorch/pull/15194)).\r\n* `Tensor.grad`: correctly handle `del`.  ([16525](https://github.com/pytorch/pytorch/pull/16525)).\r\n* `Tensor.clamp`: correctly handle NaN on CUDA.  ([15479](https://github.com/pytorch/pytorch/pull/15479)).\r\n* `Tensor.topk`: properly set launch bounds on CUDA.  ([17296](https://github.com/pytorch/pytorch/pull/17296)).\r\n* `Tensor.kthvalue`: treat NaN as bigger than any number.  ([17824](https://github.com/pytorch/pytorch/pull/17824)).\r\n* `Tensor.copy_`: Properly synchronize on src and dst sreams.  ([16966](https://github.com/pytorch/pytorch/pull/16966)).\r\n* `Tensor indexing`: Fix incorrect dimension error message.  ([16495](https://github.com/pytorch/pytorch/pull/16495)).\r\n* `Tensor.coalesce`, `Tensor.clone`, `Tensor.to_dense`: fixed for sparse 0-dimensional tensors.  ([17379](https://github.com/pytorch/pytorch/pull/17379)).\r\n* `torch.isinf`: Don't error out on integral tensors.  ([15489](https://github.com/pytorch/pytorch/pull/15489)).\r\n* `torch.argsort`, `torch.sort`: Match NumPy by considering NaNs to be larger than any number.  ([15886](https://github.com/pytorch/pytorch/pull/15886)).\r\n* `torch.geqrf`, `torch.ormqr`: when an `out` parameter is specified, dispatch to the correct function.  ([16964](https://github.com/pytorch/pytorch/pull/16964)).\r\n* `torch.cuda.get_device_name` / `torch.cuda.get_device_capability`: Fix handling of optional.  ([17222](https://github.com/pytorch/pytorch/pull/17222)).\r\n* `Tensor.tril_` / `Tensor.triu_`: properly reuse input memory.  ([17031](https://github.com/pytorch/pytorch/pull/17031)).\r\n* `torch.arange`: fix shape inconsistency between CPU and CUDA.  ([18462](https://github.com/pytorch/pytorch/pull/18462)).\r\n* `torch.empty` (and other size-based factory functions): properly enforce non-negative sizes.  ([17077](https://github.com/pytorch/pytorch/pull/17077)).\r\n* `torch.load`: support serializing / deserializing `pathlib.Path` object.  ([18562](https://github.com/pytorch/pytorch/pull/18562)).\r\n* `nn.BatchNorm`: correctly handle very large batches.  ([17047](https://github.com/pytorch/pytorch/pull/17047)).\r\n* `nn.Softmax` / `nn.LogSoftmax`: fix double backward for `torch.half`.  ([17330](https://github.com/pytorch/pytorch/pull/17330)).\r\n* `nn.Softmax`: handle empty inputs in backward.  ([17259](https://github.com/pytorch/pytorch/pull/17259)).\r\n* `nn.NLLLoss`: Fix crash when `ignore_index` is out-of-bounds on CPU.  ([17328](https://github.com/pytorch/pytorch/pull/17328)).\r\n* `nn.Softmax`, `nn.LogSoftmax`: handle 0-element inputs.  ([17651](https://github.com/pytorch/pytorch/pull/17651)).\r\n* `nn.CTCLoss`: correct error checking.  ([16269](https://github.com/pytorch/pytorch/pull/16269)).\r\n* `nn.Conv`: better report convolution size mismatch.  ([17436](https://github.com/pytorch/pytorch/pull/17436)).\r\n* `torch.nn.functional.cosine_similarity`: fix output sometimes returning result > 1.0.  ([18168](https://github.com/pytorch/pytorch/pull/18168)).\r\n* `nn.parallel.data_parallel`: Fix handling of buffers that require_grad.  ([13352](https://github.com/pytorch/pytorch/pull/13352)).\r\n* `nn.parallel.data_parallel`: would previously sometimes frees tensors before all pending operations finish. ([18465](https://github.com/pytorch/pytorch/pull/18465)).\r\n* `torch.distributed.broadcast`: fixed repeated calls leading to OOM. ([19219](https://github.com/pytorch/pytorch/pull/19219)).\r\n* `torch.multiprocessing`: fix serialization of integer `nn.Parameters`.  ([18639](https://github.com/pytorch/pytorch/pull/18639)).\r\n* `torch.multiprocessing`: Fix handling of `distributions` on CUDA.  ([16854](https://github.com/pytorch/pytorch/pull/16854)).\r\n* `torch.nonzero`: Fix for 0-dimensional tensors on CUDA.  ([17406](https://github.com/pytorch/pytorch/pull/17406)).\r\n* `torch.slogdet`: Fix `sign` requiring grad when `input` required grad.  ([16337](https://github.com/pytorch/pytorch/pull/16337)).\r\n* `torch.cuda.Stream`: Properly restore stream on destination device when switching devices.  ([17439](https://github.com/pytorch/pytorch/pull/17439)).\r\n* `torch.cuda.Stream`: Fixed synchronization issue when used with non-current device.  ([15689](https://github.com/pytorch/pytorch/pull/15689)).\r\n* `torch.cuda.Stream`: properly change device in stream context manager.  ([16128](https://github.com/pytorch/pytorch/pull/16128)).\r\n* `DataLoader`: fixed a hang when no data was read and the buffer size is smaller than the chunk size.  ([17409](https://github.com/pytorch/pytorch/pull/17409)).\r\n* `DataLoader`: `_utils.collate.default_collate` now converts bool lists to byte Tensors, not integer tensors. \r\n ([14669](https://github.com/pytorch/pytorch/pull/14669)).\r\n* `DataLoader`: ensure dataset is indexed by integers.  ([17649](https://github.com/pytorch/pytorch/pull/17649)).\r\n* `torch.sparse.mm`:  Handle transposed dense tensors in backwards.  ([18737](https://github.com/pytorch/pytorch/pull/18737)).\r\n* `torch.sparse.sum`: Fix parsing of `dim`.  ([16517](https://github.com/pytorch/pytorch/pull/16517)).\r\n* `torch.sparse.mm` / `torch.sparse.addmm`: fix broadcasting and using uninitialized data.  ([16572](https://github.com/pytorch/pytorch/pull/16572)).\r\n* `Tensor.to_sparse`: Fix for 0-dimensional tensors.  ([17406](https://github.com/pytorch/pytorch/pull/17406)).\r\n* `SparseTensor`: fix add with non-contiguous `values` tensors.  ([18179](https://github.com/pytorch/pytorch/pull/18179)).\r\n* Fix `compare_exchange_weak` in `weak_intrusive_ptr`.  ([16302](https://github.com/pytorch/pytorch/pull/16302)).\r\n* `utils.model_zoo.load_url`: Fix race condition.  ([16578](https://github.com/pytorch/pytorch/pull/16578)).\r\n* `utils.data.RandomSampler`: have `len` properly take into account `num_samples`.  ([15991](https://github.com/pytorch/pytorch/pull/15991)).\r\n* `torch.distributions`:  Fix precision issue with expansion that prefers `probs` over `logits`.  ([18614](https://github.com/pytorch/pytorch/pull/18614)).\r\n* `distributions.dirichlet.Dirichlet`: fixed an underflow issue.  ([17488](https://github.com/pytorch/pytorch/pull/17488)).\r\n* `distributions.binomial.Binomial.log_prob`: fixed numerical stability issue.  ([15962](https://github.com/pytorch/pytorch/pull/15962)).\r\n* `Caching Allocator`: Free all blocks with outstanding events on OOM-retry.  ([19222](https://github.com/pytorch/pytorch/pull/19222)).\r\n* `torch.dtype`: fix pickling issue with Python 2.  ([18045](https://github.com/pytorch/pytorch/pull/18045)).\r\n* `utils.data.DataLoader`: Fix SIGCHLD checking.  ([19421](https://github.com/pytorch/pytorch/pull/19421)).\r\n* `optim.Optimizer`: Properly copy defaults.  ([19308](https://github.com/pytorch/pytorch/pull/19308)).\r\n* `optim.lr_scheduler.CosineAnnealingLR`: Fix division-by-zero error.  ([19180](https://github.com/pytorch/pytorch/pull/19180)).\r\n* `optim.lr_scheduler.ReduceLROnPlateau`: fix bug when the argument to `step` is reused outside the function. \r\n  ([16697](https://github.com/pytorch/pytorch/pull/16697)).\r\n* `cudNN`: fix race condition with multiple threads calling into the same device.  ([15080](https://github.com/pytorch/pytorch/pull/15080)).\r\n* `cudNN`: Properly specify accumulation types.  ([16825](https://github.com/pytorch/pytorch/pull/16825)).\r\n* `cuDNN`: Fix incorrectly selecting slower algorithms in certain cases.  ([15881](https://github.com/pytorch/pytorch/pull/15881)).\r\n* `cuFFT`:  Properly handle CUDA contexts.  ([19300](https://github.com/pytorch/pytorch/pull/19300))\r\n* Fix infinite loop in reduction functions when get_max_threads is nonzero but num_threads is 1.  ([15114](https://github.com/pytorch/pytorch/pull/15114)).\r\n* Fix tensor printing bug with Python 2.  ([12732](https://github.com/pytorch/pytorch/pull/12732)).\r\n* `MKLDNN`: fix thread safety.  ([17022](https://github.com/pytorch/pytorch/pull/17022)).\r\n* **[JIT]** `floordiv`: Fix integer division and divide-by-zero semantics. ([#15813](https://github.com/pytorch/pytorch/pull/15813)).\r\n* **[JIT]** Fix bug in alias analysis that disabled optimizations even in models without mutation. ([#18416](https://github.com/pytorch/pytorch/pull/18146)).\r\n* **[JIT]** `ord()`: Fix handling of utf8 chars. ([#19423](https://github.com/pytorch/pytorch/pull/19423)).\r\n* **[JIT]** Fix error when too many parameters are passed to a fused CUDA kernel. ([#18063](https://github.com/pytorch/pytorch/pull/18063)).\r\n* **[JIT]** Fix bug where common subexpression elimination accidentally introduced aliasing to function outputs. ([#19576](https://github.com/pytorch/pytorch/pull/19576)).\r\n* **[JIT]** Fix infinite loop in `requires_grad` analysis pass. ([#18361](https://github.com/pytorch/pytorch/pull/18361)).\r\n* **[JIT]** Fix ordering of parameters for in `rnn.py`. ([#18198](https://github.com/pytorch/pytorch/pull/18198)).\r\n* **[JIT]]** Fix contiguous autodiff and AutoGradZero inconsistency ([#18633](https://github.com/pytorch/pytorch/pull/18633)).\r\n* **[JIT]** Fix error reporting in NVRTC use of the fuser. ([#18327](https://github.com/pytorch/pytorch/pull/18327)).\r\n* **[JIT]** Ensure GIL is acquired before doing module lookup on import. ([#17135](https://github.com/pytorch/pytorch/pull/17135)).\r\n* **[JIT]** Fix bug where `_unique_state_dict` could contain duplicate Tensors. ([#18139](https://github.com/pytorch/pytorch/pull/18139)).\r\n* **[C++]**: Fix module serialization issue where one submodule doesn't have any parameters, but its submodules do.  ([15033](https://github.com/pytorch/pytorch/pull/15033)).\r\n* **[C++]**: Add `Stream` and `Event` APIs.  ([15937](https://github.com/pytorch/pytorch/pull/15937)).\r\n* **[C++]**: Fix Module serialization incompatibility between Python and C++ with weight-less layers.  ([19740](https://github.com/pytorch/pytorch/pull/19740)).\r\n* **[C++]**: Properly pass `extra_cuda_cflags` to C++ extensions on Windows.  ([18638](https://github.com/pytorch/pytorch/pull/18638)).\r\n* **[C++]** Make SGD semantics match python.  ([15840](https://github.com/pytorch/pytorch/pull/15840)).\r\n* **[C++]** `torch::nn::init::orthogonal_`: match Python API.  ([18915](https://github.com/pytorch/pytorch/pull/18915)).\r\n\r\n## Deprecations\r\n* `torch.btrifact`: the deprecated `info` argument has been removed.  ([14935](https://github.com/pytorch/pytorch/pull/14935)).\r\n* `torch.potrs` has been deprecated, use `torch.cholesky_solve` instead.  Note that `upper` defaults to `False`  for `torch.cholesky_solve`, and `True` for `torch.potrs`.  ([15334](https://github.com/pytorch/pytorch/pull/15334)).\r\n* `torch.pstrf` is deprecated; use `torch.cholesky` instead.  Note that `upper` defaults to `False`  for `torch.cholesky`, and `True` for `torch.pstrf`.  ([17866](https://github.com/pytorch/pytorch/pull/17866)).\r\n* `torch.potri` is deprecated; use `torch.cholesky_inverse` instead.  Note that `upper` defaults to `False`  for `torch.cholesky_inverse`, and `True` for `torch.potri`.  ([19498](https://github.com/pytorch/pytorch/pull/19498)).\r\n* `torch.btrifact_with_info` has been deprecated; use `torch.lu` with `get_infos=True` instead.([18435](https://github.com/pytorch/pytorch/pull/18435)).\r\n* `torch.btrifact` has been deprecated; use the new name `torch.lu` instead.  ([18435](https://github.com/pytorch/pytorch/pull/18435)).\r\n* `torch.gesv` is deprecated; use the new name `torch.solve instead.  ([18060](https://github.com/pytorch/pytorch/pull/18060)).\r\n* `torch.trtrs` has been deprecated; use the new name `torch.triangular_solve` instead.  ([18213](https://github.com/pytorch/pytorch/pull/18213)).\r\n* `torch. btriunpack` has been deprecated; use the new name `torch.lu_unpack ` instead.  ([18529](https://github.com/pytorch/pytorch/pull/18529)).\r\n* `torch.btrisolve` has been deprecated; use the new name `torch.lu_solve` instead.  ([18726](https://github.com/pytorch/pytorch/pull/18726)).\r\n* **[C++]** `IntList` has been deprecated, use `IntArrayRef` instead, as it better describes the type and ownership semantics in C++.  ([16751](https://github.com/pytorch/pytorch/pull/16751)).\r\n*  **[C++]** Dispatch macros with `Type` parameters, e.g. `AT_DISPATCH_ALL_TYPES(tensor.type(), ...`, are now deprecated; use `ScalarType` instead, e.g. `AT_DISPATCH_ALL_TYPES(tensor.scalar_type(), ...`.  ([17527](https://github.com/pytorch/pytorch/pull/17527), [17996](https://github.com/pytorch/pytorch/pull/17996)).\r\n* **[C++]** the deprecated `variable_tensor_functions` have been removed.  ([15003](https://github.com/pytorch/pytorch/pull/15003)).\r\n\r\n## Performance \r\n\r\n### Highlights\r\n* `nn.BatchNorm` CPU inference speed increased up to ~19x.([19152](https://github.com/pytorch/pytorch/pull/19152)).\r\n* `nn.AdaptiveAvgPool`: speed up common-case of size=1 output by ~30x.  ([17011](https://github.com/pytorch/pytorch/pull/17011)).\r\n* `nn.EmbeddingBag` CPU performance increased by ~4x.  ([19329](https://github.com/pytorch/pytorch/pull/19329)).\r\n* `Tensor.copy_`: sped up larger tensor copy ~2-3x, small regression in small tensor copy.  ([18618](https://github.com/pytorch/pytorch/pull/18618)).\r\n* `torch.nonzero`: is now ~2x faster than numpy on CPU.  ([15190](https://github.com/pytorch/pytorch/pull/15190))\r\n* Improve caching allocator for Pascal and newer GPUs; 10-20% better memory utilization on Mask-RCNN.  ([17120](https://github.com/pytorch/pytorch/pull/17120)).\r\n* `reduction functions`: Speed up some large Tensor cases by 50-80%.  ([17428](https://github.com/pytorch/pytorch/pull/17428)).\r\n* **[JIT]** Graph fuser: better fusion for backwards graphs in the presence of broadcasting. ([#14957](https://github.com/pytorch/pytorch/pull/14957))\r\n* **[JIT]** Graph fuser: `batch_norm` fusion for inference. ([#15146](https://github.com/pytorch/pytorch/pull/15146))\r\n* **[JIT]** Graph fuser: `layer_norm` fusion for inference. ([#18266](https://github.com/pytorch/pytorch/pull/18266))\r\n\r\n\r\n### Other\r\n\r\n* `torch.abs`, `torch.frac`, `torch.repiprocal`, `torch.neg` have been vectorized and parallelized ([19041](https://github.com/pytorch/pytorch/pull/19041)).\r\n* `torch.bmm`: CPU performance increased by 2x.  ([19338](https://github.com/pytorch/pytorch/pull/19338)).\r\n* `torch.sort`: CUDA performance increased by ~2x.  ([19379](https://github.com/pytorch/pytorch/pull/19379)).\r\n* `torch.cat` on CPU is now ~4x faster in the case where inputs are contiguous and `dim` != 0.  ([17032](https://github.com/pytorch/pytorch/pull/17032)).\r\n* `torch.multinomial` fixed a 2x performance regression.  ([17121](https://github.com/pytorch/pytorch/pull/17121)).\r\n* `torch.empty` (and another factory functions): reduce overhead by 20-40%.  ([17565](https://github.com/pytorch/pytorch/pull/17565)).\r\n* `torch.linspace` has been parallelized on CPU.  ([15320](https://github.com/pytorch/pytorch/pull/15320)).\r\n* `torch.logspace` has been parallelized on CPU.  ([15438](https://github.com/pytorch/pytorch/pull/15438)).\r\n* `torch.range` has been parallelized on CPU.  ([15484](https://github.com/pytorch/pytorch/pull/15484)).\r\n* `torch.arange` has been parallelized on CPU.  ([15667](https://github.com/pytorch/pytorch/pull/15667)).\r\n* `torch.load`: avoid unnecessary CPU-to-CUDA copy.  ([17297](https://github.com/pytorch/pytorch/pull/17297)).\r\n* `reduction functions`: improve efficiency on CUDA.  ([16224](https://github.com/pytorch/pytorch/pull/16224), [17040](https://github.com/pytorch/pytorch/pull/17040)).\r\n* Speed up some GEMM cases on CPU by up to 7x.([17730](https://github.com/pytorch/pytorch/pull/17730))\r\n* Tensor iterator loop unrolling.  ([17667](https://github.com/pytorch/pytorch/pull/17667)).\r\n* `sparse/dense matrix multiply`: improve speed by ~5x.  ([16905](https://github.com/pytorch/pytorch/pull/16905)).\r\n* `distributions.MultivariateNormal`: sped up.  ([17294](https://github.com/pytorch/pytorch/pull/17294)).\r\n* **[JIT]** Graph fuser: pow scalar exponent / base autodiff, fusion ([#19324](https://github.com/pytorch/pytorch/pull/19324))\r\n* **[JIT]** Graph fuser: allow fusion of function float arguments. ([#18087](https://github.com/pytorch/pytorch/pull/18087))\r\n* **[JIT]** Shape analysis: specialize optional Tensor inputs to graphs. ([#18360](https://github.com/pytorch/pytorch/pull/18360))\r\n* **[JIT]** Shape analysis: various correctness improvements. ([#18271](https://github.com/pytorch/pytorch/pull/18271))\r\n* **[JIT]** Shape analysis: `aten::_convolution` now participates in shape analysis. ([#16837](https://github.com/pytorch/pytorch/pull/16837)]\r\n* **[JIT]** Autodiff: coverage for ops used in maskrcnn & BERT. ([#16689](https\ufffc://github.com/pytorch/pytorch/pull/16689))\r\n* **[JIT]** Autodiff: support for scalar comparison ops and `randlike`. ([#14740](https://github.com/pytorch/pytorch/pull/14740))\r\n* **[JIT]** Autodiff: support for `adaptive_avg_pool2d`. ([#15459](https://github.com/pytorch/pytorch/pull/15459))\r\n* **[JIT]** Autodiff: support for `erf` and `erfc`. ([#15139](https://github.com/pytorch/pytorch/pull/15139))\r\n* **[JIT]** Autodiff: support for `layernorm`. ([#17702](https://github.com/pytorch/pytorch/pull/17702))\r\n* **[JIT]** Autodiff: support for `tanh`. ([#17816](https://github.com/pytorch/pytorch/pull/17816))\r\n* **[JIT]** Autodiff: support for `matmul`/`dropout`. ([#17523](https://github.com/pytorch/pytorch/pull/17523))\r\n* **[JIT]** Autodiff: specialized CUDA impl for dropout. ([#17756](https://github.com/pytorch/pytorch/pull/17756))\r\n* **[JIT]** Constant folding: improved inlining of control flow. ([#16244](https://github.com/pytorch/pytorch/pull/16244))\r\n\r\n## Documentation\r\n\r\n* `Tensor.scatter_`: add documentation about `value` parameter.  ([17467](https://github.com/pytorch/pytorch/pull/17467)).\r\n* `Tensor.unfold`: correctly document `dimension` parameter, not `dim`.  ([19020](https://github.com/pytorch/pytorch/pull/19020)).\r\n* `Tensor.is_floating_point()` is now documented.  ([15704](https://github.com/pytorch/pytorch/pull/15704)).\r\n* `torch.cholesky`: Fix broken `upper` example in documentation.  ([15215](https://github.com/pytorch/pytorch/pull/15215)).\r\n* `torch.gesv`: document `out` parameter.  ([15649](https://github.com/pytorch/pytorch/pull/15649)).\r\n* `torch.mul`: better explain elementwise multiplication.  ([15664](https://github.com/pytorch/pytorch/pull/15664)).\r\n* `torch.eig`, `torch.symeig`: better explain backwards limitations.  ([15929](https://github.com/pytorch/pytorch/pull/15929)).\r\n* `torch.ormqr`: fixed output specification.  ([15694](https://github.com/pytorch/pytorch/pull/15694)).\r\n* `torch.from_numpy`: replaced usage with `torch.as_tensor` in documentation.  ([16587](https://github.com/pytorch/pytorch/pull/16587)).\r\n* `torch.mvlgamma`: Fix the constant in the docs.  ([17045](https://github.com/pytorch/pytorch/pull/17045)).\r\n* `torch.mode`: more precisely describe what is returned.  ([17069](https://github.com/pytorch/pytorch/pull/17069)).\r\n* `torch.upsample`: documentation now matches `torch.interpolate`.  ([17134](https://github.com/pytorch/pytorch/pull/17134))\r\n* `torch.arange`: correct `dtype` documentation.  ([18604](https://github.com/pytorch/pytorch/pull/18604))\r\n* `torch.cumprod`: document `out` parameter.  ([19340](https://github.com/pytorch/pytorch/pull/19340)).\r\n* `torch.nonzero`: document indices being returned lexicographically.  ([19539](https://github.com/pytorch/pytorch/pull/19539)).\r\n* `torch.nn.functional.interpolate`: better explain `aligned_corners` parameter.  ([14806](https://github.com/pytorch/pytorch/pull/14806)).\r\n* `torch.nn.functional.pad`: documentation has been made consistent with other functional ops.  ([15984](https://github.com/pytorch/pytorch/pull/15984)).\r\n* `nn.functional.grid_sample`: clarify behavior of padding.  ([19754](https://github.com/pytorch/pytorch/pull/19754)).\r\n* `nn.TripletMarginLoss`: correct type of `swap` parameter.  ([18115](https://github.com/pytorch/pytorch/pull/18115)).\r\n* `nn.CrossEntropyLoss`: clarify `ignore_index` documentation.  ([18117](https://github.com/pytorch/pytorch/pull/18117)).\r\n* `nn.CrossEntropyLoss`: the input format is more clearly explained.  ([15990](https://github.com/pytorch/pytorch/pull/15990)).\r\n* `nn.CTCLoss`: Clarify a number of ambiguities.  ([18415](https://github.com/pytorch/pytorch/pull/18415)).\r\n* `nn.BCEWithLogitsLoss`: add better explanation.  ([19212](https://github.com/pytorch/pytorch/pull/19212)).\r\n* `nn.BCEWithLogitsLoss`: better explain positive samples.  ([17258](https://github.com/pytorch/pytorch/pull/17258)).\r\n* `nn.ModuleList` / `nn.ParameterList`: update documentation.  ([17731](https://github.com/pytorch/pytorch/pull/17731)).\r\n* `nn.Module.load_state_dict`: correct semantics of `strict`.  ([17618](https://github.com/pytorch/pytorch/pull/17618))\r\n* `nn.parallel.DataParallel`: more accurately specify how different argument types are handled.  ([15993](https://github.com/pytorch/pytorch/pull/15993)).\r\n* `nn.parallel.DistributedDataParallel`: Clarified batch size requirements.  ([16010](https://github.com/pytorch/pytorch/pull/16010)).\r\n* `torch.distributed`: Document mixed-precision training.  ([15440](https://github.com/pytorch/pytorch/pull/15440)).\r\n* `torch.multiprocessing`: Include example multiprocessing code.  ([16345](https://github.com/pytorch/pytorch/pull/16345)).\r\n* `torch.autograd`: Better explain computing Jacobian-vector product.  ([15197](https://github.com/pytorch/pytorch/pull/15197)).\r\n* `torch.cuda.get_rng_state`, `torch.cuda.set_rng_state`: document taking a `device` object.  ([14324](https://github.com/pytorch/pytorch/pull/14324)).\r\n* `torch.device`: Fix example of passing `device` to tensor factory.  ([16839](https://github.com/pytorch/pytorch/pull/16839)).\r\n* `DataLoader`: update documentation to describe how workers are managed.  ([18091](https://github.com/pytorch/pytorch/pull/18091)).\r\n* Unified shape formats throughout the documentation.  ([15741](https://github.com/pytorch/pytorch/pull/15741)).\r\n* Update documentation for `reduction` arguments to use non-deprecated format.  ([17300](https://github.com/pytorch/pytorch/pull/17300)).\r\n* `mark_non_differentiable`: document correct semantics.  ([17891](https://github.com/pytorch/pytorch/pull/17891)).\r\n* Warn about memory overlaps on inplace operations.  ([17576](https://github.com/pytorch/pytorch/pull/17576)).\r\n* Fix a number of small issues with conv and pooling docstrings.  ([17052](https://github.com/pytorch/pytorch/pull/17052)).\r\n* Fix a number of small issues with padding and activation docstrings.  ([17197](https://github.com/pytorch/pytorch/pull/17197)).\r\n* **[C++]**: mention packed accessors in Tensor basics.  ([19464](https://github.com/pytorch/pytorch/pull/19464)).\r\n\r\n## ONNX\r\n\r\n### Exporting More Torch Operators to ONNX\r\n\r\n* Export torch.isnan to ONNX ([17698](https://github.com/pytorch/pytorch/pull/17698)).\r\n* Export torch.flatten to ONNX ([16240](https://github.com/pytorch/pytorch/pull/16240)).\r\n* Export torch.where, torch.ceil, torch.floor to ONNX ([18571](https://github.com/pytorch/pytorch/pull/18571)).\r\n* Export torch.narrow to ONNX ([17550](https://github.com/pytorch/pytorch/pull/17550)).\r\n* Export torch.argmax and torch torch.argmin ([17382](https://github.com/pytorch/pytorch/pull/17382), [18264](https://github.com/pytorch/pytorch/pull/18264), [18261](https://github.com/pytorch/pytorch/pull/18261)).\r\n* Export adaptive_avg_pool1D, adaptive_avg_pool2D, adaptive_avg_pool3D, adaptive_max_pool1D, adaptive_max_pool2D, adaptive_max_pool3D to ONNX ([17412](https://github.com/pytorch/pytorch/pull/17412)).\r\n* Export torch.nonzero to ONNX ([17036](https://github.com/pytorch/pytorch/pull/17036), [18047](https://github.com/pytorch/pytorch/pull/18047)).\r\n* Export torch.erf to ONNX ([16106](https://github.com/pytorch/pytorch/pull/16106)).\r\n* Export torch.split ([15092](https://github.com/pytorch/pytorch/pull/15092)).\r\n* Export torch.lt, torch.gt, torch.le, torch.ge, torch.eq, torch.ne to ONNX ([15677](https://github.com/pytorch/pytorch/pull/15677)).\r\n* Export torch.expand and torch.ne to ONNX ([15050](https://github.com/pytorch/pytorch/pull/15050)).\r\n* Export torch.nn.LogSigmoid to ONNX ([14830](https://github.com/pytorch/pytorch/pull/14830)).\r\n* Export torch.nn.RReLU to ONNX ([14781](https://github.com/pytorch/pytorch/pull/14781)).\r\n* Export torch.reshape and torch.reshape_as to ONNX ([16632](https://github.com/pytorch/pytorch/pull/16632), [16971](https://github.com/pytorch/pytorch/pull/16971)).\r\n* Replace use of ConstantLike with with ConstantOfShape ([16095](https://github.com/pytorch/pytorch/pull/16095), [16214](https://github.com/pytorch/pytorch/pull/16214)).\r\n\r\n### Extending Existing Exporting Logic\r\n\r\n* Enable dim support in torch.nn.Softmax's export ([18482](https://github.com/pytorch/pytorch/pull/18482)).\r\n* Support exporting squeeze & unsqueeze with negative dim attribute ([19297](https://github.com/pytorch/pytorch/pull/19297)).\r\n* Support exporting max_pool1d, max_pool2d, max_pool3d with indices ([16455](https://github.com/pytorch/pytorch/pull/16455)).\r\n* Add dtype support in torch.logsoftmax and torch.softmax's export ([17672](https://github.com/pytorch/pytorch/pull/17672)).\r\n* Support ceil_mode in max_pool_1d, max_pool2d, max_pool3d, avg_pool1d, avg_pool2d, avg_pool3d's export ([16769](https://github.com/pytorch/pytorch/pull/16769)).\r\n\r\n### Optimizing Exported ONNX Graph\r\n\r\n* Add constant folding in ONNX exporter ([18698](https://github.com/pytorch/pytorch/pull/18698)).\r\n* Retain the parameter names in ONNX exporter ([17551](https://github.com/pytorch/pytorch/pull/17551)).\r\n* Omit slice op if it is a non-op ([19155](https://github.com/pytorch/pytorch/pull/19155)).\r\n* Add a flag to strip doc_string from exported ONNX models ([18882](https://github.com/pytorch/pytorch/pull/18882)).\r\n* Omit torch.dropout if the model is in eval mode ([16547](https://github.com/pytorch/pytorch/pull/16547)).\r\n\r\n### Adding Utility Functions and Refactoring\r\n\r\n* Remove unused arg f from _model_to_graph(). ([19647](https://github.com/pytorch/pytorch/pull/19647)).\r\n* Add the support for stable ONNX opsets in exporter ([16068](https://github.com/pytorch/pytorch/pull/16068), [17419](https://github.com/pytorch/pytorch/pull/17419)).\r\n* Set the default ONNX opset to the latest stable opset (i.e., 9) ([17736](https://github.com/pytorch/pytorch/pull/17736)).\r\n* Add an utility function to check whether it's in the middle of ONNX export or not ([19050](https://github.com/pytorch/pytorch/pull/19050)).\r\n* Refactoring serialization of ONNX initializers to be name-based ([17830](https://github.com/pytorch/pytorch/pull/17830)).\r\n* Expose dim() on type and use it in ONNX symbolics ([15933](https://github.com/pytorch/pytorch/pull/15933)).\r\n* Add scalar_type_to_pytorch_type dict in ONNX symbolic ([15965](https://github.com/pytorch/pytorch/pull/15965)).\r\n* Add an assertion to check the number of the parameters passed to ONNX exporter ([18145](https://github.com/pytorch/pytorch/pull/18145)).\r\n\r\n### Bugfixes\r\n\r\n* Fix different types in rsub caused bug ([15707](https://github.com/pytorch/pytorch/pull/15707)).\r\n* Fix list structure supports in ONNX exporter ([19102](https://github.com/pytorch/pytorch/pull/19102)).\r\n* Fix case for `activations` attribute in nn.RNN ONNX export. ([19368](https://github.com/pytorch/pytorch/pull/19368)).\r\n* Minor fix for onnx ConstantOfShape export ([18199](https://github.com/pytorch/pytorch/pull/18199)).\r\n* Fix the torch.(reduce)min and torch.(reduce)max's export ([15241](https://github.com/pytorch/pytorch/pull/15241)).\r\n* Fixing ONNX export of logical ops to have correct output datatype ([15185](https://github.com/pytorch/pytorch/pull/15185)).\r\n* Fix typo in docstring ([18216](https://github.com/pytorch/pytorch/pull/18216)).\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.1.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.1.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.1.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/17080796", "dateCreated": "2019-04-30T23:22:19Z", "datePublished": "2019-05-01T00:09:03Z"}, {"tagName": "v1.0.1", "name": "Bug Fix Release", "authorName": "soumith", "authorType": "User", "body": "Note: our conda install commands have slightly changed. Version specifiers such as `cuda100` in `conda install pytorch cuda100 -c pytorch` have changed to `conda install pytorch cudatoolkit=10.0 -c pytorch`\r\n\r\n## Breaking Changes\r\n\r\nThere are no breaking changes in this release.\r\n\r\n## Bug Fixes\r\n\r\n### Serious\r\n\r\n- Higher order gradients for CPU Convolutions have been fixed (regressed in 1.0.0 under MKL-DNN setting) #15686\r\n- Correct gradients for non-contiguous weights in CPU Convolutions #16301\r\n- Fix ReLU on CPU Integer Tensors by fixing vec256 inversions #15634\r\n- Fix bincount for non-contiguous Tensors #15109\r\n- Fix torch.norm on CPU for large Tensors #15602\r\n- Fix eq_ to do equality on GPU (was doing greater-equal due to a typo) (#15475)\r\n- Workaround a CuDNN bug that gave wrong results in certain strided convolution gradient setups\r\n  - blacklist fft algorithms for strided dgrad (#16626)\r\n\r\n### Correctness\r\n\r\n- Fix cuda native loss_ctc for varying input length (#15798)\r\n  - this avoids NaNs in variable length settings\r\n- C++ Frontend: Fix serialization (#15033)\r\n  - Fixes a bug where (de-)/serializing a hierarchy of submodules where one submodule doesn't have any parameters, but its submodules do\r\n- Fix derivative for mvlgamma (#15049)\r\n- Fix numerical stability in log_prob for Gumbel distribution (#15878)\r\n- multinomial: fix detection and drawing of zero probability events (#16075)\r\n\r\n\r\n### Crashes\r\n\r\n- PyTorch binaries were [crashing on AWS Lambda](https://github.com/pytorch/pytorch/issues/15213) and a few other niche systems, stemming from CPUInfo handling certain warnings as errors. Updated CPUInfo with relevant fixes.\r\n- MKL-DNN is now statically built, to avoid conflicts with system versions\r\n- Allow ReadyQueue to handle empty tasks (#15791)\r\n  - Fixes a segfault with a DataParallel + Checkpoint neural network setting\r\n- Avoid integer divide by zero error in index_put_ (#14984)\r\n- Fix for model inference crash on Win10 (#15919) (#16092)\r\n- Use CUDAGuard when serializing Tensors:\r\n  - Before this change, `torch.save` and `torch.load` would initialize the CUDA context on GPU 0 if it hadn't been initialized already, even if the serialized tensors are only on GPU 1.\r\n- Fix error with handling scalars and __rpow__, for example `1 ^^ x`, where x is a PyTorch scalar (#16687)\r\n- Switch to CUDA implementation instead of CuDNN if batch size >= 65536 for affine_grid (#16403)\r\n  - CuDNN crashes when batch size >= 65536\r\n- [Distributed] TCP init method race condition fix (#15684)\r\n- [Distributed] Fix a memory leak in Gloo's CPU backend\r\n- [C++ Frontend] Fix LBFGS issue around using inplace ops (#16167)\r\n- [Hub] Fix github branch prefix v (#15552)\r\n- [Hub] url download bugfix for URLs served without Content-Length header\r\n\r\n## Performance\r\n\r\n- LibTorch binaries now ship with CuDNN enabled. Without this change, many folks saw significant perf differences while using LibTorch vs PyTorch, this should be fixed now. [#14976](https://github.com/pytorch/pytorch/pull/14976)\r\n- Make btriunpack work for high dimensional batches and faster than before (#15286)\r\n- improve performance of unique with inverse indices (#16145)\r\n- Re-enable OpenMP in binaries (got disabled because of a CMake refactor)\r\n\r\n## Other\r\n\r\n- create type hint stub files for module torch (#16089)\r\n  - This will restore auto-complete functionality in PyCharm, VSCode etc.\r\n- Fix sum_to behavior with zero dimensions (#15796)\r\n- Match NumPy by considering NaNs to be larger than any number when sorting (#15886)\r\n- Fixes various error message / settings in dynamic weight GRU / LSTMs (#15766)\r\n- C++ Frontend: Make call operator on module holder call forward (#15831)\r\n- C++ Frontend: Add the normalize transform to the core library (#15891) \r\n- Fix bug in torch::load and unpack torch::optim::detail namespace (#15926)\r\n- Implements Batched upper triangular, lower triangular (#15257)\r\n- Add torch.roll to documentation (#14880)\r\n- (better errors) Add backend checks for batch norm (#15955) \r\n\r\n\r\n## JIT\r\n\r\n- Add better support for bools in the graph fuser (#15057) \r\n- Allow tracing with fork/wait (#15184)\r\n- improve script/no script save error (#15321)\r\n- Add self to Python printer reserved words (#15318) \r\n- Better error when torch.load-ing a JIT model (#15578)\r\n- fix select after chunk op (#15672)\r\n- Add script standard library documentation + cleanup (#14912)\r\n\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.0.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.0.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.0.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/15374386", "dateCreated": "2019-02-07T07:24:24Z", "datePublished": "2019-02-07T08:51:04Z"}, {"tagName": "v1.0.0", "name": "JIT Compiler, Faster Distributed, C++ Frontend", "authorName": "soumith", "authorType": "User", "body": "## Table of Contents\r\n\r\n* **Highlights**\r\n  * JIT\r\n  * Brand New Distributed Package\r\n  * C++ Frontend [API Unstable]\r\n  * Torch Hub\r\n* **Breaking Changes**\r\n* **Additional New Features**\r\n  * N-dimensional empty tensors\r\n  * New Operators\r\n  * New Distributions\r\n  * Sparse API Improvements\r\n  * Additions to existing Operators and Distributions\r\n* **Bug Fixes**\r\n  * Serious\r\n  * Backwards Compatibility\r\n  * Correctness\r\n  * Error checking\r\n  * Miscellaneous\r\n* **Other Improvements**\r\n* **Deprecations**\r\n  * CPP Extensions\r\n* **Performance**\r\n* **Documentation Improvements**\r\n\r\n## Highlights\r\n\r\n### JIT\r\n\r\nThe JIT is a set of compiler tools for bridging the gap between research in PyTorch\r\nand production. It allows for the creation of models that can run without a dependency on the Python interpreter and which can be optimized more aggressively. Using program annotations existing models can be transformed into Torch Script, a subset of Python that PyTorch can run directly. Model code is still valid Python code and can be debugged with the standard Python toolchain. PyTorch 1.0 provides two ways in which you can make your existing code compatible with the JIT, using `torch.jit.trace` or `torch.jit.script`. Once annotated, Torch Script code can be aggressively optimized and it can be serialized for later use in our new C++ API, which doesn't depend on Python at all.\r\n\r\n```python\r\n# Write in Python, run anywhere!\r\n@torch.jit.script\r\ndef RNN(x, h, W_h, U_h, b_h):\r\n  y = []\r\n  for t in range(x.size(0)):\r\n    h = torch.tanh(x[t] @ W_h + h @ U_h + b_h)\r\n    y += [h]\r\n  return torch.stack(y), h\r\n```\r\n\r\nAs an example, see a tutorial on [deploying a seq2seq model](https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html),\r\n[loading an exported model from C++](https://pytorch.org/tutorials/advanced/cpp_export.html), or [browse the docs](https://pytorch.org/docs/jit.html).\r\n\r\n### Brand New Distributed Package\r\n\r\nThe [torch.distributed](https://pytorch.org/docs/master/distributed.html) package and [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel) module are backed by a brand new re-designed distributed library.  The main highlights of the new library are:\r\n* New `torch.distributed` is performance driven and operates entirely asynchronously for all backends: `Gloo`, `NCCL`, and `MPI`.\r\n* Significant Distributed Data Parallel performance improvements especially for hosts with slower networks such as ethernet-based hosts\r\n* Adds async support for all distributed collective operations in the [torch.distributed](https://pytorch.org/docs/master/distributed.html) package.\r\n* Adds the following CPU ops in the Gloo backend: [send](https://pytorch.org/docs/master/distributed.html#torch.distributed.send), [recv](https://pytorch.org/docs/master/distributed.html#torch.distributed.recv), [reduce](https://pytorch.org/docs/master/distributed.html#torch.distributed.reduce), [all_gather](https://pytorch.org/docs/master/distributed.html#torch.distributed.all_gather), [gather](https://pytorch.org/docs/master/distributed.html#torch.distributed.gather), [scatter](https://pytorch.org/docs/master/distributed.html#torch.distributed.scatter)\r\n* Adds [barrier](https://pytorch.org/docs/master/distributed.html#torch.distributed.barrier) op in the NCCL backend\r\n* Adds [new_group](https://pytorch.org/docs/master/distributed.html#torch.distributed.new_group) support for the NCCL backend\r\n\r\n### C++ Frontend _**[API Unstable]**_.\r\n\r\nThe C++ frontend is a pure C++ interface to the PyTorch backend that follows the API and architecture of the established Python frontend. It is intended to enable research in high performance, low latency and bare metal C++ applications. It provides equivalents to `torch.nn`, `torch.optim`, `torch.data` and other components of the Python frontend. Here is a minimal side-by-side comparison of the two language frontends:\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Python</th><th>C++</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\n<br>\r\nmodel = torch.nn.Linear(5, 1)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\r\nprediction = model.forward(torch.randn(3, 5))\r\nloss = torch.nn.functional.mse_loss(prediction, torch.ones(3, 1))\r\nloss.backward()\r\noptimizer.step()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"cpp\">\r\n#include &lt;torch/torch.h&gt;\r\n<br>\r\ntorch::nn::Linear model(5, 1);\r\ntorch::optim::SGD optimizer(model->parameters(), /*lr=*/0.1);\r\ntorch::Tensor prediction = model->forward(torch::randn({3, 5}));\r\nauto loss = torch::mse_loss(prediction, torch::ones({3, 1}));\r\nloss.backward();\r\noptimizer.step();\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nWe are releasing the C++ frontend marked as \"API Unstable\" as part of PyTorch 1.0. This means it is ready to be used for your research application, but still has some open construction sites that will stabilize over the next couple of releases. **Some parts of the API may undergo breaking changes during this time**.\r\n\r\nSee https://pytorch.org/cppdocs for detailed documentation on the greater PyTorch C++ API as well as the C++ frontend.\r\n\r\n### Torch Hub\r\n\r\nTorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\r\n\r\nTorch Hub supports publishing pre-trained models (model definitions and pre-trained weights) to a github repository using a simple hubconf.py file; see [hubconf for resnet models in pytorch/vision](https://github.com/pytorch/vision/blob/master/hubconf.py) as an example.  Once published, users can load the pre-trained models using the [torch.hub.load](https://pytorch.org/docs/master/hub.html#torch.hub.load) API.\r\n\r\nFor more details, see the [torch.hub documentation](https://pytorch.org/docs/master/hub.html).  Expect a more-detailed blog post introducing Torch Hub in the near future!\r\n\r\n## Breaking Changes\r\n\r\n* Indexing a 0-dimensional tensor will now throw an error instead of warn.  Use [tensor.item()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item) instead.  ([#11679](https://github.com/pytorch/pytorch/pull/11679)).\r\n* [torch.legacy](https://pytorch.org/docs/stable/legacy.html) is removed.  ([#11823](https://github.com/pytorch/pytorch/pull/11823)).\r\n* _torch.masked_copy__ is removed, use [torch.masked_scatter_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_scatter_) instead.  ([#9817](https://github.com/pytorch/pytorch/issues/9817)).\r\n* Operations that result in 0 element tensors may return changed shapes.\r\n    * Before: all 0 element tensors would collapse to shape _(0,)_.  For example, [torch.nonzero](https://pytorch.org/docs/stable/torch.html#torch.nonzero) is documented to return a tensor of shape _(n,z)_, where n = number of nonzero elements and z = dimensions of the input, but would always return a Tensor of shape _(0,) when no nonzero elements existed.\r\n    * Now: Operations return their documented shape.\r\n      ```\r\n      # Previously: all 0-element tensors are collapsed to shape (0,)\r\n      >>> torch.nonzero(torch.zeros(2, 3))\r\n      tensor([], dtype=torch.int64)\r\n\r\n      # Now, proper shape is returned\r\n      >>> torch.nonzero(torch.zeros(2, 3))\r\n      tensor([], size=(0, 2), dtype=torch.int64)\r\n      ```\r\n* Sparse tensor _indices_ and _values_ shape invariants are changed to be more consistent in the case of 0-element tensors.  See link for more details.  ([#9279](https://github.com/pytorch/pytorch/pull/9279)).\r\n* [torch.distributed](https://pytorch.org/docs/master/distributed.html): the TCP backend is removed, we recommend to use Gloo and MPI backends for CPU collectives and NCCL backend for GPU collectives.\r\n* Some inter-type operations (e.g. `*`) between `torch.Tensors` and NumPy arrays will now favor dispatching to the `torch` variant.  This may result in different return types.  ([#9651](https://github.com/pytorch/pytorch/pull/9651)).\r\n* Implicit `numpy` conversion no longer implicitly moves a tensor to CPU.  Therefore, you may have to explicitly move a CUDA tensor to CPU (`tensor.to('cpu')`) before an implicit conversion. ([#10553](https://github.com/pytorch/pytorch/pull/10553)).\r\n* [torch.randint](https://pytorch.org/docs/stable/torch.html#torch.randint) now defaults to using dtype _torch.int64_ rather than the default floating-point dtype.  ([#11040](https://github.com/pytorch/pytorch/pull/11040)).\r\n* [torch.tensor](https://pytorch.org/docs/master/torch.html#torch.tensor) function with a `Tensor` argument now returns a `detached` Tensor (i.e. a Tensor where `grad_fn` is `None`).  This more closely aligns with the intent of the function, which is to return a Tensor with copied data and no history.  ([#11061](https://github.com/pytorch/pytorch/pull/11061), \r\n[#11815](https://github.com/pytorch/pytorch/pull/11815)).\r\n* [torch.nn.functional.multilabel_soft_margin_loss](https://pytorch.org/docs/master/nn.html#torch.nn.functional.multilabel_soft_margin_loss) now returns Tensors of shape `(N,)` instead of `(N, C)` to match the behavior of [torch.nn.MultiMarginLoss](https://pytorch.org/docs/master/nn.html#torch.nn.MultiMarginLoss).  In addition, it is more numerically stable. \r\n ([#9965](https://github.com/pytorch/pytorch/pull/9965)).\r\n* The result type of a _torch.float16_ 0-dimensional tensor and a integer is now _torch.float16_ (was _torch.float32_ or _torch.float64_ depending on the dtype of the integer).  ([#11941](https://github.com/pytorch/pytorch/pull/11941)).\r\n* [Dirichlet](https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet) and [Categorical](https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical) distributions no longer accept scalar parameters.  ([#11589](https://github.com/pytorch/pytorch/pull/11589)).\r\n* _[CPP Extensions](https://pytorch.org/docs/master/cpp_extension.html)_: Deprecated factory functions that accept a type as the first argument and a size as a second argument argument have been removed.  Instead, use the new-style factory functions that accept the size as the first argument and [`TensorOptions`](https://github.com/pytorch/pytorch/blob/ab9a5976a025b3a56cfc64b86981ad52f8fe4137/aten/src/ATen/core/TensorOptions.h#L14) as the last argument. For example, replace your call to `at::ones(torch::CPU(at::kFloat)), {2, 3})` with `torch::ones({2, 3}, at::kCPU)`. This applies to the following functions:\r\n    - `arange`, `empty`, `eye`, `full`, `linspace`, `logspace`, `ones`, `rand`, `randint`, `randn`, `randperm`, `range`, `zeros`.\r\n* [torch.potrf](https://pytorch.org/docs/master/torch.html#torch.potrf) renamed to [torch.cholesky](https://pytorch.org/docs/master/torch.html#torch.cholesky). It has a new default (upper=False) ([#12699](https://github.com/pytorch/pytorch/pull/12699)).\r\n* Renamed `elementwise_mean` to `mean` for loss reduction functions ([#13419](https://github.com/pytorch/pytorch/pull/13419))\r\n\r\n## Additional New Features\r\n\r\n### N-dimensional empty tensors\r\n\r\n* Tensors with 0 elements can now have an arbitrary number of dimensions and support indexing and other torch operations; previously, 0 element tensors were limited to shape _(0,)_.  ([#9947](https://github.com/pytorch/pytorch/pull/9947)).  _Example_:\r\n  ```\r\n  >>> torch.empty((0, 2, 4, 0), dtype=torch.float64)\r\n  tensor([], size=(0, 2, 4, 0), dtype=torch.float64)\r\n  ```\r\n\r\n### New Operators\r\n\r\n\r\n- [torch.argsort](https://pytorch.org/docs/master/torch.html#torch.argsort) similar to [numpy.argsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html). \r\n ([#9600](https://github.com/pytorch/pytorch/pull/9600)).\r\n- [torch.pdist](https://pytorch.org/docs/master/torch.html#torch.pdist) similar to [scipy.spatial.distance.pdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html).  ([#10782](https://github.com/pytorch/pytorch/pull/10782)).\r\n- [torch.tensordot](https://pytorch.org/docs/master/torch.html#torch.tensordot) similar to [numpy.tensordot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tensordot.html).  ([#10025](https://github.com/pytorch/pytorch/pull/10025)).\r\n- [torch.broadcast_tensors](https://pytorch.org/docs/master/torch.html#torch.narrow) similar to [numpy.broadcast_arrays](https://docs.scipy.org/doc/numpy/reference/generated/numpy.broadcast_arrays.html). \r\n ([#10075](https://github.com/pytorch/pytorch/pull/10075)).\r\n- [torch.narrow](https://pytorch.org/docs/master/torch.html#torch.narrow) support for sparse tensors.\r\n([#11342](https://github.com/pytorch/pytorch/pull/11342)).\r\n- [torch.matrix_rank](https://pytorch.org/docs/master/torch.html#torch.matrix_rank) similar to  [numpy.linalg.matrix_rank](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). \r\n([#10338](https://github.com/pytorch/pytorch/pull/10338)).\r\n- [torch.matrix_power](https://pytorch.org/docs/master/torch.html#torch.matrix_power) similar to [numpy.linalg.matrix_power](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_power.html).  ([#11421](https://github.com/pytorch/pytorch/pull/11421)).\r\n- [torch.nn.CeLU](https://pytorch.org/docs/master/nn.html#torch.nn.CELU) activation.  ([#8551](https://github.com/pytorch/pytorch/pull/8551)).\r\n- [torch.nn.CTCLoss](https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss).  ([#9628]( https://github.com/pytorch/pytorch/pull/9628)).\r\n- [torch.diag_embed](https://pytorch.org/docs/master/torch.html#torch.diag_embed) ([#12447](https://github.com/pytorch/pytorch/pull/12447)).\r\n- `torch.roll` operator to match [numpy.roll](https://docs.scipy.org/doc/numpy/reference/generated/numpy.roll.html) ([#13261](https://github.com/pytorch/pytorch/pull/13261), [#13588](https://github.com/pytorch/pytorch/pull/13588), [#13874](https://github.com/pytorch/pytorch/pull/13874)).\r\n- [torch.chain_matmul](https://pytorch.org/docs/master/torch.html#torch.chain_matmul) for performing a chain of matrix multiplies ([#12380](https://github.com/pytorch/pytorch/pull/12380)).\r\n- [torch.finfo](https://pytorch.org/docs/master/type_info.html#torch.torch.finfo),\r\n  [torch.info](https://pytorch.org/docs/master/type_info.html#torch.torch.iinfo) to get more detailed information on a `dtype`, similar to [numpy.finfo](https://docs.scipy.org/doc/numpy/reference/generated/numpy.finfo.html) and [numpy.iinfo](https://docs.scipy.org/doc/numpy/reference/generated/numpy.iinfo.html) ([#12472](https://github.com/pytorch/pytorch/pull/12472)).\r\n- `Tensor.__cuda_array_interface__` to provide compatibility with numba and other CUDA projects ([#11984](https://github.com/pytorch/pytorch/pull/11984)).\r\n\r\n### New Distributions\r\n\r\n- [Weibull Distribution](https://pytorch.org/docs/master/distributions.html#torch.distributions.weibull.Weibull).  ([#9454](https://github.com/pytorch/pytorch/pull/9454)).\r\n- [NegativeBinomial Distribution](https://pytorch.org/docs/master/distributions.html#torch.distributions.negative_binomial.NegativeBinomial).     ([#9345](https://github.com/pytorch/pytorch/pull/9345)).\r\n- [torch.mvlgamma](https://pytorch.org/docs/master/torch.html#torch.mvlgamma) Multivariate Log-Gamma Distribution.  ([#9451](https://github.com/pytorch/pytorch/pull/9451)).\r\n\r\n### Sparse API Improvements\r\n- Implemented \"sparse gradient\" versions of some existing functions, see [sparse.mm](https://pytorch.org/docs/master/sparse.html#torch.sparse.mm), [sparse.sum](https://pytorch.org/docs/master/sparse.html#torch.sparse.sum), [sparse.addmm](https://pytorch.org/docs/master/sparse.html#torch.sparse.addmm) for details.  ([#14526](https://github.com/pytorch/pytorch/pull/14526), [#14661](https://github.com/pytorch/pytorch/pull/14661), [#12430](https://github.com/pytorch/pytorch/pull/12430), [#13345](https://github.com/pytorch/pytorch/pull/13345)).\r\n- `Tensor.to_sparse()` allows conversion from a dense tensor to a sparse tensor. ([#12171](https://github.com/pytorch/pytorch/pull/12171))\r\n- [torch.cat](https://pytorch.org/docs/master/torch.html#torch.cat) now supports sparse tensors.  ([#13761](https://github.com/pytorch/pytorch/pull/13761), [#13577](https://github.com/pytorch/pytorch/pull/13577)).\r\n- [torch.unsqueeze](https://pytorch.org/docs/master/torch.html#torch.unsqueeze) now works with sparse vectors (this also makes [torch.stack](https://pytorch.org/docs/master/torch.html#torch.stack) work out of the box).  ([#13760](https://github.com/pytorch/pytorch/pull/13760)).\r\n- Autograd is now supported on `values()` and [torch.sparse_coo_tensor](https://pytorch.org/docs/master/torch.html#torch.sparse_coo_tensor) (with indices and values tensors). E.g., `torch.sparse_coo_tensor(i, v).values().sum()` is differentiable w.r.t. `v`.  See the updated [torch.sparse](https://pytorch.org/docs/master/sparse.html#torch%20sparse) documentation for details.  ([#13001](https://github.com/pytorch/pytorch/pull/13001)).\r\n\r\n### Additions to existing Operators and Distributions\r\n\r\n- [torch.unique](https://pytorch.org/docs/master/torch.html#torch.unique) now accepts an optional `dim` argument.  ([#10423](https://github.com/pytorch/pytorch/pull/10423)).\r\n- [torch.norm](https://pytorch.org/docs/master/torch.html#torch.norm) now supports matrix norms. \r\n ([#11261](https://github.com/pytorch/pytorch/pull/11261)).\r\n- [torch.distributions.kl.kl_divergence](https://pytorch.org/docs/master/distributions.html#torch.distributions.kl.kl_divergence) now supports broadcasting.  ([#10533](https://github.com/pytorch/pytorch/pull/10533)).\r\n- [torch.distributions](https://pytorch.org/docs/master/distributions.html#module-torch.distributions) now support an `expand` method similar to [torch.Tensor.expand](https://pytorch.org/docs/master/tensors.html#torch.Tensor.expand).  For example: [torch.distributions.bernoulli.Bernoulli.expand](https://pytorch.org/docs/master/distributions.html#torch.distributions.bernoulli.Bernoulli.expand).  ([#11341](https://github.com/pytorch/pytorch/pull/11341)).\r\n- [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample) now support nearest neighbor interpolation and reflection padding.  ([#10051](https://github.com/pytorch/pytorch/pull/10051)).\r\n- [torch.mean](https://pytorch.org/docs/master/torch.html#torch.mean) now works across multiple dimensions. ([#14252](https://github.com/pytorch/pytorch/pull/14252)).\r\n- [torch.potrs](https://pytorch.org/docs/master/torch.html#torch.potrs) supports batching ([#13453](https://github.com/pytorch/pytorch/pull/13453)).\r\n- [torch.multiprocessing.spawn](https://pytorch.org/docs/master/multiprocessing.html#torch.multiprocessing.spawn) helper for spawning processes.  ([#13518](https://github.com/pytorch/pytorch/pull/13518)).\r\n- [torch.pow](https://pytorch.org/docs/master/torch.html#torch.pow) now allows taking derivatives when invoked with a python number as a base.  ([#12450](https://github.com/pytorch/pytorch/pull/12450)).\r\n- [Tensor.to](https://pytorch.org/docs/master/tensors.html#torch.Tensor.to) now supports a `copy` keyword argument.  ([#12571](https://github.com/pytorch/pytorch/pull/12571)).\r\n- [torch.softmax](https://pytorch.org/docs/master/nn.html#torch.nn.functional.softmax) and  and [torch.log_softmax](https://pytorch.org/docs/master/nn.html#torch.nn.functional.log_softmax) now support a `dtype` accumulation argument.  ([#11719](https://github.com/pytorch/pytorch/pull/11719)).\r\n- [torch.svd](https://pytorch.org/docs/master/torch.html#torch.svd) supports a `compute_uv` argument for optionally computing singular vectors ([#12517](https://github.com/pytorch/pytorch/pull/12517)).\r\n- [torch.inverse](https://pytorch.org/docs/master/torch.html#torch.inverse) now supports batches of tensors ([#9949](https://github.com/pytorch/pytorch/pull/9949)).\r\n- [autograd.profiler](https://pytorch.org/docs/master/autograd.html#profiler) shows demangled names on nvtx ranges.  ([#13154](https://github.com/pytorch/pytorch/pull/13154)).\r\n\r\n## Bug Fixes\r\n\r\n### Serious\r\n\r\n- [torch.nn.functional.softmin](https://pytorch.org/docs/master/nn.html#torch.nn.functional.softmin) was using the incorrect formula in 0.4.1  ([#10066](https://github.com/pytorch/pytorch/pull/10066)).\r\n- [torch.as_strided](https://pytorch.org/docs/master/torch.html#torch.as_strided) backwards (called via `view`) was incorrect with overlapping data locations.  ([#9538](https://github.com/pytorch/pytorch/pull/9538)).\r\n- Pointwise losses (e.g. [torch.nn.MSELoss](https://pytorch.org/docs/master/nn.html#torch.nn.MSELoss)) were sometimes using the wrong `reduction` method.  ([#10018](https://github.com/pytorch/pytorch/pull/10018)).\r\n- [torch.from_numpy](https://pytorch.org/docs/master/torch.html#torch.from_numpy) was not handling big-endian dtypes correctly.  ([#9508](https://github.com/pytorch/pytorch/pull/9508)).\r\n- [torch.multiprocessing](https://pytorch.org/docs/master/multiprocessing.html#module-torch.multiprocessing) now correctly handles CUDA tensors, requires_grad settings, and hooks.  ([#10220](https://github.com/pytorch/pytorch/pull/10220)).\r\n- `__rsub__` now works properly when the CUDA device is not 0.  ([#12956](https://github.com/pytorch/pytorch/pull/12956)).\r\n- Fix memory leak during packing in tuples ([#13305](https://github.com/pytorch/pytorch/pull/13305)).\r\n- [DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) fixed a couple of issues resulting in hangs.  ([#11985](https://github.com/pytorch/pytorch/pull/11985), [#12700](https://github.com/pytorch/pytorch/pull/12700)).\r\n- [torch.multinomial](https://pytorch.org/docs/master/torch.html#torch.multinomial) `replacement=False` will not properly throw an error message when there are no more categories to select ([#12490](https://github.com/pytorch/pytorch/pull/12490)).\r\n- [torch.masked_fill_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.masked_fill_) now works properly on non-contiguous tensor inputs ([#12594](https://github.com/pytorch/pytorch/pull/12594)).\r\n- `Tensor.__delitem__`: fixed a segmentation fault on  ([#12726](https://github.com/pytorch/pytorch/pull/12726)).\r\n\r\n### Backwards Compatibility\r\n\r\n- [torch.nn.Module](https://pytorch.org/docs/master/nn.html#torch.nn.Module) `load_from_state_dict` now correctly handles 1-dimensional vs 0-dimensional tensors saved from 0.3 versions.  ([#9781](https://github.com/pytorch/pytorch/pull/9781)).\r\n- Fix `RuntimeError: storages don't support slicing` when loading models saved with PyTorch 0.3.  ([#11314](https://github.com/pytorch/pytorch/pull/11314)).\r\n- [BCEWithLogitsLoss](https://pytorch.org/docs/master/nn.html?highlight=logits#torch.nn.BCEWithLogitsLoss): fixed an issue with legacy `reduce` parameter.  ([#12689](https://github.com/pytorch/pytorch/pull/12689)).\r\n\r\n### Correctness\r\n\r\n- [torch.nn.Dropout](https://pytorch.org/docs/master/nn.html#torch.nn.Dropout) fused kernel could change parameters in `eval` mode.  ([#10621](https://github.com/pytorch/pytorch/pull/10621)).\r\n- [torch.unbind](https://pytorch.org/docs/master/torch.html#torch.unbind) backwards has been fixed.  ([#9995](https://github.com/pytorch/pytorch/pull/9995)).\r\n- Fix a bug in sparse matrix-matrix multiplication when a sparse matrix is coalesced then transposed.  ([#10496](https://github.com/pytorch/pytorch/pull/10496)).\r\n- [torch.bernoulli](https://pytorch.org/docs/master/torch.html#torch.bernoulli) now handles `out=` parameters correctly, handles expanded tensors correctly, and has corrected argument validity checks on CPU.  ([#10273](https://github.com/pytorch/pytorch/pull/10273)).\r\n- [torch.Tensor.normal_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.normal_) could give incorrect results on CPU.  ([#10846](https://github.com/pytorch/pytorch/pull/10846)).\r\n- [torch.tanh](https://pytorch.org/docs/master/torch.html#torch.tanh) could return incorrect results on non-contiguous tensors.  ([#11226](https://github.com/pytorch/pytorch/pull/11226)).\r\n- [torch.log](https://pytorch.org/docs/master/torch.html#torch.log) on an expanded `Tensor` gave incorrect results on CPU.  ([#10269](https://github.com/pytorch/pytorch/pull/10269)).\r\n- [torch.logsumexp](https://pytorch.org/docs/master/torch.html#torch.logsumexp) now correctly modifies the `out` parameter if it is given.  ([#9755](https://github.com/pytorch/pytorch/pull/9755)).\r\n- [torch.multinomial](https://pytorch.org/docs/master/torch.html#torch.multinomial) with `replacement=True` could select 0 probability events on CUDA.  ([#9960](https://github.com/pytorch/pytorch/pull/9960)).\r\n- [torch.nn.ReLU](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU) will now properly propagate `NaN`. \r\n ([#10277](https://github.com/pytorch/pytorch/pull/10277)).\r\n- [torch.max](https://pytorch.org/docs/master/torch.html#torch.max) and [torch.min](https://pytorch.org/docs/master/torch.html#torch.min) could return incorrect values on input containing `inf` / `-inf`.  ([#11091](https://github.com/pytorch/pytorch/pull/11091)).\r\n- Fixed an issue with calculated output sizes of `torch.nn.Conv` modules with `stride` and `dilation`.  ([#9640](https://github.com/pytorch/pytorch/pull/9640)).\r\n- [torch.nn.EmbeddingBag](https://pytorch.org/docs/master/nn.html#torch.nn.EmbeddingBag) now correctly returns vectors filled with zeros for empty bags on CUDA.  ([#11740](https://github.com/pytorch/pytorch/pull/11740)).\r\n- Use integer math to compute output size of pooling operations ([#14405](https://github.com/pytorch/pytorch/pull/14405)).\r\n- Fix sum() on fp16 ([#13926](https://github.com/pytorch/pytorch/pull/13926)).\r\n- Remove CUDNN_BATCHNORM_SPATIAL_PERSISTENT mode for accuracy ([#13844](https://github.com/pytorch/pytorch/pull/13844)).\r\n- fix stability in bce with pos_weight formula ([#13863](https://github.com/pytorch/pytorch/pull/13863)).\r\n- Fix torch.dist for infinity, zero and minus infinity norms ([#13713](https://github.com/pytorch/pytorch/pull/13713)).\r\n- Give broadcast_coalesced tensors different version counters ([#13594](https://github.com/pytorch/pytorch/pull/13594)).\r\n- Fix flip() shape bug in CPU ([#13344](https://github.com/pytorch/pytorch/pull/13344)).\r\n- Fix more spectral norm bugs ([#13350](https://github.com/pytorch/pytorch/pull/13350)).\r\n- Fix handling of single input in gradcheck ([#13543](https://github.com/pytorch/pytorch/pull/13543)).\r\n- [torch.cuda.manual_seed](https://pytorch.org/docs/master/cuda.html#torch.cuda.manual_seed) now also sets the philox seed and offset. ([#12677](https://github.com/pytorch/pytorch/pull/12677)).\r\n- [utils.bottleneck](https://pytorch.org/docs/master/bottleneck.html) fix ZeroDivisionError([#11987](https://github.com/pytorch/pytorch/pull/11987)).\r\n- Disable [hook](https://pytorch.org/docs/master/autograd.html#torch.Tensor.register_hook) serialization ([#11705](https://github.com/pytorch/pytorch/pull/11705)).\r\n- [torch.norm](https://pytorch.org/docs/master/torch.html#torch.norm): fix negative infinity norm ([#12722](https://github.com/pytorch/pytorch/pull/12722)).\r\n- Fix [torch.isfinite](https://pytorch.org/docs/master/torch.html#torch.isfinite) for integer input ([#12750](https://github.com/pytorch/pytorch/pull/12750)).\r\n- [ConvTranspose3d](https://pytorch.org/docs/master/nn.html#torch.nn.ConvTranspose3d) fix `output_size` calculation ([#12952](https://github.com/pytorch/pytorch/pull/12952)).\r\n- [torch.randperm](https://pytorch.org/docs/master/torch.html#torch.randperm): properly use RNG mutex on CPU ([#13832](https://github.com/pytorch/pytorch/pull/13832))\r\n\r\n### Error checking\r\n\r\n- [torch.gesv](https://pytorch.org/docs/master/torch.html#torch.gesv) now properly checks LAPACK errors.  ([#11634](https://github.com/pytorch/pytorch/pull/11634)).\r\n- Fixed an issue where extra positional arguments were accepted (and ignored) in Python functions calling into C++.  ([#10499](https://github.com/pytorch/pytorch/pull/10499)).\r\n- legacy `Tensor` constructors (e.g. `torch.FloatTensor(...)`) now correctly check their `device` argument. \r\n ([#11669](https://github.com/pytorch/pytorch/pull/11669)).\r\n- Properly check that `out` parameter is a CPU `Tensor` for CPU unary ops.  ([#10358](https://github.com/pytorch/pytorch/pull/10358)).\r\n- [torch.nn.InstanceNorm1d](https://pytorch.org/docs/master/nn.html#torch.nn.InstanceNorm1d) now correctly accepts 2 dimensional inputs.  ([#9776](https://github.com/pytorch/pytorch/issues/9776)).\r\n- [torch.nn.Module.load_state_dict](https://pytorch.org/docs/master/nn.html#torch.nn.Module.load_state_dict) had an incorrect error message.  ([#11200](https://github.com/pytorch/pytorch/pull/11200)).\r\n- [torch.nn.RNN](https://pytorch.org/docs/master/nn.html#torch.nn.RNN) now properly checks that inputs and hidden_states are on the same devices.  ([#10185](https://github.com/pytorch/pytorch/pull/10185)).\r\n- [torch.nn.utils.rnn.pack_padded_sequence](https://pytorch.org/docs/master/nn.html#torch.nn.utils.rnn.pack_padded_sequence) now properly checks for out-of-order length.  ([#13933](https://github.com/pytorch/pytorch/pull/13933)).\r\n- [torch.bmm](https://pytorch.org/docs/master/torch.html#torch.bmm) now properly checks that its Tensor arguments are on compatible devices.  ([#12434](https://github.com/pytorch/pytorch/pull/12434)).\r\n- [Conv2d](https://pytorch.org/docs/master/nn.html#torch.nn.Conv2d): fixed incorrect error message for too-large kernel size ([#12791](https://github.com/pytorch/pytorch/pull/12791)).\r\n- [Tensor.expand](https://pytorch.org/docs/master/tensors.html#torch.Tensor.expand) error message now includes complete sizes.  ([#13124](https://github.com/pytorch/pytorch/pull/13124)).\r\n- Improve CUDA out-of-memory error message.  ([#13751](https://github.com/pytorch/pytorch/pull/13751)).\r\n- [torch.arange](https://pytorch.org/docs/master/torch.html#torch.arange) now properly checks for invalid ranges.  ([#13915](https://github.com/pytorch/pytorch/pull/13915))\r\n\r\n### Miscellaneous\r\n\r\n- [torch.utils.data.DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) could hang if it was not completely iterated.  ([#10366](https://github.com/pytorch/pytorch/pull/10366)).\r\n- Fixed a segfault when grad to a hook function is `None`.  ([#12028](https://github.com/pytorch/pytorch/pull/12028)).\r\n- Fixed a segfault in backwards with [torch.nn.PReLU](https://pytorch.org/docs/master/nn.html#torch.nn.PReLU) when the input does not require grad.  ([#11758](https://github.com/pytorch/pytorch/pull/11758)).\r\n- `dir(torch)` has been fixed with Python 3.7.  ([#10271](https://github.com/pytorch/pytorch/pull/10271)).\r\n- Fixed a device-side assert in [torch.multinomial](https://pytorch.org/docs/master/torch.html#torch.multinomial) when `replacement=False` and the input has fewer  nonzero elements than `num_samples`.  ([#11933](https://github.com/pytorch/pytorch/pull/11933)).\r\n- Can now properly assign a `torch.float16` dtype tensor to `.grad`.  ([#11781](https://github.com/pytorch/pytorch/pull/11781)).\r\n- Fixed `can only join a started process` error with [torch.utils.data.DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader).  ([#11432](https://github.com/pytorch/pytorch/pull/11432)).\r\n- Prevent `unexpected exit` in [torch.utils.data.DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) on `KeyboardInterrupt`.  ([#11718](https://github.com/pytorch/pytorch/pull/11718)).\r\n- [torch.einsum](https://pytorch.org/docs/master/torch.html#torch.einsum) now handles spaces consistently.  ([#9994](https://github.com/pytorch/pytorch/pull/9994)).\r\n- Fixed a broadcasting bug in [torch.distributions.studentT.StudentT](https://pytorch.org/docs/master/distributions.html#torch.distributions.studentT.StudentT).  ([#12148](https://github.com/pytorch/pytorch/pull/12148/)).\r\n- fix a printing error with large non-contiguous tensors.  ([#10405](https://github.com/pytorch/pytorch/pull/10405)).\r\n- allow empty index for scatter_* methods ([#14077](https://github.com/pytorch/pytorch/pull/14077))\r\n- [torch.nn.ModuleList](https://pytorch.org/docs/master/nn.html#torch.nn.ModuleList) now handles negative indices.  ([#13102](https://github.com/pytorch/pytorch/pull/13102)).\r\n- Minor fix to reenable nvtx sequence numbers for the forward methods of custom (Python) autograd functions ([#13876](https://github.com/pytorch/pytorch/pull/13876))\r\n- Fix handling all empty bags in CUDA embedding bag ([#13483](https://github.com/pytorch/pytorch/pull/13483))\r\n- Fix half_tensor.bernoulli_(double) ([#13474](https://github.com/pytorch/pytorch/pull/13474))\r\n- Fix cuda out of memory test ([#13864](https://github.com/pytorch/pytorch/pull/13864))\r\n- Implement NaN-propagating max/min on Vec256.  ([#13399](https://github.com/pytorch/pytorch/pull/13399)).\r\n- Fix refcounting in anomaly metadata ([#13249](https://github.com/pytorch/pytorch/pull/13249))\r\n- Fix pointwise loss broadcast ([#12996](https://github.com/pytorch/pytorch/pull/12996))\r\n- Fix copying a `nn.Parameter` ([#12886](https://github.com/pytorch/pytorch/pull/12886))\r\n\r\n## Other Improvements\r\n\r\n- [torch.cuda](https://pytorch.org/docs/master/cuda.html#module-torch.cuda) functions and [torch.nn.parallel.data_parallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.data_parallel) now accept [torch.device](https://pytorch.org/docs/master/tensor_attributes.html#torch.torch.device) objects in addition to integer device ids.  ([#10833](https://github.com/pytorch/pytorch/pull/10833), [#10189](https://github.com/pytorch/pytorch/pull/10189)).\r\n- [torch.nn.parallel.data_parallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.data_parallel) now accepts `torch.device` inputs.  ([#10189](https://github.com/pytorch/pytorch/pull/10189)).\r\n- [torch.nn.functional.log_softmax](https://pytorch.org/docs/master/nn.html#torch.nn.functional.log_softmax) is now more numerically stable.  ([#11866](https://github.com/pytorch/pytorch/pull/11866)).\r\n- Improve printing of sparse tensors and `grad_fns`.  ([#10181](https://github.com/pytorch/pytorch/pull/10181)).\r\n- Only involve CUDA device in CUDA -> CPU copy.  ([#11592](https://github.com/pytorch/pytorch/pull/11592)).\r\n- Accept numpy floating-point scalars as doubles more consistently.  ([#9659](https://github.com/pytorch/pytorch/pull/9659)).\r\n- sparse-to-sparse [copy_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.copy_) is now supported.  ([#9005](https://github.com/pytorch/pytorch/pull/9005)).\r\n- [torch.bincount](https://pytorch.org/docs/master/torch.html#torch.bincount) now supports 0 element inputs.  ([#9757](https://github.com/pytorch/pytorch/pull/9757)).\r\n- [torch.nn.functional.conv2d](https://pytorch.org/docs/master/nn.html#torch.nn.functional.conv2d) error message have been improved.  ([#11053](https://github.com/pytorch/pytorch/pull/11053)).\r\n- Allow conversion of `np.int64` to PyTorch scalar.  ([#9225](https://github.com/pytorch/pytorch/pull/9225)).\r\n- [torch.einsum](https://pytorch.org/docs/master/torch.html#torch.einsum) now handles varargs. \r\n ([#10067](https://github.com/pytorch/pytorch/pull/10067)).\r\n- [torch.symeig](https://pytorch.org/docs/master/torch.html#torch.symeig) now returns 0-filled eigenvectors when `eigenvectors=False` is passed on CUDA rather than uninitialized data.  ([#10645](https://github.com/pytorch/pytorch/pull/10645)).\r\n- [torch.utils.checkpoint](https://pytorch.org/docs/master/checkpoint.html): added an pption to preserve bitwise accuracy of gradient checkpointed vs non-checkpointed dropout.  ([#14253](https://github.com/pytorch/pytorch/pull/14253)).\r\n\r\n## Deprecations\r\n- Removed support for C extensions. Please use [cpp extensions](https://pytorch.org/docs/master/cpp_extension.html). ([#12122](https://github.com/pytorch/pytorch/pull/12122))\r\n- Delete `torch.utils.trainer` ([#12487](https://github.com/pytorch/pytorch/pull/12487))\r\n\r\n### CPP Extensions\r\n\r\n- The `torch/torch.h` header is deprecated in favor of `torch/extension.h`, which should be used in all C++ extensions going forward. Including `torch/torch.h` from a C++ extension will produce a warning. It is safe to batch replace `torch/torch.h` with `torch/extension.h`.\r\n- Usage of the following functions in C++ extensions is also deprecated:\r\n    - `torch::set_requires_grad`. Replacement: `at::Tensor`  now has a `set_requires_grad` method.\r\n    - `torch::requires_grad`. Replacement: `at::Tensor`  now has a `requires_grad` method.\r\n    - `torch::getVariableType`. Replacement: None.\r\n- Fix version.groups() ([#14505](https://github.com/pytorch/pytorch/pull/14505))\r\n- Allow building libraries with setuptools that dont have abi suffix ([#14130](https://github.com/pytorch/pytorch/pull/14130))\r\n- Missing .decode() after check_output in cpp_extensions ([#13935](https://github.com/pytorch/pytorch/pull/13935))\r\n\r\n### torch.distributed \r\n- the old (THD-backed) [torch.distributed]((https://pytorch.org/docs/master/distributed.html)) package is deprecated but still available at [torch.distributed.deprecated](https://pytorch.org/docs/master/distributed_deprecated.html).\r\n- The old (THD-backed) [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel) is deprecated but still available at `torch.nn.parallel.deprecated.DistributedDataParallel`.\r\n\r\n## Performance\r\n\r\n- \"Advanced Indexing\" performance has improved on both CPU and GPU.  ([#13420](https://github.com/pytorch/pytorch/pull/13420))\r\n- [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample) on CPU now uses vectorized operation and is now 2x~7x faster with AVX2 enabled CPUs.  ([#9961](https://github.com/pytorch/pytorch/pull/9961)).\r\n- [torch.norm](https://pytorch.org/docs/master/torch.html#torch.norm) has been vectorized and parallelized on CPU.  ([#11565](https://github.com/pytorch/pytorch/pull/11565)).\r\n- [torch.max](https://pytorch.org/docs/master/torch.html#torch.max) and [torch.min](https://pytorch.org/docs/master/torch.html#torch.min) has been parallelized on CPU.  ([#10343](https://github.com/pytorch/pytorch/pull/10343)).\r\n- [torch.nn.Threshold](https://pytorch.org/docs/master/nn.html#torch.nn.Threshold) and [torch.nn.ReLU](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU) have been parallelized on CPU. ([#13182](https://github.com/pytorch/pytorch/pull/13182))\r\n- [torch.Tensor.masked_fill_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.masked_fill_) has been parallelized on CPU.  ([#11359](https://github.com/pytorch/pytorch/pull/11359)).\r\n- [Tensor.sparse_mask](https://pytorch.org/docs/master/tensors.html#Tensor.sparse_mask) has been parallelized on CPU.  ([#13290](https://github.com/pytorch/pytorch/pull/13290)).\r\n- [torch.nn.PReLU](https://pytorch.org/docs/master/nn.html#torch.nn.PReLU) has been sped up on both CPU and GPU.\r\n([#11758](https://github.com/pytorch/pytorch/pull/11758)).\r\n- [torch.nn.KLDivLoss](https://pytorch.org/docs/master/nn.html#torch.nn.KLDivLoss) has been sped up on both CPU and GPU.  ([#10336](https://github.com/pytorch/pytorch/pull/10336)).\r\n- [torch.svd](https://pytorch.org/docs/master/torch.html#torch.svd) has been sped up on both CPU and GPU.\r\n([#11194](https://github.com/pytorch/pytorch/pull/11194)).\r\n- [torch.einsum](https://pytorch.org/docs/master/torch.html#torch.einsum) has been greatly sped up on CPU.\r\n([#11292](https://github.com/pytorch/pytorch/pull/11292)).\r\n- [torch.clamp](https://pytorch.org/docs/master/torch.html#torch.clamp) no longer does unnecessary copying.   ([#10352](https://github.com/pytorch/pytorch/pull/10352)).\r\n- [torch.add](https://pytorch.org/docs/master/torch.html#torch.add), [torch.sub](https://pytorch.org/docs/master/torch.html#torch.sub), [torch.mul](https://pytorch.org/docs/master/torch.html#torch.mul), [torch.div](https://pytorch.org/docs/master/torch.html#torch.div) are much faster for non-contiguous tensors on GPU.  ([#8919](https://github.com/pytorch/pytorch/pull/8919)).\r\n- [torch.nn.RNN](https://pytorch.org/docs/master/nn.html#torch.nn.RNN) and related Modules have been ported to C++ and are more performant.  ([#10305](https://github.com/pytorch/pytorch/pull/10305), [#10481](https://github.com/pytorch/pytorch/pull/10481)).\r\n- [autograd.Profiler](https://pytorch.org/docs/master/autograd.html#torch.autograd.profiler.profile) now has lower overhead.   ([#10969](https://github.com/pytorch/pytorch/pull/10969), [#11773](https://github.com/pytorch/pytorch/pull/11773)).\r\n- Printing large Tensors is now faster.  ([#14418](https://github.com/pytorch/pytorch/pull/14418)).\r\n\r\n## Documentation Improvements\r\n\r\n- [Reproducibility](https://pytorch.org/docs/master/notes/randomness.html) note added.  ([#11329](https://github.com/pytorch/pytorch/pull/11329)).\r\n- [CPP Extensions](https://pytorch.org/docs/master/cpp_extension.html) have [improved online documentation](https://pytorch.org/cppdocs).  Authors of C++ extensions may want to consult this documentation when writing new extensions.\r\n- [torch.Tensor.flatten](https://pytorch.org/docs/master/tensors.html#torch.Tensor.flatten) is now documented. \r\n ([#9876](https://github.com/pytorch/pytorch/issues/9876)).\r\n- [torch.digamma](https://pytorch.org/docs/master/torch.html#torch.digamma) is now documented.  ([#10967](https://github.com/pytorch/pytorch/pull/10967)).\r\n- [torch.allclose](https://pytorch.org/docs/master/torch.html#torch.allclose) is now documented.  ([#11185](https://github.com/pytorch/pytorch/pull/11185)).\r\n- [torch.eig](https://pytorch.org/docs/master/torch.html#torch.eig) return format clarified.  ([#10315](https://github.com/pytorch/pytorch/pull/10315)).\r\n- [torch.as_tensor](https://pytorch.org/docs/master/torch.html#torch.as_tensor) now includes a proper example.  ([#10309](https://github.com/pytorch/pytorch/pull/10309)).\r\n- [torch.sparse_coo_tensor](https://pytorch.org/docs/master/torch.html#torch.sparse_coo_tensor) now explains uncoalesced behavior.  ([#10308](https://github.com/pytorch/pytorch/pull/10308)).\r\n- [torch.fft](https://pytorch.org/docs/master/torch.html#torch.fft) equation has been corrected.  ([#10760](https://github.com/pytorch/pytorch/pull/10760)).\r\n- [torch.nn.LSTM](https://pytorch.org/docs/master/nn.html#torch.nn.LSTM) behavior has been clarified in the multilayer case.  ([#11896](https://github.com/pytorch/pytorch/pull/11896)).\r\n- [torch.nn.functional.dropout](https://pytorch.org/docs/master/nn.html#torch.nn.functional.dropout) documentation has been clarified.  ([#10417](https://github.com/pytorch/pytorch/pull/10417)).\r\n- [torch.nn.functional.pad](https://pytorch.org/docs/master/nn.html#torch.nn.functional.pad) documentation has been clarified.  ([#11623](https://github.com/pytorch/pytorch/pull/11623)).\r\n- Add documentation about Tensor properties `device`, `is_cuda`, `requires_grad`, `is_leaf` and `grad`. \r\n ([#14339](https://github.com/pytorch/pytorch/pull/14339))\r\n- [torch.sparse](https://pytorch.org/docs/master/sparse.html#torch%20sparse) documentation updated ([#12221](https://github.com/pytorch/pytorch/pull/12221)).\r\n- Added a quick rundown of codebase structure. ([#12693](https://github.com/pytorch/pytorch/pull/12693))\r\n- [torch.cat](https://pytorch.org/docs/master/torch.html#torch.cat) corrected parameter name to `tensors` from `seq`.  ([#12741](https://github.com/pytorch/pytorch/pull/12741))\r\n- Warn that tensor.resize_() resets strides ([#12816](https://github.com/pytorch/pytorch/pull/12816))", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.0.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.0.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.0.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/14415751", "dateCreated": "2018-12-06T23:44:56Z", "datePublished": "2018-12-07T19:19:01Z"}, {"tagName": "v1.0rc1", "name": "torch.jit, C++ API, c10d distributed", "authorName": "soumith", "authorType": "User", "body": "**This is a pre-release preview, do not rely on the tag to have a fixed set of commits, or rely on the tag for anything practical / important**\r\n\r\n## Table of Contents\r\n\r\n* [Highlights](#highlights)\r\n  * [JIT](#jit)\r\n  * [torch.distributed new \"C10D\" library](#torchdistributed-new-c10d-library)\r\n  * [C++ Frontend [API Unstable]](#c-frontend-api-unstable)\r\n* [Breaking Changes](#breaking-changes)\r\n  * [Additional New Features](#additional-new-features)\r\n    * [N-dimensional empty tensors](#n-dimensional-empty-tensors)\r\n  \t* [New Operators](#new-operators)\r\n  \t* [New Distributions](#new-distributions)\r\n  \t* [Additions to existing Operators and Distributions](#additions-to-existing-operators-and-distributions)\r\n* [Bug Fixes](#bug-fixes)\r\n  * [Serious](#serious)\r\n  * [Backwards Compatibility](#backwards-compatibility)\r\n  * [Correctness](#correctness)\r\n  * [Error checking](#error-checking)\r\n  * [Miscellaneous](#miscellaneous)\r\n* [Other Improvements](#other-improvements)\r\n* [Deprecations](#deprecations)\r\n  * [CPP Extensions](#cpp-extensions)\r\n* [Performance](#performance)\r\n* [Documentation Improvements](#documentation-improvements)\r\n\r\n## Highlights\r\n\r\n### JIT\r\n\r\nThe JIT is a set of compiler tools for bridging the gap between research in PyTorch\r\nand production. It includes a language called Torch Script (don't worry it is a subset of Python,\r\nso you'll still be writing Python), and two ways in which you can make your existing code compatible with the JIT. \r\nTorch Script code can be aggressively optimized and it can be serialized for later use in our new C++ API, which doesn't depend on Python at all.\r\n\r\n```python\r\n# Write in Python, run anywhere!\r\n@torch.jit.script\r\ndef RNN(x, h, W_h, U_h, b_h):\r\n  y = []\r\n  for t in range(x.size(0)):\r\n    h = torch.tanh(x[t] @ W_h + h @ U_h + b_h)\r\n    y += [h]\r\n  return torch.stack(y), h\r\n```\r\n\r\nAs an example, see a tutorial on [deploying a seq2seq model](https://pytorch.org/tutorials/beginner/deploy_seq2seq_hybrid_frontend_tutorial.html),\r\n[loading an exported model from C++](https://pytorch.org/tutorials/advanced/cpp_export.html), or [browse the docs](https://pytorch.org/docs/master/jit.html).\r\n\r\n### torch.distributed new \"C10D\" library\r\n\r\nThe [torch.distributed](https://pytorch.org/docs/master/distributed.html) package and [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel) module are backed by the new \"C10D\" library.  The main highlights of the new library are:\r\n* C10D is performance driven and operates entirely asynchronously for all backends: `Gloo`, `NCCL`, and `MPI`.\r\n* Significant Distributed Data Parallel performance improvements especially for slower network like ethernet-based hosts\r\n* Adds async support for all distributed collective operations in the [torch.distributed](https://pytorch.org/docs/master/distributed.html) package.\r\n* Adds [send](https://pytorch.org/docs/master/distributed.html#torch.distributed.send) and [recv](https://pytorch.org/docs/master/distributed.html#torch.distributed.recv) support in the Gloo backend\r\n\r\n### C++ Frontend _**[API Unstable]**_.\r\n\r\nThe C++ frontend is a pure C++ interface to the PyTorch backend that follows the API and architecture of the established Python frontend. It is intended to enable research in high performance, low latency and bare metal C++ applications. It provides equivalents to `torch.nn`, `torch.optim`, `torch.data` and other components of the Python frontend. Here is a minimal side-by-side comparison of the two language frontends:\r\n\r\n<p align=\"center\">\r\n  <table align=\"center\">\r\n    <tr><th>Python</th><th>C++</th></tr>\r\n    <tr valign=\"top\">\r\n      <td><sub><pre lang=\"python\">\r\nimport torch\r\n<br>\r\nmodel = torch.nn.Linear(5, 1)\r\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\r\nprediction = model.forward(torch.randn(3, 5))\r\nloss = torch.nn.functional.mse_loss(prediction, torch.ones(3, 1))\r\nloss.backward()\r\noptimizer.step()\r\n      </pre></sub></td>\r\n      <td><sub><pre lang=\"cpp\">\r\n#include &lt;torch/torch.h&gt;\r\n<br>\r\ntorch::nn::Linear model(5, 1);\r\ntorch::optim::SGD optimizer(model->parameters(), /*lr=*/0.1);\r\ntorch::Tensor prediction = model->forward(torch::randn({3, 5}));\r\nauto loss = torch::mse_loss(prediction, torch::ones({3, 1}));\r\nloss.backward();\r\noptimizer.step();\r\n      </pre></sub></td>\r\n    </tr>\r\n  </table>\r\n</p>\r\n\r\nWe are releasing the C++ frontend marked as \"API Unstable\" as part of PyTorch 1.0. This means it is ready to be used for your research application, but still has some open construction sites that will stabilize over the next month or two. **Some parts of the API may undergo breaking changes during this time**.\r\n\r\nSee https://pytorch.org/cppdocs for detailed documentation on the greater PyTorch C++ API as well as the C++ frontend.\r\n\r\n## Breaking Changes\r\n\r\n* Indexing a 0-dimensional tensor will now throw an error instead of warn.  Use [tensor.item()](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.item) instead.  ([#11679](https://github.com/pytorch/pytorch/pull/11679)).\r\n* [torch.legacy](https://pytorch.org/docs/stable/legacy.html) is removed.  ([#11823](https://github.com/pytorch/pytorch/pull/11823)).\r\n* _torch.masked_copy__ is removed, use [torch.masked_scatter_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_scatter_) instead.  ([#9817](https://github.com/pytorch/pytorch/issues/9817)).\r\n* Operations that result in 0 element tensors may return changed shapes.\r\n    * Before: all 0 element tensors would collapse to shape _(0,)_.  For example, [torch.nonzero](https://pytorch.org/docs/stable/torch.html#torch.nonzero) is documented to return a tensor of shape _(n,z)_, where n = number of nonzero elements and z = dimensions of the input, but would always return a Tensor of shape _(0,) when no nonzero elements existed.\r\n    * Now: Operations return their documented shape.\r\n      ```\r\n      # Previously: all 0-element tensors are collapsed to shape (0,)\r\n      >>> torch.nonzero(torch.zeros(2, 3))\r\n      tensor([], dtype=torch.int64)\r\n\r\n      # Now, proper shape is returned\r\n      >>> torch.nonzero(torch.zeros(2, 3))\r\n      tensor([], size=(0, 2), dtype=torch.int64)\r\n      ```\r\n* Sparse tensor _indices_ and _values_ shape invariants are changed to be more consistent in the case of 0-element tensors.  See link for more details.  ([#9279](https://github.com/pytorch/pytorch/pull/9279)).\r\n* [torch.distributed](https://pytorch.org/docs/master/distributed.html): the TCP backend is removed, we recommend to use Gloo and MPI backends for CPU collectives and NCCL backend for GPU collectives.\r\n* Some inter-type operations (e.g. `*`) between `torch.Tensors` and NumPy arrays will now favor dispatching to the `torch` variant.  This may result in different return types.  ([#9651](https://github.com/pytorch/pytorch/pull/9651)).\r\n* Implicit `numpy` conversion no longer implicitly moves a tensor to CPU.  Therefore, you may have to explicitly move a CUDA tensor to CPU (`tensor.to('cpu')`) before an implicit conversion. ([#10553](https://github.com/pytorch/pytorch/pull/10553)).\r\n* [torch.randint](https://pytorch.org/docs/stable/torch.html#torch.randint) now defaults to using dtype _torch.int64_ rather than the default floating-point dtype.  ([#11040](https://github.com/pytorch/pytorch/pull/11040)).\r\n* [torch.tensor](https://pytorch.org/docs/master/torch.html#torch.tensor) function with a `Tensor` argument now returns a `detached` Tensor (i.e. a Tensor where `grad_fn` is `None`).  This more closely aligns with the intent of the function, which is to return a Tensor with copied data and no history.  ([#11061](https://github.com/pytorch/pytorch/pull/11061), \r\n[#11815](https://github.com/pytorch/pytorch/pull/11815)).\r\n* [torch.nn.functional.multilabel_soft_margin_loss](https://pytorch.org/docs/master/nn.html#torch.nn.functional.multilabel_soft_margin_loss) now returns Tensors of shape `(N,)` instead of `(N, C)` to match the behavior of [torch.nn.MultiMarginLoss](https://pytorch.org/docs/master/nn.html#torch.nn.MultiMarginLoss).  In addition, it is more numerically stable. \r\n ([#9965](https://github.com/pytorch/pytorch/pull/9965)).\r\n* The result type of a _torch.float16_ 0-dimensional tensor and a integer is now _torch.float16_ (was _torch.float32_ or _torch.float64_ depending on the dtype of the integer).  ([#11941](https://github.com/pytorch/pytorch/pull/11941)).\r\n* [Dirichlet](https://pytorch.org/docs/stable/distributions.html#torch.distributions.dirichlet.Dirichlet) and [Categorical](https://pytorch.org/docs/stable/distributions.html#torch.distributions.categorical.Categorical) distributions no longer accept scalar parameters.  ([#11589](https://github.com/pytorch/pytorch/pull/11589)).\r\n* _[CPP Extensions](https://pytorch.org/docs/master/cpp_extension.html)_: Deprecated factory functions that accept a type as the first argument and a size as a second argument argument have been removed.  Instead, use the new-style factory functions that accept the size as the first argument and [`TensorOptions`](https://github.com/pytorch/pytorch/blob/ab9a5976a025b3a56cfc64b86981ad52f8fe4137/aten/src/ATen/core/TensorOptions.h#L14) as the last argument. For example, replace your call to `at::ones(torch::CPU(at::kFloat)), {2, 3})` with `torch::ones({2, 3}, at::kCPU)`. This applies to the following functions:\r\n    - `arange`, `empty`, `eye`, `full`, `linspace`, `logspace`, `ones`, `rand`, `randint`, `randn`, `randperm`, `range`, `zeros`.\r\n\r\n## Additional New Features\r\n\r\n### N-dimensional empty tensors\r\n\r\n* Tensors with 0 elements can now have an arbitrary number of dimensions and support indexing and other torch operations; previously, 0 element tensors were limited to shape _(0,)_.  ([#9947](https://github.com/pytorch/pytorch/pull/9947)).  _Example_:\r\n  ```\r\n  >>> torch.empty((0, 2, 4, 0), dtype=torch.float64)\r\n  tensor([], size=(0, 2, 4, 0), dtype=torch.float64)\r\n  ```\r\n\r\n### New Operators\r\n\r\n\r\n- [torch.argsort](https://pytorch.org/docs/master/torch.html#torch.argsort) similar to [numpy.argsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html). \r\n ([#9600](https://github.com/pytorch/pytorch/pull/9600)).\r\n- [torch.pdist](https://pytorch.org/docs/master/torch.html#torch.pdist) similar to [scipy.spatial.distance.pdist](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html).  ([#10782](https://github.com/pytorch/pytorch/pull/10782)).\r\n- [torch.tensordot](https://pytorch.org/docs/master/torch.html#torch.tensordot) similar to [numpy.tensordot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.tensordot.html).  ([#10025](https://github.com/pytorch/pytorch/pull/10025)).\r\n- [torch.broadcast_tensors](https://pytorch.org/docs/master/torch.html#torch.narrow) similar to [numpy.broadcast_arrays](https://docs.scipy.org/doc/numpy/reference/generated/numpy.broadcast_arrays.html). \r\n ([#10075](https://github.com/pytorch/pytorch/pull/10075)).\r\n- [torch.narrow](https://pytorch.org/docs/master/torch.html#torch.narrow) support for sparse tensors.\r\n([#11342](https://github.com/pytorch/pytorch/pull/11342)).\r\n- [torch.matrix_rank](https://pytorch.org/docs/master/torch.html#torch.matrix_rank) similar to  [numpy.linalg.matrix_rank](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_rank.html). \r\n([#10338](https://github.com/pytorch/pytorch/pull/10338)).\r\n- [torch.matrix_power](https://pytorch.org/docs/master/torch.html#torch.matrix_power) similar to [numpy.linalg.matrix_power](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.matrix_power.html).  ([#11421](https://github.com/pytorch/pytorch/pull/11421)).\r\n- [torch.nn.CeLU](https://pytorch.org/docs/master/nn.html#torch.nn.CELU) activation.  ([#8551](https://github.com/pytorch/pytorch/pull/8551)).\r\n- [torch.nn.CTCLoss](https://pytorch.org/docs/master/nn.html#torch.nn.CTCLoss).  ([#9628]( https://github.com/pytorch/pytorch/pull/9628)).\r\n\r\n### New Distributions\r\n\r\n- [Weibull Distribution](https://pytorch.org/docs/master/distributions.html#torch.distributions.weibull.Weibull).  ([#9454](https://github.com/pytorch/pytorch/pull/9454)).\r\n- [NegativeBinomial Distribution](https://pytorch.org/docs/master/distributions.html#torch.distributions.negative_binomial.NegativeBinomial).     ([#9345](https://github.com/pytorch/pytorch/pull/9345)).\r\n- [torch.mvlgamma](https://pytorch.org/docs/master/torch.html#torch.mvlgamma) Multivariate Log-Gamma Distribution.  ([#9451](https://github.com/pytorch/pytorch/pull/9451)).\r\n\r\n### Additions to existing Operators and Distributions\r\n\r\n- [torch.unique](https://pytorch.org/docs/master/torch.html#torch.unique) now accepts an optional `dim` argument.  ([#10423](https://github.com/pytorch/pytorch/pull/10423)).\r\n- [torch.norm](https://pytorch.org/docs/master/torch.html#torch.norm) now supports matrix norms. \r\n ([#11261](https://github.com/pytorch/pytorch/pull/11261)).\r\n- [torch.distributions.kl.kl_divergence](https://pytorch.org/docs/master/distributions.html#torch.distributions.kl.kl_divergence) now supports broadcasting.  ([#10533](https://github.com/pytorch/pytorch/pull/10533)).\r\n- [torch.distributions](https://pytorch.org/docs/master/distributions.html#module-torch.distributions) now support an `expand` method similar to [torch.Tensor.expand](https://pytorch.org/docs/master/tensors.html#torch.Tensor.expand).  For example: [torch.distributions.bernoulli.Bernoulli.expand](https://pytorch.org/docs/master/distributions.html#torch.distributions.bernoulli.Bernoulli.expand).  ([#11341](https://github.com/pytorch/pytorch/pull/11341)).\r\n- [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample) now support nearest neighbor interpolation and reflection padding.  ([#10051](https://github.com/pytorch/pytorch/pull/10051)).\r\n\r\n## Bug Fixes\r\n\r\n### Serious\r\n\r\n- [torch.nn.functional.softmin](https://pytorch.org/docs/master/nn.html#torch.nn.functional.softmin) was using the incorrect formula in 0.4.1.  ([#10066](https://github.com/pytorch/pytorch/pull/10066))\r\n- [torch.as_strided](https://pytorch.org/docs/master/torch.html#torch.as_strided) backwards (called via `view`) was incorrect with overlapping data locations.  ([#9538](https://github.com/pytorch/pytorch/pull/9538)).\r\n- Pointwise losses (e.g. [torch.nn.MSELoss](https://pytorch.org/docs/master/nn.html#torch.nn.MSELoss) were sometimes using the wrong `reduction` method.  ([#10018](https://github.com/pytorch/pytorch/pull/10018)).\r\n- [torch.from_numpy](https://pytorch.org/docs/master/torch.html#torch.from_numpy) was not handling big-endian dtypes correctly.  ([#9508](https://github.com/pytorch/pytorch/pull/9508)).\r\n- [torch.multiprocessing](https://pytorch.org/docs/master/multiprocessing.html#module-torch.multiprocessing) now correctly handles CUDA tensors, requires_grad settings, and hooks.  ([#10220](https://github.com/pytorch/pytorch/pull/10220)).\r\n\r\n### Backwards Compatibility\r\n\r\n- [torch.nn.Module](https://pytorch.org/docs/master/nn.html#torch.nn.Module) `load_from_state_dict` now correctly handles 1-dimensional vs 0-dimensional tensors saved from 0.3 versions.  ([#9781](https://github.com/pytorch/pytorch/pull/9781)).\r\n- Fix `RuntimeError: storages don't support slicing` when loading models saved with PyTorch 0.3.  ([#11314](https://github.com/pytorch/pytorch/pull/11314)).\r\n\r\n### Correctness\r\n\r\n- [torch.nn.Dropout](https://pytorch.org/docs/master/nn.html#torch.nn.Dropout) fused kernel could change parameters in `eval` mode.  ([#10621](https://github.com/pytorch/pytorch/pull/10621)).\r\n- [torch.unbind](https://pytorch.org/docs/master/torch.html#torch.unbind) backwards has been fixed.  ([#9995](https://github.com/pytorch/pytorch/pull/9995)).\r\n- Fix a bug in sparse matrix-matrix multiplication when a sparse matrix is coalesced then transposed.  ([#10496](https://github.com/pytorch/pytorch/pull/10496)).\r\n- [torch.bernoulli](https://pytorch.org/docs/master/torch.html#torch.bernoulli) now handles `out=` parameters correctly, handles expanded tensors correctly, and has corrected argument validity checks on CPU.  ([#10273](https://github.com/pytorch/pytorch/pull/10273)).\r\n- [torch.Tensor.normal_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.normal_) could give incorrect results on CPU.  ([#10846](https://github.com/pytorch/pytorch/pull/10846)).\r\n- [torch.tanh](https://pytorch.org/docs/master/torch.html#torch.tanh) could return incorrect results on non-contiguous tensors.  ([#11226](https://github.com/pytorch/pytorch/pull/11226)).\r\n- [torch.log](https://pytorch.org/docs/master/torch.html#torch.log) on an expanded `Tensor` gave incorrect results on CPU.  ([#10269](https://github.com/pytorch/pytorch/pull/10269)).\r\n- [torch.logsumexp](https://pytorch.org/docs/master/torch.html#torch.logsumexp) now correctly modifies the `out` parameter if it is given.  ([#9755](https://github.com/pytorch/pytorch/pull/9755)).\r\n- [torch.multinomial](https://pytorch.org/docs/master/torch.html#torch.multinomial) with `replacement=True` could select 0 probability events on CUDA.  ([#9960](https://github.com/pytorch/pytorch/pull/9960)).\r\n- [torch.nn.ReLU](https://pytorch.org/docs/master/nn.html#torch.nn.ReLU) will now properly propagate `NaN`. \r\n ([#10277](https://github.com/pytorch/pytorch/pull/10277)).\r\n- [torch.max](https://pytorch.org/docs/master/torch.html#torch.max) and [torch.min](https://pytorch.org/docs/master/torch.html#torch.min) could return incorrect values on input containing `inf` / `-inf`.  ([#11091](https://github.com/pytorch/pytorch/pull/11091)).\r\n- Fixed an issue with calculated output sizes of `torch.nn.Conv` modules with `stride` and `dilation`.  ([#9640](https://github.com/pytorch/pytorch/pull/9640)).\r\n- [torch.nn.EmbeddingBag](https://pytorch.org/docs/master/nn.html#torch.nn.EmbeddingBag) now correctly returns vectors filled with zeros for empty bags on CUDA.  ([#11740](https://github.com/pytorch/pytorch/pull/11740)).\r\n\r\n### Error checking\r\n\r\n- [torch.gesv](https://pytorch.org/docs/master/torch.html#torch.gesv) now properly checks LAPACK errors.  ([#11634](https://github.com/pytorch/pytorch/pull/11634)).\r\n- Fixed an issue where extra positional arguments were accepted (and ignored) in Python functions calling into C++.  ([#10499](https://github.com/pytorch/pytorch/pull/10499)).\r\n- legacy `Tensor` constructors (e.g. `torch.FloatTensor(...)`) now correctly check their `device` argument. \r\n ([#11669](https://github.com/pytorch/pytorch/pull/11669)).\r\n- Properly check that `out` parameter is a CPU `Tensor` for CPU unary ops.  ([#10358](https://github.com/pytorch/pytorch/pull/10358)).\r\n- [torch.nn.InstanceNorm1d](https://pytorch.org/docs/master/nn.html#torch.nn.InstanceNorm1d) now correctly accepts 2 dimensional inputs.  ([#9776](https://github.com/pytorch/pytorch/issues/9776)).\r\n- [torch.nn.Module.load_state_dict](https://pytorch.org/docs/master/nn.html#torch.nn.Module.load_state_dict) had an incorrect error message.  ([#11200](https://github.com/pytorch/pytorch/pull/11200)).\r\n- [torch.nn.RNN](https://pytorch.org/docs/master/nn.html#torch.nn.RNN) now properly checks that inputs and hidden_states are on the same devices.  ([#10185](https://github.com/pytorch/pytorch/pull/10185)).\r\n\r\n### Miscellaneous\r\n\r\n- [torch.utils.data.DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) could hang if it was not completely iterated.  ([#10366](https://github.com/pytorch/pytorch/pull/10366)).\r\n- Fixed a segfault when grad to a hook function is `None`.  ([#12028](https://github.com/pytorch/pytorch/pull/12028)).\r\n- Fixed a segfault in backwards with [torch.nn.PReLU](https://pytorch.org/docs/master/nn.html#torch.nn.PReLU) when the input does not require grad.  ([#11758](https://github.com/pytorch/pytorch/pull/11758)).\r\n- `dir(torch)` has been fixed with Python 3.7.  ([#10271](https://github.com/pytorch/pytorch/pull/10271)).\r\n- Fixed a device-side assert in [torch.multinomial](https://pytorch.org/docs/master/torch.html#torch.multinomial) when `replacement=False` and the input has fewer  nonzero elements than `num_samples`.  ([#11933](https://github.com/pytorch/pytorch/pull/11933)).\r\n- Can now properly assign a `torch.float16` dtype tensor to `.grad`.  ([#11781](https://github.com/pytorch/pytorch/pull/11781)).\r\n- Fixed `can only join a started process` error with [torch.utils.data.DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader).  ([#11432](https://github.com/pytorch/pytorch/pull/11432)).\r\n- Prevent `unexpected exit` in [torch.utils.data.DataLoader](https://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader) on `KeyboardInterrupt`.  ([#11718](https://github.com/pytorch/pytorch/pull/11718)).\r\n- [torch.einsum](https://pytorch.org/docs/master/torch.html#torch.einsum) now handles spaces consistently.  ([#9994](https://github.com/pytorch/pytorch/pull/9994)).\r\n- Fixed a broadcasting bug in [torch.distributions.studentT.StudentT](https://pytorch.org/docs/master/distributions.html#torch.distributions.studentT.StudentT).  ([#12148](https://github.com/pytorch/pytorch/pull/12148/)).\r\n- fix a printing error with large non-contiguous tensors.  ([#10405](https://github.com/pytorch/pytorch/pull/10405)).\r\n\r\n## Other Improvements\r\n\r\n- [torch.cuda](https://pytorch.org/docs/master/cuda.html#module-torch.cuda) functions and [torch.nn.parallel.data_parallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.data_parallel) now accept [torch.device](https://pytorch.org/docs/master/tensor_attributes.html#torch.torch.device) objects in addition to integer device ids.  ([#10833](https://github.com/pytorch/pytorch/pull/10833), [#10189](https://github.com/pytorch/pytorch/pull/10189)).\r\n- [torch.nn.parallel.data_parallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.data_parallel) now accepts `torch.device` inputs.  ([#10189](https://github.com/pytorch/pytorch/pull/10189)).\r\n- [torch.nn.functional.log_softmax](https://pytorch.org/docs/master/nn.html#torch.nn.functional.log_softmax) is now more numerically stable.  ([#11866](https://github.com/pytorch/pytorch/pull/11866)).\r\n- Improve printing of sparse tensors and `grad_fns`.  ([#10181](https://github.com/pytorch/pytorch/pull/10181)).\r\n- Only involve CUDA device in CUDA -> CPU copy.  ([#11592](https://github.com/pytorch/pytorch/pull/11592)).\r\n- Accept numpy floating-point scalars as doubles more consistently.  ([#9659](https://github.com/pytorch/pytorch/pull/9659)).\r\n- sparse-to-sparse [copy_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.copy_) is now supported.  ([#9005](https://github.com/pytorch/pytorch/pull/9005)).\r\n- [torch.bincount](https://pytorch.org/docs/master/torch.html#torch.bincount) now supports 0 element inputs.  ([#9757](https://github.com/pytorch/pytorch/pull/9757)).\r\n- [torch.nn.functional.conv2d](https://pytorch.org/docs/master/nn.html#torch.nn.functional.conv2d) error message have been improved.  ([#11053](https://github.com/pytorch/pytorch/pull/11053)).\r\n- Allow conversion of `np.int64` to PyTorch scalar.  ([#9225](https://github.com/pytorch/pytorch/pull/9225)).\r\n- [torch.einsum](https://pytorch.org/docs/master/torch.html#torch.einsum) now handles varargs. \r\n ([#10067](https://github.com/pytorch/pytorch/pull/10067)).\r\n- [torch.symeig](https://pytorch.org/docs/master/torch.html#torch.symeig) now returns 0-filled eigenvectors when `eigenvectors=False` is passed on CUDA rather than uninitialized data.  ([#10645](https://github.com/pytorch/pytorch/pull/10645)).\r\n\r\n## Deprecations\r\n\r\n### CPP Extensions\r\n\r\n- The `torch/torch.h` header is deprecated in favor of `torch/extension.h`, which should be used in all C++ extensions going forward. Including `torch/torch.h` from a C++ extension will produce a warning. It is safe to batch replace `torch/torch.h` with `torch/extension.h`.\r\n- Usage of the following functions in C++ extensions is also deprecated:\r\n    - `torch::set_requires_grad`. Replacement: `at::Tensor`  now has a `set_requires_grad` method.\r\n    - `torch::requires_grad`. Replacement: `at::Tensor`  now has a `requires_grad` method.\r\n    - `torch::getVariableType`. Replacement: None.\r\n\r\n### torch.distributed \r\n- the old (THD-backed) [torch.distributed]((https://pytorch.org/docs/master/distributed.html)) package is deprecated but still available at [torch.distributed.deprecated](https://pytorch.org/docs/master/distributed_deprecated.html).\r\n- The old (THD-backed) [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel) is deprecated but still available at `torch.nn.parallel.deprecated.DistributedDataParallel`.\r\n\r\n## Performance\r\n\r\n- [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample) on CPU now uses vectorized operation and is now 2x~7x faster with AVX2 enabled CPUs.  ([#9961](https://github.com/pytorch/pytorch/pull/9961)).\r\n- [torch.norm](https://pytorch.org/docs/master/torch.html#torch.norm) has been vectorized and parallelized on CPU.  ([#11565](https://github.com/pytorch/pytorch/pull/11565)).\r\n- [torch.max](https://pytorch.org/docs/master/torch.html#torch.max) and [torch.min](https://pytorch.org/docs/master/torch.html#torch.min) has been parallelized on CPU.  ([#10343](https://github.com/pytorch/pytorch/pull/10343)).\r\n- [torch.Tensor.masked_fill_](https://pytorch.org/docs/master/tensors.html#torch.Tensor.masked_fill_) has been parallelized on CPU.  ([#11359](https://github.com/pytorch/pytorch/pull/11359)).\r\n- [torch.nn.PReLU](https://pytorch.org/docs/master/nn.html#torch.nn.PReLU) has been sped up on both CPU and GPU.\r\n([#11758](https://github.com/pytorch/pytorch/pull/11758)).\r\n- [torch.nn.KLDivLoss](https://pytorch.org/docs/master/nn.html#torch.nn.KLDivLoss) has been sped up on both CPU and GPU.  ([#10336](https://github.com/pytorch/pytorch/pull/10336)).\r\n- [torch.svd](https://pytorch.org/docs/master/torch.html#torch.svd) has been sped up on both CPU and GPU.\r\n([#11194](https://github.com/pytorch/pytorch/pull/11194)).\r\n- [torch.einsum](https://pytorch.org/docs/master/torch.html#torch.einsum) has been greatly sped up on CPU.\r\n([#11292](https://github.com/pytorch/pytorch/pull/11292)).\r\n- [torch.clamp](https://pytorch.org/docs/master/torch.html#torch.clamp) no longer does unnecessary copying.   ([#10352](https://github.com/pytorch/pytorch/pull/10352)).\r\n- [torch.add](https://pytorch.org/docs/master/torch.html#torch.add), [torch.sub](https://pytorch.org/docs/master/torch.html#torch.sub), [torch.mul](https://pytorch.org/docs/master/torch.html#torch.mul), [torch.div](https://pytorch.org/docs/master/torch.html#torch.div) are much faster for non-contiguous tensors on GPU.  ([#8919](https://github.com/pytorch/pytorch/pull/8919)).\r\n- [torch.nn.RNN](https://pytorch.org/docs/master/nn.html#torch.nn.RNN) and related Modules have been ported to C++ and are more performant.  ([#10305](https://github.com/pytorch/pytorch/pull/10305), [#10481](https://github.com/pytorch/pytorch/pull/10481)).\r\n- [Profiler](https://pytorch.org/docs/master/autograd.html#torch.autograd.profiler.profile) now has lower overhead.   ([#10969](https://github.com/pytorch/pytorch/pull/10969), [#11773](https://github.com/pytorch/pytorch/pull/11773)).\r\n\r\n## Documentation Improvements\r\n\r\n- [Reproducibility](https://pytorch.org/docs/master/notes/randomness.html) note added.  ([#11329](https://github.com/pytorch/pytorch/pull/11329)).\r\n- [CPP Extensions](https://pytorch.org/docs/master/cpp_extension.html) have [improved online documentation](https://pytorch.org/cppdocs).  Authors of C++ extensions may want to consult this documentation when writing new extensions.\r\n- [torch.Tensor.flatten](https://pytorch.org/docs/master/tensors.html#torch.Tensor.flatten) is now documented. \r\n ([#9876](https://github.com/pytorch/pytorch/issues/9876)).\r\n- [torch.digamma](https://pytorch.org/docs/master/torch.html#torch.digamma) is now documented.  ([#10967](https://github.com/pytorch/pytorch/pull/10967)).\r\n- [torch.allclose](https://pytorch.org/docs/master/torch.html#torch.allclose) is now documented.  ([#11185](https://github.com/pytorch/pytorch/pull/11185)).\r\n- [torch.eig](https://pytorch.org/docs/master/torch.html#torch.eig) return format clarified.  ([#10315](https://github.com/pytorch/pytorch/pull/10315)).\r\n- [torch.as_tensor](https://pytorch.org/docs/master/torch.html#torch.as_tensor) now includes a proper example.  ([#10309](https://github.com/pytorch/pytorch/pull/10309)).\r\n- [torch.sparse_coo_tensor](https://pytorch.org/docs/master/torch.html#torch.sparse_coo_tensor) now explains uncoalesced behavior.  ([#10308](https://github.com/pytorch/pytorch/pull/10308)).\r\n- [torch.fft](https://pytorch.org/docs/master/torch.html#torch.fft) equation has been corrected.  ([#10760](https://github.com/pytorch/pytorch/pull/10760)).\r\n- [torch.nn.LSTM](https://pytorch.org/docs/master/nn.html#torch.nn.LSTM) behavior has been clarified in the multilayer case.  ([#11896](https://github.com/pytorch/pytorch/pull/11896)).\r\n- [torch.nn.functional.dropout](https://pytorch.org/docs/master/nn.html#torch.nn.functional.dropout) documentation has been clarified.  ([#10417](https://github.com/pytorch/pytorch/pull/10417)).\r\n- [torch.nn.functional.pad](https://pytorch.org/docs/master/nn.html#torch.nn.functional.pad) documentation has been clarified.  ([#11623](https://github.com/pytorch/pytorch/pull/11623)).\r\n- Various mathematical formulas have been clarified.  ([#11106](https://github.com/pytorch/pytorch/pull/11106)).\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v1.0rc1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v1.0rc1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v1.0rc1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/13194853", "dateCreated": "2018-10-02T04:54:52Z", "datePublished": "2018-10-02T06:28:33Z"}, {"tagName": "v0.4.1", "name": "Spectral Norm, Adaptive Softmax, faster CPU ops, anomaly detection (NaNs, etc.), Lots of bug fixes, Python 3.7 and CUDA 9.2 support", "authorName": "soumith", "authorType": "User", "body": "### Table of Contents\r\n\r\n- Breaking Changes\r\n- New Features\r\n  - Neural Networks\r\n      - Adaptive Softmax, Spectral Norm, etc.\r\n  - Operators\r\n      - torch.bincount, torch.as_tensor, ...\r\n  - torch.distributions\r\n      - Half Cauchy, Gamma Sampling, ...\r\n  - Other\r\n      - Automatic anomaly detection (detecting NaNs, etc.)\r\n- Performance\r\n  - Faster CPU ops in a wide variety of cases\r\n- Other improvements\r\n- Bug Fixes\r\n- Documentation Improvements\r\n\r\n## Breaking Changes\r\n\r\n- [`torch.stft`](https://pytorch.org/docs/stable/torch.html#torch.stft)  has changed its signature to be consistent with librosa https://github.com/pytorch/pytorch/pull/9497\r\n    + Before: `stft(signal, frame_length, hop, fft_size=None, normalized=False, onesided=True, window=None, pad_end=0)`\r\n    + After: `stft(input, n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=True)`\r\n    + `torch.stft` is also now using FFT internally and is much faster.\r\n- [`torch.slice`](https://pytorch.org/docs/stable/torch.html#torch.slice)  is removed in favor of the tensor slicing notation https://github.com/pytorch/pytorch/pull/7924\r\n- [`torch.arange`](https://pytorch.org/docs/stable/torch.html#torch.arange)  now does dtype inference: any floating-point argument is inferred to be the default `dtype`; all integer arguments are inferred to be `int64`. https://github.com/pytorch/pytorch/pull/7016\r\n- [`torch.nn.functional.embedding_bag`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.embedding_bag)'s old signature embedding_bag(weight, input, ...) is deprecated, embedding_bag(input, weight, ...) (consistent with torch.nn.functional.embedding) should be used instead\r\n- [`torch.nn.functional.sigmoid`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.sigmoid)  and [`torch.nn.functional.tanh`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.tanh)  are deprecated in favor of [`torch.sigmoid`](https://pytorch.org/docs/stable/torch.html#torch.sigmoid)  and [`torch.tanh`](https://pytorch.org/docs/stable/torch.html#torch.tanh)  https://github.com/pytorch/pytorch/pull/8748\r\n- Broadcast behavior changed in an (very rare) edge case: `[1] x [0]` now broadcasts to `[0]` (used to be `[1]`) https://github.com/pytorch/pytorch/pull/9209\r\n\r\n## New Features\r\n\r\n### Neural Networks\r\n\r\n- Adaptive Softmax [`nn.AdaptiveLogSoftmaxWithLoss`](https://pytorch.org/docs/stable/nn.html#adaptivelogsoftmaxwithloss) https://github.com/pytorch/pytorch/pull/5287\r\n\r\n   ```python\r\n   >>> in_features = 1000\r\n   >>> n_classes = 200\r\n   >>> adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(in_features, n_classes, cutoffs=[20, 100, 150])\r\n   >>> adaptive_softmax\r\n   AdaptiveLogSoftmaxWithLoss(\r\n     (head): Linear(in_features=1000, out_features=23, bias=False)\r\n     (tail): ModuleList(\r\n       (0): Sequential(\r\n         (0): Linear(in_features=1000, out_features=250, bias=False)\r\n         (1): Linear(in_features=250, out_features=80, bias=False)\r\n       )\r\n       (1): Sequential(\r\n         (0): Linear(in_features=1000, out_features=62, bias=False)\r\n         (1): Linear(in_features=62, out_features=50, bias=False)\r\n       )\r\n       (2): Sequential(\r\n         (0): Linear(in_features=1000, out_features=15, bias=False)\r\n         (1): Linear(in_features=15, out_features=50, bias=False)\r\n       )\r\n     )\r\n   )\r\n   >>> batch = 15\r\n   >>> input = torch.randn(batch, in_features)\r\n   >>> target = torch.randint(n_classes, (batch,), dtype=torch.long)\r\n   >>> # get the log probabilities of target given input, and mean negative log probability loss\r\n   >>> adaptive_softmax(input, target) \r\n   ASMoutput(output=tensor([-6.8270, -7.9465, -7.3479, -6.8511, -7.5613, -7.1154, -2.9478, -6.9885,\r\n           -7.7484, -7.9102, -7.1660, -8.2843, -7.7903, -8.4459, -7.2371],\r\n          grad_fn=<ThAddBackward>), loss=tensor(7.2112, grad_fn=<MeanBackward1>))\r\n   >>> # get the log probabilities of all targets given input as a (batch x n_classes) tensor\r\n   >>> adaptive_softmax.log_prob(input)  \r\n   tensor([[-2.6533, -3.3957, -2.7069,  ..., -6.4749, -5.8867, -6.0611],\r\n           [-3.4209, -3.2695, -2.9728,  ..., -7.6664, -7.5946, -7.9606],\r\n           [-3.6789, -3.6317, -3.2098,  ..., -7.3722, -6.9006, -7.4314],\r\n           ...,\r\n           [-3.3150, -4.0957, -3.4335,  ..., -7.9572, -8.4603, -8.2080],\r\n           [-3.8726, -3.7905, -4.3262,  ..., -8.0031, -7.8754, -8.7971],\r\n           [-3.6082, -3.1969, -3.2719,  ..., -6.9769, -6.3158, -7.0805]],\r\n          grad_fn=<CopySlices>)\r\n   >>> # predit: get the class that maximize log probaility for each input\r\n   >>> adaptive_softmax.predict(input)  \r\n   tensor([ 8,  6,  6, 16, 14, 16, 16,  9,  4,  7,  5,  7,  8, 14,  3])\r\n   ```\r\n- Add spectral normalization [`nn.utils.spectral_norm`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.spectral_norm) https://github.com/pytorch/pytorch/pull/6929\r\n    \r\n    ```python\r\n    >>> # Usage is similar to weight_norm\r\n    >>> convT = nn.ConvTranspose2d(3, 64, kernel_size=3, pad=1)\r\n    >>> # Can specify number of power iterations applied each time, or use default (1)\r\n    >>> convT = nn.utils.spectral_norm(convT, n_power_iterations=2)\r\n    >>>\r\n    >>> # apply to every conv and conv transpose module in a model\r\n    >>> def add_sn(m):\r\n            for name, c in m.named_children():\r\n\t             m.add_module(name, add_sn(c))    \r\n\t         if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\r\n\t             return nn.utils.spectral_norm(m)\r\n\t         else:\r\n\t             return m\r\n    \r\n    >>> my_model = add_sn(my_model)\r\n    ```\r\n- [`nn.ModuleDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ModuleDict) and [`nn.ParameterDict`](https://pytorch.org/docs/stable/nn.html#torch.nn.ParameterDict) containers https://github.com/pytorch/pytorch/pull/8463\r\n- Add `nn.init.zeros_` and `nn.init.ones_` https://github.com/pytorch/pytorch/pull/7488\r\n- Add sparse gradient option to pretrained embedding https://github.com/pytorch/pytorch/pull/7492\r\n- Add max pooling support to [`nn.EmbeddingBag`](https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag) https://github.com/pytorch/pytorch/pull/5725\r\n- Depthwise convolution support for MKLDNN https://github.com/pytorch/pytorch/pull/8782\r\n- Add `nn.FeatureAlphaDropout` (featurewise Alpha Dropout layer) https://github.com/pytorch/pytorch/pull/9073\r\n\r\n### Operators\r\n\r\n- [`torch.bincount`](https://pytorch.org/docs/stable/torch.html#torch.bincount)  (count frequency of each value in an integral tensor) https://github.com/pytorch/pytorch/pull/6688\r\n\r\n    ```python\r\n    >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\r\n\t>>> weights = torch.linspace(0, 1, steps=5)\r\n\t>>> input, weights\r\n\t(tensor([4, 3, 6, 3, 4]),\r\n\t tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\r\n\r\n\t>>> torch.bincount(input)\r\n\ttensor([0, 0, 0, 2, 2, 0, 1])\r\n\r\n\t>>> input.bincount(weights)\r\n\ttensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\r\n    ```\r\n- [`torch.as_tensor`](https://pytorch.org/docs/stable/torch.html#torch.as_tensor)  (similar to `torch.tensor` but never copies unless necessary) https://github.com/pytorch/pytorch/pull/7109\r\n\r\n    ```python\r\n    >>> tensor = torch.randn(3, device='cpu', dtype=torch.float32)\r\n    >>> torch.as_tensor(tensor)                       # doesn't copy\r\n    >>> torch.as_tensor(tensor, dtype=torch.float64)  # copies due to incompatible dtype\r\n    >>> torch.as_tensor(tensor, device='cuda')        # copies due to incompatible device\r\n    >>> array = np.array([3, 4.5])\r\n    >>> torch.as_tensor(array)                        # doesn't copy, sharing memory with the numpy array\r\n    >>> torch.as_tensor(array, device='cuda')         # copies due to incompatible device\r\n    ```\r\n- [`torch.randperm`](https://pytorch.org/docs/stable/torch.html#torch.randperm)  for CUDA tensors https://github.com/pytorch/pytorch/pull/7606\r\n- [`nn.HardShrink`](https://pytorch.org/docs/stable/nn.html?torch.nn.Hardshrink) for CUDA tensors https://github.com/pytorch/pytorch/pull/8117\r\n- [`torch.flip`](https://pytorch.org/docs/stable/torch.html#torch.flip)  (flips a tensor along specified dims) https://github.com/pytorch/pytorch/pull/7873\r\n- [`torch.flatten`](https://pytorch.org/docs/stable/torch.html#torch.flatten)  (flattens a contiguous range of dims) https://github.com/pytorch/pytorch/pull/8578\r\n- [`torch.pinverse`](https://pytorch.org/docs/stable/torch.html#torch.pinverse)  (computes svd-based pseudo-inverse) https://github.com/pytorch/pytorch/pull/9052\r\n- [`torch.meshgrid`](https://pytorch.org/docs/stable/torch.html#torch.meshgrid)  https://github.com/pytorch/pytorch/pull/8581\r\n- [`torch.unique`](https://pytorch.org/docs/stable/torch.html#torch.unique)  for CUDA tensors https://github.com/pytorch/pytorch/pull/8899\r\n- [`torch.erfc`](https://pytorch.org/docs/stable/torch.html#torch.erfc)  (complementary error function) https://github.com/pytorch/pytorch/pull/9366/files\r\n- [`torch.isinf`](https://pytorch.org/docs/stable/torch.html#torch.isinf)  and [`torch.isfinite`](https://pytorch.org/docs/stable/torch.html#torch.isfinite)  https://github.com/pytorch/pytorch/pull/9169 https://github.com/pytorch/pytorch/pull/9487\r\n- [`torch.reshape_as`](https://pytorch.org/docs/stable/torch.html#torch.reshape_as)  https://github.com/pytorch/pytorch/pull/9452\r\n- Support backward for target tensor in [`torch.nn.functional.kl_div`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.kl_div)  https://github.com/pytorch/pytorch/pull/7839\r\n- [`torch.logsumexp`](https://pytorch.org/docs/stable/torch.html#torch.logsumexp)  https://github.com/pytorch/pytorch/pull/7254\r\n- Add batched linear solver to `torch.gesv` https://github.com/pytorch/pytorch/pull/6100\r\n- [`torch.sum`](https://pytorch.org/docs/stable/torch.html#torch.sum)  now supports summing over multiple dimensions https://github.com/pytorch/pytorch/pull/6152/files\r\n- [`torch.diagonal`](https://pytorch.org/docs/stable/torch.html#torch.diagonal)  [`torch.diagflat`](https://pytorch.org/docs/stable/torch.html#torch.diagflat)  to take arbitrary diagonals with numpy semantics https://github.com/pytorch/pytorch/pull/6718\r\n- [`tensor.any`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor.any) and [`tensor.all`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor.all) on `ByteTensor` can now accept `dim` and `keepdim` arguments https://github.com/pytorch/pytorch/pull/4627\r\n\r\n### Distributions\r\n\r\n- Half Cauchy and Half Normal https://github.com/pytorch/pytorch/pull/8411\r\n- Gamma sampling for CUDA tensors https://github.com/pytorch/pytorch/pull/6855\r\n- Allow vectorized counts in Binomial Distribution https://github.com/pytorch/pytorch/pull/6720\r\n\r\n### Misc\r\n\r\n- Autograd automatic anomaly detection for `NaN` and errors occuring in backward. Two functions [detect_anomaly](https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly) and [set_detect_anomaly](https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly) are provided for this. https://github.com/pytorch/pytorch/pull/7677\r\n- Support `reversed(torch.Tensor)` https://github.com/pytorch/pytorch/pull/9216\r\n- Support `hash(torch.device)` https://github.com/pytorch/pytorch/pull/9246\r\n- Support `gzip` in [`torch.load`](https://pytorch.org/docs/stable/torch.html#torch.load)  https://github.com/pytorch/pytorch/pull/6490\r\n\r\n## Performance\r\n\r\n- Accelerate bernoulli number generation on CPU https://github.com/pytorch/pytorch/pull/7171\r\n- Enable cuFFT plan caching (80% speed-up in certain cases) https://github.com/pytorch/pytorch/pull/8344\r\n- Fix unnecessary copying in `bernoulli_` https://github.com/pytorch/pytorch/pull/8682\r\n- Fix unnecessary copying in `broadcast` https://github.com/pytorch/pytorch/pull/8222\r\n- Speed-up multidim `sum` (2x~6x speed-up in certain cases) https://github.com/pytorch/pytorch/pull/8992\r\n- Vectorize CPU `sigmoid` (>3x speed-up in most cases) https://github.com/pytorch/pytorch/pull/8612\r\n- Optimize CPU `nn.LeakyReLU` and `nn.PReLU` (2x speed-up) https://github.com/pytorch/pytorch/pull/9206\r\n- Vectorize `softmax` and `logsoftmax` (4.5x speed-up on single core and 1.8x on 10 threads) https://github.com/pytorch/pytorch/pull/7375\r\n- Speed up `nn.init.sparse` (10-20x speed-up) https://github.com/pytorch/pytorch/pull/6899\r\n\r\n\r\n## Improvements\r\n\r\n### Tensor printing\r\n\r\n- Tensor printing now includes `requires_grad` and `grad_fn` information https://github.com/pytorch/pytorch/pull/8211\r\n- Improve number formatting in tensor print https://github.com/pytorch/pytorch/pull/7632\r\n- Fix scale when printing some tensors https://github.com/pytorch/pytorch/pull/7189\r\n- Speed up printing of large tensors https://github.com/pytorch/pytorch/pull/6876\r\n\r\n### Neural Networks\r\n\r\n- `NaN` is now propagated through many activation functions https://github.com/pytorch/pytorch/pull/8033\r\n- Add `non_blocking` option to nn.Module.to https://github.com/pytorch/pytorch/pull/7312\r\n- Loss modules now allow target to require gradient https://github.com/pytorch/pytorch/pull/8460\r\n- Add `pos_weight` argument to `nn.BCEWithLogitsLoss` https://github.com/pytorch/pytorch/pull/6856\r\n- Support `grad_clip` for parameters on different devices https://github.com/pytorch/pytorch/pull/9302\r\n- Removes the requirement that input sequences to `pad_sequence` have to be sorted https://github.com/pytorch/pytorch/pull/7928\r\n- `stride` argument for `max_unpool1d`, `max_unpool2d`, `max_unpool3d` now defaults to `kernel_size` https://github.com/pytorch/pytorch/pull/7388\r\n- Allowing calling grad mode context managers (e.g., `torch.no_grad`, `torch.enable_grad`) as decorators https://github.com/pytorch/pytorch/pull/7737\r\n- `torch.optim.lr_scheduler._LRSchedulers` `__getstate__` include optimizer info https://github.com/pytorch/pytorch/pull/7757\r\n- Add support for accepting `Tensor` as input in `clip_grad_*` functions https://github.com/pytorch/pytorch/pull/7769\r\n- Return `NaN` in `max_pool`/`adaptive_max_pool` for `NaN` inputs https://github.com/pytorch/pytorch/pull/7670\r\n- `nn.EmbeddingBag` can now handle empty bags in all modes https://github.com/pytorch/pytorch/pull/7389\r\n- `torch.optim.lr_scheduler.ReduceLROnPlateau` is now serializable https://github.com/pytorch/pytorch/pull/7201\r\n- Allow only tensors of floating point dtype to require gradients https://github.com/pytorch/pytorch/pull/7034 and https://github.com/pytorch/pytorch/pull/7185\r\n- Allow resetting of BatchNorm running stats and cumulative moving average https://github.com/pytorch/pytorch/pull/5766\r\n- Set the gradient of `LP-Pool`ing to zero if the sum of all input elements to the power of p is zero https://github.com/pytorch/pytorch/pull/6766\r\n\r\n### Operators\r\n\r\n- Add ellipses ('...') and diagonals (e.g. 'ii\u2192i') to [`torch.einsum`](https://pytorch.org/docs/stable/torch.html#torch.einsum)  https://github.com/pytorch/pytorch/pull/7173\r\n- Add `to` method for `PackedSequence` https://github.com/pytorch/pytorch/pull/7319\r\n- Add support for `__floordiv__` and `__rdiv__` for integral tensors https://github.com/pytorch/pytorch/pull/7245\r\n- [`torch.clamp`](https://pytorch.org/docs/stable/torch.html#torch.clamp)  now has subgradient 1 at min and max https://github.com/pytorch/pytorch/pull/7049\r\n- [`torch.arange`](https://pytorch.org/docs/stable/torch.html#torch.arange)  now uses NumPy-style type inference: https://github.com/pytorch/pytorch/pull/7016\r\n- Support infinity norm properly in [`torch.norm`](https://pytorch.org/docs/stable/torch.html#torch.norm)  and [`torch.renorm`](https://pytorch.org/docs/stable/torch.html#torch.renorm)  https://github.com/pytorch/pytorch/pull/6969\r\n- Allow passing an output tensor via `out=` keyword arugment in [`torch.dot`](https://pytorch.org/docs/stable/torch.html#torch.dot)  and [`torch.matmul`](https://pytorch.org/docs/stable/torch.html#torch.matmul)  https://github.com/pytorch/pytorch/pull/6961\r\n\r\n### Distributions\r\n\r\n- Always enable grad when calculating `lazy_property` https://github.com/pytorch/pytorch/pull/7708\r\n\r\n### Sparse Tensor\r\n\r\n- Add log1p for sparse tensor https://github.com/pytorch/pytorch/pull/8969\r\n- Better support for adding zero-filled sparse tensors https://github.com/pytorch/pytorch/pull/7479\r\n\r\n### Data Parallel\r\n\r\n- Allow modules that return scalars in `nn.DataParallel` https://github.com/pytorch/pytorch/pull/7973\r\n- Allow `nn.parallel.parallel_apply` to take in a list/tuple of tensors https://github.com/pytorch/pytorch/pull/8047\r\n\r\n### Misc\r\n\r\n- `torch.Size` can now accept PyTorch scalars https://github.com/pytorch/pytorch/pull/5676 \r\n- Move `torch.utils.data.dataset.random_split` to torch.utils.data.random_split, and `torch.utils.data.dataset.Subset` to `torch.utils.data.Subset` https://github.com/pytorch/pytorch/pull/7816\r\n- Add serialization for `torch.device` https://github.com/pytorch/pytorch/pull/7713\r\n- Allow copy.deepcopy of `torch.(int/float/...)*` dtype objects https://github.com/pytorch/pytorch/pull/7699\r\n- [`torch.load`](https://pytorch.org/docs/stable/torch.html#torch.load)  can now take a `torch.device` as map location https://github.com/pytorch/pytorch/pull/7339\r\n\r\n## Bug Fixes\r\n\r\n- Fix [`nn.BCELoss`](https://pytorch.org/docs/stable/nn.html?torch.nn.BCELoss) sometimes returning negative results https://github.com/pytorch/pytorch/pull/8147\r\n- Fix `tensor._indices` on scalar sparse tensor giving wrong result https://github.com/pytorch/pytorch/pull/8197\r\n- Fix backward of `tensor.as_strided` not working properly when input has overlapping memory https://github.com/pytorch/pytorch/pull/8721\r\n- Fix `x.pow(0)` gradient when x contains 0 https://github.com/pytorch/pytorch/pull/8945\r\n- Fix CUDA [`torch.svd`](https://pytorch.org/docs/stable/torch.html#torch.svd)  and [`torch.eig`](https://pytorch.org/docs/stable/torch.html#torch.eig)  returning wrong results in certain cases https://github.com/pytorch/pytorch/pull/9082\r\n- Fix `nn.MSELoss` having low precision https://github.com/pytorch/pytorch/pull/9287\r\n- Fix segmentation fault when calling `torch.Tensor.grad_fn` https://github.com/pytorch/pytorch/pull/9292\r\n- Fix [`torch.topk`](https://pytorch.org/docs/stable/torch.html#torch.topk)  returning wrong results when input isn't contiguous https://github.com/pytorch/pytorch/pull/9441\r\n- Fix segfault in convolution on CPU with large `inputs` / `dilation` https://github.com/pytorch/pytorch/pull/9274\r\n- Fix `avg_pool2/3d` `count_include_pad` having default value `False` (should be `True`) https://github.com/pytorch/pytorch/pull/8645\r\n- Fix `nn.EmbeddingBag`'s `max_norm` option https://github.com/pytorch/pytorch/pull/7959\r\n- Fix returning scalar input in Python autograd function https://github.com/pytorch/pytorch/pull/7934\r\n- Fix THCUNN `SpatialDepthwiseConvolution` assuming contiguity https://github.com/pytorch/pytorch/pull/7952\r\n- Fix bug in seeding random module in `DataLoader` https://github.com/pytorch/pytorch/pull/7886\r\n- Don't modify variables in-place for [`torch.einsum`](https://pytorch.org/docs/stable/torch.html#torch.einsum)  https://github.com/pytorch/pytorch/pull/7765\r\n- Make return uniform in lbfgs step https://github.com/pytorch/pytorch/pull/7586\r\n- The return value of `uniform.cdf()` is now clamped to `[0..1]` https://github.com/pytorch/pytorch/pull/7538\r\n- Fix advanced indexing with negative indices https://github.com/pytorch/pytorch/pull/7345\r\n- `CUDAGenerator` will not initialize on the current device anymore, which will avoid unnecessary memory allocation on `GPU:0` https://github.com/pytorch/pytorch/pull/7392\r\n- Fix `tensor.type(dtype)` not preserving device https://github.com/pytorch/pytorch/pull/7474\r\n- Batch sampler should return the same results when used alone or in dataloader with `num_workers` > 0 https://github.com/pytorch/pytorch/pull/7265\r\n- Fix broadcasting error in LogNormal, TransformedDistribution https://github.com/pytorch/pytorch/pull/7269\r\n- Fix [`torch.max`](https://pytorch.org/docs/stable/torch.html#torch.max) and [`torch.min`](https://pytorch.org/docs/stable/torch.html#torch.min)  on CUDA in presence of `NaN` https://github.com/pytorch/pytorch/pull/7052\r\n- Fix [`torch.tensor`](https://pytorch.org/docs/stable/torch.html#torch.tensor) device-type calculation when used with CUDA https://github.com/pytorch/pytorch/pull/6995\r\n- Fixed a missing `'='` in `nn.LPPoolNd` repr function https://github.com/pytorch/pytorch/pull/9629\r\n\r\n## Documentation\r\n\r\n- Expose and document `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck` https://github.com/pytorch/pytorch/pull/8166\r\n- Document `tensor.scatter_add_` https://github.com/pytorch/pytorch/pull/9630\r\n- Document variants of [`torch.add`](https://pytorch.org/docs/stable/torch.html#torch.add) and `tensor.add_`, e.g. `tensor.add(value=1, other)` -> Tensor https://github.com/pytorch/pytorch/pull/9027\r\n- Document [`torch.logsumexp`](https://pytorch.org/docs/stable/torch.html#torch.logsumexp)  https://github.com/pytorch/pytorch/pull/8428\r\n- Document [`torch.sparse_coo_tensor`](https://pytorch.org/docs/stable/torch.html#torch.sparse_coo_tensor)  https://github.com/pytorch/pytorch/pull/8152\r\n- Document [`torch.utils.data.dataset.random_split`](https://pytorch.org/docs/stable/data.html?torch.utils.data.random_split) https://github.com/pytorch/pytorch/pull/7676\r\n- Document [`torch.nn.GroupNorm`](https://pytorch.org/docs/stable/nn.html?torch.nn.GroupNorm) https://github.com/pytorch/pytorch/pull/7086\r\n- A lot of other various documentation improvements including RNNs, `ConvTransposeNd`, `Fold`/`Unfold`, `Embedding`/`EmbeddingBag`, Loss functions, etc.\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.4.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.4.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.4.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/12123311", "dateCreated": "2018-07-26T00:28:04Z", "datePublished": "2018-07-26T19:09:19Z"}, {"tagName": "v0.4.0", "name": "Trade-off memory for compute, Windows support, 24 distributions with cdf, variance etc., dtypes, zero-dimensional Tensors, Tensor-Variable merge, , faster distributed, perf and bug fixes, CuDNN 7.1", "authorName": "soumith", "authorType": "User", "body": "# PyTorch 0.4.0 release notes\r\n\r\n### Table of Contents\r\n\r\n- Major Core Changes\r\n    - Tensor / Variable merged\r\n    - Zero-dimensional Tensors\r\n    - dtypes\r\n    - migration guide\r\n- New Features\r\n  - Tensors\r\n      - Full support for advanced indexing\r\n      - Fast Fourier Transforms\r\n  - Neural Networks\r\n      - Trade-off memory for compute\r\n      - bottleneck - a tool to identify hotspots in your code\r\n  - torch.distributions\r\n      - 24 basic probability distributions\r\n      - Added cdf, variance, entropy, perplexity etc.\r\n  - Distributed Training\r\n      - Launcher utility for ease of use\r\n      - NCCL2 backend\r\n  - C++ Extensions\r\n  - Windows Support\r\n  - ONNX Improvements\r\n      - RNN support\r\n- Performance improvements\r\n- Bug fixes\r\n\r\n## Major Core changes\r\n\r\nHere is a summary of the updates to the most important core features users will use daily.\r\n\r\n**Major Changes and Potentially Breaking Changes:**\r\n* ``Tensors`` and ``Variables`` have merged\r\n* Some operations now return 0-dimensional (scalar) ``Tensors``\r\n* Deprecation of the ``volatile`` flag\r\n\r\n**Improvements:**\r\n* ``dtypes``, ``devices``, and Numpy-style ``Tensor`` creation functions added\r\n* Support for writing device-agnostic code\r\n\r\n\r\nWe wrote a [migration guide](http://pytorch.org/2018/04/22/0_4_0-migration-guide.html) that should help you transition your code to new APIs and style. Please read it if you have code in a previous version of PyTorch that you would like to migrate.\r\n\r\n**Please read the [migration guide](http://pytorch.org/2018/04/22/0_4_0-migration-guide.html) if you have code in a previous version of PyTorch that you would like to migrate.**\r\n**Please read the [migration guide](http://pytorch.org/2018/04/22/0_4_0-migration-guide.html) if you have code in a previous version of PyTorch that you would like to migrate.**\r\n**Please read the [migration guide](http://pytorch.org/2018/04/22/0_4_0-migration-guide.html) if you have code in a previous version of PyTorch that you would like to migrate.**\r\n\r\nThe contents of this section (Major Core changes) are included in the [migration guide](http://pytorch.org/2018/04/22/0_4_0-migration-guide.html).\r\n\r\n### Merging [``Tensor``](http://pytorch.org/docs/0.4.0/tensors.html) and ``Variable`` classes\r\n\r\n``torch.autograd.Variable`` and [``torch.Tensor``](http://pytorch.org/docs/0.4.0/tensors.html) are now the same class.  More precisely, [``torch.Tensor``](http://pytorch.org/docs/0.4.0/tensors.html) is capable of tracking history and behaves like the old ``Variable``; ``Variable`` wrapping continues to work as before but returns an object of type [``torch.Tensor``](http://pytorch.org/docs/0.4.0/tensors.html).  This means that you don't need the ``Variable`` wrapper everywhere in your code anymore.\r\n\r\n#### The `type()` of a [``Tensor``](http://pytorch.org/docs/0.4.0/tensors.html) has changed\r\n\r\nNote also that the ``type()`` of a Tensor no longer reflects the data type. Use ``isinstance()`` or ``x.type()`` instead:\r\n\r\n```python\r\n>>> x = torch.DoubleTensor([1, 1, 1])\r\n>>> print(type(x)) # was torch.DoubleTensor\r\n<class 'torch.autograd.variable.Variable'>\r\n>>> print(x.type())  # OK: 'torch.DoubleTensor'\r\n'torch.DoubleTensor'\r\n>>> print(isinstance(x, torch.DoubleTensor))  # OK: True\r\nTrue\r\n```\r\n\r\n#### When does [``autograd``](http://pytorch.org/docs/0.4.0/autograd.html) start tracking history now?\r\n\r\n``requires_grad``, the central flag for [``autograd``](http://pytorch.org/docs/0.4.0/autograd.html), is now an attribute on ``Tensor``s. Let's see how this change manifests in code.\r\n\r\n[``autograd``](http://pytorch.org/docs/0.4.0/autograd.html) uses the same rules previously used for ``Variable``s. It starts tracking history when any input ``Tensor`` of an operation has ``requires_grad=True``. For example,\r\n\r\n```python\r\n>>> x = torch.ones(1)  # create a tensor with requires_grad=False (default)\r\n>>> x.requires_grad\r\nFalse\r\n>>> y = torch.ones(1)  # another tensor with requires_grad=False\r\n>>> z = x + y\r\n>>> # both inputs have requires_grad=False. so does the output\r\n>>> z.requires_grad\r\nFalse\r\n>>> # then autograd won't track this computation. let's verify!\r\n>>> z.backward()\r\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\r\n>>>\r\n>>> # now create a tensor with requires_grad=True\r\n>>> w = torch.ones(1, requires_grad=True)\r\n>>> w.requires_grad\r\nTrue\r\n>>> # add to the previous result that has require_grad=False\r\n>>> total = w + z\r\n>>> # the total sum now requires grad!\r\n>>> total.requires_grad\r\nTrue\r\n>>> # autograd can compute the gradients as well\r\n>>> total.backward()\r\n>>> w.grad\r\ntensor([ 1.])\r\n>>> # and no computation is wasted to compute gradients for x, y and z, which don't require grad\r\n>>> z.grad == x.grad == y.grad == None\r\nTrue\r\n```\r\n\r\n##### Manipulating ``requires_grad`` flag\r\n\r\nOther than directly setting the attribute, you can change this flag **in-place** using [``my_tensor.requires_grad_(requires_grad=True)``](http://pytorch.org/docs/0.4.0/tensors.html#torch.Tensor.requires_grad_), or, as in the above example, at creation time by passing it in as an argument (default is ``False``), e.g.,\r\n\r\n```python\r\n>>> existing_tensor.requires_grad_()\r\n>>> existing_tensor.requires_grad\r\nTrue\r\n>>> my_tensor = torch.zeros(3, 4, requires_grad=True)\r\n>>> my_tensor.requires_grad\r\nTrue\r\n```\r\n\r\n#### What about ``.data``?\r\n\r\n``.data`` was the primary way to get the underlying ``Tensor`` from a ``Variable``. After this merge, calling ``y = x.data`` still has similar semantics. So ``y`` will be a ``Tensor`` that shares the same data with ``x``, is unrelated with the computation history of ``x``, and has ``requires_grad=False``.\r\n\r\nHowever, ``.data`` can be unsafe in some cases. Any changes on ``x.data`` wouldn't be tracked by ``autograd``, and the computed gradients would be incorrect if ``x`` is needed in a backward pass. A safer alternative is to use [``x.detach()``](http://pytorch.org/docs/0.4.0/autograd.html#torch.Tensor.detach), which also returns a ``Tensor`` that shares data with ``requires_grad=False``, but will have its in-place changes reported by ``autograd`` if ``x`` is needed in backward.\r\n\r\n\r\n### Some operations now return 0-dimensional (scalar) ``Tensors``\r\n\r\nPreviously, indexing into a ``Tensor`` vector (1-dimensional tensor) gave a Python number but indexing into a ``Variable`` vector gave (incosistently!) a vector of size ``(1,)``!  Similar behavior existed with reduction functions, i.e. `tensor.sum()` would return a Python number, but `variable.sum()` would retun a vector of size `(1,)`.\r\n\r\nFortunately, this release introduces proper scalar (0-dimensional tensor) support in PyTorch!  Scalars can be created using the new `torch.tensor` function (which will be explained in more detail later; for now just think of it as the PyTorch equivalent of `numpy.array`).  Now you can do things like:\r\n\r\n```python\r\n>>> torch.tensor(3.1416)         # create a scalar directly\r\ntensor(3.1416)\r\n>>> torch.tensor(3.1416).size()  # scalar is 0-dimensional\r\ntorch.Size([])\r\n>>> torch.tensor([3]).size()     # compare to a vector of size 1\r\ntorch.Size([1])\r\n>>>\r\n>>> vector = torch.arange(2, 6)  # this is a vector\r\n>>> vector\r\ntensor([ 2.,  3.,  4.,  5.])\r\n>>> vector.size()\r\ntorch.Size([4])\r\n>>> vector[3]                    # indexing into a vector gives a scalar\r\ntensor(5.)\r\n>>> vector[3].item()             # .item() gives the value as a Python number\r\n5.0\r\n>>> sum = torch.tensor([2, 3]).sum()\r\n>>> sum\r\ntensor(5)\r\n>>> sum.size()\r\ntorch.Size([])\r\n```\r\n\r\n#### Accumulating losses\r\n\r\nConsider the widely used pattern ``total_loss += loss.data[0]`` before 0.4.0. ``loss`` was a ``Variable`` wrapping a tensor of size ``(1,)``, but in 0.4.0 ``loss`` is now a scalar and has ``0`` dimensions. Indexing into a scalar doesn't make sense (it gives a warning now, but will be a hard error in 0.5.0): use ``loss.item()`` to get the Python number from a scalar.\r\n\r\nNote that if you don't convert to a Python number when accumulating losses, you may find increased memory usage in your program. This is because the right-hand-side of the above expression used to be a Python float, while it is now a zero-dim Tensor.  The total loss is thus accumulating Tensors and their gradient history, which may keep around large autograd graphs for much longer than necessary.\r\n\r\n\r\n### Deprecation of ``volatile`` flag\r\n\r\nThe ``volatile`` flag is now deprecated and has no effect. Previously, any computation that involves a ``Variable`` with ``volatile=True`` won't be tracked by ``autograd``. This has now been replaced by [a set of more flexible context managers](http://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation) including ``torch.no_grad()``, ``torch.set_grad_enabled(grad_mode)``, and others.\r\n\r\n```python\r\n>>> x = torch.zeros(1, requires_grad=True)\r\n>>> with torch.no_grad():\r\n...     y = x * 2\r\n>>> y.requires_grad\r\nFalse\r\n>>>\r\n>>> is_train = False\r\n>>> with torch.set_grad_enabled(is_train):\r\n...     y = x * 2\r\n>>> y.requires_grad\r\nFalse\r\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\r\n>>> y = x * 2\r\n>>> y.requires_grad\r\nTrue\r\n>>> torch.set_grad_enabled(False)\r\n>>> y = x * 2\r\n>>> y.requires_grad\r\nFalse\r\n```\r\n\r\n\r\n### [``dtypes``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype), [``devices``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device) and NumPy-style creation functions\r\n\r\nIn previous versions of PyTorch, we used to specify data type (e.g. float vs double), device type (cpu vs cuda) and layout (dense vs sparse) together as a \"tensor type\". For example, ``torch.cuda.sparse.DoubleTensor`` was the ``Tensor`` type respresenting``double`` data type, living on CUDA devices, and with [COO sparse tensor layout](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)).\r\n\r\nIn this release, we introduce [``torch.dtype``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype), [``torch.device``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device) and [``torch.layout``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout) classes to allow better management of these properties via NumPy-style creation functions.\r\n\r\n#### [``torch.dtype``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype)\r\n\r\nBelow is a complete list of available [``torch.dtype``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.dtype)s (data types) and their corresponding tensor types.\r\n\r\n| Data type                 | ``torch.dtype``                        | Tensor types              |\r\n|:------------------------- |:-------------------------------------- | :------------------------ |\r\n| 32-bit floating point     | ``torch.float32`` or ``torch.float``   | ``torch.*.FloatTensor``   |\r\n| 64-bit floating point     | ``torch.float64`` or ``torch.double``  | ``torch.*.DoubleTensor``  |\r\n| 16-bit floating point     | ``torch.float16`` or ``torch.half``    | ``torch.*.HalfTensor``    |\r\n| 8-bit integer (unsigned)  | ``torch.uint8``                        | ``torch.*.ByteTensor``    |\r\n| 8-bit integer (signed)    | ``torch.int8``                         | ``torch.*.CharTensor``    |\r\n| 16-bit integer (signed)   | ``torch.int16``   or ``torch.short``   | ``torch.*.ShortTensor``   |\r\n| 32-bit integer (signed)   | ``torch.int32``   or ``torch.int``     | ``torch.*.IntTensor``     |\r\n| 64-bit integer (signed)   | ``torch.int64``   or ``torch.long``    | ``torch.*.LongTensor``    |\r\n\r\nUse [``torch.set_default_dtype``](http://pytorch.org/docs/0.4.0/torch.html#torch.set_default_dtype) and [``torch.get_default_dtype``](http://pytorch.org/docs/0.4.0/torch.html#torch.get_default_dtype) to manipulate default ``dtype`` for floating point tensors.\r\n\r\n#### [``torch.device``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device)\r\n\r\nA [``torch.device``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device) contains a device type (``'cpu'`` or ``'cuda'``) and optional device ordinal (id) for the device type. It can be initilized with ``torch.device('{device_type}')`` or ``torch.device('{device_type}:{device_ordinal}')``.\r\n\r\nIf the device ordinal is not present, this represents the current device for the device type; e.g., ``torch.device('cuda')`` is equivalent to ``torch.device('cuda:X')`` where ``X`` is the result of ``torch.cuda.current_device()``.\r\n\r\n#### [``torch.layout``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout)\r\n\r\n[``torch.layout``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.layout) represents the data layout of a [``Tensor``](http://pytorch.org/docs/0.4.0/tensors.html). Currently``torch.strided`` (dense tensors) and ``torch.sparse_coo`` (sparse tensors with COO format) are supported.\r\n\r\n### Creating ``Tensor``s\r\n\r\n[Methods that create a ``Tensor``](http://pytorch.org/docs/0.4.0/torch.html#creation-ops) now also take in ``dtype``, ``device``, ``layout``, and ``requires_grad`` options to specify the desired attributes on the returned ``Tensor``. For example,\r\n\r\n```python\r\n>>> device = torch.device(\"cuda:1\")\r\n>>> x = torch.randn(3, 3, dtype=torch.float64, device=device)\r\ntensor([[-0.6344,  0.8562, -1.2758],\r\n        [ 0.8414,  1.7962,  1.0589],\r\n        [-0.1369, -1.0462, -0.4373]], dtype=torch.float64, device='cuda:1')\r\n>>> x.requires_grad  # default is False\r\nFalse\r\n>>> x = torch.zeros(3, requires_grad=True)\r\n>>> x.requires_grad\r\nTrue\r\n```\r\n\r\n#### [``torch.tensor``](http://pytorch.org/docs/0.4.0/torch.html#torch.tensor)\r\n[``torch.tensor``](http://pytorch.org/docs/0.4.0/torch.html#torch.tensor) is one of the newly added [tensor creation methods](http://pytorch.org/docs/0.4.0/torch.html#creation-ops). It takes in array like data of all kinds and copies the contained values into a new ``Tensor``. As mentioned earlier, [``torch.tensor``](http://pytorch.org/docs/0.4.0/torch.html#torch.tensor) is the PyTorch equivalent of NumPy's ``numpy.array`` constructor.  Unlike the ``torch.*Tensor`` methods, you can also create zero-dimensional ``Tensor``s (aka scalars) this way (a single python number is treated as a Size in the``torch.*Tensor``  methods). Moreover, if a ``dtype`` argument isn't given, it will infer the suitable ``dtype`` given the data. It is the recommended way to create a tensor from existing data like a Python list. For example,\r\n\r\n```python\r\n>>> cuda = torch.device(\"cuda\")\r\n>>> torch.tensor([[1], [2], [3]], dtype=torch.half, device=cuda)\r\ntensor([[ 1],\r\n        [ 2],\r\n        [ 3]], device='cuda:0')\r\n>>> torch.tensor(1)               # scalar\r\ntensor(1)\r\n>>> torch.tensor([1, 2.3]).dtype  # type inferece\r\ntorch.float32\r\n>>> torch.tensor([1, 2]).dtype    # type inferece\r\ntorch.int64\r\n```\r\n\r\nWe've also added more tensor creation methods. Some of them have ``torch.*_like`` and/or ``tensor.new_*`` variants.\r\n\r\n1. ``torch.*_like`` takes in an input ``Tensor`` instead of a shape. It returns a ``Tensor`` with same attributes as the input ``Tensor`` by default unless otherwise specified:\r\n\r\n    ```python\r\n    >>> x = torch.randn(3, dtype=torch.float64)\r\n    >>> torch.zeros_like(x)\r\n    tensor([ 0.,  0.,  0.], dtype=torch.float64)\r\n    >>> torch.zeros_like(x, dtype=torch.int)\r\n    tensor([ 0,  0,  0], dtype=torch.int32)\r\n    ```\r\n\r\n2. ``tensor.new_*`` can also create ``Tensor``s with same attributes as ``tensor``, but it always takes in a shape argument:\r\n\r\n    ```python\r\n    >>> x = torch.randn(3, dtype=torch.float64)\r\n    >>> x.new_ones(2)\r\n    tensor([ 1.,  1.], dtype=torch.float64)\r\n    >>> x.new_ones(4, dtype=torch.int)\r\n    tensor([ 1,  1,  1,  1], dtype=torch.int32)\r\n    ```\r\n\r\nTo specify the desired shape, you can either use a tuple (e.g., ``torch.zeros((2, 3))``) or variable arguments (e.g., ``torch.zeros(2, 3)``) in most cases.\r\n\r\n| Name                                                       | Returned ``Tensor``                                       | ``torch.*_like`` variant | ``tensor.new_*`` variant |\r\n|:-----------------------------------------------------------|-----------------------------------------------------------|--------------------------|--------------------------|\r\n| [``torch.empty``](http://pytorch.org/docs/0.4.0/torch.html#torch.empty)                                            | unintialized memory                                       | \u2714                        | \u2714                        |\r\n| [``torch.zeros``](http://pytorch.org/docs/0.4.0/torch.html#torch.zeros)                                            | all zeros                                                 | \u2714                        | \u2714                        |\r\n| [``torch.ones``](http://pytorch.org/docs/0.4.0/torch.html#torch.ones)                                             | all ones                                                  | \u2714                        | \u2714                        |\r\n| [``torch.full``](http://pytorch.org/docs/0.4.0/torch.html#torch.full)                                             | filled with a given value                                 | \u2714                        | \u2714                        |\r\n| [``torch.rand``](http://pytorch.org/docs/0.4.0/torch.html#torch.rand)                                             | i.i.d. continuous ``Uniform[0, 1)``                       | \u2714                        |                          |\r\n| [``torch.randn``](http://pytorch.org/docs/0.4.0/torch.html#torch.randn)                                            | i.i.d. ``Normal(0, 1)``                                   | \u2714                        |                          |\r\n| [``torch.randint``](http://pytorch.org/docs/0.4.0/torch.html#torch.randint)                                          | i.i.d. discrete Uniform in given range                    | \u2714                        |                          |\r\n| [``torch.randperm``](http://pytorch.org/docs/0.4.0/torch.html#torch.randperm)                                         | random permutation of ``{0, 1, ..., n - 1}``              |                          |                          |\r\n| [``torch.tensor``](http://pytorch.org/docs/0.4.0/torch.html#torch.tensor)                                           | copied from existing data (`list`, NumPy `ndarray`, etc.) |                          | \u2714                        |\r\n| [``torch.from_numpy``](http://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy)*                                      | from NumPy ``ndarray`` (sharing storage without copying)  |                          |                          |\r\n| [``torch.arange``](http://pytorch.org/docs/0.4.0/torch.html#torch.arange), <br>[``torch.range``](http://pytorch.org/docs/0.4.0/torch.html#torch.range), and <br>[``torch.linspace``](http://pytorch.org/docs/0.4.0/torch.html#torch.linspace)  | uniformly spaced values in a given range                  |                          |                          |\r\n| [``torch.logspace``](http://pytorch.org/docs/0.4.0/torch.html#torch.logspace)                                         | logarithmically spaced values in a given range            |                          |                          |\r\n| [``torch.eye``](http://pytorch.org/docs/0.4.0/torch.html#torch.eye)                                              | identity matrix                                           |                          |                          |\r\n\r\n*: [``torch.from_numpy``](http://pytorch.org/docs/0.4.0/torch.html#torch.from_numpy) only takes in a NumPy ``ndarray`` as its input argument.\r\n\r\n\r\n\r\n### Writing device-agnostic code\r\n\r\nPrevious versions of PyTorch made it difficult to write code that was device agnostic (i.e. that could run on both CUDA-enabled and CPU-only machines without modification).\r\n\r\nPyTorch 0.4.0 makes this easier in two ways:\r\n* The `device` attribute of a Tensor gives the [``torch.device``](http://pytorch.org/docs/0.4.0/tensor_attributes.html#torch.torch.device) for all Tensors (`get_device` only works for CUDA tensors)\r\n* The `to` method of ``Tensors`` and ``Modules`` can be used to easily move objects to different devices (instead of having to call `cpu()` or `cuda()` based on the context)\r\n\r\n\r\nWe recommend the following pattern:\r\n```python\r\n# at beginning of the script\r\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n...\r\n\r\n# then whenever you get a new Tensor or Module\r\n# this won't copy if they are already on the desired device\r\ninput = data.to(device)\r\nmodel = MyModule(...).to(device)\r\n```\r\n\r\n## Tensors\r\n\r\n### Full support for Advanced indexing\r\n\r\nPyTorch now has full support for advanced indexing, following numpy's advanced indexing rules. The following examples are now possible:\r\n\r\n```python\r\na = torch.rand(10, 10, 10, 10)\r\n\r\n# the indexing elements can have other shapes than 1\r\nb = a[[[3, 2]], :, [[1, 3]]]\r\n\r\n# broadcasting also supported in the indices, as well as lists,\r\n# negative indices, slices, elipses, numbers\r\nc = a[[1, -2], 2:4, :, [1]]\r\n\r\n# can also support tensors as indices\r\nindex = torch.tensor([2, 4])\r\nd = a[index]\r\n\r\n# and the indices can be on the GPU\r\n# or CPU\r\ne = a[index.cuda()]\r\nf = a.cuda()[index]\r\n\r\n\r\nmask = torch.rand(10) > 0.5\r\n# we can now index with a mask that has fewer\r\n# dimensions than the indexing tensor\r\nc = a[mask, :5]\r\n```\r\n\r\n### Fast Fourier Transform\r\n- Add new FFT methods [#5856](https://github.com/pytorch/pytorch/pull/5856)\r\n- Add ``torch.stft`` (short time Fourier transform) and hann/hamming/bartlett window functions. [#4095](https://github.com/pytorch/pytorch/pull/4095)\r\n- Support arbitrary number of batch dimensions in *FFT [#6528](https://github.com/pytorch/pytorch/pull/6528) \r\n\r\n### New and updated Torch operators\r\n\r\n* Added `torch.log2` and `torch.log10` [#6272](https://github.com/pytorch/pytorch/pull/6272)\r\n* Added `torch.isnan` [#5273](\r\nhttps://github.com/pytorch/pytorch/pull/5273)\r\n* Add `torch.reshape`, which is similar to `numpy.reshape`. It is roughly equivalent to `tensor.contiguous().view()`, but avoids copying in certain cases [#5575](https://github.com/pytorch/pytorch/pull/5575)\r\n* Add CPU implementation of `torch.unique`, which outputs the unique elements of a Tensor [#5503](https://github.com/pytorch/pytorch/pull/5503)\r\n* Add `torch.det`, `torch.logdet` and `torch.slogdet`, for computing the (log-)determinant of square 2D tensors. For negative determinants, `torch.logdet` returns `nan`, while `torch.slogdet` returns the sign of the log-determinant and the log of the absolute value of the determinant. [#3816](https://github.com/pytorch/pytorch/pull/3816) and [#5393](https://github.com/pytorch/pytorch/pull/5393)\r\n* Add `nn.functional.gumbel_softmax`, which lets you use the reparametrization trick for discrete variables [#3341](https://github.com/pytorch/pytorch/pull/3341)\r\n* Add `torch.take` and `Tensor.put_`. Those functions are equivalent to numpy.take and numpy.put, and are the base for full support of advanced indexing in PyTorch [#3263](https://github.com/pytorch/pytorch/pull/3263)\r\n* Add `torch.randint`, similar to `numpy.random.randint` [#6136](https://github.com/pytorch/pytorch/pull/6136)\r\n* Add `torch.diagonal` and `torch.diagflat`, similar to `numpy.diagonal` and `numpy.diagflat`. They are meant as a replacement for `torch.diag`, which handled both the cases of constructing a diagonal tensor as well as extracting the diagonal of a matrix [#5622](https://github.com/pytorch/pytorch/pull/5622)\r\n* Add `torch.einsum`, equivalent to `numpy.einsum`. einsum allows you to perform operations using Einstein's notation. [#5503](https://github.com/pytorch/pytorch/pull/5503)\r\n```python\r\na = torch.arange(0, 9).reshape(3, 3)\r\n# the following transposes a\r\nb = torch.einsum('ij->ji', (a,))\r\n```\r\n- Add ``torch.expm1``, a numerically stable ``exp(x)-1`` for small ``x``. [#4350](https://github.com/pytorch/pytorch/pull/4350)\r\n- Allow users to specify individual split sizes with ``torch.split`` [#3837](https://github.com/pytorch/pytorch/pull/3837)\r\n- Add ``torch.where(condition, tensor1, tensor2)`` that returns a tensors of elements selected from  ``tensor1`` or ``tensor2`` based on ``condition``. [#4259](https://github.com/pytorch/pytorch/pull/4259), [#4259](https://github.com/pytorch/pytorch/pull/4259)\r\n- Add ``Tensor.norm(dim)`` for sparse tensors. [#4882](https://github.com/pytorch/pytorch/pull/4882)\r\n- Implement ``torch.neg`` for all types. [#4075](https://github.com/pytorch/pytorch/pull/4075)\r\n- Implement gradient calculation for ``torch.trtrs``. [#3972](https://github.com/pytorch/pytorch/pull/3972)\r\n- Deprecate out-of-place ``Tensor.resize`` and ``Tensor.resize_as``. These have weird semantics and are hard to use correctly. Please use their in-place variants ``Tensor.resize_`` and ``Tensor.resize_as_``. [#4886](https://github.com/pytorch/pytorch/pull/4886)\r\n\r\n###  Rename `async` argument in ``.cuda()`` to `non_blocking`\r\n\r\nThe `async` keyword argument in conversion calls is now deprecated in PyTorch, and it has been replaced by `non_blocking`.  This was necessary because `async` will be a keyword in Python 3.7\r\n\r\n\r\n## Neural Networks\r\n\r\n\r\n### A new autograd container that lets you trade compute for memory\r\n\r\nThe new `checkpoint` container allows you to only store a subset of the outputs necessary for backpropagation. If an output is missing (to save memory), the `checkpoint` container will recompute the intermediate outputs from the closest checkpoint, so that memory usage can be reduced (with an increase in computation time).\r\nHere is an example:\r\n```python\r\n# input\r\ninput = torch.rand(1, 10)\r\n# suppose we have a very deep model\r\nlayers = [nn.Linear(10, 10) for _ in range(1000)]\r\nmodel = nn.Sequential(*layers)\r\noutput = model(input)\r\n```\r\nThe above model uses a lot of memory, because it needs to keep the intermediate values of every operation for backpropagation. `checkpoint` lets your reduce the memory requirements:\r\n\r\n```python\r\n\r\n# create the input tensors and set the requires_grad=True\r\n# NOTE: the requires_grad=True for the input is a current\r\n# limitation of checkpointing. At least one of the \r\n# model inputs should have requires_grad=True. \r\n# If you don't do it, you might have empty gradients.\r\ninput = torch.rand(1, 10, requires_grad=True)\r\nlayers = [nn.Linear(10, 10) for _ in range(1000)]\r\n\r\n# define function that will define where\r\n# we will checkpoint and store\r\n# intermediate gradients. In this case,\r\n# we will only store one intermediate\r\n# gradient, in the middle of the\r\n# model\r\n\r\ndef run_first_half(*args):\r\n    x = args[0]\r\n    for layer in layers[:500]:\r\n        x = layer(x)\r\n    return x\r\n\r\ndef run_second_half(*args):\r\n    x = args[0]\r\n    for layer in layers[500:-1]:\r\n        x = layer(x)\r\n    return x\r\n\r\n# now uses the new checkpoint functionality\r\nfrom torch.utils.checkpoint import checkpoint\r\n\r\nx = checkpoint(run_first_half, input)\r\nx = checkpoint(run_second_half, x)\r\n# last output need to be run without checkpoint\r\nx = layers[-1](x)\r\nx.sum.backward()  # works!\r\n```\r\nFor sequential modules (which can have arbitrary blocks inside), a helper function `checkpoint_sequential` is provided, which takes care of the most common use-cases:\r\n```python\r\ninput = torch.rand(1, 10, requires_grad=True)\r\nlayers = [nn.Linear(10, 10) for _ in range(1000)]\r\nmodel = nn.Sequential(*layers)\r\n\r\nfrom torch.utils.checkpoint import checkpoint_sequential\r\n\r\n# split in two blocks\r\nnum_segments = 2\r\nx = checkpoint_sequential(model, num_segments, input)\r\nx.sum().backward()  # works!\r\n```\r\n\r\n### bottleneck - a tool to identify hotspots in your code\r\n\r\n``torch.utils.bottleneck`` ([#5216](https://github.com/pytorch/pytorch/pull/5216), [#6425](https://github.com/pytorch/pytorch/pull/6425)) is a tool that can be used as an initial step for\r\ndebugging bottlenecks in your program. It summarizes runs of your script with \r\nthe Python profiler and PyTorch\u2019s autograd profiler. See the [bottleneck docs](http://pytorch.org/docs/master/bottleneck.html) for more details.\r\n\r\n### reduce=False Losses\r\nAs of this release, all of our loss functions support the ``reduce`` keyword. Specifying ``reduce=False`` gives a Tensor per unit of loss instead of a single reduced loss. [#4924](https://github.com/pytorch/pytorch/pull/4924), [#5346](https://github.com/pytorch/pytorch/pull/5346), [#5646](https://github.com/pytorch/pytorch/pull/5646), [#4231](https://github.com/pytorch/pytorch/pull/4231), [#4705](https://github.com/pytorch/pytorch/pull/4705),  [#5680](https://github.com/pytorch/pytorch/pull/5680)\r\n\r\n\r\n### New modules and module improvements\r\n\r\n* Add `DistributedDataParallelCPU`. This is similar to `DistributedDataParallel`, but with specific support for models running on the CPU (contrary to `DistributedDataParallel`, which targets GPU), and supports `mpi`, `gloo` and `tcp` backends [#5919](https://github.com/pytorch/pytorch/pull/5919).\r\n* Add [Group Normalization](https://arxiv.org/abs/1803.08494) (`nn.GroupNorm`), an alternative to batch normalization that doesn't suffer from the same issues as `BatchNorm` for small batch sizes\r\n* Add [Layer Normalization](https://arxiv.org/abs/1607.06450) (``nn.LayerNorm``), an alternative for batch normalization often used in NLP tasks. [#4922](https://github.com/pytorch/pytorch/pull/4922)\r\n* Add Local Response Normalization (``nn.LocalResponseNorm``). [#4922](https://github.com/pytorch/pytorch/pull/4922)\r\n* `MaxPool3d` now supports double backwards. MaxPool3d and MaxUnpool3d now use indices consistent with the rest of the pooling layers. [#5328](https://github.com/pytorch/pytorch/pull/5328)\r\n* All loss functions now support a reduce argument to return a batch of losses. [#264](https://github.com/pytorch/pytorch/issues/264)\r\n* Add util to clip gradient value in torch.nn.utils.clip_grad and add param to He initialization scheme in `torch.nn.init`. [#6173](https://github.com/pytorch/pytorch/pull/6173)\r\n* Renamed ``torch.nn.init.*`` methods to have an underscore in the end, as they operate in-place, and deprecate the old versions [6093](https://github.com/pytorch/pytorch/pull/6093)\r\n* Added support for returning dictionaries in `DataParallel` [#6113](https://github.com/pytorch/pytorch/pull/6113)\r\n* Added support for N-D tensors in `torch.nn.Bilinear` [#5764](https://github.com/pytorch/pytorch/pull/5764)\r\n* Add `Embedding.from_pretrained` factory. This allows to initialize an Embedding layer with an existing tensor, bypassing the initial random initialization of its weights.\r\n* You can now slice ``nn.Sequential``, ``nn.ModuleList``, and ``nn.ParameterList`` [#4491](https://github.com/pytorch/pytorch/pull/4491)\r\n* Registered ``nn.Module`` integer parameters and buffers are now immune to ``module.float()``, ``module.double()`` ``module.half()`` calls. [#3820](https://github.com/pytorch/pytorch/pull/3820)\r\n\r\n## torch.distributions\r\n`torch.distributions` has expanded to include 24 [basic probability distributions](http://pytorch.org/docs/0.4.0/distributions.html): `Bernoulli`, `Beta`, `Binomial`, `Categorical`, `Cauchy`, `Chi2`, `Dirichlet`, `Exponential`, `FisherSnedecor`, `Gamma`, `Geometric`, `Gumbel`, `Laplace`, `LogNormal`, `Multinomial`, `MultivariateNormal`, `Normal`, `OneHotCategorical`, `Pareto`, `Poisson`, `RelaxedBernoulli`, `RelaxedOneHotCategorical`, `StudentT`, and `Uniform`.\r\n\r\nThe [`Distribution`](http://pytorch.org/docs/0.4.0/distributions.html#distribution) interface has expanded to include many methods including `.cdf()`, `.icdf()`, `.mean()`, `.variance()`, `.entropy()`, and `.perplexity()`. Distributions now split tensor dimensions into [`sample_shape`](http://pytorch.org/docs/0.4.0/distributions.html#torch.distributions.distribution.Distribution.sample)+[`batch_shape`](http://pytorch.org/docs/0.4.0/distributions.html#torch.distributions.distribution.Distribution.batch_shape)+[`event_shape`](http://pytorch.org/docs/0.4.0/distributions.html#torch.distributions.distribution.Distribution.event_shape). Most continuous distributions now also implement a differentiable `.rsample()` method to compute [pathwise derivatives](http://pytorch.org/docs/0.4.0/distributions.html#pathwise-derivative) aka the reparameterization trick (check `.has_rsample` for availability):\r\n```python\r\n>>> loc = torch.tensor(0., requires_grad=True)\r\n>>> scale = torch.tensor(1., requires_grad=True)\r\n>>> samples = Normal(loc, scale).rsample(sample_shape=(1000,))\r\n>>> loss = (samples - 0.5).pow(4).mean()  # average over 1000 monte carlo samples\r\n>>> grad(loss, [loc, scale])\r\n(tensor(-7.5092), tensor(15.2704))\r\n```\r\nMost discrete distributions implement an [`.enumerate_support()`](http://pytorch.org/docs/0.4.0/distributions.html#torch.distributions.distribution.Distribution.enumerate_support) method to make it easy to sum over all possible sample values (check `.has_enumerate_support` for availability).\r\n\r\n[`kl_divergence`](http://pytorch.org/docs/0.4.0/distributions.html#module-torch.distributions.kl) is defined for many pairs of distributions, e.g.\r\n```python\r\n>>> x = torch.tensor(1.0, requires_grad=True)\r\n>>> kl = kl_divergence(Uniform(-x, x), Normal(0., 1.))\r\n>>> grad(kl, [x])[0]\r\ntensor(-0.6667)\r\n```\r\n\r\n### Distribution Transforms\r\nNew distributions can be created by combining [`TransformedDistribution`](http://pytorch.org/docs/0.4.0/distributions.html#transformeddistribution) with any number of [`Transform`](http://pytorch.org/docs/0.4.0/distributions.html#torch.distributions.transforms.Transform) objects from the [`torch.distributions.transforms`](http://pytorch.org/docs/0.4.0/distributions.html#module-torch.distributions.transforms) library, including: `ExpTransform`, `PowerTransform`, `SigmoidTransform`, `AbsTransform`, `AffineTransform`, `SoftmaxTransform`, `StickBreakingTransform`, `LowerCholeskyTransform`, and their inverses via the [`.inv`](http://pytorch.org/docs/0.4.0/distributions.html#torch.distributions.transforms.Transform.inv) property.\r\n\r\n### Distribution Constraints\r\nDistributions provide metadata about the constraints of their `.support` and about their arguments (`.arg_constraints`). These [`Constraint`](http://pytorch.org/docs/0.4.0/distributions.html#module-torch.distributions.constraints) objects are registered with transforms using [`transform_to()` and `biject_to()`](http://pytorch.org/docs/0.4.0/distributions.html#module-torch.distributions.constraint_registry). Together constraints and transforms make it easy to specify new distributions in a generic way\r\n```python\r\n>>> scale = torch.tensor(1., requires_grad=True)\r\n>>> p = Normal(0., scale)\r\n>>> assert p.arg_constraints['scale'] == constraints.positive\r\n>>> prior = TransformedDistribution(Normal(0., 1.),\r\n...                                 transform_to(constraints.positive))\r\n```\r\nConstraints in the [`torch.distributions.constraints`](http://pytorch.org/docs/0.4.0/distributions.html#module-torch.distributions.constraints) library include: `boolean`, `greater_than(lower_bound)`, `integer_interval(lower_bound, upper_bound)`, `interval(lower_bound, upper_bound)`, `lower_cholesky`, `lower_triangular`, `nonnegative_integer`, `positive`, `positive_definite`, `positive_integer`, `real`, `real_vector`, `simplex`, and `unit_interval`. \r\n\r\n### Distributed\r\n\r\n#### Helper utility for launching Distributed Training jobs\r\n\r\nWe have added an utility function to help launch jobs on a distributed setup.\r\nIn order to launch a script that leverages `DistributedDataParallel` on either single-node multiple-nodes, we can make use of torch.distributed launch as follows\r\n```\r\npython -m torch.distributed.launch my_script.py --arg1 --arg2 --arg3\r\n```\r\n\r\nThe script simplifies day to day usability of the `distributed` package.\r\n\r\nYou can read about it's usage here: http://pytorch.org/docs/stable/distributed.html#launch-utility\r\n\r\n#### A new distributed backend based on NCCL 2.0\r\n\r\nPyTorch now has a new distributed backend, which leverages NCCL 2.0 for maximum speed.\r\nIt also provides new APIs for collective operations on multiple GPUs.\r\nYou can enable the new backend via\r\n```python\r\ntorch.distributed.init_process_group(\"nccl\")\r\n```\r\n\r\n#### Other distributed improvements\r\n\r\n- Coalesce many small broadcasts to improve performance [#4978](https://github.com/pytorch/pytorch/pull/4978)\r\n- Add mixed-precision support for distributed training [#4891](https://github.com/pytorch/pytorch/pull/4891)\r\n- Release NCCL distributed backend. Previously it was marked as ``experimental``. [#4921](https://github.com/pytorch/pytorch/pull/4921)\r\n- Enable Infiniband support for Gloo data channel with automatic IB device detection [#4795](https://github.com/pytorch/pytorch/pull/4795)\r\n\r\n\r\n### C++ extensions\r\nPreviously, the official way of writing extensions using C or CUDA for custom modules was through the cffi extension. The drawback of this method was that it required a separate step for compiling the CUDA kernels, which could be a bit messy.\r\n\r\nPyTorch now provides a better system for [writing your own C++ / CUDA extensions](http://pytorch.org/docs/master/cpp_extension.html). Example implementations using this new extension support can be found in the [pytorch/cpp_extensions](https://github.com/pytorch/extension-cpp) repo.\r\n\r\nWe provide two compilation modes:\r\n- ahead of time compilation: you write a `setup.py` script using the new `CppExtension` or `CUDAExtension`, which is an extension of `setuptools.Extension` module;\r\n- just-in-time compilation: you pass the list of C++ / CUDA files that you want to compile to `torch.utils.cpp_extension.load`, and it will compile on the fly and cache the libraries for you. Here is an example illustrating how easy it is to implement an extension:\r\n\r\nIn C++\r\n```cpp\r\n// my_implementation.cpp\r\n#include <torch/torch.h>\r\n#include <unordered_set>\r\n\r\n// can use templates as well. But let's keep it\r\n// simple\r\nusing scalar_t = float;\r\n\r\nat::Tensor unique_float(at::Tensor input_) {\r\n  // only works for floats\r\n  AT_ASSERT(input_.type().scalarType() == at::ScalarType::Float, \"input must be a float tensor\");\r\n  // and CPU tensors\r\n  AT_ASSERT(!input_.type().is_cuda(), \"input must be a CPU tensor\");\r\n  \r\n  // make the input contiguous, to simplify the implementation\r\n  at::Tensor input = input_.contiguous();\r\n  \r\n  // get the pointer that holds the data\r\n  scalar_t* input_data = input.data<scalar_t>();\r\n  // let's use a function from the std library to implement\r\n  // the unique function\r\n  std::unordered_set<scalar_t> set(input_data, input_data + input.numel());\r\n  \r\n  // create the output tensor, with size set.size()\r\n  at::Tensor output = input.type().tensor({static_cast<int64_t>(set.size())});\r\n  scalar_t* output_data = output.data<scalar_t>();\r\n  // copy the content of the set to the output tensor\r\n  std::copy(set.begin(), set.end(), output_data);\r\n  \r\n  return output;\r\n}\r\n\r\n// this defines the functions exposed to Python\r\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\r\n  m.def(\"unique_float\", &unique_float, \"Unique for float tensors\");\r\n}\r\n```\r\nAnd then in Python\r\n```python\r\nimport torch\r\nfrom torch.utils.cpp_extension import load as load_ext\r\n# pass the source files, they will be compiled on the fly \r\n# and will return a python module\r\n_C = load_ext('my_unique_lib', sources=['my_implementation.cpp'])\r\n\r\n# now can use the functions implemented in C++\r\nunique = _C.unique_float\r\n\r\na = torch.tensor([1.0, 2.0, 1.0])\r\nprint(unique(a))\r\n# tensor([ 2.,  1.])\r\n```\r\n\r\n### Windows support\r\n\r\nPyTorch now officially supports Windows. We provide pre-compiled Conda binaries and pip wheels for Python 3.5 and 3.6.\r\nPyTorch on Windows doesn't support `distributed` training and might be a tad bit slower than Linux / OSX because Visual Studio supports an older version of OpenMP.\r\n\r\nAs always, you can use the commands at http://pytorch.org to install PyTorch on Windows\r\nWe have an FAQ that answers most questions you might have around Windows here: http://pytorch.org/docs/stable/notes/windows.html\r\n\r\n\r\n### ONNX Improvements\r\n\r\n\r\n#### New ONNX operators\r\n- Support export `torch.max(input, dim)` and `torch.min(input, dim)` [#6220](https://github.com/pytorch/pytorch/pull/6220)\r\n- Add symbolic for `ReLU` to support exporting to ONNX [#5759](https://github.com/pytorch/pytorch/pull/5759)\r\n- Add `sum`, `prod`, `sqrt` and improve `log_softmax` [#4579](https://github.com/pytorch/pytorch/pull/4579)\r\n- Add ONNX support for `InstanceNorm` [#4626](https://github.com/pytorch/pytorch/pull/4626)\r\n- Add ONNX symbolic for `Elu` [#3453](https://github.com/pytorch/pytorch/pull/3453)\r\n- Add ONNX symbolic for `UpsamplingNearest2d` [#3450](https://github.com/pytorch/pytorch/pull/3450)\r\n\r\n#### Improvements\r\n- Print source location when ONNX export fails for a node [#5652](https://github.com/pytorch/pytorch/pull/5652)\r\n- Export onnx protobuf bindings to python [#6651](https://github.com/pytorch/pytorch/pull/6651)\r\n- Support `output_padding` in `ConvTranspose` [#4583](https://github.com/pytorch/pytorch/pull/4583)\r\n\r\n#### Better RNN support\r\nPyTorch can now export a subset of RNNs to ONNX [#4409](https://github.com/pytorch/pytorch/pull/4409)\r\n\r\n- Add Elman RNN export to ONNX [#4613](https://github.com/pytorch/pytorch/pull/4613)\r\n- Support batch-first in ONNX export of padded sequences [#5360](https://github.com/pytorch/pytorch/pull/5360)\r\n- Bidirectional Elman RNN export to ONNX [#5120](https://github.com/pytorch/pytorch/pull/5120)\r\n- Handle sequence lengths correctly when exporting RNNs to ONNX [#4695](https://github.com/pytorch/pytorch/pull/4695)\r\n- Support GRU export to ONNX [#4390](https://github.com/pytorch/pytorch/pull/4390)\r\n\r\n#### Bugfixes\r\n- Fix a bug in ONNX symbolic of 3d average pooling [#6101](https://github.com/pytorch/pytorch/pull/6101)\r\n- Fix onnx export of replication/reflection pad [#4263](https://github.com/pytorch/pytorch/pull/4263)\r\n\r\n\r\n### Miscellaneous improvements\r\n* implement ``__dir__`` for Tensors, so that editors can automatically auto-complete and query for the possible fields in Tensors\r\n\r\n* Add ``numpy()`` and ``from_numpy()`` to ``HalfTensor``\r\n* Enable `TensorDataset` to have any number of input tensors.\r\n\r\n* Add `padding_value` to `torch.nn.utils.rnn.pad_sequence`\r\n* Add `total_length` option to `pack_padded_sequence`, which is useful when using `DataParallel`, as we can ensure that we have sequences of the same length.\r\n* Improve numerical precision of `torch.arange`, making it consistent with `numpy.arange`\r\n* `torch.load()` and `torch.save()` support arbitrary file-like object\r\n* `torch.nn.functional.grid_sample` now supports 2D (spatial) and 3D (volumetric) inputs\r\n* set python random seed in `DataLoader` workers, in order to improve experiment reproducibility\r\n\r\n* Add `__delitem__` to `nn.Sequential`. Now one can delete arbitrary elements of a `nn.Sequential`.\r\n\r\nFor example:\r\n\r\n```python\r\nmodel = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 2))\r\ndel model[1]  # deletes nn.ReLU\r\n```\r\n\r\n* `ReduceLROnPlateau` is now serializable [#5300](https://github.com/pytorch/pytorch/pull/5300)\r\n\r\n* Add option to flush denormal numbers on CPU. [#5294](https://github.com/pytorch/pytorch/pull/5294)\r\n* PyTorch now exposes the gradients of conv1d, conv2d and conv3d with respect to the input and the weights [#5408](https://github.com/pytorch/pytorch/pull/5408)\r\n* Add support for calling `pack_padded_sequence` with either list or with a Tensor [#5133](https://github.com/pytorch/pytorch/pull/5133)\r\n- Support negative indexing for ``padding_idx`` in ``nn.Embedding`` [#4496](https://github.com/pytorch/pytorch/pull/4496)\r\n- Implement backward pass for ``pack_padded_sequence`` [#4512](https://github.com/pytorch/pytorch/pull/4512)\r\n- Add ``nn.utils.rnn.pad_sequence`` and ``nn.utils.rnn.pack_sequence`` to pad lists of variable length Tensors with ``0`` and to pack a list of variable length Tensors.\r\n- Add ``torch.cuda.memory_cached``, ``torch.cuda.max_memory_cached``, ``torch.cuda.memory_allocated``, and ``torch.cuda.max_memory_allocated`` methods\r\n  for checking CUDA memory usage [#4511](https://github.com/pytorch/pytorch/pull/4511)\r\n- Allow viewing on noncontiguous tensors if the new view size is compatible with the tensor's original size and stride. [#4062](https://github.com/pytorch/pytorch/pull/4062)\r\n- ``NLLLoss`` and ``CrossEntropyLoss`` now support more than 2 dimensions. [#4654](https://github.com/pytorch/pytorch/pull/4654)\r\n\r\n- Add an option to not show ``model_zoo`` download progress bar [#4135](https://github.com/pytorch/pytorch/pull/4135)\r\n- You can now assign modules to indices of ``nn.Sequential``. [#4931](https://github.com/pytorch/pytorch/pull/4931)\r\n- You can create tensors with a numpy ``np.longlong`` array [#4367](https://github.com/pytorch/pytorch/pull/4367)\r\n- Change the autograd execution order to use good heuristics. This greatly improves memory usage for large models. [#4746](https://github.com/pytorch/pytorch/pull/4746)\r\n\r\n- Add AMSgrad mode to ``Adam`` and ``SparseAdam`` optmizers. [#4034](https://github.com/pytorch/pytorch/pull/4034)\r\n\r\n- Better ``torch.autograd.profiler`` support for CUDA profiling using the ``cudaEvent`` API. [#3734](https://github.com/pytorch/pytorch/pull/3734)\r\n\r\n- ``torch.set_num_threads`` also sets the respective MKL option so you won't need to use an environment variable to control it. [#4949](https://github.com/pytorch/pytorch/pull/4949)\r\n\r\n\r\n## Performance improvements\r\n\r\n- Speed up CPU ``nn.EmbeddingBag``, making training overall 30% faster [#5433](https://github.com/pytorch/pytorch/pull/5433)\r\n- Move ``nn.MarginRankingLoss``, `nn.CosineEmbeddingLoss`, `nn.HingeEmbeddingLoss`, and `nn.TripletMarginLoss` from Python to our ATen backend, resulting in some cases up to a 3x performance gains.\r\n[#5346](https://github.com/pytorch/pytorch/pull/5346),  [#5646](https://github.com/pytorch/pytorch/pull/5646), [#5080](https://github.com/pytorch/pytorch/pull/5080), [#5680](https://github.com/pytorch/pytorch/pull/5680)\r\n- Implement ``pin_memory()`` as a NativeFunction [#4094](https://github.com/pytorch/pytorch/pull/4094)\r\n- Save ``self.numel()`` for backward computation instead of ``self`` to save memory [#5747](https://github.com/pytorch/pytorch/pull/5747)\r\n- Rearrange dimensions for pointwise operations for up to 10x better performance in one case. [#4174](https://github.com/pytorch/pytorch/pull/4174)\r\n- Vectorize `normal_` for a 5-6x speed up in a small case [#4312](https://github.com/pytorch/pytorch/pull/4312)\r\n- Allowing usage of GPU Direct within PyTorch for the Broadcast operation [#4183](https://github.com/pytorch/pytorch/pull/4183)\r\n- Speed-up ``nn.Linear`` for the 3D input case [#5279](https://github.com/pytorch/pytorch/pull/5279)\r\n- Speed up `Conv3D` on the CPU by parallelizing ``vol2col`` and ``col2vol`` [#4824](https://github.com/pytorch/pytorch/pull/4824)\r\n- Add AVX2 implementation for sigmoid function, showing around 10x speedup [#5010](https://github.com/pytorch/pytorch/pull/5010)\r\n- Use fast integer division algorithm to avoid division ops inside kernels. [#5054](https://github.com/pytorch/pytorch/pull/5054)\r\n- Improve occupancy for CUDA random number generation [#5710](https://github.com/pytorch/pytorch/pull/5710)\r\n- Add optimization to norm for common norms [#5722](https://github.com/pytorch/pytorch/pull/5722)\r\n- Add a fast fused GLU backward [#5782](https://github.com/pytorch/pytorch/pull/5782)\r\n- Optimize unique sorting by using ``std::vector+sort`` instead of ``std::set``, giving up to 5x speedup. [#5913](https://github.com/pytorch/pytorch/pull/5913)\r\n- Speed up sum over a dimension [#6026](https://github.com/pytorch/pytorch/pull/6026)\r\n- Enable MKLDNN convolution forward and backward. [#6062](https://github.com/pytorch/pytorch/pull/6062)\r\n- Parallelize non-contiguous point-wise operations with OpenMP [#2764](https://github.com/pytorch/pytorch/pull/2764)\r\n- Add cudnn Tensor Core ops to RNNs for Volta [#3409](https://github.com/pytorch/pytorch/pull/3409)\r\n- Vectorize ``exp``, ``log``, ``sin``, ``cos`` [#6078](https://github.com/pytorch/pytorch/pull/6078)\r\n- Reuse intermediate results over multiple backwards grad_inputs [#3526](https://github.com/pytorch/pytorch/pull/3526)\r\n\r\n### Distributed\r\n- DistributedDataParallel: 10% of NCCL backend perf improvements with mixed-precision support [#5064](https://github.com/pytorch/pytorch/pull/5064)\r\n- Slightly improve DistributedDataParallel (single-GPU binding) multi-process distributed training performance [#4870](https://github.com/pytorch/pytorch/pull/4870)\r\n\r\n\r\n## Bug fixes\r\n\r\n### torch operators\r\n- Improve ``torch.digamma`` precision near poles [#6517](https://github.com/pytorch/pytorch/pull/6517)\r\n- Fix incorrect behavior of ``Tensor.random_`` on negative inputs [#6463](https://github.com/pytorch/pytorch/pull/6463)\r\n- Fix undefined behavior in backward pass for ``tensor.permute(dims)`` with negative dims [#5945](https://github.com/pytorch/pytorch/pull/5945)\r\n- Fix integer overflow in ``torch.remainder`` operator (it would break with a divisor above ``2**48``) [#5906](https://github.com/pytorch/pytorch/pull/5906)\r\n- Fix memory leak in ``torch.bmm`` [#5744](https://github.com/pytorch/pytorch/pull/5744)\r\n- Make dimension checker of `scatter_add_` consistent with `scatter_`'s [#5659](https://github.com/pytorch/pytorch/pull/5659)\r\n- Fix CPU ``torch.multinomial`` with noncontiguous probability tensor input (previously, it would overwrite input data)[#5093](https://github.com/pytorch/pytorch/pull/5093)\r\n- Fix CUDA ``torch.multinomial`` using incorrect strides and being able to select zero-probability events. [#5774](https://github.com/pytorch/pytorch/pull/5774), [#5238](https://github.com/pytorch/pytorch/pull/5238)\r\n- Support empty index tensor for ``index_select`` [#3429](https://github.com/pytorch/pytorch/pull/3429)\r\n- Support empty indices tensor in CUDA ``Tensor.put_`` [#4486](https://github.com/pytorch/pytorch/pull/4486)\r\n- Improve stability of ``torch.cat`` with empty tensors [#3602](https://github.com/pytorch/pytorch/pull/3602), [#5971](https://github.com/pytorch/pytorch/pull/5971), [#5819](https://github.com/pytorch/pytorch/pull/5819)\r\n- Fix ``torch.fft`` in the case where any of the input dimensions is not aligned [#6118](https://github.com/pytorch/pytorch/pull/6118)\r\n- Improve the CUDA btrifact error message [#5644](https://github.com/pytorch/pytorch/pull/5644)\r\n- Return zeros for eigenvector tensor when not requested in ``torch.symeig``[#3411](https://github.com/pytorch/pytorch/pull/3411)\r\n- Fix ``torch.btrifact`` on tensors. [#4318](https://github.com/pytorch/pytorch/pull/4318)\r\n- Fix ``torch.pstrf`` on tensors. [#4883](https://github.com/pytorch/pytorch/pull/4883)\r\n- Fix memory leak in `torch.median` [6889](https://github.com/pytorch/pytorch/pull/6889)\r\n- Fix SVD backward on non-square matrices when `some=False` [6870](https://github.com/pytorch/pytorch/pull/6870)\r\n\r\n### core\r\n- Detect re-initialization of ``_C`` shared library that would often result in segfaults on exit [#6232](https://github.com/pytorch/pytorch/pull/6232)\r\n- Fix indexing with all zero ByteTensors [#3926](https://github.com/pytorch/pytorch/pull/3926)\r\n- Only allow dense floating-point types as the default tensor type. [#5674](https://github.com/pytorch/pytorch/pull/5674)\r\n- Initialize CUDA before setting CUDA tensor types as default to prevent crash [#4788](https://github.com/pytorch/pytorch/pull/4788)\r\n- Fix a bug where ``from_dlpack`` fails if CUDA is not initialized. [#4182](https://github.com/pytorch/pytorch/pull/4182)\r\n- Fix crash in creating a CUDA tensor with a numpy array [#5850](https://github.com/pytorch/pytorch/pull/5850)\r\n- Fix broken sharing of empty tensor in multiprocessing on some OSes [#6229](https://github.com/pytorch/pytorch/pull/6229)\r\n\r\n### autograd\r\n- Restore allow_unused functionality: throw error when differentiated input is unused or unreachable. [#6553](https://github.com/pytorch/pytorch/pull/6553)\r\n- Fix ``output_nr`` not being incremented correctly. This caused crashes in the backward pass of operations that don't ``requires_grad`` on some inputs. [#4812](https://github.com/pytorch/pytorch/pull/4812)\r\n- Fix nvprof parsing in the ``torch.autograd.profiler`` [#5840](https://github.com/pytorch/pytorch/pull/5840)\r\n\r\n### nn layers\r\n- Support only specifying size in certain dimension for adaptive pooling [#3127](https://github.com/pytorch/pytorch/pull/3127)\r\n- Fix reflection padding boundary checks to not cause invalid memory access [#6438](https://github.com/pytorch/pytorch/pull/6438)\r\n- Improve error messages for ``NLLLoss``. [#5299](https://github.com/pytorch/pytorch/pull/5299), [#6072](https://github.com/pytorch/pytorch/pull/6072)\r\n- Fix ``kl_div`` backward on CUDA. Previously it would not respect ``gradOutput`` when computing ``gradInput``. [#5814](https://github.com/pytorch/pytorch/pull/5814)\r\n- Fix incorrect ``bias`` size assert for ``Linear`` [#5992](https://github.com/pytorch/pytorch/pull/5992)\r\n- Fix incorrect ``nn.functional.convNd`` and ``nn.functional.conv_transposeNd`` error message [#5701](https://github.com/pytorch/pytorch/pull/5701)\r\n- Check that shape for input and target matches instead of number of elements for some loss functions [#5085](https://github.com/pytorch/pytorch/pull/5085)\r\n- Fix ``torch.diag`` backward returning square grad with non-square input [#4538](https://github.com/pytorch/pytorch/pull/4538)\r\n- Fix convolution type mismatch error message [#5815](https://github.com/pytorch/pytorch/pull/5815)\r\n- Add ``align_corners`` option to linearly interpolating upsampling and make the default upsampling behavior more consistent with other frameworks [#5927](https://github.com/pytorch/pytorch/pull/5927)\r\n- Prevent numerical issues with ``poisson_nll_loss`` when log_input=False [#3336](https://github.com/pytorch/pytorch/pull/3336)\r\n\r\n### CUDA\r\n- Ensure convolution weights are contiguous to fix CUDA ``ConvTranspose`` double backward [#4543](https://github.com/pytorch/pytorch/pull/4543)\r\n- Fix CUDA double backwards [#4460](https://github.com/pytorch/pytorch/pull/4460)\r\n\r\n### sparse\r\n- Fix embedding with ``sparse=True`` [#4686](https://github.com/pytorch/pytorch/pull/4686)\r\n- Fix sparse embedding backward when input contains only ``padding_idx`` [#6211](https://github.com/pytorch/pytorch/pull/6211)\r\n- Handle copying empty sparse tensors to/from CPU, GPU. [#5361](https://github.com/pytorch/pytorch/pull/5361)\r\n\r\n### dataloader\r\n- Add argument checks to the  ``torch.utils.data.Sampler`` classes, fixing a bug where ``DataLoader`` tries to load the entire dataset on non-integer ``batch_size``. [#6249](https://github.com/pytorch/pytorch/pull/6249)\r\n- Set ``dataloader.batch_size = None`` when batch_sampler is given, fixing a bug where ``DataLoader`` would report ``batch_size`` as ``1``. [#6108](https://github.com/pytorch/pytorch/pull/6108)\r\n- Improve signal handling in ``DataLoader`` [#4643](https://github.com/pytorch/pytorch/pull/4643)\r\n- Ignore ``FileNotFoundError`` when shutting down [#5380](https://github.com/pytorch/pytorch/pull/5380)\r\n- Make preprocessing deterministic [#4640](https://github.com/pytorch/pytorch/pull/4640)\r\n\r\n### optim\r\n- Cast tensors when loading optimizer state dicts to improve usability [#3658](https://github.com/pytorch/pytorch/pull/3658)\r\n- List model parameters in deterministic order to improve stability of ``load_state_dict()`` [#6031](https://github.com/pytorch/pytorch/pull/6031)\r\n- Add parameter range checks for all optimizers [#6000](https://github.com/pytorch/pytorch/pull/6000)\r\n- Fix ``AMSGrad`` mode for ``SparseAdam`` [#4314](https://github.com/pytorch/pytorch/pull/4314)\r\n\r\n### distributed and multi-gpu\r\n- Fix a number of distributed training errors caused by a detach in place error [#5829](https://github.com/pytorch/pytorch/pull/5829)\r\n- Don't modify requires_grad when running DataParallel in no_grad mode [#5880](https://github.com/pytorch/pytorch/pull/5880)\r\n- Add GPU guard for ``broadcast_coalesce`` for Distributed Data Parallel stability [#5655](https://github.com/pytorch/pytorch/pull/5655)\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.4.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.4.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.4.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/10702684", "dateCreated": "2018-05-30T21:37:47Z", "datePublished": "2018-04-24T20:49:48Z"}, {"tagName": "v0.3.1", "name": "Bug fixes and performance improvements", "authorName": "soumith", "authorType": "User", "body": "## Binaries\r\n\r\n- Removed support for CUDA capability 3.0 and 5.0 (they still work for source builds for now, but the commitment to support this forward is removed)\r\n- Stop binary releases for CUDA 7.5\r\n- Add CPU-only binary releases that are 10x smaller in size than the full binary with CUDA capabilities.\r\n\r\nAs always, links to our binaries are on http://pytorch.org\r\n\r\n## New features\r\n- Add [Cosine Annealing Learning Rate Scheduler](http://pytorch.org/docs/0.3.1/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR)   https://github.com/pytorch/pytorch/pull/3311\r\n- add `reduce` argument to `PoissonNLLLoss` to be able to compute unreduced losses   https://github.com/pytorch/pytorch/pull/3770\r\n- Allow `target.requires_grad=True` in `l1_loss` and `mse_loss` (compute loss wrt `target`)   https://github.com/pytorch/pytorch/pull/3876\r\n- Add [`random_split`](https://github.com/pytorch/pytorch/blob/master/torch/utils/data/dataset.py#L105-L113) that randomly splits a dataset into non-overlapping new datasets of given lengths https://github.com/pytorch/pytorch/pull/4435\r\n- Introduced scopes to annotate ONNX graphs to have better [TensorBoard visualization of models](https://github.com/lanpa/tensorboard-pytorch) https://github.com/pytorch/pytorch/pull/5153\r\nAllow `map_location` in `torch.load` to be a string, such as `map_location='cpu'` or `map_location='cuda:2'`   https://github.com/pytorch/pytorch/pull/4203\r\n\r\n## Bug Fixes\r\n\r\n#### Data Loader / Datasets / Multiprocessing\r\n- Made DataLoader workers more verbose on bus error and segfault. Additionally, add a `timeout` option to the DataLoader, which will error if sample loading time exceeds the given value. https://github.com/pytorch/pytorch/pull/3474\r\n- DataLoader workers used to all have the same random number generator (RNG) seed because of the semantics of `fork` syscall. Now, each worker will have it's RNG seed set to `base_seed + worker_id` where `base_seed` is a random int64 value generated by the parent process. You may use `torch.initial_seed()` to access this value in `worker_init_fn`, which can be used to set other seeds (e.g. NumPy) before data loading. `worker_init_fn` is an optional argument that will be called on each worker subprocess with the worker id as input, after seeding and before data loading https://github.com/pytorch/pytorch/pull/4018\r\n- Add additional signal handling in DataLoader worker processes when workers abruptly die.\r\n- Negative value for n_workers now gives a ValueError https://github.com/pytorch/pytorch/pull/4019\r\n- fixed a typo in `ConcatDataset.cumulative_sizes` attribute name https://github.com/pytorch/pytorch/pull/3534\r\n- Accept longs in default_collate for dataloader in python 2   https://github.com/pytorch/pytorch/pull/4001\r\n- Re-initialize autograd engine in child processes   https://github.com/pytorch/pytorch/pull/4158\r\n- Fix distributed dataloader so it pins memory to current GPU not GPU 0.   https://github.com/pytorch/pytorch/pull/4196\r\n\r\n#### CUDA / CuDNN\r\n- allow cudnn for fp16 batch norm https://github.com/pytorch/pytorch/pull/4021\r\n- Use `enabled` argument in `torch.autograd.profiler.emit_nvtx` (was being ignored)   https://github.com/pytorch/pytorch/pull/4032\r\n- Fix cuBLAS arguments for fp16 `torch.dot` https://github.com/pytorch/pytorch/pull/3660\r\n- Fix CUDA index_fill_ boundary check with small tensor size   https://github.com/pytorch/pytorch/pull/3953\r\n- Fix CUDA Multinomial checks   https://github.com/pytorch/pytorch/pull/4009\r\n- Fix CUDA version typo in warning  https://github.com/pytorch/pytorch/pull/4175\r\n- Initialize cuda before setting cuda tensor types as default   https://github.com/pytorch/pytorch/pull/4788\r\n- Add missing lazy_init in cuda python module   https://github.com/pytorch/pytorch/pull/4907\r\n- Lazy init order in set device, should not be called in getDevCount   https://github.com/pytorch/pytorch/pull/4918\r\n- Make torch.cuda.empty_cache() a no-op when cuda is not initialized   https://github.com/pytorch/pytorch/pull/4936\r\n\r\n#### CPU\r\n- Assert MKL ld* conditions for ger, gemm, and gemv   https://github.com/pytorch/pytorch/pull/4056\r\n\r\n#### torch operators\r\n- Fix `tensor.repeat` when the underlying storage is not owned by `torch` (for example, coming from numpy)   https://github.com/pytorch/pytorch/pull/4084\r\n- Add proper shape checking to torch.cat   https://github.com/pytorch/pytorch/pull/4087\r\n- Add check for slice shape match in index_copy_ and index_add_.   https://github.com/pytorch/pytorch/pull/4342\r\n- Fix use after free when advanced indexing tensors with tensors   https://github.com/pytorch/pytorch/pull/4559\r\n- Fix triu and tril for zero-strided inputs on gpu   https://github.com/pytorch/pytorch/pull/4962\r\n- Fix blas addmm (gemm) condition check   https://github.com/pytorch/pytorch/pull/5048\r\n- Fix topk work size computation   https://github.com/pytorch/pytorch/pull/5053\r\n- Fix reduction functions to respect the stride of the output   https://github.com/pytorch/pytorch/pull/4995\r\n- Improve float precision stability of `linspace` op, fix 4419.   https://github.com/pytorch/pytorch/pull/4470\r\n\r\n#### autograd\r\n- Fix python gc race condition with THPVariable_traverse   https://github.com/pytorch/pytorch/pull/4437\r\n\r\n#### nn layers\r\n- Fix padding_idx getting ignored in backward for Embedding(sparse=True)   https://github.com/pytorch/pytorch/pull/3842\r\nFix cosine_similarity's output shape   https://github.com/pytorch/pytorch/pull/3811\r\n- Add rnn args check   https://github.com/pytorch/pytorch/pull/3925\r\n- NLLLoss works for arbitrary dimensions https://github.com/pytorch/pytorch/pull/4654\r\n- More strict shape check on Conv operators https://github.com/pytorch/pytorch/pull/4637\r\n- Fix maxpool3d / avgpool3d crashes   https://github.com/pytorch/pytorch/pull/5052\r\n- Fix setting using running stats in InstanceNorm*d   https://github.com/pytorch/pytorch/pull/4444\r\n\r\n#### Multi-GPU\r\n- Fix DataParallel scattering for empty lists / dicts / tuples   https://github.com/pytorch/pytorch/pull/3769\r\n- Fix refcycles in DataParallel scatter and gather (fix elevated memory usage)  https://github.com/pytorch/pytorch/pull/4988\r\n- Broadcast output requires_grad only if corresponding input requires_grad   https://github.com/pytorch/pytorch/pull/5061\r\n\r\n#### core\r\n- Remove hard file offset reset in load()   https://github.com/pytorch/pytorch/pull/3695\r\n- Have __sizeof__ account for size of stored elements   https://github.com/pytorch/pytorch/pull/3821\r\n- Fix undefined FileNotFoundError   https://github.com/pytorch/pytorch/pull/4384\r\n- make torch.set_num_threads also set MKL threads (take 2)   https://github.com/pytorch/pytorch/pull/5002\r\n\r\n#### others\r\n- Fix wrong learning rate evaluation in CosineAnnealingLR in Python 2   https://github.com/pytorch/pytorch/pull/4656\r\n\r\n## Performance improvements\r\n- slightly simplified math in IndexToOffset   https://github.com/pytorch/pytorch/pull/4040\r\n- improve performance of maxpooling backwards   https://github.com/pytorch/pytorch/pull/4106\r\n- Add cublas batched gemm support.   https://github.com/pytorch/pytorch/pull/4151\r\n- Rearrange dimensions for pointwise operations for better performance.   https://github.com/pytorch/pytorch/pull/4174\r\n- Improve memory access patterns for index operations.   https://github.com/pytorch/pytorch/pull/4493\r\n- Improve CUDA softmax performance   https://github.com/pytorch/pytorch/pull/4973\r\n- Fixed double memory accesses of several pointwise operations.   https://github.com/pytorch/pytorch/pull/5068\r\n\r\n## Documentation and UX Improvements\r\n- Better error messages for blas ops with cuda.LongTensor   https://github.com/pytorch/pytorch/pull/4160\r\n- Add missing trtrs, orgqr, ormqr docs   https://github.com/pytorch/pytorch/pull/3720\r\n- change doc for Adaptive Pooling   https://github.com/pytorch/pytorch/pull/3746\r\n- Fix MultiLabelMarginLoss docs   https://github.com/pytorch/pytorch/pull/3836\r\n- More docs for Conv1d Conv2d   https://github.com/pytorch/pytorch/pull/3870\r\n- Improve Tensor.scatter_ doc   https://github.com/pytorch/pytorch/pull/3937\r\n- [docs] rnn.py: Note zero defaults for hidden state/cell   https://github.com/pytorch/pytorch/pull/3951\r\n- Improve Tensor.new doc   https://github.com/pytorch/pytorch/pull/3954\r\n- Improve docs for torch and torch.Tensor   https://github.com/pytorch/pytorch/pull/3969\r\n- Added explicit tuple dimensions to doc for Conv1d.   https://github.com/pytorch/pytorch/pull/4136\r\n- Improve svd doc   https://github.com/pytorch/pytorch/pull/4155\r\n- Correct instancenorm input size   https://github.com/pytorch/pytorch/pull/4171\r\n- Fix StepLR example docs   https://github.com/pytorch/pytorch/pull/4478\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.3.1", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.3.1", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.3.1", "url": "https://api.github.com/repos/pytorch/pytorch/releases/9664431", "dateCreated": "2018-02-09T17:07:43Z", "datePublished": "2018-02-14T00:36:58Z"}, {"tagName": "v0.3.0", "name": "Performance improvements, new layers, ship models to other frameworks (via ONNX), CUDA9, CuDNNv7, lots of bug fixes", "authorName": "soumith", "authorType": "User", "body": "## Table of contents\r\n\r\n- Breaking changes: removed `reinforce()`\r\n- New features\r\n  - Unreduced losses\r\n  - A profiler for the autograd engine\r\n  - More functions support Higher order gradients\r\n  - New features in Optimizers\r\n  - New layers and nn functionality\r\n  - New Tensor functions and Features\r\n  - Other additions\r\n- API changes\r\n- Performance improvements\r\n  - Big reduction in framework overhead (helps small models)\r\n  - 4x to 256x faster Softmax/LogSoftmax\r\n  - More...\r\n- Framework Interoperability\r\n  - DLPack Interoperability\r\n  - Model Exporter to ONNX (ship PyTorch to Caffe2, CoreML, CNTK, MXNet, Tensorflow)\r\n- Bug Fixes (a lot of them)\r\n\r\n## Breaking changes\r\n\r\nStochastic functions, i.e. `Variable.reinforce()` were removed because of their limited functionality and broad performance implications. The motivation for stochastic functions was to avoid book-keeping of sampled values. In practice, users were still book-keeping in their code for various reasons. We constructed an alternative, equally effective API, but did not have a reasonable deprecation path to the new API. Hence this removal is a breaking change.\r\n\r\nWe introduce the [torch.distributions](http://pytorch.org/docs/0.3.0/distributions.html) package to replace Stochastic functions.\r\n\r\nYour previous code typically looked like this:\r\n\r\n```python\r\nprobs = policy_network(state)\r\naction = probs.multinomial()\r\nnext_state, reward = env.step(action)\r\naction.reinforce(reward)\r\naction.backward()\r\n```\r\nThis is the new equivalent code:\r\n\r\n```python\r\nprobs = policy_network(state)\r\n# NOTE: categorical is equivalent to what used to be called multinomial\r\nm = torch.distributions.Categorical(probs)\r\naction = m.sample()\r\nnext_state, reward = env.step(action)\r\nloss = -m.log_prob(action) * reward\r\nloss.backward()\r\n```\r\n\r\n## New features\r\n\r\n### Unreduced losses\r\n\r\nNow, Some loss functions can compute per-sample losses in a mini-batch\r\n  - By default PyTorch sums losses over the mini-batch and returns a single scalar loss. This was limiting to users.\r\n  - Now, a subset of loss functions allow specifying `reduce=False` to return individual losses for each sample in the mini-batch\r\n  - Example: `loss = nn.CrossEntropyLoss(..., reduce=False)`\r\n  - Currently supported losses: `MSELoss`, `NLLLoss`, `NLLLoss2d`, `KLDivLoss`, `CrossEntropyLoss`, `SmoothL1Loss`, `L1Loss`\r\n  - More loss functions will be covered in the next release\r\n\r\n### An in-built Profiler in the autograd engine\r\n\r\nWe built a low-level profiler to help you identify bottlenecks in your models\r\n\r\nLet us start with an example:\r\n\r\n```\r\n>>> x = Variable(torch.randn(1, 1), requires_grad=True)\r\n>>> with torch.autograd.profiler.profile() as prof:\r\n...     y = x ** 2\r\n...     y.backward()\r\n>>> # NOTE: some columns were removed for brevity\r\n... print(prof)\r\n--------------------------------  ----------  ---------\r\nName                               CPU time   CUDA time\r\n-------------------------------   ----------  ---------\r\nPowConstant                        142.036us    0.000us\r\nN5torch8autograd9GraphRootE         63.524us    0.000us\r\nPowConstantBackward                184.228us    0.000us\r\nMulConstant                         50.288us    0.000us\r\nPowConstant                         28.439us    0.000us\r\nMul                                 20.154us    0.000us\r\nN5torch8autograd14AccumulateGradE   13.790us    0.000us\r\nN5torch8autograd5CloneE              4.088us    0.000us\r\n```\r\n\r\nThe profiler works for both CPU and CUDA models.\r\nFor CUDA models, you have to run your python program with a special `nvprof` prefix. For example:\r\n\r\n```\r\nnvprof --profile-from-start off -o trace_name.prof -- python <your arguments>\r\n\r\n# in python\r\n>>> with torch.cuda.profiler.profile():\r\n...     model(x) # Warmup CUDA memory allocator and profiler\r\n...     with torch.autograd.profiler.emit_nvtx():\r\n...         model(x)\r\n```\r\n\r\nThen, you can load `trace_name.prof` in PyTorch and print a summary profile report.\r\n\r\n```\r\n>>> prof = torch.autograd.profiler.load_nvprof('trace_name.prof')\r\n>>> print(prof)\r\n```\r\n\r\n[Read additional documentation here](http://pytorch.org/docs/0.3.0/autograd.html#profiler)\r\n\r\n\r\n### Higher order gradients\r\n\r\nAdded higher-order gradients support for the following layers\r\n\r\n- ConvTranspose, AvgPool1d, AvgPool2d, LPPool2d, AvgPool3d, MaxPool1d, MaxPool2d, AdaptiveMaxPool, AdaptiveAvgPool, FractionalMaxPool2d, MaxUnpool1d, MaxUnpool2d, nn.Upsample, ReplicationPad2d, ReplicationPad3d, ReflectionPad2d\r\n- PReLU, HardTanh, L1Loss, SoftSign, ELU, RReLU, Hardshrink, Softplus, SoftShrink, LogSigmoid, Softmin, GLU\r\n- MSELoss, SmoothL1Loss, KLDivLoss, HingeEmbeddingLoss, SoftMarginLoss, MarginRankingLoss, CrossEntropyLoss\r\n- DataParallel\r\n\r\n### Optimizers\r\n\r\n- [optim.SparseAdam](http://pytorch.org/docs/0.3.0/optim.html#torch.optim.SparseAdam): Implements a lazy version of Adam algorithm suitable for sparse tensors.\r\n  - In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.\r\n- Optimizers now have an [add_param_group](http://pytorch.org/docs/0.3.0/optim.html#torch.optim.Optimizer.add_param_group) function that lets you add new parameter groups to an already constructed optimizer.\r\n\r\n### New layers and nn functionality\r\n\r\n- Added AdpativeMaxPool3d and AdaptiveAvgPool3d\r\n- Added LPPool1d\r\n- [F.pad](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad) now has support for:\r\n    - 'reflection' and 'replication' padding on 1d, 2d, 3d signals (so 3D, 4D and 5D Tensors)\r\n    - constant padding on n-d signals\r\n- nn.Upsample now works for 1D signals (i.e. B x C x L Tensors) in `nearest` and `linear` modes.\r\n- [grid_sample](http://pytorch.org/docs/0.3.0/nn.html?highlight=grid_sampler#torch.nn.functional.grid_sample) now allows padding with the border value via `padding_mode=\"border\"`. `grid_sample` expects a grid in the range of `[-1, 1]`, and if the values are out of these bounds, padding with the value `0.0` is applied by default. However, in a lot of cases, using the border value (i.e. the nearest valid value) helps improve accuracy of the overall model.\r\n- Introducing `nn.utils.parameters_to_vector` and `nn.utils.vector_to_parameters`\r\n  - `parameters_to_vector` takes `net.parameters()` and return a 1D vector that contains all the parameters\r\n  - `vector_to_parameters` takes a vector of flattened parameters and copies the values over to a network's parameters\r\n  - Convenient for some reinforcement learning algorithms, such as cross-entropy method, TRPO etc., which need to pull all network parameters as one big vector, modify them, and put the modified vector back.\r\n- Allow user to not specify certain input dimensions for `AdaptivePool*d` and infer them at runtime.\r\n    - For example:\r\n    ```python\r\n    # target output size of 10x7\r\n    m = nn.AdaptiveMaxPool2d((None, 7))\r\n    ```\r\n- DataParallel container on CPU is now a no-op (instead of erroring out)\r\n\r\n\r\n### New Tensor functions and features\r\n- Introduced `torch.erf` and `torch.erfinv` that compute the error function and the inverse error function of each element in the Tensor.\r\n- adds broadcasting support to bitwise operators\r\n- Added `Tensor.put_` and `torch.take` similar to `numpy.take` and `numpy.put`.\r\n  - The take function allows you to linearly index into a tensor without viewing it as a 1D tensor\r\nfirst. The output has the same shape as the indices.\r\n  - The put function copies value into a tensor also using linear indices.\r\n  - Differences from `numpy` equivalents:\r\n    - `numpy.take` has an optional axis argument, which behaves like `index_select`. This `axis` argument is not yet present.\r\n    - `numpy.put` repeats the values if necessary to make them as long as indices. This behavior is not yet replicated.\r\n- add `zeros` and `zeros_like` for sparse Tensors.\r\n- 1-element Tensors can now be casted to Python scalars. For example: `int(torch.Tensor([5]))` works now.\r\n\r\n### Other additions\r\n\r\n- Added `torch.cuda.get_device_name` and `torch.cuda.get_device_capability` that do what the names say. Example:\r\n    ```python\r\n    >>> torch.cuda.get_device_name(0)\r\n    'Quadro GP100'\r\n    >>> torch.cuda.get_device_capability(0)\r\n    (6, 0)\r\n    ```\r\n- If one sets ` torch.backends.cudnn.deterministic = True`, then the CuDNN convolutions use deterministic algorithms\r\n- `torch.cuda_get_rng_state_all` and `torch.cuda_set_rng_state_all` are introduced to let you save / load the state of the random number generator over all GPUs at once\r\n- `torch.cuda.emptyCache()` frees the cached memory blocks in PyTorch's caching allocator. This is useful when having long-running ipython notebooks while sharing the GPU with other processes.\r\n\r\n\r\n## API changes\r\n\r\n- `softmax` and `log_softmax` now take a `dim` argument that specifies the dimension in which slices are taken for the softmax operation. `dim` allows negative dimensions as well (`dim = -1` will be the last dimension)\r\n- `torch.potrf` (Cholesky decomposition) is now differentiable and defined on `Variable`\r\n- Remove all instances of `device_id` and replace it with `device`, to make things consistent\r\n- `torch.autograd.grad` now allows you to specify inputs that are unused in the autograd graph if you use `allow_unused=True`\r\n   This gets useful when using `torch.autograd.grad` in large graphs with lists of inputs / outputs\r\n   For example:\r\n   ```python\r\n   x, y = Variable(...), Variable(...)\r\n   torch.autograd.grad(x * 2, [x, y]) # errors\r\n   torch.autograd.grad(x * 2, [x, y], allow_unused=True) # works\r\n   ```\r\n- `pad_packed_sequence` now allows a `padding_value` argument that can be used instead of zero-padding\r\n- `Dataset` now has a `+` operator (which uses `ConcatDataset`). You can do something like `MNIST(...) + FashionMNIST(...)` for example, and you will get a concatenated dataset containing samples from both.\r\n- `torch.distributed.recv` allows Tensors to be received from any sender (hence, `src` is optional). `recv` returns the rank of the sender.\r\n- adds `zero_()` to `Variable`\r\n- `Variable.shape` returns the size of the Tensor (now made consistent with Tensor)\r\n- `torch.version.cuda` specifies the CUDA version that PyTorch was compiled with\r\n- Add a missing function `random_` for CUDA.\r\n- torch.load and torch.save can now take a `pathlib.Path` object, which is a standard Python3 typed filepath object\r\n- If you want to load a model's `state_dict` into another model (for example to fine-tune a pre-trained network), `load_state_dict` was strict on matching the key names of the parameters. Now we provide a `strict=False` option to `load_state_dict` where it only loads in parameters where the keys match, and ignores the other parameter keys.\r\n- added `nn.functional.embedding_bag` that is equivalent to `nn.EmbeddingBag`\r\n\r\n\r\n## Performance Improvements\r\n\r\n- The overhead of `torch` functions on Variables was around 10 microseconds. This has been brought down to ~1.5 microseconds by moving most of the core autograd formulas into C++ using our ATen library. This speeds-up models that are very small, such as small LSTMs and other common models seen in NLP.\r\n- softmax and log_softmax are now [4x to 256x faster](https://github.com/pytorch/pytorch/pull/3245#issue-267805013) on the GPU after rewriting the gpu kernels\r\n- 2.5x to 3x performance improvement of the distributed AllReduce (gloo backend) by enabling GPUDirect\r\n- nn.Embedding's renorm option is much faster on the GPU. For embedding dimensions of `100k x 128` and a batch size of 1024, it is 33x faster.\r\n- All pointwise ops now use OpenMP and get multi-core CPU benefits\r\n- Added dedicated CUDA kernels for group convolutions where `groups == nInputPlane` (depthwise convolution). Speedups range from 5x to 1000x for tested layer sizes. See the [benchmark table](https://github.com/pytorch/pytorch/pull/3057#issuecomment-336519873) for more details as well as [this table](https://github.com/pytorch/pytorch/pull/3265#issue-268106225).\r\n- Fixed `optim.SGD`'s memory usage for sparse gradients (for ex. `nn.Embedding(..., sparse=True)`), reducing the usage on a user-provided test script by 10x.\r\n- Optional NNPack integration for faster CPU convolutions (not part of binaries)\r\n- Reduce overhead of broadcasting if Tensors aren't broadcastable\r\n- `torch.nn.utils.weight_norm` over the right-most dimensions is faster\r\n- Backward of `torch.norm` is sped up by ~1.5x\r\n- Improve the performance of `pack_padded_sequence`\r\n- Add a single-argument version of `torch.arange`. For example `torch.arange(10)`\r\n\r\n## Framework Interoperability\r\n\r\n### DLPack Interoperability\r\n\r\n[DLPack Tensors](https://github.com/dmlc/dlpack) are cross-framework Tensor formats. We now have `torch.utils.to_dlpack(x)` and `torch.utils.from_dlpack(x)` to convert between DLPack and torch Tensor formats. The conversion has zero memory copy and hence is very efficient.\r\n\r\n### Model exporter to ONNX\r\n\r\n[ONNX](http://onnx.ai) is a common model interchange format that can be executed in Caffe2, CoreML, CNTK, MXNet, Tensorflow at the moment. PyTorch models that are ConvNet-like and RNN-like (static graphs) can now be shipped to the ONNX format.\r\n\r\n- There is a new module torch.onnx (http://pytorch.org/docs/0.3.0/onnx.html) which provides the API for exporting ONNX models.\r\n\r\n- The operations supported in this release are:\r\n  - add, sub (nonzero alpha not supported), mul, div, cat, mm, addmm, neg, tanh, sigmoid, mean, t, transpose, view, split, squeeze\r\n  - expand (only when used before a broadcasting ONNX operator; e.g., add)\r\n  - prelu (single weight shared among input channels not supported)\r\n  - threshold (non-zero threshold/non-zero value not supported)\r\n  - Conv, ConvTranspose, BatchNorm, MaxPool, RNN, Dropout, ConstantPadNd, Negate\r\n  - elu, leaky_relu, glu, softmax, log_softmax, avg_pool2d\r\n  - unfold (experimental support with ATen-Caffe2 integration)\r\n  - Embedding (no optional arguments supported)\r\n  - RNN\r\n  - FeatureDropout (training mode not supported)\r\n  - Index (constant integer and tuple indices supported)\r\n\r\n## Usability Improvements\r\n\r\n  - More cogent error messages during indexing of Tensors / Variables\r\n  Breaking changes\r\n  - Add proper error message for specifying dimension on a tensor with no dimensions\r\n  - better error messages for Conv*d input shape checking\r\n  - More user-friendly error messages for LongTensor indexing\r\n  - Better error messages and argument checking for Conv*d routines\r\n  - Trying to construct a Tensor from a Variable fails more appropriately\r\n  - If you are using a PyTorch binary with insufficient CUDA version, then a `warning` is printed to the user.\r\n  - Fixed incoherent error messages in `load_state_dict`\r\n  - Fix error message for type mismatches with sparse tensors\r\n\r\n## Bug fixes\r\n\r\n#### torch\r\n\r\n- Fix CUDA lazy initialization to not trigger on calls to `torch.manual_seed` (instead, the calls are queued and run when CUDA is initialized)\r\n\r\n#### Tensor\r\n\r\n- if `x` is 2D, `x[[0, 3],]` was needed to trigger advanced indexing. The trailing comma is no longer needed, and you can do `x[[0, 3]]`\r\n- `x.sort(descending=True)` used to incorrectly fail for Tensors. Fixed a bug in the argument checking logic to allow this.\r\n- Tensor constructors with numpy input: `torch.DoubleTensor(np.array([0,1,2], dtype=np.float32))`\r\n  - torch will now copy the contents of the array in a storage of appropriate type.\r\n  - If types match, it will share the underlying array (no-copy), with equivalent semantics to initializing a tensor with another tensor.\r\n  - On CUDA, `torch.cuda.FloatTensor(np.random.rand(10,2).astype(np.float32))` will now work by making a copy.\r\n- `ones_like` and `zeros_like` now create Tensors on the same device as the original Tensor\r\n- `torch.multinomial` on the CPU would reshape the input `prob_dist` in-place. Fixed this to make sure the `prob_dist` input's shape is unchanged after the call to `multinomial`\r\n- `expand` and `expand_as` allow expanding an empty Tensor to another empty Tensor\r\n- when `[..., None, ...]` was given (i.e. newaxis placement in indexing was specified), PyTorch had different behavior from NumPy. This is made consistent with NumPy in all cases.\r\n- Fix exponential distribution implementation to never sample infinity - cuRAND returns numbers in (0, 1]\r\n- torch.HalfTensor supports `numpy()` and `torch.from_numpy`\r\n- Add additional size checking for `torch.scatter`\r\n- fix `torch.tril` and `torch.triu` on the GPU for storage-offset Tensors (would return incorrect result).\r\n- Fix a memory leak in CUDA qr decomposition\r\n- Fix stream-awareness issues in THCUNN kernels\r\n- Fix kwargs parsing in `torch.topk`\r\n- Fixed `random_` on CPU (which previously had a max value of 2^32) for DoubleTensor and LongTensor\r\n- Fix `ZeroDivisionError: float division by zero` when printing certain Tensors\r\n- `torch.gels` when `m > n` had a truncation bug on the CPU and returned incorrect results. Fixed.\r\n- Add a check in tensor.numpy() that checks if no positional arguments are passed\r\n- Before a Tensor is moved to CUDA pinned memory, added a check to ensure that it is `contiguous`\r\n- `any` and `all` work on empty Tensors on the cpu (previously errored out)\r\n- Fix `symeig` on CUDA for large matrices. The bug is that not enough space was being allocated for the workspace, causing some undefined behavior.\r\n- Improved the numerical stability of `torch.var` and `torch.std` by using Welford's algorithm\r\n- The Random Number Generator returned `uniform` samples with inconsistent bounds (inconsistency in cpu implementation and running into a cublas bug).\r\n  - Now, all `uniform` sampled numbers will return within the bounds `[0, 1)`, across all types and devices\r\n- Fix `torch.svd` to not segfault on large CUDA Tensors (fixed an overflow error in the magma bindings)\r\n- Allows empty index Tensor for `index_select` (instead of erroring out)\r\n- Previously when `eigenvector=False`, `symeig` returns some unknown value for the eigenvectors. Now we zero them out.\r\n\r\n#### sparse\r\n\r\n- Fix bug with 'coalesced' calculation in sparse 'cadd'\r\n- Fixes `.type()` not converting indices tensor.\r\n- Fixes sparse tensor coalesce on the GPU in corner cases\r\n\r\n\r\n#### autograd\r\n\r\n- Fixed crashes when calling backwards on leaf variable with requires_grad=False\r\n- fix bug on Variable `type()` around non-default GPU input.\r\n- when `torch.norm` returned `0.0`, the gradient was `NaN`. We now use the subgradient at `0.0`, so the gradient is `0.0`.\r\n- Fix an correctness issue with advanced indexing and higher-order gradients\r\n- `torch.prod`'s backward was failing on the GPU due to a type error, fixed.\r\n- Advanced Indexing on Variables now allows the index to be a LongTensor backed Variable\r\n- Variable.cuda() and Tensor.cuda() are consistent in kwargs options\r\n\r\n#### optim\r\n\r\n- `torch.optim.lr_scheduler` is now imported by default.\r\n\r\n#### nn\r\n\r\n- Returning a dictionary from a nn.Module's forward function is now supported (used to throw an error)\r\n- When `register_buffer(\"foo\", ...)` is called, and self.foo already exists, then instead of silently failing, now raises a `KeyError`\r\n- Fixed loading of older checkpoints of RNN/LSTM which were missing `_data_ptrs` attributes.\r\n- `nn.Embedding` had a hard error when using the `max_norm` option. This is fixed now.\r\n- when using the `max_norm` option, the passed-in indices are written upon (by the underlying implementation). To fix this, pass a clone of the indices to the renorm kernel.\r\n- `F.affine_grid` now can take non-contiguous inputs\r\n- EmbeddingBag can accept both 1D and 2D inputs now.\r\n- Workaround a CuDNN bug where batch sizes greater than 131070 fail in CuDNN BatchNorm\r\n- fix nn.init.orthogonal to correctly return orthonormal vectors when rows < cols\r\n- if BatchNorm has only `1` value per channel in total, raise an error in training mode.\r\n- Make cuDNN bindings respect the current cuda stream (previously raised incoherent error)\r\n- fix grid_sample backward when gradOutput is a zero-strided Tensor\r\n- Fix a segmentation fault when reflection padding is out of Tensor bounds.\r\n- If LogSoftmax has only 1 element, `-inf` was returned. Now this correctly returns `0.0`\r\n- Fix pack_padded_sequence to accept inputs of arbitrary sizes (not just 3D inputs)\r\n- Detect pointer aliasing in cuDNN RNN flatten_parameters and avoid that path.\r\n- Fixed ELU higher order gradients when applied in-place\r\n- Workaround a CuDNN RNN bug for half-precision\r\n- Prevent numerical issues with `poisson_nll_loss` when `log_input=False` by adding a small epsilon\r\n\r\n#### distributed and multi-gpu\r\n\r\n- Allow kwargs-only inputs to DataParallel. This used to fail: `n = nn.DataParallel(Net()); out = n(input=i)`\r\n- DistributedDataParallel calculates num_samples correctly in python2\r\n- Fix the case of DistributedDataParallel when 1-GPU per process is used.\r\n- Fixed DataParallel to specify GPUs that don't include GPU-0\r\n- DistributedDataParallel's exit doesn't error out anymore, the daemon flag is set.\r\n- Fix a bug in DistributedDataParallel in the case when model has no `buffers` (previously raised incoherent error)\r\n- Fix `__get_state__` to be functional in `DistributedDataParallel` (was returning nothing)\r\n- Fix a deadlock in the NCCL bindings when GIL and CudaFreeMutex were starving each other\r\n\r\n#### Others\r\n\r\n- `model.zoo.load_url` now first attempts to use the `requests` library if available, and then falls back to `urllib`\r\n- Fix error when default_collate is passed a collection of `numpy.str_`\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.3.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.3.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.3.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/8770325", "dateCreated": "2017-12-04T08:00:43Z", "datePublished": "2017-12-05T01:57:11Z"}, {"tagName": "v0.2.0", "name": "Higher order gradients, Distributed PyTorch, Broadcasting, Advanced Indexing, New Layers and more", "authorName": "soumith", "authorType": "User", "body": "Here comes the next major release of PyTorch, just in time for ICML.  Install it today from our website http://pytorch.org\r\nPackage documentation for this release is available at [http://pytorch.org/docs/0.2.0/](http://pytorch.org/docs/0.2.0/)\r\n\r\nWe're introducing long-awaited features such as Broadcasting, Advanced Indexing, Higher-order gradients and finally: Distributed PyTorch.\r\n\r\n**Due to introducing Broadcasting, the code behavior for certain broadcastable situations is different from behavior in 0.1.12. This might lead to silent bugs in your existing code. We've provided easy ways of identifying this ambiguous code in the *Important Breakages and Workarounds* section.**\r\n\r\nTable of contents:\r\n- Tensor Broadcasting (numpy-style)\r\n- Advanced Indexing for Tensors and Variables\r\n- Higher-order gradients\r\n- Distributed PyTorch (multi-node training, etc.)\r\n- Neural Network layers and features: SpatialTransformers, WeightNorm, EmbeddingBag, etc.\r\n- New in torch and autograd: matmul, inverse, etc.\r\n- Easier debugging, better error messages\r\n- Bug Fixes\r\n- **Important Breakages and Workarounds**\r\n\r\n## Tensor Broadcasting (numpy-style)\r\n\r\nIn short, if a PyTorch operation supports broadcasting, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\r\n\r\nPyTorch Broadcasting semantics [closely follow numpy-style broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html#module-numpy.doc.broadcasting); if you are familiar with numpy broadcasting, things should just work as expected.\r\n\r\n### General Semantics\r\n\r\nTwo tensors are \u201cbroadcastable\u201d if the following rules hold:\r\n- Each tensor has at least one dimension.\r\n- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\r\n\r\nFor Example:\r\n\r\n```python\r\n>>> x=torch.FloatTensor(5,7,3)\r\n>>> y=torch.FloatTensor(5,7,3)\r\n# same shapes are always broadcastable (i.e. the above rules always hold)\r\n\r\n# can line up trailing dimensions\r\n>>> x=torch.FloatTensor(5,3,4,1)\r\n>>> y=torch.FloatTensor(  3,1,1)\r\n\r\n# x and y are broadcastable.\r\n# 1st trailing dimension: both have size 1\r\n# 2nd trailing dimension: y has size 1\r\n# 3rd trailing dimension: x size == y size\r\n# 4th trailing dimension: y dimension doesn't exist\r\n\r\n# but:\r\n>>> x=torch.FloatTensor(5,2,4,1)\r\n>>> y=torch.FloatTensor(  3,1,1)\r\n# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\r\n```\r\n\r\nIf two tensors x, y are \"broadcastable\", the resulting tensor size is calculated as follows:\r\n- If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\r\n- Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\r\n\r\nFor Example:\r\n\r\n```python\r\n# can line up trailing dimensions to make reading easier\r\n>>> x=torch.FloatTensor(5,1,4,1)\r\n>>> y=torch.FloatTensor(  3,1,1)\r\n>>> (x+y).size()\r\ntorch.Size([5, 3, 4, 1])\r\n\r\n# error case\r\n>>> x=torch.FloatTensor(5,2,4,1)\r\n>>> y=torch.FloatTensor(  3,1,1)\r\n>>> (x+y).size()\r\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\r\n```\r\n\r\nMore details [can be found on the PyTorch documentation site](http://pytorch.org/docs/0.2.0/notes/broadcasting.html).  Also, each torch function lists its broadcasting semantics in the documentation.\r\n\r\n## Advanced Indexing for Tensors and Variables\r\n\r\nPyTorch now supports a subset of NumPy style [advanced indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing). This allows users to select arbitrary indices at each dimension of the Tensor, including non-adjacent indices and duplicate indices, using the same `[]`-style operation. This allows for a more flexible indexing strategy without needing calls to PyTorch's `Index[Select, Add, ...]`  functions.\r\n\r\nLet's look at some examples:\r\n\r\n```python\r\nx = torch.Tensor(5, 5, 5)\r\n```\r\n\r\n**Pure Integer Array Indexing - specify arbitrary indices at each dimension**\r\n\r\n```python\r\nx[[1, 2], [3, 2], [1, 0]]\r\n--> yields a 2-element Tensor (x[1][3][1], x[2][2][0])\r\n```\r\n\r\n**also supports broadcasting, duplicates**\r\n\r\n```python\r\nx[[2, 3, 2], [0], [1]]\r\n--> yields a 3-element Tensor (x[2][0][1], x[3][0][1], x[2][0][1])\r\n```\r\n\r\n**arbitrary indexer shapes allowed**\r\n\r\n```python\r\nx[[[1, 0], [0, 1]], [0], [1]].shape\r\n--> yields a 2x2 Tensor [[x[1][0][1], x[0][0][1]],\r\n                         [x[0][0][1], x[1][0][1]]]\r\n```\r\n\r\n**can use colon, ellipse**\r\n\r\n```python\r\nx[[0, 3], :, :]\r\nx[[0, 3], ...]\r\n--> both yield a 2x5x5 Tensor [x[0], x[3]]\r\n```\r\n\r\n**also use Tensors to index!**\r\n\r\n```python\r\ny = torch.LongTensor([0, 2, 4])\r\nx[y, :, :]\r\n--> yields a 3x5x5 Tensor [x[0], x[2], x[4]]\r\n```\r\n\r\n**selection with less than ndim, note the use of comma**\r\n\r\n```python\r\nx[[1, 3], ]\r\n--> yields a 2x5x5 Tensor [x[1], x[3]]\r\n```\r\n\r\n## Higher order gradients\r\n\r\nNow you can evaluate higher order differentials in PyTorch. For example, you can compute Hessian-Vector products, penalize the norm of the gradients of your model, implement Unrolled GANs and Improved WGANs, etc.\r\n\r\nIn the `0.2` release, we've enabled the ability to compute higher order gradients for all of `torch.XXX` functions and the most popular `nn`layers. The rest will be covered in the next release.\r\n\r\nHere's a short example that penalizes the norm of the weight gradients of a Resnet-18 model, so that the volume of weights is slow-changing.\r\n\r\n```python\r\nimport torch\r\nfrom torchvision.models import resnet18\r\nfrom torch.autograd import Variable\r\n\r\nmodel = resnet18().cuda()\r\n\r\n# dummy inputs for the example\r\ninput = Variable(torch.randn(2,3,224,224).cuda(), requires_grad=True)\r\ntarget = Variable(torch.zeros(2).long().cuda())\r\n\r\n# as usual\r\noutput = model(input)\r\nloss = torch.nn.functional.nll_loss(output, target)\r\n\r\ngrad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)\r\n# torch.autograd.grad does not accumuate the gradients into the .grad attributes\r\n# It instead returns the gradients as Variable tuples.\r\n\r\n# now compute the 2-norm of the grad_params\r\ngrad_norm = 0\r\nfor grad in grad_params:\r\n    grad_norm += grad.pow(2).sum()\r\ngrad_norm = grad_norm.sqrt()\r\n\r\n# take the gradients wrt grad_norm. backward() will accumulate\r\n# the gradients into the .grad attributes\r\ngrad_norm.backward()\r\n\r\n# do an optimization step\r\noptimizer.step()\r\n```\r\n\r\nWe see two new concepts here:\r\n\r\n1. [torch.autograd.grad](http://pytorch.org/docs/master/autograd.html#torch.autograd.grad) is a function that takes in [outputs, list of inputs (for which you want gradients)], and returns the gradients wrt. these inputs as a tuple, rather than accumulating the gradients into the `.grad` attributes. This is useful if you want to further operate on the gradients.\r\n2. You can operate on the gradients, and call `backward()` on them.\r\n\r\nThe list of `nn` layers that support higher order gradients are:\r\n- `AvgPool*d`, `BatchNorm*d`, `Conv*d`, `MaxPool1d,2d`, `Linear`, `Bilinear`\r\n- `pad`, `ConstantPad2d`, `ZeroPad2d`, `LPPool2d`,  `PixelShuffle`\r\n- `ReLU6`, `LeakyReLU`, `PReLU`, `Tanh`, `Tanhshrink`, `Threshold`, `Sigmoid`, `HardTanh`, `ELU`, `Softsign`, `SeLU`\r\n- `L1Loss`, `NLLLoss`, `PoissonNLLLoss`, `LogSoftmax`, `Softmax2d`\r\nThe rest will be enabled in the next release.\r\n\r\nTo enable higher order gradients, we've introduced a new style of writing `autograd.Function` (the current/old style of writing functions is fully backward compatible). [You can read more about the new style of functions here](http://pytorch.org/docs/0.2.0/notes/extending.html).\r\n\r\nMost of you dont write your own `autograd.Function`s, they are low-level primitives that introduce\r\nnew operations to the autograd engine, where you specify the forward and backward calls.\r\n\r\n## Distributed PyTorch\r\n\r\nWe introduce the [torch.distributed](http://pytorch.org/docs/0.2.0/distributed.html) package that allows you to exchange Tensors among multiple machines. Using this package, you can scale your network training over multiple machines and larger mini-batches. For example, you are given the primitives to implement [Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/abs/1706.02677).\r\n\r\nThe `distributed` package follows an MPI-style programming model. This means that there are functions provided to you such as `send`, `recv`, `all_reduce` that will exchange Tensors among nodes (machines).\r\n\r\nFor each of the machines to first identify each other and assign unique numbers to each other (ranks), we provide simple initialization methods:\r\n- shared file system (requires that all processes can access a single file system)\r\n- IP multicast (requires that all processes are in the same network)\r\n- environment variable (requires you to manually assign ranks and know an address of a node reachable from all processes)\r\n\r\nOur package documentation contains more details on initialization and available backends, but here's an example of initializing using a multicast address:\r\n\r\n```python\r\nimport torch.distributed as dist\r\n\r\ndist.init_process_group(backend='tcp',\r\n                        init_method='tcp://[ff15:1e18:5d4c:4cf0:d02d:b659:53ba:b0a7]:23456',\r\n                        world_size=4)\r\n\r\nprint('Hello from process {} (out of {})!'.format(\r\n        dist.get_rank(), dist.get_world_size()))\r\n```\r\n\r\nThis would print `Hello from process 2 (out of 4)`on the 3rd machine.\r\n\r\nWorld size is the number of processes that will participate in the job. Each will be assigned a rank, which is a number between 0 and world_size - 1, unique within this job. It will serve as a process identifier and will be used instead of an address to, for example, specify to which process should a tensor be sent.\r\n\r\nHere's a snippet that shows how simple point-to-point communication can be performed:\r\n\r\n```python\r\n# All processes (receiving ones too!) need to have tensors of appropriate\r\n# size preallocated.\r\nx = torch.Tensor(10)\r\nif dist.get_rank() == 0:\r\n    x.normal_()\r\n    # Send x to process with rank 1\r\n    dist.send(x, dst=1)\r\nelse:  # rank == 1\r\n    # Receive data from process with rank 0 and save result in x\r\n    dist.recv(x, src=0)\r\n```\r\n\r\nAsynchronous p2p functions (`isend`, `irecv`) are available too.\r\n\r\nHowever, some communication patterns appear so often that more efficient collective calls have been developed. They typically engage the whole process group and are much faster than naive algorithms using `send`/`recv`. One example is `all_reduce`:\r\n\r\n```python\r\nx = torch.Tensor([dist.get_rank()])\r\n# Add tensors from all processes such that they all receive the result.\r\n# x is an input and output to this operation.\r\ndist.all_reduce(x)\r\n```\r\n\r\nThe distributed package is fairly low-level, so that it allows to implement more advanced algorithms and tailor the code to very specific purposes, but data-parallel training is such a common one that we have created high-level helpers for it.\r\n\r\nHence, we've introduced `DistributedDataParallel`, which is meant to be a nearly drop-in replacement for nn.DataParallel.\r\nHere's a code snippet demonstrating changes necessary to add it to existing training code:\r\n\r\n```python\r\n# Wrap model in DistributedDataParallel (CUDA only for the moment)\r\nmodel = torch.nn.parallel.DistributedDataParallel(model.cuda())\r\n\r\n# Use a DistributedSampler to restrict each process to a distinct subset\r\n# of the dataset.\r\ntrain_dataset = ...\r\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\r\ntrain_loader = torch.utils.data.DataLoader(\r\n    train_dataset, batch_size=args.batch_size, num_workers=args.workers,\r\n    pin_memory=True, sampler=train_sampler)\r\n\r\nfor epoch in range(args.num_epochs):\r\n    # Use .set_epoch() method to reshuffle the dataset partition at every iteration\r\n    train_sampler.set_epoch(epoch)\r\n    # training loop\r\n    ...\r\n```\r\n\r\nYou can see a fuller [Imagenet training example here](https://github.com/pytorch/examples/tree/master/imagenet)\r\n\r\n## New nn layers: SpatialTransformers, WeightNorm, EmbeddingBag, etc.\r\n\r\n#### New features\r\n- [forward_pre_hook](http://pytorch.org/docs/master/nn.html#torch.nn.Module.register_forward_pre_hook) is introduced to execute user-specified closures right before a forward function is called.\r\n- Convenient access to non-leaf gradients:\r\nCurrently, to access and inspect gradients of intermediate values, we have to use `hooks`. This is not convenient for doing simple inspections. Hence, we introduce `retain_grad`. It is best explained via an example:\r\n\r\n```python\r\ninput = Variable(torch.rand(1, 3), requires_grad=True)\r\nh1 = input * 3\r\nout = (h1 * h1).sum()\r\n\r\nh1.retain_grad()\r\nout.backward()\r\n\r\nprint(h1.grad)\r\n# without calling retain_grad(), h1.grad is None\r\n```\r\n- DataParallel now supports dicts as inputs\r\n\r\n#### New Layers\r\n\r\n- Spatial Transformer Networks via `F.grid_sample` and `F.affine_grid`\r\n- `nn.SeLU` and `nn.AlphaDropout` are introduced, from the paper: [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n- `nn.GLU` (Gated Linear Unit) is introduced from the paper [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)\r\n- [Weight Normalization](https://arxiv.org/abs/1602.07868) is now implemented via [torch.utils.weight_norm](http://pytorch.org/docs/master/nn.html#torch.nn.utils.weight_norm).\r\n- You can now ignore specific target indices while computing `cross_entropy_loss` and `nll_loss` using the `ignore_index` argument. This is a cheap and useful way of implementing masking, where you can have a `mask` index that is ignored in computing the loss.\r\n- `F.normalize` implements dimension-wise renormalization\r\n- `F.upsample` and `nn.Upsample` consolidate multiple Upsampling layers into one function. It implements 2d and 3d bilinear/trilinear/nearest upsampling.\r\n- `nn.EmbeddingBag`: When build bag-of-words models, doing an `Embedding` followed by `Sum` or `Mean` is common. For variable length sequences, computing bags of embeddings involves masking. We provide a singe `nn.EmbeddingBag` which is much more efficent and faster to compute bags of embeddings, especially for variable length sequences.\r\n- Numerically stable Binary Cross-Entropy loss via `bce_with_logits`\r\n- A negative log-likelihood loss with Poisson distribution of the target via `PoissonNLLLoss`\r\n- `cosine_similarity`: Returns cosine similarity between x1 and x2, computed along dim.\r\n\r\n#### training utilities\r\n\r\n*Learning Rate Schedulers:* [torch.optim.lr_scheduler](http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate) provides several dumb and smart methods to adjust the current learning rate. They are quite convenient while experimenting, giving a proxy for what you as the user would likely want to do.\r\n\r\nThere are various strategies provided, which can be used depending on the appropriate situation, more can be read in the [package docs](http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate):\r\n - ReduceLROnPlateau, LambdaLR, StepLR, MultiStepLR, ExponentialLR\r\n\r\n\r\n*`ConcatDataset`* that is a convenient dataset meta-class that can merge and concatenate two individual datasets.\r\n\r\n## New in torch and autograd\r\n\r\n- All reduce functions such as `sum` and `mean`now default to squeezing the reduced dimension. For example `torch.sum(torch.randn(10, 20), 0)` returns a 1D Tensor.\r\n- `x.shape`, similar to numpy. A convenience `property` that is equivalent to `x.size()`\r\n- `torch.matmul`, similar to np.matmul\r\n- bitwise and, or, xor, lshift, rshift\r\n- autograd support for `inverse`, `gesv`, `cumprod`, `atan2`\r\n- unbiased `var` and `std` now available via keyword argument option\r\n- `torch.scatter_add` - torch.scatter, except when duplicate indices are encountered, the values are summed.\r\n- torch.median behaves similar to torch.sum when no arguments are given, i.e. it reduces all the dimensions and returns a single median value of the flattened Tensor.\r\n- masked_copy_ has been renamed to masked_scatter_ (with deprecation on masked_copy_)\r\n- torch.manual_seed now seeds all CUDA devices as well\r\n- You can now specify the random number generator object via keyword arguments `torch.rand(1000, generator=gen)`\r\n\r\n## Bug-fixes and small improvements\r\n\r\n- Now we emit an error when a Variable is converted to a bool. For example:\r\n\r\n```\r\nb = Variable(torch.zeros(1))\r\nif b[0]: # errors now\r\n```\r\n\r\n- Fix correctness bugs in qr decomposition on CUDA.\r\n- Support for IBM PowerPC64 platform\r\n- Check that the CuDNN version at compile-time is the same version at run-time.\r\n- Improve error message in CUDA forked subprocess\r\n- Faster transposed-copy on CPU\r\n- Improve error messages in InstanceNorm\r\n- Add more argument checking for various routines, especially BatchNorm and Convolution routines.\r\n- Better error messages around shape reporting across the CPU backend.\r\n- Support more than 8 GPUs per machine (work-around a CUDA p2p restriction)\r\n- Improve error message when accessing attributes that don't exist\r\n- t() of Variable consistent with Tensor\r\n- prevent divide-by-zero when dropout p=1\r\n- fix sharing of CUDA tensors on non-current devices\r\n- when BN epsilon < allowed CuDNN value, fallback to THNN\r\n- Fix thread-trashing when using different number of threads for MKL and OMP\r\n- improve memory usage when using CuDNN RNN\r\n- Fix ZeroPad2d backwards with negative padding\r\n- add dummy tensor.data property, to provide interpretable error message to users\r\n- Fix in-place division for Python3\r\n- Raise error when call from_numpy on 0-dim array\r\n- Empty Tensors dont error out when shared across multiprocessing\r\n- fix baddbmm for expanded tensors\r\n- Let parallel_apply accept arbitrary inputs\r\n- keyword arguments in Tensor and Variable are now consistent\r\n- fix torch.inverse when Magma is not available\r\n- Add logical not operator for ByteTensor\r\n- add device asserts in scatter/gather kernels\r\n\r\n## Important Breakages and Workarounds\r\n\r\nAs you've read, we've introduced two important changes that are not\r\nbackward compatible:\r\n- Numpy-style Broadcasting\r\n- Reduction functions such as `sum(1)` now default to `keepdim=False`\r\n\r\nWe provide different levels of Python warnings that you can enable to alert you if you are using deprecated behavior or if the behavior of your code has changed.\r\n\r\n#### tl;dr\r\nHere is a code snippet that you can add to the top of your scripts.\r\nAdding this code will generate warnings highlighting incompatible code.\r\n\r\nFix your code to no longer generate warnings.\r\n\r\n```python\r\n# insert this to the top of your scripts (usually main.py)\r\nimport sys, warnings, traceback, torch\r\ndef warn_with_traceback(message, category, filename, lineno, file=None, line=None):\r\n    sys.stderr.write(warnings.formatwarning(message, category, filename, lineno, line))\r\n    traceback.print_stack(sys._getframe(2))\r\nwarnings.showwarning = warn_with_traceback; warnings.simplefilter('always', UserWarning);\r\ntorch.utils.backcompat.broadcast_warning.enabled = True\r\ntorch.utils.backcompat.keepdim_warning.enabled = True\r\n```\r\nOnce all warnings disappear, you can remove the code snippet.\r\n\r\n#### More elaborately\r\n\r\nNow, let us see the three incompatible changes with examples.\r\n\r\n##### Using the (now deprecated) 1-dimensional view pointwise function\r\n\r\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes, as long as the number of elements in each tensor was equal.  The pointwise operation would then be carried out by viewing each tensor as 1-dimensional. PyTorch now supports broadcasting. The \u201c1-dimensional\u201d pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are not broadcastable, but have the same number of elements.\r\n\r\nFor example:\r\n\r\n```python\r\n>>> torch.add(torch.ones(4), torch.ones(2,2))\r\n__main__:1: UserWarning: self and other not broadcastable, but have the same\r\nnumber of elements.  Falling back to deprecated pointwise behavior.\r\n2\r\n2\r\n2\r\n2\r\n[torch.FloatTensor of size 4]\r\n```\r\n\r\n##### Broadcasting in code where it didn't happen before\r\nThe introduction of broadcasting can cause backwards incompatible changes in the case where two tensors do not have the same shape,\r\nbut are broadcastable and have the same number of elements.\r\n\r\nFor example:\r\n\r\n```python\r\n>>> torch.add(torch.ones(4,1), torch.randn(4))\r\n```\r\n\r\nwould previously produce a Tensor with size: `torch.Size([4,1])`,\r\nbut now produces a Tensor with size: `torch.Size([4,4])`.\r\n\r\nIn order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist, you may set `torch.utils.backcompat.broadcast_warning.enabled` to `True`, which will generate a python warning in such cases.\r\n\r\nFor Example:\r\n\r\n```python\r\n>>> torch.utils.backcompat.broadcast_warning.enabled=True\r\n>>> torch.add(torch.ones(4,1), torch.ones(4))\r\n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\r\n```\r\nNote that this setting can trigger warnings for valid uses of broadcasting (including in library code), so you probably want to turn this warning off after migrating your code.\r\n\r\n##### KeepDim=False for Reduction Functions\r\n\r\nTo get a warning when using a dimensional reduction function with the default keepdim argument, set `torch.utils.backcompat.keepdim_warning.enabled` to `True`.  For example:\r\n\r\n```python\r\n>>> torch.sum(torch.ones(2,3), 1)\r\n__main__:1: UserWarning: backwards compatibility: call to \"sum\" uses default value for keepdim which has changed default to False.  Consider passing as kwarg.\r\n3\r\n3\r\n[torch.FloatTensor of size 2]\r\n```\r\n\r\nAs with `torch.utils.backcompat.broadcast_warning.enabled`, this warning can trigger from valid code, so you most likely want to disable this warning after migrating your code.\r\n\r\nNote also that using `keepdim=False` can cause your existing code to \"just work\" with broadcasting.  For example:\r\n\r\n```python\r\n# behavior with (old) keepdim=True, causes accidental broadcast\r\n>>> torch.add(torch.ones(4), torch.ones(4,4).sum(dim=1, keepdim=True))\r\n5  5  5  5\r\n5  5  5  5\r\n5  5  5  5\r\n5  5  5  5\r\n[torch.FloatTensor of size 4x4]\r\n\r\n# new behavior with keepdim=False is equivalent to non-broadcasted result\r\n>>> torch.add(torch.ones(4), torch.ones(4,4).sum(dim=1, keepdim=False))\r\n5\r\n5\r\n5\r\n5\r\n[torch.FloatTensor of size 4]\r\n```\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.2.0", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.2.0", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.2.0", "url": "https://api.github.com/repos/pytorch/pytorch/releases/7295746", "dateCreated": "2017-08-28T14:41:55Z", "datePublished": "2017-08-28T14:43:31Z"}, {"tagName": "v0.1.12", "name": "Sparse support for CUDA, bug fixes, performance improvements", "authorName": "soumith", "authorType": "User", "body": "API Changes\r\n-----------\r\n- `torch.range` is deprecated in favor of `torch.arange` which is consistent with numpy and python range.\r\n- On sparse Tensors, `contiguous` is renamed to `coalesce` and `coalesce` is now made out-of-place.\r\n  (a reminder that Sparse API is still experimental and evolving, so we dont provide backward-compability).\r\n\r\nNew Features\r\n------------\r\n\r\n### New layers and functions\r\n- `torch.topk` is now supported for all CUDA types, not just `torch.cuda.FloatTensor`.\r\n- Added a three-way ranking loss: [nn.TripletMarginLoss](http://pytorch.org/docs/nn.html#tripletmarginloss)\r\n- Added per-instance normalization layers: [nn.InstanceNorm1d](http://pytorch.org/docs/nn.html#instancenorm1d), [nn.InstanceNorm2d](http://pytorch.org/docs/nn.html#instancenorm2d), [nn.InstanceNorm3d](http://pytorch.org/docs/nn.html#instancenorm3d)\r\n  Each channel is treated as an instance to normalize, and mean-subtraction and std-division is done. This is useful when dealing with larger images and smaller mini-batches where BatchNorm like effects are desired.\r\n- `nn.ZeroPad2d` and `nn.ConstantPad2d` are added.\r\n- `nn.Bilinear` is added, which computes `Y = X1 * W * X2 + b`\r\n\r\n### Negative dimension support for all functions\r\nEvery single function that took a dimension argument will also allow taking negative dimensions.\r\n\r\nA negative dimension will index the tensor from the last dimension.\r\n\r\nFor example:\r\n\r\n```\r\nx = torch.randn(10, 20, 30)\r\ny = torch.mean(x, dim = -1)\r\n```\r\n\r\nHere, since `x` has 3 dimensions, and `dim = -1`, the last dimension, i.e. `dim=3` is picked for taking a mean.\r\n\r\nThe functions with dimension arguments are:\r\n```\r\nnarrow, transpose, size, cat, chunk, gather, index_select, split, squeeze,\r\nstack, unbind, unsqueeze, cumprod, cumsum, mean, median, mode, norm, prod, std,\r\nsum, var, kthvalue, max, min, sort, topk, renorm,\r\nindex_add, index_copy, index_fill, scatter, select, unfold\r\n```\r\n\r\n### CUDA support for Sparse Tensors, faster CPU sparse\r\n\r\nNow a part of the `torch.sparse` API is also supported for `torch.cuda.sparse.*Tensor`.\r\n\r\nFunctions that are supported on CUDA are:\r\n```\r\nsparse_mask, to_dense, coalesce, transpose, spaddmm\r\nspcadd, mul, div, cadd, csub, cmul\r\n```\r\n\r\n`nn.Embedding` now supports sparse even on CUDA (with the `sparse=True` flag) leveraging these sparse functions.\r\n\r\nA new hybrid matrix-multiply `hspmm` operation that multiplies a sparse matrix with a dense matrix and returns a matrix in the form of a hybrid tensor (i.e. 1 sparse dimension, 1 dense dimension).\r\n\r\nSeveral of the CPU sparse functions have more efficient implementations.\r\n\r\nIn a quickly hacked up Embedding classifier training script by @martinraison we see CUDA sparse performing as well as CUDA dense:\r\nhttps://gist.github.com/martinraison/1e7c18c6f6eda87f1cb4995b0e6a22a5\r\n\r\nTable times of seconds / batch\r\n\r\n_      | CPU  | CUDA\r\n-------|------|------\r\nDense  | 10   | 0.86\r\nSparse | 0.15 | 0.13\r\n\r\n### named_parameters to filter out specific parameter types\r\n\r\nLet's say that you want to add weight decay to all parameters of your model except for the biases. How do you get only the biases of your model?\r\nWe introduce [nn.Module.named_parameters](http://pytorch.org/docs/nn.html#torch.nn.Module.named_parameters) for this.\r\nIt joins `named_children` and `named_modules` in helping you filter specific attributes of models.\r\n\r\nExample of filtering out biases of a model and give them weight_decay of 0:\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.optim as optim\r\nm = nn.Sequential(\r\n      nn.Linear(10, 20),\r\n      nn.ReLU(),\r\n      nn.Linear(20, 20),\r\n      nn.ReLU(),\r\n    )\r\nweights, biases = [], []\r\nfor name, p in m.named_parameters():\r\n   if 'bias' in name:\r\n       biases += [p]\r\n   else:\r\n       weights += [p]\r\n\r\noptim.SGD([\r\n  {'params': weights},\r\n  {'params': biases, weight_decay=0}\r\n], lr=1e-2, momentum=0.9, weight_decay=1e-5)\r\n```\r\n\r\nPerformance Improvements\r\n------------------------\r\n- `cumsum` and `cumprod` have been significantly made faster on the GPU via using some thrust primitives where appropriate.\r\n- `LSTMCell` and `GRUCell` are now significantly faster on the GPU via a fused kernel\r\n- The default Algorithm for CuDNN has been changed to `PRECOMP_GEMM` which is a\r\n  much faster algorithm that takes a tiny bit of workspace. Previously, it used to\r\n  be `IMPLICIT_GEMM` which took zero workspace, but was significantly slower.\r\n- 5% to 10% improvement in data loader by collating batches directly into shared memory.\r\n- SVD is now computed on the GPU via divide-and-conquer (sgesdd) which gives a 2x to 5x speedup.\r\n- The commonly used function `expand` has been moved to C, to have better performance in smaller models.\r\n\r\nBug Fixes\r\n---------\r\n- Added contiguous checks on weight and bias for a large range of THNN functions\r\n- make the range of `random_` correct when both lower and upper bound are specified\r\n- `parallel_apply` now can take arguments that are unhashable\r\n- Reshape `grad` correctly in the Dot function (inputs don't have to be 1D vectors...)\r\n- Added `Variable.type_as`\r\n- Unify argument names of `norm` and `renorm` to have `p=norm_type, dim=dim`\r\n- `btrisolve` works on CPU doubles\r\n- ipython autocomplete for torch.nn.Module fixed via implementing `__dir__`\r\n- `device_ids` can now be `None` again in `F.data_parallel` and will use all available GPUs\r\n- workaround cudnn bugs in BatchNorm (<5.1.10) and Dilation (6.0.20)\r\n- Padding bugfix in Conv1d CPU\r\n- `remainder` and `cremainder` are fixed for integer types\r\n- fix memory leak in `btrisolve` and `getri`\r\n- If nn.Module's source cant be retrieved because of any exception,\r\n  handle serialization to be non-fatal\r\n- `collate_fn` now retains the type of the numpy array\r\n- `is_tensor` and `is_storage` are now fixed for old-style Python classes\r\n- `torch.cat` now supports keyword arguments\r\n- CUDA collectives supported coalescing, but the inputs were all assumed\r\n  to be of the same Tensor type. This is fixed.\r\n- Fix a deadlock bug in autograd because of an underlying glibc bug in specific\r\n  linux distros (ArchLinux in particular)\r\n- `abs` is now fixed for `char` and `short` cuda types\r\n- fix `torch.diag` autograd when giving a dimension argument\r\n- fix grouped convolution on CPU when `bias=False`\r\n- expose `dilated` convolutions for `ConvTranspose*d`\r\n- Fix a bug in `HingeEmbeddingLoss` where `margin` can now be specified via kwargs\r\n\r\nImproved error messages\r\n-----------------------\r\n- Fix errors and messages when no CUDA devices are available.\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.1.12", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.1.12", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.1.12", "url": "https://api.github.com/repos/pytorch/pytorch/releases/6258732", "dateCreated": "2017-05-01T19:55:29Z", "datePublished": "2017-05-02T22:26:49Z"}, {"tagName": "v0.1.11", "name": "CuDNN v6, new layers, lots of bugfixes", "authorName": "soumith", "authorType": "User", "body": "## Minor API Changes\r\n\r\n- in `optim.Adamax`, the default learning rate and epsilon have been made\r\n  consistent with Lasagne, Keras and TF.\r\n  - Previous: `(lr=1e-2, eps=1e-38)`\r\n  - Current : `(lr=2e-3, eps=1e-8)`\r\n- **Make `random_` range exclusive** (it used to be exclusive when only the upper bound was specified, and inclusive when both were given).\r\n- `torch.cat` now **disallows catting along inexistent dimensions**\r\n  (to make it consistent with numpy and Variable cat)\r\n- `torch.utils.clip_grad_norm` now returns the total norm (say, for logging purposes).\r\n\r\n## Performance Improvements\r\n- Reduce DataParallel overhead on >4 GPUs\r\n  - Improve broadcast/reduce performance by coalescing tensors\r\n- `nn.Embedding`'s backward performance increased for batch sizes > 1024\r\n\r\n## New Features\r\n**torch**\r\n- Batch triangular factorization and solves have been interfaced (CPU and GPU) and\r\n  are available under `torch.btrifact` and `torch.btrisolve`. [See documentation\r\n  for usage](http://pytorch.org/docs/torch.html#torch.btrifact)\r\n- All RNG functions now have `generator` specifiable via a keyword argument\r\n- `torch.mode` is now supported on the GPU via a high-performance kernel.\r\n\r\n**autograd, nn and optim**  \r\n- CuDNN v6 integrated:\r\n  - Faster Dilated Convolutions (and less memory hungry)\r\n  - 1D FFT-based Convolutions\r\n  - Significant performance improvement for Softmax layers\r\n  - Speedups across many functions\r\n  - Improved CuDNN error messages\r\n  - We will integrate persistent RNNs in the next release\r\n- `torch.trace`, `torch.cumsum`, `torch.cross` are now implemented in autograd  \r\n- `nll_loss` now supports Spatial inputs (i.e. 4d inputs BCHW) and computes\r\n    channel-wise cross-entropy.\r\n- `nn.PReLU` now supports all dimensional Tensors, not just 1d and 2d.\r\n- add `nn.PairwiseDistance` and `F.pairwise_distance` that compute batchwise\r\n    pairwise distance between two vectors.\r\n- Adaptive Max and Average Pooling added for 1d, 2d inputs via\r\n    `nn.AdaptiveMaxPooling1d`, `nn.AdaptiveAvgPooling2d`, etc.\r\n- RMSProp now has `momentum` and a `centered` option. If `centered` is True,\r\n  the gradient is normalized by an estimation of it's variance. (Graves 2013)\r\n\r\n**utils**  \r\n- `WeightedRandomSampler` has been added as a custom sampler for the DataLoader.\r\n  It samples elements from `[0,..,len(weights)-1]` with the given probabilities\r\n  and is useful to sample from unbalanced datasets where some classes have\r\n  many more samples than others. [See the docs](http://pytorch.org/docs/data.html)\r\n  for more details\r\n- DataLoader now allows returning of numpy arrays\r\n\r\n\r\n## Bug Fixes\r\n*torch*\r\n- When loading GPU checkpoints from disk with storage location remapping,\r\n  `torch.cuda` was still attempted to be imported. This is now fixed, and\r\n  you can load GPU checkpoints on machines with no GPUs or CUDA.\r\n- Work around an OSX `fread` bug where loading checkpoints of each Tensor > 1GB\r\n  would give an error.\r\n- Fixed a in `torch.cat` where it now does not\r\n  accept `reverse` (it's not a `PySequence`)\r\n  For example:\r\n  ```\r\n  l = [Variable(torch.ones(1,3)*i) for i in range(3)]\r\n  torch.cat(reversed(l), 0) # errors now\r\n  ```\r\n- Fix a memory leak in `torch.from_numpy`\r\n- GPU svd returned a larger matrix than expected in the `some` mode.\r\n  This is now fixed to match CPU behavior.\r\n- Fix a bug in CPU max that was introduced in the previous release.\r\n\r\n**autograd, nn and optim**  \r\n- Reassigning attributes in modules correctly works now.\r\n  This example used to not work correctly, `l.a` always remained `None`.\r\n  Now it works as one would expect:\r\n  ```python\r\n  l = nn.Linear(10, 20)\r\n  l.a = None\r\n  l.a = nn.Parameter(torch.randn(2))\r\n  # l.a is correctly updated\r\n  ```\r\n- Fix bug where adding a hook could replace an existing hook\r\n- Fix `nn.Embedding` and `nn.CosineEmbeddingLoss` to work without\r\n  error on non-float CUDA (half, double)\r\n- Fix a bug in `nn.Embedding` when the `max_norm` option was used. Some of the\r\n  indices were not respecting `max_norm` and this is fixed.\r\n- Fix corner-case in `Variable`'s SetItem where gradient was of incorrect shape.\r\n  `x.grad` used to be of shape 20, because `y[1]`` was of shape 20.\r\n  ```\r\n  x = Variable(torch.randn(1, 20), requires_grad=True)\r\n  y = Variable(torch.zeros(10, 20))\r\n  y[1] = x\r\n  ```\r\n- Fix a segfault in Conv1d when input doesn't require grad.\r\n- Assertions in `pack_padded_sequence` to check that sequence is of length > 0\r\n- `torch.prod`'s autograd forumlae were incorrect if the Tensor had 0. This\r\n  formula has been fixed.\r\n- Variable `expand` and `expand_as` had incorrect dimension inference when using\r\n  broadcasting semantics. The formula has been fixed in these cases.\r\n- Fix a size mismatch in `CosineEmbeddingLoss`. [See this issue](https://github.com/pytorch/pytorch/issues/1058) for more details.\r\n- Fixed a bug in LBFGS that caused it to use uninitialized locals. [See issue](https://github.com/pytorch/pytorch/issues/1039)\r\n- Add assertions for negative padding in `nn.Conv*` functions.\r\n- Fix the sttdev gradient formula for the stochastic function `normal`.\r\n\r\n**other**\r\n- Fix issue when returning strings from the DataLoader when `pin_memory=True`\r\n- Binaries no longer dependent on needing a `libcudart.so` at runtime.\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.1.11", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.1.11", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.1.11", "url": "https://api.github.com/repos/pytorch/pytorch/releases/5938760", "dateCreated": "2017-03-31T16:19:59Z", "datePublished": "2017-03-31T16:27:41Z"}, {"tagName": "v0.1.10", "name": "Variable-length RNNs, Better Indexing, Sparse Tensors, Faster CPU, Many Bug Fixes", "authorName": "soumith", "authorType": "User", "body": "New Features\r\n------------\r\n\r\n### Indexing and Broadcasting Improvements\r\n\r\n- Add broadcasting semantics to `expand` / `expand_as`.\r\n  - Previously, `expand` had no ability to add new dimensions, and `unsqueeze`\r\n    had to be used to first create singleton dimensions before expansion.\r\n  - Now, singleton dimensions are automatically prepended to the shape of\r\n    the tensor if a matching dimension is found.\r\n    Here's an example:\r\n    ```python\r\n    x = torch.rand(5)\r\n    y = torch.rand(4, 8, 5)\r\n    z = x.expand_as(y) # z is of shape (4, 8, 5)\r\n\r\n    x = torch.rand(1, 8, 1)\r\n    z.expand_as(y) # z is of shape (4, 8, 5)\r\n    ```\r\n- Unsqueeze dimensions using None indexing\r\n  ```python\r\n  a = torch.randn(10)\r\n  b = a.unsqueeze(0)\r\n  b = a[None, :]     # Equivalent operations\r\n  ```\r\n- Indexing with steps is supported (only positive steps)\r\n  ```python\r\n  In [1]: a = torch.randn(10)\r\n  In [2]: a\r\n  Out[2]:\r\n\r\n     0.1338\r\n     1.0789\r\n     1.2302\r\n    -1.3343\r\n    -0.4676\r\n     1.3511\r\n    -0.4374\r\n    -1.0611\r\n    -0.1528\r\n    -1.3994\r\n    [torch.FloatTensor of size 10]\r\n\r\n  In [3]: a[0:10:3]\r\n  Out[3]:\r\n\r\n     0.1338\r\n    -1.3343\r\n    -0.4374\r\n    -1.3994\r\n    [torch.FloatTensor of size 4]\r\n  ```\r\n\r\n### Variable-length mini-batches in Recurrent Networks\r\n`nn.RNN`, `nn.LSTM`, `nn.GRU` now support mini-batches where sequences are of variable\r\nlengths.\r\nYou can pass an input of type [`PackedSequence`](http://pytorch.org/docs/nn.html#packedsequence)\r\ninto these layers.\r\nA `PackedSequence` holds data and a list of sequence sizes of a packed sequence batch.\r\nFor example, a `PackedSequence` will hold an input mini-batch of such sequences:\r\n```\r\na b c d e\r\na b c d e f g h\r\na b\r\na b c d\r\n```\r\nHere, each input row is of variable length.\r\n\r\nYou can construct a `PackedSequence` using the provided function\r\n[`pack_padded_sequence`](http://pytorch.org/docs/nn.html#torch.nn.utils.rnn.pack_padded_sequence)\r\n\r\n`pack_padded_sequence` takes a `Variable` containing padded sequences, i.e. a `Tensor`\r\nof `T x B x *`, where `B` is the size of the mini-batch, and each input is either of\r\nlength `T` or is padded to length `T`. It also takes a list of lengths of each input.\r\nFrom these, it constructs a `PackedSequence`\r\n\r\nFor example, it will take [8, 5, 4, 2] and and an input `8 x 4 x 128`\r\nthat corresponds to:\r\n```\r\na b c d e f g h\r\na b c d e 0 0 0\r\na b c d 0 0 0 0\r\na b 0 0 0 0 0 0\r\n```\r\n\r\nThe output of the RNN layers will also be a `PackedSequence`, which can then be inverted\r\nback to a padded Tensor using the inverse function:\r\n[`pad_packed_sequence`](http://pytorch.org/docs/nn.html#torch.nn.utils.rnn.pad_packed_sequence)\r\n\r\n\r\n### Sparse Tensors (CPU)\r\nOriginal goals:\r\n- ability to propagate sparse updates in a network (e.g. for updating an embedding matrix)\r\n- ability to efficiently compute \"bag-of-words\" sentence embeddings (e.g. weighted average of word embeddings)\r\n\r\nImplemented features:\r\n- enable backpropagation of sparse gradients without conversion to dense tensors. In most cases a runtime exception is thrown when mixing different gradient types for the same variable\r\n- add some methods for `THSTensor`: `zero`, elementwise `add` and `mul`, scalar `mul` and `div`\r\n- make `addcmul` method of `THTensor` compatible with sparse operands\r\n- make `spmm` method accessible from Python as `dsmm`\r\n- `sparse_mask` method on `THTensor`. This produces a sparse tensor from a dense tensor,\r\n   by using a sparse tensor as a mask. A value is only present in the output sparse\r\n   tensor if it also exists in the mask.\r\n- update `optim.Adagrad` to use sparse updates when possible.\r\n- **leave `Variable`'s gradient to `None` by default.**\r\n  This is because there is no canonical zero gradient anymore (it could be dense or\r\n  sparse, and if it is sparse we don't know how many dimensions are sparse)\r\n- N-dimensional values for sparse tensors:\r\n  - Basically for things like applying sparse updates to embedding matrices, only the\r\n    first dimension (the one that corresponds to the word index) is sparse. The other\r\n    dimension is always dense (only whole embedding vectors are updated). An elegant\r\n    solution is to make the `values` tensor N-dimensional instead of 1-dimensional.\r\n    For an embedding matrix, the sparse gradient will have a `values` tensor of\r\n    size `nnz * embedding_size` instead of just `nnz`.\r\n\r\n### Common weight initialization methods for neural networks\r\nBy default, all `Linear` and `Conv` layers in PyTorch are initialized according to\r\na scheme proposed by [LeCun'98](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf).\r\n\r\nHowever, there are several other commonly used initialization methods.\r\nWe now support many other methods via `torch.nn.init`.\r\nSupported methods include:\r\n[`uniform`, `normal`, `constant`, `xavier_uniform`, `xavier_normal`, `kaiming_uniform`,\r\n`kaiming_normal`, `orthogonal`, `sparse`](http://pytorch.org/docs/nn.html#torch-nn-init)\r\n\r\nHere's an example of using these initialization methods:\r\n```python\r\nimport math\r\nfrom torch import nn\r\n\r\nclass Net(nn.Module):\r\n  def __init__(self):\r\n     super(Net, self).__init__()\r\n     self.conv1 = nn.Conv2d(5, 10, (3, 3))\r\n     nn.init.xavier_uniform(self.conv1.weight, gain=math.sqrt(2.0))\r\n     nn.init.constant(self.conv1.bias, 0.1)\r\n\r\nnetwork = Net()\r\n```\r\n\r\n### Other features\r\n- Added a gradient checker utility `torch.autograd.gradcheck` that can\r\n  be used to check your implementations. Here's a small example:\r\n  ```python\r\n  from torch.autograd import Variable, gradcheck\r\n  inputs = Variable(torch.randn(4, 4), requires_grad=True)\r\n  gradcheck(lambda x: 2*x.diag(), (inputs,), eps=1e-3)\r\n  ```\r\n- Add a [clip_grad_norm](http://pytorch.org/docs/nn.html#torch.nn.utils.clip_grad_norm) utility to easily clip gradients via constraints on their norms.\r\n- Document `nn.ModuleList` and `nn.ParameterList` that are immensely useful when\r\n  storing a list of modules in a `Container`\r\n- Optimizers have backward-compatiblity for old checkpoints.\r\n  `__set_state__` and `__get_state__` introduced into optimizers.\r\n- Add Nesterov momentum to `optim.SGD` via [`nesterov=True` kwarg](http://pytorch.org/docs/optim.html#torch.optim.SGD)\r\n- DataParallel supports multiple inputs and keyword args (which are also scattered)\r\n  ```\r\n  m = nn.DataParallel(model)\r\n  # Now valid\r\n  m(x, y, option=z)\r\n  ```\r\n  See the [documentation](http://pytorch.org/docs/nn.html?highlight=dataparallel#torch.nn.DataParallel) for exact behavior.\r\n- DataLoader's `default_collate` now also supports numpy arrays\r\n- Added `F.pad` that supports Constant, Reflection and Replication padding in a single\r\n  interface: [http://pytorch.org/docs/nn.html#pad](http://pytorch.org/docs/nn.html#pad)\r\n- `train()` now optionally supports a boolean argument. For example `model.train(False)`\r\n  will set it to `eval` mode and `model.train(True)` sets it to `train` mode.\r\n- Added a `DataLoader` sampler: `SubsetRandomSampler`that takes a list of indices\r\n  in it's constructor and randomly samples from these indices. Useful when you\r\n  want to sample only a particular subset of your dataset.\r\n- Transpose supports negative dimensions. For example:\r\n  ```python\r\n  a = torch.randn(2, 3)\r\n  b = a.transpose(0, 1)   # both are equivalent\r\n  b = a.transpose(-2, -1) # both are equivalent\r\n  ```\r\n\r\nPerformance Improvements\r\n------------------------\r\n- CPU Tensor backend gets faster\r\n  - Explicit AVX, AVX2 and improved SSE intrinsics to speedup copy, fill, add, mul, div\r\n  - Much improved speed for all apply and reduce operations to have better cache hits\r\n  - Added OpenMP in TH_TENSOR_APPLY* operations\r\n  - Overall, 2x to 10x+ faster on a lot of operations, closer to Numpy speeds\r\n  - Runtime dispatch of intrinsics based on CPU features (easy to ship binaries)\r\n- Serialization Improvements\r\n    - Fixed bugs on serialization for Tensors > 2GB\r\n    - 5x to 10x faster serialization (no longer Tarring Tensors)\r\n\r\nBug Fixes\r\n---------\r\n- Multi-GPU CuDNN RNN now has separate dropout descriptors per GPU\r\n- NLLLoss2d has proper shape checks on GPU and stable sizeAverage formulation\r\n- LogSoftmax2d has a more stable formula\r\n- Fix prodall (prod without dim arguments) to not average\r\n- Return correct number of gradients from cuDNN RNN\r\n- NLLLoss2d has support for weights\r\n- Fix Unpooling bug for MaxPool1d\r\n- Fix Indexing when using only an ellipsis\r\n```python\r\nx = torch.randn(2,2,2,2)\r\nx[...] # used to fail, fixed now.\r\n```\r\n- expose stateless methods (`torch.*`` methods) for `torch.cuda.HalfTensor`\r\n- Prevent creation of reference cycles (and hence improve memory usage) when\r\n  leaf variables were using in-place operations.\r\n- Fix gradient computation for the indexing operation in the case of sending in\r\n  `LongTensor`.\r\n- Fix a reshaping bug in the grad_input of basic operations such as `+, -, *, /` etc.\r\n  This used to fail, but is fixed now:\r\n  ```python\r\n  x = Variable(torch.randn(4, 6), requires_grad=True)\r\n  b = Variable(torch.rand(12, 1) + 1e-2, requires_grad=True)\r\n  (x + b.mm(Variable(torch.rand(1, 2) + 1e-2))).sum().backward()\r\n  ```\r\n- Revert partial indexing with `LongTensor` to return to numpy-compatibility\r\n- References to some Tensors in `BatchNorm` and `Conv` are now freed to improve\r\n  memory usage in certain situations. ResNet-152 finetuning with batch_size 16\r\n  used to consume the same amount of memory as batch 256 after this fix.\r\n- Fix a bug where `requires_grad` was being propagated forward differently in\r\n  CPU mode and CUDA mode.\r\n- Fix bugs in `torch.multinomial` on CUDA, where in rare cases, the sampling\r\n  lead to nonsensical values\r\n- Allow backprop through CuDNN RNN in `eval()` mode.\r\n- Support `np.int16` in conversion to `ShortTensor`\r\n- Enable multithreading in MKL (was disabled previously due to a cmake bug).\r\n\r\nImproved error messages\r\n-----------------------\r\n- Print a readable error message when arguments are on different GPUs\r\n- Add better error message for conversion of CUDA tensors to numpy\r\n- Add checks for reward type and size in StochasticFunction\r\n", "tarballUrl": "https://api.github.com/repos/pytorch/pytorch/tarball/v0.1.10", "zipballUrl": "https://api.github.com/repos/pytorch/pytorch/zipball/v0.1.10", "htmlUrl": "https://github.com/pytorch/pytorch/releases/tag/v0.1.10", "url": "https://api.github.com/repos/pytorch/pytorch/releases/5748328", "dateCreated": "2017-03-05T19:30:13Z", "datePublished": "2017-03-15T05:58:21Z"}], "confidence": [1.0], "technique": "GitHub API"}}